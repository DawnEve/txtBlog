R中的统计学

负二项分布，常用于拟合RNAseq测序结果。


本文记录R和数理统计学。

还有一个paper中遇到的统计学: scSeq/NGS_statistics.txt 
一个纯粹统计学: Math/Math-statistics.txt

薛毅和陈立萍的《统计建模与R软件》（清华大学出版社，2007年）中



参考 stata 的统计功能: https://baike.baidu.com/item/stata/7400048?fr=aladdin

[推荐]生物统计学与R极简手册 https://kaopubear.top/learnR/r_and_statistics_basic.html



========================================
统计学和R书单、资源
----------------------------------------

1.教科书
(1). 教科书《R语言与统计分析》 汤银才 主编 447页pdf


(2). 教科书《统计建模与R软件》上下册 共643页 薛毅陈立萍 编著
目录在PDF7页，


(3).《R Programming for Data Science》
https://bookdown.org/rdpeng/rprogdatascience/
podcast: 《Not So Standard Deviations》 http://nssdeviations.com/





2. 网络教程
(1) 学习R编程
https://iowiki.com/r/r_poisson_regression.html


(2) 北大李程老师的基因组学分析课程
http://3d-genome.life/

https://mp.weixin.qq.com/s?__biz=MzAxMDkxODM1Ng==&mid=2247497700&idx=1&sn=c3ba289b61f458923daaf179e572925f&chksm=9b4bb75fac3c3e49cb199176c2396966ff4a0a92424f94449fcc8369e0ae47a90cacbc9341fe&scene=132#wechat_redirect


(3) R基础统计学
http://www.sthda.com/english/wiki/r-basic-statistics


(4) R 基础统计学
http://www.r-tutor.com/elementary-statistics





========================================
R 统计相关函数: 分布与密度曲线、组合数、随机数
----------------------------------------
R起家于统计。统计是R的强项，加之灵活的包，使得R能快速实现最新的统计理论与算法。
概率论是统计学的基础，而R有许多用于处理概率、概率分布以及随机变量的机制。
本文前半部分展示怎么计算分位数的概率，计算概率的分位数，生成给定分布的随机变量，绘制分布图等。

1.分布的名称
R对每个概率分布都有一个简称。这个名称用于识别与分布相联系的函数。
例如，正态分布的名称是“norm”，它是这些函数名称的词根：

函数名	目的
dnorm	正态概率密度
pnorm	正态分布函数
qnorm	正态分位数函数
rnorm	正态分布的随机数


下表描述了一些常见的离散分布：
离散分布名称	 R函数	 参数
二项分布	 binom	 n：试验次数；p：一次试验中事件发生的概率
几何分布	 geom	 p:一次试验中事件发生的概率
超几何分布	 hyper	m：白球个数；n：黑球个数；k：抽取球的个数 
负二项分布	 nbinom	 size:发生的试验个数；或者prob：事件发生概率，或mu：均值
泊松分布	 pois	 lambda：均值


连续分布
连续分布名称	 R函数	 参数
正态分布	 norm	 mean:均值；sd：标准差
学生t分布	 t	 df:自由度
卡方分布	 chisq	 df：自由度
F分布	 f	 df1：第一自由度；df2：第二自由度；
指数分布	 exp	 rate：发生率
均匀分布	 unif	 min：左边界； max：右边界
贝塔分布	 beta	 shape1：形状1；shape2：形状2
gamma分布	 gamma	 rate：发生率；或者rate:发生率，或者scale:大小
柯西分布	cauchy 	 location：位置；scale：大小
对数正态分布	 lnorm	 meanlog：对数均值；sdlong：对数标准差
逻辑分布	 logis	 location：位置：scale：大小
威布尔分布	 weibull	 shape：形状；scale：大小
wilcoxon分布	 wilcox	 m=第一个样本的样本量；n=第二个样本的样本量

使用前最好查看帮助文档！有一些参数不是想象中的那样。比如：指数分布的参数，我们可能认为是beta，结果R的惯例是把指数分布定义为rate=1/beta。

查找帮助的方法：
> ?Normal  #显示与正态分布相关的函数；
> ?TDist  #学生t分布（这是特殊名字）



2.计算组合数
choose(5,3) #从5个中拿出3个:[1] 10
这些成为二项式系数。

生成组合数的方法
combn(1:5, 3)  #生成值为1~5，一次取出3个的所有组合：
> combn(1:5, 3)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    1    1    1    1    1    1    2    2    2     3
[2,]    2    2    2    3    3    4    3    3    4     4
[3,]    3    4    5    4    5    5    4    5    5     5
也可以取出非数字的组合：
> combn(c("t1","t2","t3","t4","t5"), 3)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,] "t1" "t1" "t1" "t1" "t1" "t1" "t2" "t2" "t2" "t3" 
[2,] "t2" "t2" "t2" "t3" "t3" "t4" "t3" "t3" "t4" "t4" 
[3,] "t3" "t4" "t5" "t4" "t5" "t5" "t4" "t5" "t5" "t5"




3.生成随机数
rnorm(1)  #生成一个正态随机数；
> runif(3, min=-1,max=3) #生成[-3,3]之间的均匀分布的随机数
[1]  2.887974  2.147100 -0.365687

R还可以动态的生成均值随机的随机数：
生成10个正态随机数，他们的均值本身是超参数为u=0和sigma=0.2的正态分布的随机数：
> means=rnorm(10,mean=0,sd=0.2)
> rnorm(10,mean=means,sd=1)
[1]  0.2199548 -0.2308713 -0.1428610 -0.2473340  0.1745677 -0.9327783  0.2941705
[8]  1.6410314  0.3827223  1.0588883


生成可再生的随机数
用set.seed(正整数) #来记录一个已知状态，每次生成的随机数一样；
#生成可再生的随机数
> set.seed(100)#任意正整数
> rpois(2,lambda=10)
[1]  8 10
> set.seed(100)#任意正整数
> rpois(2,lambda=10)
[1]  8 10





4.计算离散分布的概率：
(1).随机变量X符合二项分布，每次试验发生的概率0.5，试验10次发生7次的概率：
> dbinom(7,size=10,prob=0.5) #d开头的密度函数：P(X=x)
[1] 0.1171875

x=7的累计概率是：
> pbinom(7,size=10,prob=0.5) #p开头的是分布函数P(X <= x)
[1] 0.9453125

添加参数lower.tail=FALSE，找到右尾概率：
> pbinom(7,size=10,prob=0.5, lower.tail=FALSE)  #也就是X > 7的概率
[1] 0.0546875

怎么计算3 < X <= 7 的概率呢？
> pbinom(7,size=10,prob=0.5) - pbinom(3,size=10,prob=0.5)
[1] 0.7734375

> pbinom(c(3,7),size=10,prob=0.5)
[1] 0.1718750 0.9453125
> diff( pbinom(c(3,7),size=10,prob=0.5) )
[1] 0.7734375





5.连续分布的概率：
分布		分布函数: P(X <= x)
正态分布		pnorm(x,mean,sd)
学生t分布		pt(x,df)
指数分布		pexp(x,rate)
gamma分布		pgamma(x,shape,rate)
卡方分布		pchisq(x,df)

如人的身高如何N(70， 3)，一个人低于66英寸的概率？
> pnorm(66, mean=70,sd=3)
[1] 0.09121122


转换概率为分位数
计算分位数的一个常用例子是置信区间的计算。
如果我们需要知道一个标准正态变量95%的执行区间（alpha=0.05），那么我们需要概率为alpha/2=0.025和(1-alpha)/2=0.975的分位数：
> qnorm(0.025)
[1] -1.959964
> qnorm(0.975)
[1] 1.959964
> qnorm(0.975, mean=10,sd=100)
[1] 205.9964





6.绘制密度函数：
需要绘制概率密度函数，用字母“d”加概率核心单词。
有助于研究概率的分布情况，是否偏斜、是否对称等。

例1: (绘制概率密度图某一个区间，并标上阴影)
#绘制密度函数：
par(mfrow=c(1,1))#整个画布不分区
x=seq(from=-3, to=3,length.out=100);
y=dnorm(x)
plot(x,y,type="l",main = "std Distribution",xlab="Density",ylab="Quantile",col="orange")
abline(h=0)#画一条水平线

#闭合一个多边形
region.x=x[1<=x & x<=2]
region.y=y[1<=x & x<=2]
#加入其实和结束
region.x=c(region.x[1], region.x, tail(region.x,1))
region.y=c(          0, region.y,                0)
#填充
polygon(region.x, region.y, density=10)
#polygon(region.x, region.y, density=-1, col="orange")




例2:研究F分布中参数对分布的影响：
#F分布的密度图,两个参数，都是自由度。
#第一个参数越大，越接近于正态分布，>=3则双支；
#第二个参数越大越接近于正态分布
#输出分布图
x=seq(from=0,to=3,length.out=100);
#ylim=c(0,0.2)
par(mfrow=c(2,2))
b=0.8

plot(x,df(x,2,16), main="F", type="l");abline(h=b);
plot(x,df(x,3,16), main="F", type="l");abline(h=b);
plot(x,df(x,20,16), main="F", type="l");abline(h=b);
plot(x,df(x,30,16), main="F", type="l");abline(h=b);
#结束分布图




例3:研究t分布受参数自由度的影响：
########
#t分布的密度图
#输出分布图
x=seq(from=-5,to=5,length.out=100);
ylim=c(0,0.4)
par(mfrow=c(2,2))
a=0; b=0.395;
plot(x,dnorm(x,mean=0,sd=1), main="Normal", type="l", ylim=ylim);abline(v=a);abline(h=b);
plot(x,dt(x,df=1), main="dt", type="l", ylim=ylim);abline(v=a);abline(h=b);
plot(x,dt(x,df=5), main="dt", type="l", ylim=ylim);abline(v=a);abline(h=b);
plot(x,dt(x,df=10), main="dt", type="l", ylim=ylim);abline(v=a);abline(h=b);
#结束分布图



例4: 卡方分布受参数的影响：
#卡方分布的密度图
#输出分布图
x=seq(from=0,to=30,length.out=100);
ylim=c(0,0.2)
par(mfrow=c(2,2))

plot(x,dchisq(x,df=2), main="Chisq", type="l", ylim=ylim)
plot(x,dchisq(x,df=4), main="Chisq", type="l", ylim=ylim)
plot(x,dchisq(x,df=6), main="Chisq", type="l", ylim=ylim)
plot(x,dchisq(x,df=8), main="Chisq", type="l", ylim=ylim)





========================================
R常见用途: 探索性数据分析、统计推断、回归分析、机器学习、可视化报告
----------------------------------------
https://www.imooc.com/video/8778 该视频目前只讲了大概能干啥，没讲怎么做，比较水。


1. 发布平台
(1)github: 发布各种源代码，写作编程等；

(2)RPubs: https://www.rpubs.com/ 貌似是RStudio的产品。
Easy web publishing from R
Write R Markdown documents in RStudio.
Share them here on RPubs. (It’s free, and couldn’t be simpler!)


Prerequisites
You'll need R itself, RStudio (v0.96.230 or later), and the knitr package (v0.5 or later).

Instructions
1)In RStudio, create a new R Markdown document by choosing File | New | R Markdown.
选择R markdown和notebook没区别，个人倾向于使用后者。

2)Click the Knit HTML button in the doc toolbar to preview your document.
注意，保存时不要输入后缀名，会自动生成.Rmd后缀名。

3)In the preview window, click the Publish button.
打开一个新网页，需要输入用户名和密码。
不能更新！！只能删掉旧的，重新上传。
删除方法是打开web页，左下角可以删除。



(3)申请用户名:
https://www.rpubs.com/dawnEve
测试页:
http://rpubs.com/dawnEve/test001
http://rpubs.com/dawnEve/test002



(4)搜索方法：
google搜索“??+RPubs”即可。(如: heatmap rpubs)
找到: https://www.rpubs.com/tskam/heatmap




2. 探索性数据分析 - 其实就是作图
https://github.com/angelayuan/Exploratory-Data-Analysis

散点图
柱状图
折线图


3. 统计推断
基于数据得到正式结论的过程，从不确定和抽样中，得到有一定可信度(错误率控制在5%以内)的结论。
例子: 药效


4. 回归分析
线性模型拟合数据
- 预测变量
- 结果变量

父母的身高，预测孩子的身高。


5. 机器学习
训练模型 + 预测
例子: 分类模型。

机器学习是一门高深的学问，没有深入研究，不可能实现。
不过R包可以大大降低使用机器学习的门槛。

# BiocManager::install("caret")
library(caret)

# 训练集建立模型，测试集测试效果
inTrain=createDataPartition(y=training$classe, p=0.7, list=F)
training_set=training[inTrain,]
cv_set=training[-inTrain,]

# 使用训练集，建立随机森林
rf_fit=randomForest(class~., data=training_set)

# 把新数据集带入模型
test_pred=predict(rf_fit, newdata=testing)
test_pred





6. 开发数据产品，发布报告
- R制作html，调用Google charts, 交互式html图表;
- Manipulate包，实现人机交互;
- rCharts包，使用R制作交互式js可视化产品;
- Shiny包，制作嵌入式网页的交互式R程序的平台 https://www.shinyapps.io/
- Slidify包，制作和发布基于R的报告(类似ppt)
- Rmarkdown 可以生成html报告

实例: https://angelayuan.shinyapps.io/predict_bodyfat/







========================================
|-- 常见的R统计函数
----------------------------------------
1.统计函数   作用
max(x) 返回向量x中最大的元素
min(x) 返回向量x中最小的元素
which.max(x) 返回向量x中最大元素的下标
which.min(x) 返回向量x中最小元素的下标
mean(x) 计算样本(向量)x的均值
median(x) 计算样本(向量)x的中位数
mad(x) 计算中位绝对离差
var(x) 计算样本(向量)x的方差
sd(x) 计算向量x的标准差
...More(汤银才 P41)

table() 返回向量中每个元素的个数
prop.table() 输入表格，返回比例
	默认是全表的总和是1.
	指定 margin=1 表示一行的和是1
	指定 margin=2 表示一列的和是1



(1)Median Absolute Deviation
#绝对中位差实际求法是用原数据减去中位数后得到的新数据的绝对值的中位数。
#但绝对中位差常用来估计标准差，估计标准差=1.4826*绝对中位差。

#R语言中返回的是估计的标准差。
#例如：原数据{2，3，4，5，6}中位数4，新数据{2，1，0，1，2}，即{0，1，1，2，2}。
#所以中位数是1。也就是绝对中位差是1.

#而R返回的是1*1.4826=1.4826

#mad(c(2,3,4,5,6))[1] 1.4826
#R语言里的绝对中位差还要乘上一个比例因子：constant=1.4826
#也可以不乘
mad(c(2,3,4,5,6), constant=1) #1
mad(c(2,3,4,5,6,100), constant=1) #[1] 1.5
#也就是相对于标准差，mad对少量异常值不太敏感
#https://www.zhihu.com/question/56537218/answer/163638714








2. 单细胞计数和百分比计算

(1) 一个维度
> table(scObj$seurat_clusters) #返回每个元素及出现的个数
  0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 
1432 1305 1104 1007  991  661  617  523  328  251  243  127  121  114   77   33   32 

> prop.table( table(scObj$seurat_clusters) ) #输入表格数据，计算比例
          0           1           2           3           4           5           6           7 
0.159714477 0.145549855 0.123131831 0.112313183 0.110528664 0.073722953 0.068815525 0.058331474 
          8           9          10          11          12          13          14          15 
0.036582646 0.027994646 0.027102387 0.014164622 0.013495427 0.012714700 0.008587999 0.003680571 
         16 
0.003569039 


(2) 两个维度
> table(scObj$seurat_clusters, scObj$orig.ident)
     PTC_A PTC_B
  0   1426     6
  1      0  1305
  2    779   325
  3    625   382
  4    917    74
  5      2   659
  6    144   473
略
第一个元素是行，第二个元素是列


> prop.table( table(scObj$seurat_clusters, scObj$orig.ident) ) #默认全表总和是1
            PTC_A        PTC_B
  0  0.1590452822 0.0006691947
  1  0.0000000000 0.1455498550
  2  0.0868837832 0.0362480482
  3  0.0697077850 0.0426053982
  4  0.1022752621 0.0082534017
  5  0.0002230649 0.0734998885
  6  0.0160606737 0.0527548517
略
> prop.table( table(scObj$seurat_clusters, scObj$orig.ident) ) |> sum()
[1] 1


第二个参数 margin:
a vector giving the margins to split by. E.g., for a matrix 1 indicates rows, 2 indicates columns, c(1, 2) indicates rows and columns. When x has named dimnames, it can be a character vector selecting dimension names.

> prop.table( table(scObj$seurat_clusters, scObj$orig.ident), margin = c(1) ) #每行的和是1
           PTC_A       PTC_B
  0  0.995810056 0.004189944
  1  0.000000000 1.000000000
  2  0.705615942 0.294384058
  3  0.620655412 0.379344588
  4  0.925327952 0.074672048
  5  0.003025719 0.996974281
  6  0.233387358 0.766612642

> prop.table( table(scObj$seurat_clusters, scObj$orig.ident), margin = c(2) ) |> colSums() #每列的和是1
PTC_A PTC_B 
    1     1 
> prop.table( table(scObj$seurat_clusters, scObj$orig.ident), margin = c(2) )
    
            PTC_A        PTC_B
  0  0.2869215292 0.0015015015
  1  0.0000000000 0.3265765766
  2  0.1567404427 0.0813313313
  3  0.1257545272 0.0955955956
  4  0.1845070423 0.0185185185
  5  0.0004024145 0.1649149149
  6  0.0289738431 0.1183683684









========================================
|-- R做标准化与归一化，scale()函数、sweep()函数
----------------------------------------

一、任务: 按列进行min-max标准化
#数据集
x<-cbind(c(1,2,3,4),c(5,5,10,20),c(3,6,9,12))



(1)#自己写标准化
x_min_temp<-apply(x,2,min) 
x_min<-matrix(rep(x_min_temp,4),byrow=TRUE,ncol=3)#需要输入行数和列数
abs(x-x_min)#当前值减去均值
x_extreme_temp<-apply(x,2,max)-apply(x,2,min)
x_extreme<-matrix(rep(x_extreme_temp,4),byrow=TRUE,ncol=3)#需要输入行数和列数
abs(x-x_min)/x_extreme


(2) 上面一堆，可以一行搞定：
apply(x, 2, function(x){
  (x-min(x))/(max(x)-min(x))
})


(3) 使用sweep函数
center <- sweep(x, 2, apply(x, 2, min),'-') #在列的方向上减去最小值，不加‘-’也行
R <- apply(x, 2, max) - apply(x,2,min)   #算出极差，即列上的最大值-最小值
x_star<- sweep(center, 2, R, "/")        #把减去均值后的矩阵在列的方向上除以极差向量



# 输出结果是一样的
#         [,1]      [,2]      [,3]
#[1,] 0.0000000 0.0000000 0.0000000
#[2,] 0.3333333 0.0000000 0.3333333
#[3,] 0.6666667 0.3333333 0.6666667
#[4,] 1.0000000 1.0000000 1.0000000






2. sweep函数
#sweep函数更简洁、易懂，且不需要输入行数和列数，二者性能也差不多

sweep(x, MARGIN, STATS, FUN = "-", check.margin = TRUE, ...)
参数解释:
  x 数组
  margin 1是行，2是列
  stats 将要减去的向量
  fun 要使用的函数，默认是减
#


(1) sweep再举一个例子
m<-matrix(c(1:9),byrow=TRUE,nrow=3)
#第一行都加1，第二行都加4，第三行都加7
sweep(m, 1, c(1,4,7), "+")  









3. scale函数，这个比较简单，直接看例子
scale(x, center = TRUE, scale = TRUE)


原始数据
t0=iris[1:3, 1:4]
t0


(1) default: center=T, scale=T

先 center，再scale，默认就是 z标准化，(x-mu)/sd: 
	(a-mean(a)) / sd(a)

t1=scale(t0)
t1
scale(t1, center = T, scale = T) #default

# 手工实现
# Z = (x - X_bar) / sd(x)
t1B=apply(t0, 2, function(x){
  ( x-mean(x) ) / sd(x)
})
t1B
t1
all(abs(t1B-t1) < 1e-2, na.rm = T) #T




(2) 仅center: 每个元素 - 该列的均值
仅减去mu 

t2=scale(t0, center=T, scale=F)
t2
# 手工实现
t2B=apply(t0, 2, function(x){
  x-mean(x)
})
t2B
# 判断相等
all(abs(t2B-t2) < 1e-2) #T

t2
# 同样的实现：传入每列的均值
scale(t0, center=colMeans(t0), scale=F)



(3) 仅 scale：每个元素 / 该列的sd?
t3=scale(t0, center=F, scale=T)
t3
# 手工实现
t3B=apply(t0, 2, function(x){
  x/sqrt(
    sum(x**2) / ( length(x) -1 )
  )
})
t3B
t3
all(abs(t3B-t3) < 1e-2) #T

# 到底除以的是什么？
sqrt(
  sum(t0[,1]**2) / ( length(t0[,1]) -1 )
) #6.004582 仅scale时，除以该值。均方根 root-mean-square
t0[,1] / 6.004582
t3[,1]
sqrt(
  sum(t0[,1]**2) / ( length(t0[,1]) )
) #4.90272 #不是这个值

#
sd(t0[,1]) #0.2
sqrt(
  sum( scale(t0[,1], center=T,scale=F) **2) / ( length(t0[,1]) - 1 )
) #0.2
sqrt(
  sum( scale(t0[,1], center=T,scale=F) **2) / ( length(t0[,1]) )
) #0.1632993



(4) center 和 scale 都不做，那就是原样输出
t4=scale(t0, center=F, scale=F)
t4
all(abs(t4 - t0)<1e-2) #T












========================================
|-- 归一化和标准化的区别？ z-score标准化
----------------------------------------
1. 归一化（Normalization）
1).把数据变为（0，1）之间的小数。主要是为了方便数据处理，因为将数据映射到0～1范围之内，可以使处理过程更加便捷、快速。 
2).把有量纲表达式变换为无量纲表达式，成为纯量。经过归一化处理的数据，处于同一数量级，可以消除指标之间的量纲和量纲单位的影响，提高不同数据指标之间的可比性。 

主要算法： 
1).线性转换，即min-max归一化（常用方法） y=(x-min)/(max-min) 
2). 对数函数转换 y=log10(x)，
	测序数据一般先加1防止0： y=log10(x+1)
3).反余切函数转换 y=atan(x)*2/PI 

4) 把0-无穷大压缩到[0,1)之间: x/(1+x), 特点是越大压缩的越厉害。




2.标准化（Standardization） 
数据的标准化是将数据按比例缩放，使之落入一个小的特定区间。 

主要方法： 
1).z-score标准化，即零-均值标准化（常用方法） y=(x-μ)/σ 是一种统计的处理，基于正态分布的假设，将数据变换为均值为0、标准差为1的标准正态分布。但即使数据不服从正态分布，也可以用此法。特别适用于数据的最大值和最小值未知，或存在孤立点。

2).小数定标标准化 y=x/10^j （j确保max(|y|)<1） 通过移动x的小数位置进行标准化 
3).对数Logistic模式 y=1/(1+e^(-x))






3. z-score 计算方法
(1) 计算公式是 z= (X-M) / (Sigma / sqrt(n))
X = Population Mean
M = Sample Mean
Sigma = Population Standard Deviation
n = number of sample instances
https://www.analyticsvidhya.com/blog/2019/09/everything-know-about-p-value-from-scratch-data-science/

我的理解: 对于没有重复的实验，n=1，也就是 z=(x-mean)/sd


例子: 射箭选手训练后的成绩，是否又显著提高？
           Before   After
Years      10        1
Mean       74       78
Std Dev     8        5 
Observations >1000   60
Z-Score comes out to be – 3.87.


> (74-78)/ (8/sqrt(60))
[1] -3.872983

观察次数超过30可以使用z-score法。
也就是本次次数越多，值越大，则p越小。



###
计算p值
查z-score表找p值。也即是标准正态分布的p值表。
两头小，中间大的曲线。

> pnorm(-3.872983)
[1] 5.375566e-05

或者使用公式 f(x) = 1 - f(-x)
> pnorm(3.872983) #左侧的曲线下面积
[1] 0.9999462
> 1-pnorm(3.872983)
[1] 5.375566e-05








refer:
http://blog.csdn.net/yitianguxingjian/article/details/51820758



========================================
|-- 使用R做随机抽样: 模拟——随机数、抽样、线性模型
----------------------------------------
为了可重复，设置随机数种子为一个具体值
set.seed(1)


1.
(1)Simulation
今天学习的内容是模拟 —— Simulation，在统计和一些其他的应用中很重要，所以在这里介绍一些R语言中可以做模拟的函数。

用于模拟已知概率分布的数字和变量
有些函数可以直接生成符合某种概率分布的随机数字或变量，例如：
rnorm()：指定一个均值和标准差，即可生成符合正态分布的随机数字变量
rpois()：从已知平均发生率（rate）的泊松分布中生成泊松随机变量

一共有四类基本的函数和概率分布函数相关，它们的前缀分别是d, r, p以及q：
d：用来估计密度（density）
r：用来产生随机数字（random）
p：估计累计分布（cumulative distribution）
q：估计分位数（quantile）
每种分布都有以上这四种前缀构成的函数，比如rpois(), dpois(), ppois(), qpois()等。

每种函数都有不同的参数，拿正态分布的四个函数举例：
dnorm(x, mean = 0, sd = 1, log = FALSE)
## 可以求密度的对数值

pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
## 可以对概率求对数；计算该分布的左尾，如果填FALSE就是计算右尾

qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
## 可以对概率求对数；计算该分布的左尾，如果填FALSE就是计算右尾

rnorm(n, mean = 0, sd = 1)
## n是你想生成的随机变量的个数

所有函数都需要我们给定均值和标准差，以此确定实际的概率分布，如果不指定，那么默认属于标准正态分布（均值为0，标准差为1）
生成最简单的服从标准正态分布的随机数：
> ## Simulate standard Normal random numbers
> x <- rnorm(10)   
> x
 [1]  0.01874617 -0.18425254 -1.37133055 -0.59916772  0.29454513
 [6]  0.38979430 -1.20807618 -0.36367602 -1.62667268 -0.25647839

修改参数：
> x <- rnorm(10, 20, 2) 
> x
 [1] 22.20356 21.51156 19.52353 21.97489 21.48278 20.17869 18.09011
 [8] 19.60970 21.85104 20.96596
> summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  18.09   19.75   21.22   20.74   21.77   22.20
#



(2)设置随机数字生成种子
set.seed()函数可以用来设置随机数字生成种子（seed）。
这种方法可产生相同的随机数，也叫“伪随机数”。它怎么用？

举例
我们可以设置种子为任意整数，然后生成随机数字如下：
> set.seed(1)
> rnorm(5)
[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078
> set.seed(2)
> rnorm(5)
[1] -0.89691455  0.18484918  1.58784533 -1.13037567 -0.08025176
> set.seed(5)
> rnorm(5)
[1] -0.84085548  1.38435934 -1.25549186  0.07014277  1.71144087

我们看到种子设定值不同时随机数字也不同，但是再设定相同种子就会出现完全一样的随机数字：
> set.seed(1)
> rnorm(5)
[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078

这个操作能让你重复得到之前生成的随机数字，可以让我们的模拟过程可重复，所以在生成随机数字之前最好设定一个种子。

再拿泊松分布举例：
生成不同发生率的10个随机变量：
> rpois(10, 1)    ## Counts with a mean of 1
 [1] 0 0 1 1 2 1 1 4 1 2
> rpois(10, 2)    ## Counts with a mean of 2
 [1] 4 1 2 0 1 1 0 1 4 1
> rpois(10, 20)   ## Counts with a mean of 20
 [1] 19 19 24 23 22 24 23 20 11 22
#

估计泊松分布的累积分布函数：

## 在平均发生率为2的泊松分布中，出现小于等于2的随机变量的概率是多少
> ppois(2,2) 
[1] 0.6766764
## 在平均发生率为2的泊松分布中，出现小于等于4的随机变量的概率是多少
> ppois(4,2)
[1] 0.947347
## 在平均发生率为2的泊松分布中，出现小于等于6的随机变量的概率是多少
> ppois(6,2)
[1] 0.9954662




(3)Simulating a Linear Model
我们再来看怎样从简单的线性模型中提取随机值

如果x是单一自变量
举例：
## 首先记得先设定一个种子
> set.seed(20)

## 设置自变量x取值，属于标准正态分布
> x <- rnorm(100)

## 随机噪声e是属于标准差为2的正态分布
> e <- rnorm(100, 0, 2)

## 设置线性方程的回归系数和截距
> y <- 0.5 + 2 * x + e

## 输出概要结果
> summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-6.4084 -1.5402  0.6789  0.6893  2.9303  6.5052 

## 画出图表
> plot(x,y)
这就是通过回归模型模拟出的x和y的图表，可以看出明显的线性关系。





(4)如果x是二元变量
x如果是二元变量，可以代表两种性别、两种实验处理（可以是实验组和对照组）这一类情况

使用二项分布函数rbinom()

## 重新设定种子
> set.seed(10)

## 随机取100个二元变量数据，并设置参数
> x <- rbinom(100, 1, 0.5)

## 取符合正态分布的随机变量
> e <- rnorm(100, 0, 2)

## 生成线性模型
> y <- 0.5 + 2 *x + e

## 查看结果和作图
> summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-3.4936 -0.1409  1.5767  1.4322  2.8397  6.9410 
> plot(x, y)

可以看到，x是二元变量，y依旧是连续的，符合正态分布






(5)从复杂模型中模拟取值
假设结果y服从于一个平均值为μ的泊松分布，log(μ)服从一个截距为β0，斜率β1的线性函数，x是其中的自变量

> set.seed(1)
> x <- rnorm(100)    

## 模拟线型变量log(μ)
> log.mu <- 0.5 + 0.3 * x

## 使用rpois函数来计算y，取幂
> y <- rpois(100, exp(log.mu))

## 查看结果
> summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.00    1.00    1.00    1.55    2.00    6.00 
> plot(x, y)

可以看到x越大y越大，呈线性关系






(6)Random Sampling
这里使用的函数是sample()，可以从你给定的一组对象中，随机抽取样本

假如我们提供一个数值向量，sample()可以从中随机抽取样本。因此我们可以人为确定一个分布，指定给一个向量并从中取样

举例：
从整数1到10中取样，取出的数字不放回，如果重复去取样两次，得到的数字是不重复的，对字母取样也是同理：

> set.seed(1)
> sample(1:10, 4)
[1] 3 4 5 7
> sample(1:10, 4)
[1] 3 9 8 5

> ## Doesn't have to be numbers
> sample(letters, 5)    
[1] "q" "b" "e" "x" "p"



如果只提供向量，不指定任何条件，返回结果就是这些整数重新排列，数字内部无重复：
> ## Do a random permutation
> sample(1:10)          
 [1]  4  7 10  6  9  2  8  3  1  5
> sample(1:10)
 [1]  2  3  4  1  9  5 10  8  6  7
#


有放回的取样，设置参数replace = TRUE，因为有放回，所以可以看到数字内部有重复：
> ## Sample w/replacement
> sample(1:10, replace = TRUE)  
 [1] 2 9 7 8 2 8 5 9 7 8
#







2.Simulation 小结
- 使用R中的函数从指定的概率分布中取样
使用rnorm(), rpois(), rbinom()等。

常用分布包括正态分布（normal）、泊松分布（poission）、二项分布（binomial）、指数分布（exponential）、伽马分布（gamma）等。

- sample函数可用来从指定向量中随机取样
- 无论何时记得用seed生成种子



ref:
https://mp.weixin.qq.com/s/6VrRjH4tx6VSqNIJ8qw7Kg

视频课程 R Programming by Johns Hopkins University: https://www.coursera.org/learn/r-programming/home/welcome

讲义 Programming for Data Science: https://bookdown.org/rdpeng/rprogdatascience/




========================================
|-- 非线性拟合 nls()函数：非线性最小二乘(nls)拟合，不是lm拟合
----------------------------------------
1.非线性拟合 nls()函数
x=1:12;x
y1=c(7098.00, 7498,7848,8254,8761,8801.12,8951.32,9325.03,9680.90,10200,11000,12360.74)
plot(y1~x, col="red")

fit1=nls(y1~a*b^x, start=list(a=1,b=2))
lines(seq(1,12,by=0.1), predict(fit1, data.frame(x=seq(1,12,by=0.1))), col="#0096ff", lty="dotted" )
#end




2. 酶联免疫拟合曲线
http://www.qinms.com/work/elisa.html

Y = (A - D) / [1 + (X/C)^B] + D


x=seq(-5,5,0.5)
plot(x, 1/(1+exp(3*x))-0.5, type='l') #有极限的s曲线
plot(x, 1/(1+(5*x))^(-3), type='l') #y没有上下限的S曲线




3. RNA 稳定性的 放线菌素D法
详见: Bio/b1_assays.txt: RNA稳定性检测（放线菌素D (Actinomycin D, ActD) 处理法）




4. 最小二乘法 R^2 的计算 
https://www.codenong.com/14530770/








========================================
分布和检验
----------------------------------------
1. R中如何查看t.test,anova,wilcox.test,kruskal.test的源码

stats:::t.test.default
stats:::t.test.formula

stats:::anova.glm
stats:::anova.lm

ref: https://bbs.pinggu.org/thread-3067738-1-1.html




========================================
|-- 连续型分布: 正态分布、均匀分布、指数分布
----------------------------------------
1. 正态分布 Normal Distribution
dnorm(x, mean, sd) 概率密度
pnorm(x, mean, sd) 累积
qnorm(p, mean, sd) 分位数(由左侧曲线下面积p，查x值)
rnorm(n, mean, sd) 产生随机数

Density, distribution function, quantile function and random generation for the normal distribution with mean equal to mean and standard deviation equal to sd.

以下是上述功能中使用的参数的说明 -
	x是数字的向量。
	p是概率的向量。
	n是观察次数（样本量）。
	mean是样本数据的平均值。 它的默认值为零。
	sd是标准偏差。 它的默认值是1。
#


(1)dnorm() 概率密度曲线, 钟形曲线，y值最大的时候该x出现的可能性最高。
对于给定的平均值和标准偏差，此函数给出每个点处的概率分布的高度。

# Create a sequence of numbers between -5 and 5 incrementing by 0.1.
x <- seq(-5, 5, by = 0.1);x
# Choose the mean as 0 and standard deviation as 1.
y <- dnorm(x, mean = 0, sd = 1)
plot(x,y, type='o')


(2)pnorm()，累积曲线，S形单调递增曲线
此函数给出正态分布随机数的概率小于给定数字的值。 它也被称为“累积分布函数”。

x <- seq(-5,5,by = 0.1)
y <- pnorm(x, mean = 0, sd = 1)
plot(x,y, type='o', col='red')


(3)qnorm() 分位数
此函数获取概率值并给出其累积值与概率值匹配的数字。

qnorm(0.05, 0,1) #x=-1.644854 时左侧曲线下面积为0.05
qnorm(0.95, 0,1) #x=1.644854 时左侧曲线下面积为0.95
qnorm(1-(1e-16), 0,1) #x=8.209536 时左侧曲线下面积为1-(1e-16)

#
x <- seq(-5,5,by = 0.1)
y <- dnorm(x, mean = 0, sd = 1)
plot(x,y)
abline(v=qnorm(0.05, 0,1), col='red', lty=2)
abline(v=qnorm(0.95, 0,1), col='blue', lty=2)


(4)rnorm() 产生随机数
此函数用于生成分布正常的随机数。 它将样本大小作为输入并生成许多随机数。 我们绘制直方图以显示生成的数字的分布。

y <- rnorm(200)
hist(y,n=50) #频率直方图也是钟形






========================================
|-- 离散型分布: 二项分布/负二项分布、几何分布/超几何分布
----------------------------------------

1. 二项分布 Binomial Distribution

二项分布模型用于找出事件成功的概率，该事件在一系列实验中仅具有两种可能的结果。 例如，投掷硬币总是给出头部或尾部。 在二项分布期间估计在重复投掷硬币10次时准确找到3个头的概率。

dbinom(x, size, prob)
pbinom(x, size, prob)
qbinom(p, size, prob)
rbinom(n, size, prob)

参数说明:
	x是数字的向量。
	p是概率的向量。
	n是观察次数。
	size是试验次数。
	prob是每次试验成功的概率
#


(1)每个点的概率密度分布
x <- seq(0,50,by = 1);x
y <- dbinom(x,50,0.5) #抛硬币正面向上的概率是0.5，抛50次，正面向上的概率。
plot(x,y)

(2)pbinom() 此函数给出事件的累积概率。 它是表示小于等于给定值出现的概率。
x <- seq(0,50,by = 1);x
y <- pbinom(x,50,0.5)
plot(x,y)

#
pbinom(25,50,0.5) #累积概率: 25次及25次以下朝上的概率
# 则正好25次朝上的概率:
pbinom(25,50,0.5) -pbinom(24,50,0.5)  #0.1122752
choose(50,25)*0.5**25*0.5**25 #0.1122752


(3)qbinom() 分位数
abline(v=qbinom(0.05,50,1/2), col='red', lty=2)
abline(v=qbinom(0.95,50,1/2), col='blue', lty=2)

#
qbinom(0.05,50,1/2) #[1] 19 概率密度曲线左侧面积达到0.05时的x值为19
qbinom(0.95,50,1/2) #[1] 31


(4) rbinom() 产生随机数
rbinom(8,50,0.5) #每次正面向上0.5，抛50次，正面向上的次数。产生8个次数。
# [1] 22 23 25 26 24 23 25 29

hist(rbinom(800,50,0.5)) #产生足够多的时，这些次数分布符合二项分布的密度曲线

# 画抽样的密度图
plot(density(rbinom(800,50,0.5)))
# 定义的概率密度图
x <- seq(0,50,by = 1);x
y <- dbinom(x,50,0.5)
points(x,y, col='red',lty=2) #和样本密度曲线重叠





========================================
|-- *** 泊松分布
----------------------------------------


========================================
|-- t检验的三个假设；T-statistics (t统计量)的计算和意义
----------------------------------------

0. t检验的检验假设 有 3个:
假设1：两个样本是否独立？
是的，因为来自两组的样本无关。


假设2：两组中每组的数据是否服从正态分布？
我们将使用with()和shapiro.test()的函数来为每组样本计算Shapiro-Wilk测试。

使用Shapiro-Wilk正态性检验：假设数据和正态分布无差异：
# Shapiro-Wilk normality test for Men's weights
with(my_data, shapiro.test(weight[group == "Man"]))# p = 0.089

# Shapiro-Wilk normality test for Women's weights
with(my_data, shapiro.test(weight[group == "Woman"])) # p = 0.52

输出结果中，两个p值大于显着性水平0.05，说明两组数据的分布与正态分布没有显着差异。数据分部符合正态分布的假设检验成立。
请注意，如果数据不是正态分布的，建议使用非参数两样本Wilcoxon秩检验。（后面的推送会介绍）


假设3: 这两个总体是否符合方差齐性？
A: 我们将使用F检验来检验方差齐性。可以使用var.test()函数执行以下操作：假设两组方差是一致的：
res.ftest <- var.test(weight ~ group, data = my_data)
res.ftest

> a1=c(20, 30, 10); a2=c(1, 2, 3);
> var.test(a1, a2)

	F test to compare two variances

data:  a1 and a2
F = 100, num df = 2, denom df = 2, p-value = 0.0198
alternative hypothesis: true ratio of variances is not equal to 1
95 percent confidence interval:
    2.564103 3900.000000
sample estimates:
ratio of variances 
               100 

F检验为 p = 0.0198 。它小于显着性水平alpha = 0.05。因此，否定原假设。
两组数据的方差之间有显著差异。因此我们认为两组方差不相等（方差非齐性）。






1. 使用R语言计算 t统计量

(1) 方差齐性时（默认不齐性）
> t.test(c(20,30,10), c(1,2,3), var.equal = T)

	Two Sample t-test

data:  c(20, 30, 10) and c(1, 2, 3)
t = 3.1022, df = 4, p-value = 0.03614
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  1.890237 34.109763
sample estimates:
mean of x mean of y 
       20         2 




(2) 当两组方差不相等（方差不齐）时，可以使用校正的student-t检验方法，即Welch t检验比较两组差异：
> t.test(c(20,30,10), c(1,2,3))

	Welch Two Sample t-test

data:  c(20, 30, 10) and c(1, 2, 3)
t = 3.1022, df = 2.04, p-value = 0.08787
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -6.502039 42.502039
sample estimates:
mean of x mean of y 
       20         2 

> t.test(c(20,30,10), c(1,2,3))$statistic 
       t 
3.102219

> t.test(c(20,30,10), c(1,2,3))$p.value
[1] 0.0878746





2. 手工计算

(1) 方差齐性时（默认不齐性）
> a1=c(20, 30, 10); a2=c(1, 2, 3);
> s2=sum( (a1-mean(a1))**2 + (a2-mean(a2))**2 ) / (3+3 -2)
> (mean(a1) - mean(a2)) / sqrt(s2/3 + s2/3)
[1] 3.102219

计算公式: https://zhuanlan.zhihu.com/p/150479876

t = (m1 - m2) / sqrt( s^2/n1 + s^2/n2 )
其中 
m1和m2分别是A、B两组样本均值
n1和n2分别是A、B两组样本量
s是样本的标准差，可以由以下公式计算获得

s^2 = [ 累加(x1-x1_bar)^2  +  累加(x2-x2_bar)^2 ] / (n1+n2-2) 




(2) 当两组方差不相等（方差不齐）时
> a1=c(20, 30, 10); a2=c(1, 2, 3); (mean(a1) - mean(a2)) / sqrt( var(a1)/3 + var(a2)/3  )
[1] 3.102219


计算公式: https://zhuanlan.zhihu.com/p/150479876

t = (m1 - m2) / sqrt( s1^2/n1 + s2^2/n2 )

其中，
m1和m2分别是A、B两组样本均值
n1和n2分别是A、B两组样本量
s1,s2是样本的标准差，可以由以下公式计算获得

s1^2 = 累加(x1-x1_bar)^2 / (n1-1)

> var(a1)
[1] 100

> sum( (a1-mean(a1))**2 )/(3-1)
[1] 100












========================================
|-- t 检验比非参方法(wilcox)检出能力要强: 优先用参数检验
----------------------------------------

1. 使用场景的区别
(1) T检验，亦称student t检验（Student's t test），主要用于样本含量较小（例如n<30），总体标准差σ未知的正态分布资料。

T检验是用于小样本（样本容量小于30）的两个平均值差异程度的检验方法。它是用T分布理论来推断差异发生的概率，从而判定两个平均数的差异是否显著。


(2) wilcoxon秩和检验（也称Mann-Whitney U检验）

曼-惠特尼U检验又称“曼-惠特尼秩和检验”，是由H.B.Mann和D.R.Whitney于1947年提出的。它假设两个样本分别来自除了总体均值以外完全相同的两个总体，目的是检验这两个总体的均值是否有显著的差别。

曼-惠特尼秩和检验可以看作是对两均值之差的参数检验方式的T检验或相应的大样本正态检验的代用品。由于曼-惠特尼秩和检验明确地考虑了每一个样本中各测定值所排的秩，它比符号检验法使用了更多的信息。

Two data samples are independent if they come from distinct populations and the samples do not affect each other. Using the Mann-Whitney-Wilcoxon Test, we can decide whether the population distributions are identical without assuming them to follow the normal distribution.

结论: Mann-Whitney-Wilcoxon Test可不考虑样本是否符合正态分布，可用于两独立样本均值差异性检验



如果警告“无法精確計算带连结的p值“这是因为数据中存在重复的值，一旦去掉重复值，警告就不会出现。


ref: http://blog.sina.com.cn/s/blog_6e60da090101g6r2.html






2.检出能力试验
# make data
n=200
dt=data.frame(
  a1=rnorm(n),
  a2=rnorm(n),
  a2=rnorm(n),
  
  b1=rnorm(n),
  b2=rnorm(n),
  b2=rnorm(n)
)
head(dt)

#1. for t test
test.t=apply(dt, 1, function(x){
  t.test(x[1:3], x[4:6])$p.value
})
head(test.t)
table(test.t<0.05)
#FALSE  TRUE 
#194     6 

#2. for wilcox test
test.w=apply(dt, 1, function(x){
  wilcox.test(x[1:3], x[4:6])$p.value
})
head(test.w)
table(test.w<0.05)
#FALSE 
#200

# 结论: 倾向于使用t test，除非不满足条件才退而求其次使用非参方法。
# t test 检出能力更强，比非参方法能检测出更多的显著条目，
# 但是 t-test 要求数据符合正态分布。
# 而非参方法没有任何预设条件，代价就是检出能力弱。






========================================
|-- F-test 检验方差齐性: var.test(x, y)
----------------------------------------
方差齐性检验的前提是2个数据符合正态分布。


1. 正态分布的检验
原假设H0: 不是正态分布。
然后看是否推翻原假设。

(1) p<0.05，推翻H0。是正态分布
> shapiro.test(iris$Sepal.Length)

	Shapiro-Wilk normality test

data:  iris$Sepal.Length
W = 0.97609, p-value = 0.01018


(2) p>0.05，不能推翻H0。不是正态分布
> shapiro.test(c(1,2,3,4))

	Shapiro-Wilk normality test

data:  c(1, 2, 3, 4)
W = 0.99291, p-value = 0.9719

(3) p<0.05，推翻H0。是正态分布。擦边球。
> shapiro.test(c(1,2,3,4,2,3,3,1,2,3,2,3,3,1))

	Shapiro-Wilk normality test

data:  c(1, 2, 3, 4, 2, 3, 3, 1, 2, 3, 2, 3, 3, 1)
W = 0.87449, p-value = 0.04854

#看柱状图，是否两头低，中间高
hist(c(1,2,3,4,2,3,3,1,2,3,2,3,3,1))






2. F检验
(1) F统计量
S^2=求和( (x-x_bar)^2 ) /(n-1)
F=S1^2/S2^2;


(2) R代码计算方法
# 准备数据
n1=c(1,2,3,4);n1
n2=c(10,20.30,40);n2

# 1.使用公式
s1=sum( ( n1-mean(n1) )**2) /(length(n1)-1); s1
s2=sum( ( n2-mean(n2) )**2) /(length(n2)-1); s2
# 计算统计量
f=s1/s2;f #0.007172675
# 计算p值
pf(f, length(n1)-1, length(n2)-1) #0.001098215


# 2.使用函数
rs=var.test(n1,n2, alternative="less");rs
rs$p.value #0.001098215





(3) 如果是双端检验呢
pf(f, length(n1)-1, length(n2)-1)   #0.001098215 #单端
# 双端检验的p值，就是单端检验的p值的2倍。
pf(f, length(n1)-1, length(n2)-1) * 2 #0.002196429 #双端

#
rs=var.test(n1,n2, alternative="less");rs #默认是双端检验
rs$p.value #0.002196429 p值较大，是单端的2倍







ref:
http://blog.fens.me/r-test-f/





========================================
|-- KS-检验（Kolmogorov-Smirnov test） -- 检验数据是否符合某种分布 (如何在R里检验一组数据是否符合泊松分布？)
----------------------------------------

KS检验是一种非参数检验，常用于判断样本与预先给定的分布是否一致，或两个样本的概率分布是否不同。换言之，就是观测得到的样本，声称其服从某一分布是否可信。
K-S检验方法能够利用样本数据推断样本来自的总体是否服从某一理论分布，是一种拟合优度的检验方法，适用于探索连续型随机变量的分布。


KS 检验经常画的图: https://www.cnblogs.com/jiangkejie/p/11572205.html




1. 用rpois()生成n个数据,x为要检验的数据
> x = c(0.6,1.1,1.6,2.2,2.4,2.9,3.4,4.0,4.3,4.8)
> n=length(x); m=0.2; y<-rpois(n,m)
> ks.test(x,y)

	Two-sample Kolmogorov-Smirnov test

data:  x and y
D = 0.9, p-value = 0.0006071
alternative hypothesis: two-sided

拒绝原假设，认为分布不同。






2. 实例2
https://zhuanlan.zhihu.com/p/146781665

> x = c(0.6,1.1,1.6,2.2,2.4,2.9,3.4,4.0,4.3,4.8)
> ks.test(x,"punif",0,5) #第二个参数可以是待验证的分布类型，比如说 punif 均匀分布，pnorm 正态分布等等

	One-sample Kolmogorov-Smirnov test

data:  x
D = 0.14, p-value = 0.9744
alternative hypothesis: two-sided

D值是一样的；p值大于0.05，不拒绝原假设H0，认为数据服从(0, 5)上的均匀分布。


(2) 直接生成y效果类似

> y<-runif(length(x), 0, 5)
> ks.test(x, y)

	Two-sample Kolmogorov-Smirnov test

data:  x and y
D = 0.2, p-value = 0.9945
alternative hypothesis: two-sided




ref: 
https://bbs.pinggu.org/thread-1409823-1-1.html
https://www.cnblogs.com/arkenstone/p/5496761.html
https://zhuanlan.zhihu.com/p/292678346






========================================
回归 LM & GLM
----------------------------------------

广义回归的资料
family = c("gaussian", "binomial", "poisson", "multinomial", "cox", "mgaussian")




========================================
|-- 一元线性回归: ggplot2点图+线性趋势线+公式+R2+p值
----------------------------------------

1. 示例1

data("faithful")
model.lm<-lm(formula = waiting ~ eruptions, data = faithful)
summary(model.lm)
#对于一元线性回归方程y=ax+b，Intercept是指的截距，x对应的是系数。

l <- list(a = as.numeric(format(coef(model.lm)[1], digits = 4)),
          b = as.numeric(format(coef(model.lm)[2], digits = 4)),
          r2 = format(summary(model.lm)$r.squared, digits = 4),
          p = format(summary(model.lm)$coefficients[2,4], digits = 4))
eq <- substitute(italic(y) == a + b %.% italic(x)~","~ #怎么换行呢？
                   italic(R)^2~"="~r2~","~italic(P)~"="~p, l)
eq
#
library(ggplot2)
p <- ggplot(faithful,aes(x=eruptions,y=waiting)) + 
  geom_point() + theme_bw()+
  stat_smooth(method='lm',formula = y~x,colour='red')
p + geom_text(aes(x=4, y=50, label=as.character(as.expression(eq))), 
              parse = TRUE)
#




ref:
https://blog.csdn.net/weixin_43948357/article/details/105336901



========================================
|-- lowess和loess方法: 局部多项式回归 Local Polynomial Regression Fitting(loess), Scatter Plot Smoothing(lowess)
----------------------------------------

1. 二维变量之间的关系研究是很多统计方法的基础，例如回归分析通常会从一元回归讲起，然后再扩展到多元情况。局部加权回归散点平滑法（locally weighted scatterplot smoothing，LOWESS或LOESS）是查看二维变量之间关系的一种有力工具。

LOWESS主要思想是取一定比例的局部数据，在这部分子集中拟合多项式回归曲线，这样我们便可以观察到数据在局部展现出来的规律和趋势；而通常的回归分析往往是根据全体数据建模，这样可以描述整体趋势，但现实生活中规律不总是（或者很少是）教科书上告诉我们的一条直线。我们将局部范围从左往右依次推进，最终一条连续的曲线就被计算出来了。显然，曲线的光滑程度与我们选取数据比例有关：比例越少，拟合越不光滑（因为过于看重局部性质），反之越光滑。

LOESS and LOWESS (locally weighted scatterplot smoothing) are two strongly related non-parametric regression methods that combine multiple regression models in a k-nearest-neighbor-based meta-model. "LOESS" is a later generalization of LOWESS; 


Lowess和Loess都是非参数回归方法，Loess相比Lowess更加灵活和有用。Lowess通过窗口来考虑周边数据的影响，其预测值由窗口中的数据决定，窗口外的数据其贡献为0。但这种方法对野值非常敏感，Loess方法相比Lowess是一种更加robust的方法，其不仅仅考虑的局部的权重，还提出了robust权重，此权重主要是对野值进行加权，当某数据点被判断是野值后，其robust权重被设置为0，无贡献。

LOWESS本质上就是（加权）局部回归，所以理论上只要回归能做，LOWESS就能做，但我没见过你说的这种情况。LOWESS的初衷是为了检查散点图中的趋势（它具有较好的耐抗性，离群点的影响不大），而散点图通常是连续变量对连续变量的图。若因变量是离散变量，那么散点图本身的意义就不大了，仅仅在一些非常特殊的情况下可能有用，例如因变量为二分类。

Lowess and Loess的详细解释:
http://streaming.stat.iastate.edu/~stat416/LectureNotes/handout_LOWESS.pdf

Lowess and Loess in WiKi:
http://en.wikipedia.org/wiki/Local_regression



loess(): Fit a polynomial surface determined by one or more numerical predictors, using local fitting.

LOWESS(): This function performs the computations for the LOWESS smoother which uses locally-weighted polynomial regression (see the references).










2.
局部多项式回归拟合是对两维散点图进行平滑的常用方法，它结合了传统线性回归的简洁性和非线性回归的灵活性。当要估计某个响应变量值时，先从其预测变量附近取一个数据子集，然后对该子集进行线性回归或二次回归，回归时采用加权最小二乘法，即越靠近估计点的值其权重越大，最后利用得到的局部回归模型来估计响应变量的值。用这种方法进行逐点运算得到整条拟合曲线。 

在R语言中进行局部多项式回归拟合是利用loess函数，我们以cars数据集做为例子来看下使用方法。该数据中speed表示行驶速度，dist表示刹车距离。用loess来建立模型时重要的两个参数是span和degree，
- span表示数据子集的获取范围，取值越大则数据子集越多，曲线越为平滑。
span: the parameter α which controls the degree of smoothing.

- degree表示局部回归中的阶数，1表示线性回归，2表示二次回归，也可以取0，此时曲线退化为简单移动平均线。
degree: the degree of the polynomials to be used, normally 1 or 2. (Degree 0 is also allowed, but see the ‘Note’.)



(1)
# code 1
# 这里我们设span取0.4和0.8，从下图可见取值0.8的蓝色线条较为平滑。
plot(cars,pch=19)
model1=loess(dist~speed,data=cars,span=0.4)
lines(cars$speed,model1$fit,col='red',lty=2,lwd=2)
model2=loess(dist~speed,data=cars,span=0.8)
lines(cars$speed,model2$fit,col='blue',lty=2,lwd=2)


# code 2
# 当模型建立后，也可以类似线性回归那样进行预测和残差分析 
x=5:25
predict(model2,data.frame(speed=x))
plot(model2$resid~model2$fit)


# code 3
# 查看degree的影响
plot(cars,pch=19)
model0=loess(dist~speed,data=cars,degree=0)
lines(cars$speed,model0$fit,col='green',lty=1,lwd=2)

model1=loess(dist~speed,data=cars,degree=1)
lines(cars$speed,model1$fit,col='red',lty=3,lwd=2)

model2=loess(dist~speed,data=cars,degree=2)
lines(cars$speed,model2$fit,col='blue',lty=2,lwd=2)




## 还看到一个 poly 函数拟合曲线的，使用ggplot2绘制。不知道有啥区别。 //todo
data1 <- cars
for (i in 1:3) {
  mdl <- lm(dist ~ poly(speed, degree=i), data = data1)
  data1[,2+i] <- predict(mdl,data1)
}

# 作图
library(ggplot2)
ggplot(data1)+
  geom_point(aes(x=speed,y=dist))+
  geom_line(data=data1, aes(x=speed,y=V3),color="red")+
  geom_line(data=data1, aes(x=speed,y=V4),color="blue")+
  geom_line(data=data1, aes(x=speed,y=V5),color="green")+
  scale_fill_discrete(breaks=c("trt1","ctrl","trt2"))+
  guides(fill=guide_legend(title=NULL))+
  theme_bw()
#


(2)
# R语言中另一个类似的函数是lowess，它在绘图上比较方便，但在功能上不如loess强大和灵活。 
plot(cars,pch=19) 
lines(lowess(cars),lty=2,lwd=2) 



LOESS的优势是并不需要确定具体的函数形式，而是让数据自己来说话，其缺点在于需要大量的数据和运算能力。LOESS作为一种平滑技术，其目的是为了探寻响应变量和预测变量之间的关系，所以LOESS更被看作一种数据探索方法，而不是作为最终的结论。



ref:
https://blog.csdn.net/bbbeoy/article/details/72124019





========================================
|-- 使用Lasso（套索算法） 缩减基因组变量（去除多重共线性、选择变量）
----------------------------------------


文献中看到：
The ‘‘glmnet’’ R package was used to perform the LASSO Cox regression model analysis. Complete details are provided in the Supplementary Materials, http://links.lww.com/SLA/B161.
lars包[4]也实现了改进的lasso。

1.
Construction of ISGC using LASSO Cox Regression Model

LASSO is a popular method for regression of high-dimensional predictors.3-5 The method uses an L1 penalty to shrink some regression coefficients to exactly zero. The penalty parameter l, called the tuning parameter, controls the amount of shrinkage. 

Lasso是高维度回归预测的常用方法。该方法使用L1罚分来把一些回归因子缩减到精确的0。这个罚分参数lamada叫做转换参数，控制着缩减的程度。



The larger the l value, the fewer the number of predictors selected. 

这个lamada值越大，预测选择的参数越少。



LASSO has been extended and broadly applied to the Cox proportional hazard regression model for survival analysis with high-dimensional data. LASSO can also be used for optimal selection of markers in high-dimensional data with a strong prognostic value and low correlation among each other to prevent overfitting. 

Lasso被扩展，并广泛应用到高维度数据生存分析的Cox比例风险回归模型中。Lasso也被用于在高纬度数据中选择有最强预后值、低关联度的最优marker，防止过渡拟合。



We adopted the penalized Cox regression model with LASSO penalty to simultaneously achieve shrinkage and variable selection. 

我们采用Lasso罚分式的Cox回归模型来模拟同时实现缩减和变量筛选。



Five-time cross validations were used to determine the optimal values of l. We selected l via 1-SE (standard error) criteria, i.e., the optimal l is the largest value for which the partial likelihood deviance is within one SE of the smallest value of partial likelihood deviance. 

5倍交叉验证被用于选择最优的lamada。我们通过1-SE(standard error)来选择lamada，最优的lamada是最小偏似然离差的1倍SE内最大lamada值。



Thus, we plotted the partial likelihood deviance versus log (l), where l is the tuning parameter. A value l = 0.176 with log (l) = -1.738 was chosen by cross-validation via the 1-SE criteria. A vertical line was drawn at log (l) = -1.738, which corresponds to the optimal value l = 0.176 (Figure S5). The optimal tuning parameter resulted in five non-zero coefficients. Five features, CD3IM, CD3CT, CD8IM, CD45ROCT, and CD66bIM, with coefficients 0.14855447, 0.02054805, 0.04325494, 0.09574467, and 0.17309582, respectively, were selected in the LASSO Cox regression model (Figure 1D).

所以，我们画了偏似然离差vs log[lamda]，这里的lamda就是调整参数。通过1SE标准的交叉验证，我们选择了lamda=0.176，log[lamda]=-1.738。对应于lamda=0.176，也就是log[lamda]=-1.738位置画一条竖线。最优调整参数产生了5个非零系数。通过Lasso cox回归模型，选择了5个特征CD3IM, CD3CT, CD8IM, CD45ROCT, 和 CD66bIM, 系数分别是 0.14855447, 0.02054805, 0.04325494, 0.09574467, 和 0.17309582。


We investigated the prognostic or predictive accuracy of the ISGC using time-dependent ROC analysis. The AUC at different cutoff times was used to measure prognostic or predictive accuracy. The “survival ROC” package were used to perform the time-dependent ROC curve analysis.

我们研究了预后、使用时间依赖的ROC分析预测了ISgc的精度。 在不同时间截断点的AUC被用于衡量预后或者预测精度。“生存ROC”包被用于进行时间依赖的ROC曲线分析。




2.
X-tile plots offer a single and intuitive method to evaluate the association between variables and survival. The X-tile program can automatically choose the optimum data cutoff on the basis of the highest χ? value (minimum p value) formed by Kaplan–Meier survival analysis and log-rank test. We selected the optimum cutoff score for the density of each feature using X-tile software (version 3.6.1) based on the association with the patients’ DFS.

X-tile点图提供了单个的、直观的方法来评估变量二号生存率的相关性。X-tile程序可以根据最高卡方值自动选择最优的数据cutoff值（最低p-value），这是KM-生存分析和log-rank检验中的一部分。我们使用X-tile软件（3.6.1版本），选择了每个特征的最优cutoff值，依据是与病人DFS的关联。



3.
主要思路：
1.Lasso通过最小Cp值选择变量；
2.使用coef()获取变量的系数； 这个系数是标准化过的吗？
3.怎么求回归方程的截距？
用predict把零向量代进去，就有截距值了。predict(laa,t(c(0,0,0,0)))$fit

用R做Lasso如果使用lars，那么得到的系数是非标准化的，即原先变量的系数。（不信你把标准化的预处理后做下回归，会发现不一样的）。要求截距，确实用predict比较方便，当然根据数理推导代入公式也可以的。

http://f.dataguru.cn/forum.php?mod=viewthread&tid=265747&extra=&highlight=lasso&page=2
关于Fitting the Penalized Cox Model：https://cran.r-project.org/web/packages/glmnet/vignettes/Coxnet.pdf
X-tile画图： http://medicine.yale.edu/lab/rimm/research/software.aspx






一、Lasso概述
lasso estimate的提出是Robert Tibshirani在1996年JRSSB上的一篇文章Regression shrinkage and selection via lasso[2]。全称是least absolute shrinkage and selection operator。该方法是一种压缩估计。它通过构造一个罚函数得到一个较为精炼的模型，使得它压缩一些系数，同时设定一些系数为零。因此保留了子集收缩的优点，是一种处理具有复共线性数据的有偏估计。Lasso算法是一种能够实现指标集合精简的估计方法。LASSO的主要作用是降维[1,4]，排除多重共线性，进行变量选择，是有偏估计[5]。


基本思想是在回归系数的绝对值之和小于一个常数的约束条件下，使残差平方和最小，从而能够产生某些严格等于0的回归系数，得到可以解释的模型。其想法可以用如下的最优化问题来表述：


其他降维方法，如逐步回归有可能遗漏最优方程，而Lasso在错误率方面有着无可替代的优势[3]，可用于初级基因组学中经常出现的高纬度数据。传统的回归方法（最小二乘、逐步回归）用来处理业务背景相对熟悉、影响因素比较明确且不容易出现异常点的实际问题较为合适，而对于业务背景未知特别是海量自变量的问题（如基因、文本挖掘、语音识别等），甚至自变量超过观测数的问题，基于收缩机制的Lasso方法是更好的选择。



 lasso estimate具有shrinkage和selection两种功能，shrinkage这个不用多讲，本科期间学过回归分析的同学应该都知道岭估计会有shrinkage的功效，lasso也同样。关于selection功能，Tibshirani提出，当t" role="presentation" style="box-sizing: border-box; display: inline; line-height: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px;position:inherit;" >

//todo 




========================================
|-- R语言做岭回归
----------------------------------------
ridge regression可以用来处理下面两类问题：一是数据点少于变量个数；二是变量间存在共线性。

当变量间存在共线性的时候，最小二乘回归得到的系数不稳定，方差很大。这是因为系数矩阵X与它的转置矩阵相乘得到的矩阵不能求得其逆矩阵，而ridge regression通过引入参数lambda，使得该问题得到解决。在R语言中，MASS包中的函数lm.ridge()可以很方便的完成。它的输入矩阵X始终为n x p 维，不管是否包含常数项。

Usage
lm.ridge(formula, data, subset, na.action, lambda = 0, model = FALSE,
         x = FALSE, y = FALSE, contrasts = NULL, ...)
#
lambda: A scalar or vector of ridge constants.



ridge3.3<-lm.ridge(y~.-1,longley,lambda=seq(0,0.02,0.0001))
#标准化数据要-1没截距项




##############
# code 1
#install.packages("MASS")
library('MASS')
longley 
names(longley)[1] <- "y"
lm.ridge(y ~ ., longley)
##                   GNP          Unemployed    Armed.Forces     Population      Year          Employed 
## 2946.85636017    0.26352725    0.03648291    0.01116105       -1.73702984   -1.41879853    0.23128785 
plot(lm.ridge(y ~ ., longley, lambda = seq(0,0.1,0.001)))

select(lm.ridge(y ~ ., longley, lambda = seq(0,0.1,0.0001)))
# modified HKB estimator is 0.006836982 
# modified L-W estimator is 0.05267247 
# smallest value of GCV  at 0.0057 





##############
# code 2
# 做岭回归，对于标准化后的数据模型不包含截距项，其中lambda为岭参数k的所有取值
ridge3.3<-lm.ridge(y~.,longley,lambda=seq(0,0.02,0.0001)) #seq(0,0.2,0.001)
str(ridge3.3)
plot(ridge3.3)

#
ridge.sol=ridge3.3
matplot(x=ridge3.3$lambda, y=t(ridge3.3$coef), 
        xlab = expression(lamdba), ylab= "Cofficients", type = "l", lty = 1:20) # lty = 1:20可加可不加，设置线的形状.
#作出lambdaGCV取最小值时的那条竖直线
abline(v = ridge3.3$lambda[which.min(ridge3.3$GCV)]) #GCV是什么？
#上图在lambad在0.006左右，自变量的系数值趋于稳定。
#下面的语句绘出lambda同GCV之间关系的图形：
plot(ridge.sol$lambda, ridge.sol$GCV, type = "l", xlab = expression(lambda), ylab = expression(beta))
abline(v = ridge.sol$lambda[which.min(ridge.sol$GCV)]) 
#语句ridge.sol$coef[which.min(ridge.sol$GCV)]  为找到GCV最小时对应的系数
ridge.sol$lambda[which.min(ridge.sol$GCV)] #[1] 0.0057

#在上面的代码中，还可以调整lambda = seq(0, 1, length = 2000)的范围及大小，
#如果lambad只是一个值，则得到的图像为空，只有在lambad变化的时候，才能得到岭迹曲线。







##############
# code 3 https://blog.csdn.net/li603060971/article/details/49508279
library('MASS')
head(longley)
#先标准化数据
data.norm=as.data.frame(apply(longley,2,scale) )
names(data.norm)[1] <- "y"
head(data.norm)
# y   GNP Unemployed Armed.Forces Population       Year   Employed
#
la=seq(0,1,0.01)
ridge.rs=lm.ridge(y~.-1, data=data.norm, lambda=la) #标准化数据要-1没截距项
head(ridge.rs)
plot(ridge.rs)
coef(ridge.rs)
#
#删除Population 由-变+
ridge.rs2<-lm.ridge(y~.-Population-1, data=data.norm,
                    lambda=seq(0,1,0.001))#岭回归
plot(ridge.rs2)
coef(ridge.rs2)
#
#删除year 由-变+
ridge.rs3<-lm.ridge(y~.-Population-Year-1, data=data.norm,
                    lambda=seq(0,1,0.001))#岭回归
plot(ridge.rs3)
coef(ridge.rs3) #全为正了
#
select(ridge.rs3)
#modified HKB estimator is 0.05941355 
#modified L-W estimator is 0.03466392 
#smallest value of GCV  at 0.479
lm.ridge(y~.-Population-Year-1, data=data.norm,lambda=0.479)
#     GNP      Unemployed   Armed.Forces  Employed 
# 0.4546171    0.1769713    0.1210725    0.3684340


#
# 下面利用ridge包中的linearRidge()函数进行自动选择岭回归参数
#install.packages('ridge')
library(ridge)
mod <- linearRidge(y ~ ., data = data.norm)
summary(mod)
## Call:
## linearRidge(formula = y ~ ., data = data.norm)
## 
## Coefficients:
##                Estimate Scaled estimate Std. Error (scaled) t value (scaled) Pr(>|t|)    
## (Intercept)  -3.106e-18              NA                  NA               NA       NA    
## GNP           3.995e-01       1.547e+00           3.419e-01            4.526  6.0e-06 ***
## Unemployed    1.026e-01       3.972e-01           2.323e-01            1.710   0.0873 .  
## Armed.Forces  8.903e-02       3.448e-01           1.765e-01            1.953   0.0508 .  
## Population   -1.825e-02      -7.068e-02           4.898e-01            0.144   0.8853    
## Year          2.897e-01       1.122e+00           2.493e-01            4.500  6.8e-06 ***
## Employed      2.195e-01       8.502e-01           4.630e-01            1.836   0.0663 .  
## ---
## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
## 
## Ridge parameter: 0.01046912, chosen automatically, computed using 2 PCs
## 
## Degrees of freedom: model 3.67 , variance 3.218 , residual 4.123 

#从模型运行结果看，测岭回归参数值为0.01046912，2个变量的系数显著（GNP，Year）
#最后，利用Lasso回归解决共线性问题






##############
## code 4
library(lars)
## Loaded lars 1.2
x = as.matrix(longley[, 2:7])
y = as.matrix(longley[, 1])
(laa = lars(x, y, type = "lar")) #lars函数值用于矩阵型数据
## Call:
## lars(x = x, y = y, type = "lar")
## R-squared: 0.993 
## Sequence of LAR moves:
##      GNP Year Armed.Forces Unemployed Employed Population
## Var    1    5            3          2        6          4
## Step   1    2            3          4        5          6
# 由此可见，LASSO的变量选择依次是 GNP Year Armed.Forces Unemployed Employed Population
plot(laa)#绘出图
summary(laa)  #给出Cp值
## LARS/LAR
## Call: lars(x = x, y = y, type = "lar")
##   Df     Rss        Cp
## 0  1 1746.86 1210.0561
## 1  2 1439.51  996.6871
## 2  3   32.31   12.6400
## 3  4   23.18    8.2425
## 4  5   22.91   10.0505
## 5  6   22.63   11.8595
## 6  7   12.84    7.0000
## 根据课上对Cp含义的解释（衡量多重共线性，其值越小越好），我们取到第3步，使得Cp值最小，也就是选择GNP, Year, Armed.Forces这三个变量。

## -> 最后的是lar方法 不是lasso。type值变为lasso才对。
(las = lars(x, y, type = "lasso"))
## Call:
## lars(x = x, y = y, type = "lasso")
## R-squared: 0.993 
## Sequence of LASSO moves:
##      GNP Year Armed.Forces Unemployed Employed Population Year Employed Employed Year Employed Employed
## Var    1    5            3          2        6          4   -5       -6        6    5       -6        6
## Step   1    2            3          4        5          6    7        8        9   10       11       12
plot(las)
summary(las)
## LARS/LASSO
## Call: lars(x = x, y = y, type = "lasso")
##    Df     Rss        Cp
## 0   1 1746.86 1210.0561
## 1   2 1439.51  996.6871
## 2   3   32.31   12.6400
## 3   4   23.18    8.2425
## 4   5   22.91   10.0505
## 5   6   22.63   11.8595


哪个指标判断共线性？






========================================
五类模型的变量选择可采用R语言的glmnet包来解决。这五类模型分别是： //todo
----------------------------------------
1. 二分类logistic回归模型
2. 多分类logistic回归模型
3.Possion模型
4.Cox比例风险模型
5.SVM










ref:
https://blog.csdn.net/orchidzouqr/article/details/53582801
https://blog.csdn.net/jiabiao1602/article/details/39338181













========================================
广义线性回归模型
----------------------------------------
1. 对线性模型做了2个方面的推广
- 通过设定一个连接函数，将响应变量的期望与线性自变量相联系；
- 对误差的分布给出一个误差函数。


2. 广义线性模型的三个概念
- 线性自变量：第i个响应变量的期望值E(yi) 只是通过线性自变量 BetaT.xi 而依赖于 xi。
	其中如通常一样，Beta是未知参数的 (p+1) x 1 向量，可能包含截距。
- 连接函数：说明线性自变量和 E(yi) 的关系，给出了线性模型的推广。
- 误差函数: 说明广义线性模型的最后一部分随机成分，我们保留样本为相互独立的假设，但去掉了可加和正态误差的假设。
	可以从指数分布族中选一个作为误差函数。



3. GLM 的常见的连接函数和误差函数
对于正态线性模型，假设 yi 是正态分布，均值为 xiT.Beta，未知方差 sigma^2；
如果假设 yi 是 Poisson 随机变量，均值为 exp(xiT.beta)，我们得到 Poisson 回归模型。














========================================
cox回归的全称 cox proportional hazards regression model
----------------------------------------
1. 概述
COX回归模型，又称“比例风险回归模型(proportional hazards model，简称Cox模型)”，以生存结局和生存时间为因变量，可同时分析众多因素对生存期的影响。

通过KM和log-rank test检验的方法，只能够处理单个二分类因素的生存数据。当想探究多个因素或者离散型变量对生存时间的影响时，我们就需要借助于cox回归方法。



2.用到的R函数：
数据类型处理和转换：
	factor()
	as.numeric()
	as.formula()
	ldply()
Cox 回归：
	Surv()
	coxph()

library(survival)
data(lung)
#分析离散型变量
res.cox1 = coxph(Surv(time, status) ~ sex, data=lung)
#分析连续性变量
res.cox2 = coxph(Surv(time, status) ~ age, data=lung)
#分析多个变量
res.cox3 = coxph(Surv(time, status) ~ age + sex + ph.ecog, data=lung)

summary(res.cox3)

这里的exp(coef)就是HR（hazard ratio，风险率），lower .95和upper .95为95%的置信区间

首先查看likehood ration test , wald test, logrank test三种检验方法的p值，p值小于0.05, 这个回归方程是统计学显著的。说明在这么多自变量中包含了对生存时间具有影响的因素。

然后查看每个自变量的p值，可以看到sex和ph.ecog这两个变量的p值小于0.05，而age的p值大于0.05, 说明sex和ph,ecog这两个变量对生存时间的影响更加显著。

最后查看自变量的coef等指标，coef就是偏回归系数，exp(coef)就是HR。
- sex的HR值小于1，该数据集中1=male, 2= female, HR表示的是数值大的风险/数值小的风险，在这里就是female/ male， 说明female死亡的相对较低。HR的值约为0.58, 说明female的死亡风险只占了male的58%， 相比male, female的死亡风险降低了42%。
- ph.ecog的HR值大于1， 说明随着ph.ecog数值的增加，死亡风险会增加。


可视化结果
sex_df = with(lung,
	data.frame(
		sex=c(1,2),
		age=rep(mean(age, na.rm=T, 2)),
		ph.ecog=c(1,1)
	)
)
sex_df
fit=surfit(res.cox, newdata=sex_df)
ggsurvplot(fit, data=lung, pval="HR:0.58", legend.labs=c("Sex=1", "Sex=2"))





3. 题目
为研究某基因（代号ABC）的表达与患者肿瘤恶性程度的相关性，于是回顾性收集了80例患者的信息和肿瘤样品的石蜡切片，通过免疫组化检测了ABC在肿瘤组织中的表达，数据见Excel“基线资料数据-作业”，请你帮忙进行以下分析：

- 单因素Cox回归分析各个因素与患者生存的关系。
- 多因素Cox回归分析各个因素与患者生存的关系。



ref:
https://blog.csdn.net/weixin_43569478/article/details/108079548



========================================
|-- 单因素cox回归分析: Univariate Cox
----------------------------------------
univariate [ˌjuːnɪˈveərɪɪt] adj. [数] 单变量的


1. 单个单因素cox回归分析

# 安装并加载所需的R包
# install.packages("rms") #?
library(survival)

# 加载数据 Or TCGA AML dataset
pbc <- pbc[pbc$status %in% c(0, 1), ]

res.cox <- coxph(Surv(time, status) ~ age, data = pbc)
res.cox
## Call:
## coxph(formula = Surv(time, status) ~ age, data = pbc)
## 
##         coef exp(coef) se(coef)     z        p
## age -0.08115   0.92205  0.02305 -3.52 0.000431
## 
## Likelihood ratio test=14.04  on 1 df, p=0.0001786
## n= 257, number of events= 25

★ 结果显示P值是有意义的。

summary(res.cox)
## Call:
## coxph(formula = Surv(time, status) ~ age, data = pbc)
## 
##   n= 257, number of events= 25 
## 
##         coef exp(coef) se(coef)     z Pr(>|z|)    
## age -0.08115   0.92205  0.02305 -3.52 0.000431 ***
## ---
## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
## 
##     exp(coef) exp(-coef) lower .95 upper .95
## age    0.9221      1.085    0.8813    0.9647
## 
## Concordance= 0.712  (se = 0.042 )
## Likelihood ratio test= 14.04  on 1 df,   p=2e-04
## Wald test            = 12.39  on 1 df,   p=4e-04
## Score (logrank) test = 13.16  on 1 df,   p=3e-04

★ exp(coef)就是风险比HR的意思；Lower95和upper95是HR置信区间的上下限。

HR值大于1，提示暴露因素是阳性事件发生的促进因素；
HR值小于1，提示暴露因素是阳性事件发生的阻碍因素；
HR值等于1，提示暴露因素对阳性事件的发生无影响。


summary(res.cox)
> summary(res.cox)$logtest['pvalue']
     pvalue 
0.001839014






2. 批量单因素cox回归分析

一般我们的关注的特征都比较多，用上面的代码一个一个来做单因素cox回归分析效率太低了，下面我们来看看如何批量做单因素cox回归分析。

# 总结做单因素cox回归分析的特征
covariates <- c("age", "sex", "bili", "albumin", "copper", "alk.phos", "ast", "trig", "platelet", "protime", "stage")

# 分别对每一个变量，构建生存分析的公式
univ_formulas <- sapply(covariates,
                        function(x) as.formula(paste('Surv(time, status)~', x)))

# 循环对每一个特征做cox回归分析
univ_models <- lapply( univ_formulas, function(x){coxph(x, data = pbc)})

# 提取HR，95%置信区间和p值
univ_results <- lapply(univ_models,
                       function(x){ 
                         x <- summary(x)
                         #获取p值
                         p.value<-signif(x$wald["pvalue"], digits=2)
                         #获取HR
                         HR <-signif(x$coef[2], digits=2);
                         #获取95%置信区间
                         HR.confint.lower <- signif(x$conf.int[,"lower .95"], 2)
                         HR.confint.upper <- signif(x$conf.int[,"upper .95"],2)
                         HR <- paste0(HR, " (", 
                                      HR.confint.lower, "-", HR.confint.upper, ")")
                         res<-c(p.value,HR)
                         names(res)<-c("p.value","HR (95% CI for HR)")
                         return(res)
                       })

# 转换成数据框，并转置
res <- t(as.data.frame(univ_results, check.names = FALSE))
as.data.frame(res)
##          p.value HR (95% CI for HR)
## age      0.00043   0.92 (0.88-0.96)
## sex         0.38    0.58 (0.17-1.9)
## bili     8.6e-06      1.2 (1.1-1.3)
## albumin     0.02    0.3 (0.11-0.83)
## copper   0.00014            1 (1-1)
## alk.phos    0.71            1 (1-1)
## ast        0.061            1 (1-1)
## trig       0.073            1 (1-1)
## platelet    0.05            1 (1-1)
## protime     0.42     0.8 (0.46-1.4)
## stage     0.0059        2 (1.2-3.3)
write.table(as.data.frame(res), file="univariate_cox_result.txt", quote=F, sep="\t")



ref: https://www.jianshu.com/p/d431eda56d32
https://blog.csdn.net/weixin_43569478/article/details/108079548






========================================
多因素cox回归分析：Multivariate Cox
----------------------------------------
multivariate [ˌmʌltɪˈveərɪɪt] adj. [数][统计] 多元的；[数] 多变量的 n. 多元；多变量

前面是单独看每一个特征是否跟生存相关，而多因素cox回归是同时检测多个特征是否与生存相关。
一般先通过单因素cox回归分析找出与生存显著相关的特征，然后基于这些特征再去做多因素cox回归分析，或者做LASSO分析。

1. 示例
res.cox <- coxph(Surv(time, status) ~ age + bili + albumin + copper + platelet + stage, data =  pbc)
x <- summary(res.cox)
x
## Call:
## coxph(formula = Surv(time, status) ~ age + bili + albumin + copper + 
##     platelet + stage, data = pbc)
## 
##   n= 183, number of events= 19 
##    (74 observations deleted due to missingness)
## 
##               coef exp(coef)  se(coef)      z Pr(>|z|)   
## age      -0.077663  0.925277  0.027365 -2.838  0.00454 **
## bili      0.258600  1.295115  0.079748  3.243  0.00118 **
## albumin  -1.290163  0.275226  0.787549 -1.638  0.10138   
## copper    0.004702  1.004713  0.002130  2.208  0.02725 * 
## platelet  0.003291  1.003296  0.002755  1.195  0.23225   
## stage     0.736305  2.088205  0.332024  2.218  0.02658 * 
## ---
## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
## 
##          exp(coef) exp(-coef) lower .95 upper .95
## age         0.9253     1.0808   0.87696    0.9763
## bili        1.2951     0.7721   1.10771    1.5142
## albumin     0.2752     3.6334   0.05879    1.2884
## copper      1.0047     0.9953   1.00053    1.0089
## platelet    1.0033     0.9967   0.99789    1.0087
## stage       2.0882     0.4789   1.08932    4.0031
## 
## Concordance= 0.879  (se = 0.029 )
## Likelihood ratio test= 34.32  on 6 df,   p=6e-06
## Wald test            = 31.38  on 6 df,   p=2e-05
## Score (logrank) test = 42.22  on 6 df,   p=2e-07

★ stage，copper ，bili及age变量有显著意义
p值多用 wald test。


pvalue=signif(as.matrix(x$coefficients)[,5],2)
HR=signif(as.matrix(x$coefficients)[,2],2)
low=signif(x$conf.int[,3],2)
high=signif(x$conf.int[,4],2)

multi_res=data.frame(p.value=pvalue,
                     HR=paste(HR," (",low,"-",high,")",sep=""),
                     stringsAsFactors = F
)
multi_res
##          p.value               HR
## age       0.0045 0.93 (0.88-0.98)
## bili      0.0012    1.3 (1.1-1.5)
## albumin   0.1000 0.28 (0.059-1.3)
## copper    0.0270          1 (1-1)
## platelet  0.2300          1 (1-1)
## stage     0.0270      2.1 (1.1-4)

write.table(multi_res, file="multivariate_cox_result.txt", quote=F, sep="\t")







========================================
|-- 单因素 cox 回归、多因素 cox 回归、生存分析、logistic回归的区别与联系
----------------------------------------
1. KM生存分析、COX回归分析都是分析与病人预后有关的因素的方法，那它们两者之间有什么区别呢？

我们来回忆一下KM生存分析的一般流程是根据某种因素比如性别、肿瘤组织学分级、某个基因表达量的高低将样本分成几组，然后比较各个组别之间的生存的差别，计算p值。那就要求我们要分析的变量无论本身是分类变量也好，还是转化成分类变量也好，总之得是分类变量，这样才能对样本进行分组嘛，而且无论你是根据性别分为两组也好，还是根据组织学分级分为3-4组也好，KM生存分析一次只能分析一个变量对生存的影响，也就是说，KM生存分析本质也是一个单变量生存分析。

这样，把KM简单拉一遍之后，COX回归分析的优势就很明显了：
	首先它能同时分析多个因素与生存的关系，也就是多因素COX回归分析；
	其次，它能分析连续型变量对生存的影响，我们示例数据中分析的就是基因在样本中的表达情况（连续型变量）对样本生存的影响。




2. KM生存分析是单变量分析，单因素COX分析也是单变量分析，那它们的分析结果一样吗？

library(survival)
library(survminer)

# dataset
data(lung)
head(lung)

dat=lung
str(dat)
vars=colnames(dat)

#1) 批量单因素cox分析----
#设置p值的阈值
pfilter <- 0.05   
#新建空白数据框
uniresult <- data.frame()
# 使用for循环对输入数据中的100个基因依次进行单因素COX分析
#单因素COX回归分析中p值＜0.05的基因，其分析结果输入到之前新建的空白数据框uniresult中
for(i in 4:length(vars)){
  message(i, vars[i])
  unicox <- coxph(Surv(time = time, event = status) ~ dat[,i], data = dat)
  unisum<- summary(unicox)
  pvalue <- round(unisum$coefficients[,5],3)
  if(pvalue<pfilter){
    uniresult <- rbind(uniresult,
                       cbind(gene=vars[i],
                             HR=unisum$coefficients[,2],
                             L95CI=unisum$conf.int[,3],
                             H95CI=unisum$conf.int[,4],
                             pvalue=unisum$coefficients[,5]
                       ))
  }
}

#
cox_gene=uniresult$gene
cox_gene
# [1] "age"       "sex"       "ph.ecog"   "ph.karno"  "pat.karno"


#2) 对显著的做多因素cox回归分析 ----
multicox <- coxph(
  #Surv(time = time,event = status) ~ cox_gene, 
  as.formula( paste0("Surv(time = time,event = status) ~ ", paste(cox_gene, collapse = "+") ) ), 
  data = dat)
multisum <- summary(multicox)
# 多因素COX回归分析是同时分析多个元素（这里是12个基因）与样本的生存的关系，
# 所以代码相对于单因素COX回归分析会更加简单，不用for循环，一行代码就搞定了
#gene <- colnames(unigene)[3:ncol(unigene)]
multiresult <- data.frame(gene=cox_gene,
                          HR=multisum$coefficients[,2],
                          L95CI=multisum$conf.int[,3],
                          H95CI=multisum$conf.int[,4],
                          pvalue=multisum$coefficients[,5])
multiresult
multiresult2 <- subset(multiresult, pvalue<pfilter)
multiresult2
multiresult2$gene #"sex"     "ph.ecog"
#保存多因素COX回归分析结果
write.csv(multiresult, file = "result/multiresult.csv",row.names = F)

# 对于单因素、多因素COX回归分析的结果，我们可以绘制森林图来进行可视化 //todo


#3) 批量生存分析 ----
keep=c()
for(i in 4:length(vars)){
  message(i, vars[i])
  tmp=dat[, c(2,3, i)]
  tmp$group=ifelse(tmp[,3]>median(tmp[,3], na.rm = T), "high", "low")
  fit=survfit(Surv(time, status)~group, data=tmp)
  p=surv_pvalue(fit=fit, data=tmp, method = "log-rank")[,2]
  if(p<0.05){ keep=c(keep, i) }
}
km_gene=vars[keep]
km_gene
# [1] "sex"       "ph.ecog"   "ph.karno"  "pat.karno"


比较显著的基因/因素：
	单因素 cox 回归: cox_gene #[1] "age"       "sex"       "ph.ecog"   "ph.karno"  "pat.karno"
	多因素 cox 回归: multiresult2$gene #"sex"     "ph.ecog"
	生存分析: km_gene # [1] "sex"       "ph.ecog"   "ph.karno"  "pat.karno"

KM生存分析、单因素COX回归分析筛选出的基因不完全相同。
这是因为，虽然都是分析某个因素，但他们内部的算法是不同的，所以结果有出入也很正常。我们根据文献选择更适合自己研究的分析即可。

基因表达量不是连续变量吗？怎么才能用KM生存分析呢？按某个值（比如中位数）分为高低两组。






3. 为什么大多数生信文章中都是先进行单因素COX回归分析再进行多因素COX回归分析？

我们进行单因素COX回归分析的时候一次只纳入一个变量，就不能排除有这么一种情况存在：变量a影响生存其实是通过影响变量b而导致的，也就是说变量a其实对生存没有直接的影响。那么，就需要将单因素COX 分析筛选出来的显著变量继续纳入多因素COX比例风险模型，从而排除掉类似变量a的这种混杂因素。

单因素COX回归分析像是对变量的一个初步筛选，多因素COX回归分析筛选出来的才是独立预后因素。









4. 为什么需要用 Lasso + Cox 生存分析模式

https://blog.51cto.com/u_16099177/10026925

单因素Cox分析筛选关联的变量，然后构建多因素Cox模型进一步确认变量与生存的关联是否独立。

多因素方差分析结果和单因素有时候相反？
但这样做没有考虑变量之间多重共线性的影响，有时候我们甚至会发现单因素和多因素Cox回归得到的风险比是矛盾的，这是变量之间多重共线性导致模型失真的结果。并且，(Least absolute shrinkage and selection operator)回归首先进行变量的筛选，然后构建Cox回归模型分析预后影响，这就是Lasso + Cox 生存分析模式。

因此，当变量之间存在多重共线性，或者变量个数大于样本量时，需要用Lasso(Least absolute shrinkage and selection operator)回归首先进行变量的筛选，然后构建Cox回归模型分析预后影响，这就是Lasso + Cox 生存分析模式。



具体代码参考：
# Lasso & cox
outputRoot="E:\\research\\AML-SCCPDH\\output\\"
keyword="AML_Lasso_Cox"

library(survival)
library(survminer)


#1. load data----
# (1)fpkm ====
fpkm=read.table(paste0(outputRoot, "../TCGA-LAML.htseq_fpkm.tsv"), 
                row.names = 1, header = T, check.names = F )
fpkm[1:3,1:4]
dim(fpkm) # 60483   151

# rm last A or B, to be the same as clin
colnames(fpkm) = sapply( colnames(fpkm), function(x){
  substr(x, 1, nchar(x)-1)
} ) |> as.character()


# (2) gene anno ====
anno=read.table(paste0(outputRoot, "../gencode.v22.annotation.gene.probeMap"), row.names = 1, header = T )
anno[1:3,]
dim(anno) # 60483     5

# (3) clinic ====
clin=read.table(paste0(outputRoot, "../survival_LAML_survival.txt"), 
                row.names = 1, 
                header = T, sep="\t", check.names = F )
clin[1:3, ]
dim(clin) #186 10
#rm NA in time
clin=clin[which(!is.na(clin$OS.time)),]
dim(clin) #186 10
#
table(clin$OS.time |> is.na())
table(clin$OS.time < 0)
table(clin$OS.time == 0)

# sub 0 to 1
hist(clin$OS.time, n=100)
clin$OS.time = ifelse(clin$OS.time==0, 1, clin$OS.time)



# (4) connect ====
patient = intersect( colnames(fpkm), rownames(clin))
length(patient) # 140

fpkm = fpkm[,patient]
dim(fpkm) #60483   140

clin = clin[patient,]
dim(clin) #140  10

fpkm = as.matrix(fpkm)
identical(rownames(clin), colnames(fpkm)) #T

#colnames(meta)[c(1,3)] = c("event","time") #OS, OS.time

# fpkm add gene as rownames
geneslist=anno$gene |> unique()
fpkm2=t(sapply(geneslist, function(symbol){
  enId=anno[which(anno$gene==symbol), ] |> rownames()
  enId=enId[ enId %in% rownames(fpkm) ]
  fpkm[enId, , drop=F] |> colSums()
}) ) |> as.data.frame()
dim(fpkm2)
fpkm2[1:3, 1:4]





#2. Cox & Lasso ----

## (1) 批量单因素cox 回归----
vars=rownames(fpkm2)
head(vars)
length(vars) #58387
#设置p值的阈值
pfilter <- 0.05
#新建空白数据框
uniresult <- data.frame()
# 使用for循环对输入数据中的100个基因依次进行单因素COX分析
#单因素COX回归分析中p值＜0.05的基因，其分析结果输入到之前新建的空白数据框uniresult中
for(i in 1:length(vars)){
  symbol=vars[i]
  # rm all 0, and sd=9 rows
  if(0 == var( unlist(fpkm2[symbol,]) ) ){
    next;
  }
  dif=data.frame(
    time=clin$OS.time,
    status=clin$OS,
    val=fpkm2[symbol,] |> as.numeric() 
  )
  #print(head(dif))
  
  unicox <- coxph(Surv(time = time, event = status) ~ val, data = dif)
  unisum<- summary(unicox)
  pvalue <- round(unisum$coefficients[,5],3)
  if(pvalue<pfilter){
    #message(i, "\t", symbol)
    #message("\t sig")
    uniresult <- rbind(uniresult,
                       cbind(gene=symbol,
                             HR=unisum$coefficients[,2],
                             L95CI=unisum$conf.int[,3],
                             H95CI=unisum$conf.int[,4],
                             pvalue=unisum$coefficients[,5]
                       ))
  }
}
head(uniresult)
dim(uniresult) #8977    5
#
cox_gene=uniresult$gene
cox_gene |> head()


## (2) 构建模型 Surv ====
modleY=Surv(time=clin$OS.time, event = clin$OS) |> data.matrix()
head(modleY)


## (3) Lass  ====
#install.packages("glmnet")
library(glmnet)
fit=glmnet(x=t(fpkm2), y=modleY, family = "cox")
# negative event times encountered; not permitted for Cox family #time不能为0，可填充为1或2
plot(fit)
plot(fit, xvar="lambda", label=T)


## (4) 十折交叉验证刷选最佳 lambda ====
set.seed(2024)
lasso_fit=cv.glmnet(x=t(fpkm2), y=modleY, family="cox", type.measure = "deviance", nfolds = 10)
plot(lasso_fit)
#plot(lasso_fit, xvar="lambda", label=T)
lasso_fit
#    Lambda Index Measure     SE Nonzero
#min 0.1885    17   7.581 0.4251      23
#1se 0.3967     1   7.874 0.4220       0


## (5) 选择最佳 lambda 值（1se对应的 lambda）====
lambda.1se=lasso_fit$lambda.1se
lasso_fit$lambda.min #0.1884849
lambda.1se #0.3967426


## (6) 使用1se的 lambda 重新建模 ====
model_lasso_1se = glmnet(x=t(fpkm2), y=modleY, family="cox", 
                            type.measure = "deviance", nfolds = 10,
                            lambda = lambda.1se )
# 拿到建模使用的基因
gene_1se = rownames(model_lasso_1se$beta)[as.numeric(model_lasso_1se$beta)!=0]
#没有基因！！和第(4)步类似

# 使用 min 再次建模
model_lasso_min = glmnet(x=t(fpkm2), y=modleY, family="cox", 
                         type.measure = "deviance", nfolds = 10,
                         lambda = lasso_fit$lambda.min )
# 拿到建模使用的基因
gene_min = rownames(model_lasso_min$beta)[as.numeric(model_lasso_min$beta)!=0]
gene_min |> length() #23个
gene_min |> jsonlite::toJSON()
# "UBE2Q1","MYADML","RP11-745L13.2","CTD-2194D22.3","TREML2","CCND3","RP3-486I3.7","TWIST1","MIR4285","MZT1P2","PTP4A3","CREB3","LINC00702","FIBP","RP11-522N14.2","RP11-182J1.13","SOCS1","TGIF1","TCF15","CBR1","AF064858.11","RP11-402P6.3","RP1-20I3.3"








5. cox 回归与logistic 回归的区别

(1) logistic回归 vs 线性回归
logistic回归，与线性回归并成为两大回归，应用范围一点不亚于线性回归，甚至有青出于蓝之势。

因为logistic回归太好用了，而且太有实际意义了。解释起来直接就可以说，如果具有某个危险因素，发病风险增加2.3倍，听起来多么地让人通俗易懂。线性回归相比之下其实际意义就弱了。

(2) logistic 回归
logistic 回归与线性回归恰好相反，因变量一定要是分类变量，不可能是连续变量。分类变量既可以是二分类，也可以是多分类，多分类中既可以是有序，也可以是无序。二分类logistic回归有时候根据研究目的又分为条件ogistic回归和非条件logistic回归。
	条件 logistic 回归用于配对资料的分析，
	非条件logistic 回归用于非配对资料的分析，也就是直接随机抽样的资料。
无序多分类logistic回归有时候也成为多项logit 模型，有序logistic回归有时也称为累积比数logit 模型。

(3) cox 回归
cox回归，cox回归的因变量就有些特殊，因为他的因变量必须同时有2个，
	一个代表状态，必须是分类变量，
	一个代表时问，应该是连续变量。
只有同时具有这两个变量，才能用cox回归分析。
cox回归主要用于生存资料的分析，生存资料至少有两个结局变量，一是死亡状态，是活着还是死亡？二是死亡时问，如果死亡，什么时问死亡？如果活着，从开始观察到结束时有多久了？所以有了这两个变量，就可以考虑用cox回归分析。



(4) 总结
- 都可以用来筛选影响因素
- 都有OR值或者RR值
- 应变量不一样：Cox回归的应变量是生存时间*Cencor(结局)，而logistic回归应变量是分类资料，比如二分类。
- 条件 logistic 回归分析与 cox 回归分析有相似的地方，
	SAS 程序相同；
	SPSS里面条件logistic回归分析就是借用Cox比例风险模块进行分析
	logistic回归是cox回归的一个特例
	当全部个体都有结局时，两者的结果（beta）是一样的。
	cox回归可以考察生存函数，而logistic不可以。
		补充：在SPSS里，配对logistic回归的模型，是在cox回归里完成的。















ref:
1.单因素和多因素cox回归分析 https://zhuanlan.zhihu.com/p/339754573?ivk_sa=1024320u
https://www.jianshu.com/p/0fa217a09a70
https://blog.51cto.com/u_16099177/10026925
https://www.jianshu.com/p/f829b9badb28



========================================
相关系数 r 的显著性检验：t=r * sqrt(n-2) / sqrt(1-r*r) 符合自由度为 df=n-2 的t分布
----------------------------------------
1. 相关系数的检验
(1) 手动计算 r 值

a=c(1,3,3,4,5); b=c(10,20,30,40,40)

# 根据相关系数的定义，r=
sum( (a-mean(a)) * (b-mean(b)) ) / sqrt( sum( (a-mean(a))**2 )/(length(a)-1) ) /sqrt( sum( (b-mean(b))**2 )/(length(b)-1) ) / (length(a)-1) # 0.9307578

sum( (a-mean(a)) * (b-mean(b)) ) / sd( a-mean(a) ) / sd(b-mean(b)) / (length(a)-1) #0.9307578

cor(a, b) #0.9307578

cor.test(a, b) #cor=0.9307578, p-value = 0.02164

# 为了简便，使用 -mean() 后的数据，也叫 autoscale 后的数据
a1=a-mean(a)
b1=b-mean(b)

sum(a1*b1) / sqrt(var(a1))/sqrt(var(b1))/(length(a1)-1) #0.9307578

sum(a1*b1) / sd(a1)/sd(b1)/(length(a1)-1) #0.9307578，是一样的

cor.test(a1, b1) #cor=0.9307578, p-value=0.02164, t=4.4091



(2) 手动算p值
# 相关系数
r=cor(a1, b1); r #0.9307578

根据某个定义: 
	相关系数r的t统计量：t统计量=r * sqrt(n-2) / sqrt(1-r*r)
	符合自由度为n-2的t分布

# 1)计算t统计量
t_statistic =r*sqrt(length(a1)-2) / sqrt(1-r**2); t_statistic  #t 统计量 4.409082

# 2)计算t统计量在自由度为n-2时的p值
2*( pt(-abs(t_statistic), df=length(a1)-2) ) #0.02164347
# p值 和 t统计量 都和 cor.test() 结果一致




========================================
|-- 与PC1显著相关的基因有哪些？
----------------------------------------
1. 概述
(1)求相关系数r，及r的显著性：同上。

(2) 简化
Yamamoto H., Fujimori T., Sato H., Ishikawa G., Kami K., Ohashi Y. (2014). "Statistical hypothesis testing of factor loading in principal component analysis and its application to metabolite set enrichment analysis". BMC Bioinformatics, (2014) 15(1):51.

loadings和r成比例：r = sqrt(lambda) * W / sd(Xk)
	和硬算cor.test(PC1_score, scale.data.gene)结果是一致的，且节省算力。
	假设 autoscale 后的数据的 sd=1，可以忽略分母，我测试发现假设不成立，有少量sd小于1。忽略分母会造成显著的基因数减少。

> [11] Practical Multivariate Analysis, Afifi A, May S, Clark VA: 5th editionLondon: Chapman and Hall/CRC; 2011:364–366.

* 本文的R包没有做p值矫正，做了PC1和每个基因的相关显著性检验，不能不做p值矫正！
* 直接看3，给出使用 Seurat 数据的求和PC显著相关的基因的封装函数。





2. PBMC3k+R base PCA ----

## (0) Do PCA ====
scale.data=scObj@assays$RNA@scale.data[scObj@assays$RNA@var.features, ]
dim(scale.data) #2000 2700
scale.data[1:2, 1:4]
#       AAACATACAACCAC-1 AAACATTGAGCTAC-1 AAACATTGATCAGC-1 AAACCGTGCTTCCG-1
#PPBP         -0.1429670       -0.1429670       -0.1429670         2.914618
#S100A9       -0.6451891       -0.6451891       -0.6451891         1.465444

#pca.results = prcomp(t(scale.data), scale. = T, center = T)
pca.results = prcomp(t(scale.data), scale. = F, center = T)
str(pca.results)



## (1) rk=sqrt(lambda)*W / sd(Xk) ====
# rk 是PC1和第k个变量的r值
# lambda 是PC1对应的特征值
# W是PC1对应的每个变量的 loading
# Xk是第k个变量的autoscale后的值

# 1)提取 PC1 的因子加载
loadings <- pca.results$rotation[,1]  # PC1 的加载

# 计算相关系数 r 和样本大小 n
lambda = pca.results$sdev ** 2

correlations <- sqrt(lambda[1]) * as.numeric(loadings)

# 如果假设 sd(Xp)
sd.arr=apply(scale.data, 1, sd)
hist(sd.arr, n=100) #并不是都是1，why?

correlations2=correlations
for(i in 1:length(correlations)){
  correlations2[i] = correlations[i] / sd.arr[i]
}

n <- ncol(scale.data); n  # 样本大小

cal_t_from_corr=function(correlations, n){
  # 计算 t 统计量
  t_statistics <- (correlations * sqrt(n - 2)) / sqrt(1 - correlations^2)
  
  # 计算 p 值
  p_values <- 2 * pt(-abs(t_statistics), df = n - 2)
  
  hist(p_values, n=100)
  
  # 创建结果数据框
  results <- data.frame(Variable = names(loadings), 
                        Loading = loadings, 
                        t_statistic = t_statistics, 
                        p_value = p_values)
  
  # 筛选显著变量（例如 p < 0.05）
  results$sig="no"
  results[results$p_value < 0.05, ]$sig="yes"
  results
}

results=cal_t_from_corr(correlations2, n)
# 输出显著变量
table(results$sig)
#no  yes 
#961 1039 


# ignore sd
results_=cal_t_from_corr(correlations, n)
table(results_$sig)
#  no  yes 
#1024  976






## (2) ignore lambda ====
results2=cal_t_from_corr(loadings, n)
# 输出显著变量
table(results2$sig)
#  no  yes 
#1858  142





## (3) R base calc r/p ====
# 1)提取PC1的loading
loadings <- pca.results$rotation[, 1]  # PC1 的加载
length(loadings)#[1] 2000

# 2) 原始矩阵: scale 后的
dat=scale.data |> as.data.frame()
dim(dat) # 2000 2700
dat[1:4, 1:5]

# 3) 计算PC1的打分 in each cell
score=sapply(dat, function(x){
  sum( x * loadings )
})

length(score) #2700
score[1:4]

# 3) 计算PC1打分和第k列的相关系数
r_2=sapply( data.frame(t(dat)), function(x){
  cor(x, score )
})

length(r_2)
head(r_2)
hist(r_2, n=100)

results3=cal_t_from_corr(r_2, n)
# 输出显著变量
table(results3$sig)
# no  yes 
#961 1039 





## (4) use pkg mseapca ====
library(mseapca)
r_3 <- pca_loading(pca.results)
r_3$loading |> str()
table(r_3$loading$p.value[,1]<0.05)
#FALSE  TRUE 
#1024   976 

# same genes? yes
results[which(results$sig=="yes"),]$Variable |> length()
r_3$loading$p.value[ r_3$loading$p.value[,1]<0.05, ] |> rownames() |> length()

intersect(results[which(results$sig=="yes"),]$Variable, 
        r_3$loading$p.value[ r_3$loading$p.value[,1]<0.05, ] |> rownames()) |> length()
#













3. 从 Seurat 对象计算和某PC显著的基因的显著性

#' Calc p value of each gene with one given PC
#'
#' @param object Seurat object
#' @param pc_num like 1 for "PC_1"
#' @param p.adj.method default fdr
#' @param verbose default no output
#' @param p.val.threshold p value significant threshold
#'
#' @return
#' @export
#'
#' @examples
#' withPC1=SigGenesWithPC(scObj, pc_num = 2)
SigGenesWithPC=function(object, pc_num=1, p.adj.method="fdr", p.val.threshold=0.05, verbose=F){
  #rk=sqrt(lambda)*W / sd(Xk)
  # rk 是PC1和第k个变量的r值
  # lambda 是PC1对应的特征值
  # W是PC1对应的每个变量的 loading
  # Xk是第k个变量的autoscale后的值
  
  # 1)提取 PC1 的因子加载
  loadings <- object@reductions$pca@feature.loadings[,pc_num]  # PC1 的加载
  
  # 2) 样本大小 n
  n <- ncol(scale.data); n  # 样本大小
  
  # 3) autoscale 后的数据
  scale.data=object@assays$RNA@scale.data[object@assays$RNA@var.features,]
  
  # 4)计算相关系数 r 和
  lambda = object@reductions$pca@stdev[pc_num] ** 2
  correlations <- sqrt(lambda) * as.numeric(loadings)
  
  sd.arr=apply(scale.data, 1, sd)
  if(verbose) { hist(sd.arr, n=100) } #并不是都是1，why?
  
  for(i in 1:length(correlations)){
    correlations[i] = correlations[i] / sd.arr[i]
  }
  
  # 5) 计算 t 统计量
  t_statistics <- (correlations * sqrt(n - 2)) / sqrt(1 - correlations^2)
  
  # 6) 计算 p 值
  p_values <- 2 * pt(-abs(t_statistics), df = n - 2)
  if(verbose) { hist(p_values, n=100) }
  
  # 7)创建结果数据框
  dat.use <- data.frame(Variable = names(loadings), 
                        Loading = loadings, 
                        t_statistic = t_statistics, 
                        pc=pc_num,
                        p_val = p_values)
  # 8)p 值矫正
  dat.use$p_val_adj = p.adjust(dat.use$p_val, method = p.adj.method)
  
  # 筛选显著变量（例如 p < 0.05）
  dat.use$sig=ifelse(dat.use$p_val_adj < p.val.threshold, "yes", "no")
  
  # 9) order by Loading 
  dat.use=dat.use[order(-dat.use$Loading),]
  dat.use
}

withPC1=SigGenesWithPC(scObj, pc_num = 2, p.val.threshold = 0.01)
head(withPC1)
#         Variable   Loading t_statistic pc         p_val     p_val_adj sig
#CD79A       CD79A 0.1244749    34.49119  2 2.703414e-216 6.007586e-214 yes
#MS4A1       MS4A1 0.1130108    30.16768  2 1.561846e-172 2.402839e-170 yes
#TCL1A       TCL1A 0.1061570    27.79212  2 1.035920e-149 1.218729e-147 yes
#HLA-DQA1 HLA-DQA1 0.1031493    26.79132  2 2.104518e-140 2.338353e-138 yes
#HLA-DRA   HLA-DRA 0.1022300    26.49010  2 1.219714e-137 1.283910e-135 yes
#HLA-DQB1 HLA-DQB1 0.1007367    26.00524  2 3.128123e-133 2.979165e-131 yes

tail(withPC1)
table(withPC1$sig)
#  no  yes 
#1239  761
table(withPC1$sig, withPC1$Loading>0)
#    FALSE TRUE
#no    853  386
#yes   650  111

hist(withPC1$p_val_adj, n=100)

library(ggplot2)
ggplot(withPC1, aes(x=Variable, y=Loading, fill=sig))+
  geom_col()+
  scale_x_discrete(limits=withPC1$Variable, labels = NULL)+
  scale_fill_manual(values = c("red", "grey"), breaks = c("yes","no") )+
  ggtitle("p.adj<0.01")

# get sig genes: all
withPC1[which(withPC1$sig=="yes"), ]$Variable |> jsonlite::toJSON()
# get sig genes: pos sig
withPC1[which(withPC1$sig=="yes" & withPC1$Loading>0), ]$Variable |> jsonlite::toJSON()
# get sig genes: neg sig
withPC1[which(withPC1$sig=="yes" & withPC1$Loading<0), ]$Variable |> jsonlite::toJSON()






========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------




========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------


========================================
----------------------------------------

========================================
----------------------------------------

