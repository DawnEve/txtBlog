R与机器学习


========================================
常用R脚本 及 R ML优秀参考资料
----------------------------------------
GC含量统计：http://www.biotrainee.com/forum.php?mod=viewthread&tid=625&page=2#pid1904


1. pdf经典书籍



2. 网站资料和课程


R机器学习教程 https://bradleyboehmke.github.io/HOML/

关于统计和算法的网站: 
UCLA 统计学院: https://stats.idre.ucla.edu/r/dae/logit-regression/


sklearn: https://qinqianshan.com/machine_learning/sklearn/


Dr. Shirin Elsinghorst: Biologist turned Bioinformatician turned Data Scientist
https://shirinsplayground.netlify.app/categories/machine-learning/



机器学习算法的基本原理-附Python和R语言代码 https://juejin.cn/post/6844903688520073223





3. 视频资料








========================================
ROC曲线的意义
----------------------------------------
ROC曲线的意义
http://www.cnblogs.com/emanlee/archive/2011/05/29/2062280.html

　　ROC曲线指受试者工作特征曲线(receiver operating characteristic curve), 是反映敏感性和特异性连续变量的综合指标,是用构图法揭示敏感性和特异性的相互关系，它通过将连续变量设定出多个不同的临界值，从而计算出一系列敏感性和特异性，再以敏感性为纵坐标、（1-特异性）为横坐标绘制成曲线，曲线下面积越大，诊断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为敏感性和特异性均较高的临界值. 　　
SPSS统计软件包的10.0版本有ROC曲线的统计功能。ROC曲线真阳性率为纵坐标,假阳性率为横坐标,在座标上由无数个临界值求出的无数对真阳性率和假阳性率作图构成,计算ROC曲线下面积AUCROC来评价诊断效率。


========================================
|-- 画ROC曲线(ROCR包和pROC包)
----------------------------------------
AUC为ROC曲线下方的面积。一般AUC大于0.75就能够说明模型是比较合理的了。


1.
(1)library(ROCR)
data(ROCR.simple)
pred <- prediction(ROCR.simple$predictions, ROCR.simple$labels)
perf <- performance(pred,"tpr","fpr")
str(perf)

##AUC值,ROC曲线下面积为AUC，用来评价分类器的综合性能，该数值取0-1之间，越大越好。
#https://blog.csdn.net/Hellolijunshy/article/details/79991385
auc <- performance(pred,'auc');auc_value=auc@y.values[[1]]
auc_value=round(auc_value,2)
auc_value #0.84

plot(perf,colorize=TRUE,main=paste0('AUC=',auc_value))
abline(a=c(0,0),b=c(1,1),col="grey")


(2)#用ggplot2画图
plotdata=data.frame(
  x=unlist(perf@x.values),
  y=unlist(perf@y.values)
)
g=ggplot(plotdata, aes(x,y, color=x))+
  geom_path(size=1)+
  labs(x="False positive rate",y="True positive rate",title="ROC Curve")+
  scale_color_gradient(name="False positive rate", low="blue", high="red")+
  theme(plot.title=element_text(face="bold",size=10))
g
#



2.
library(pROC)
roc_curve=roc(pre_test$Y~probability)
str(roc_curve)
plot(1-roc_curve$specificities, roc_curve$sensitivities,type="o",
     xlab="1-specificities",ylab="sensitivities",main="ROC Curve")
abline(a=0,b=1,col="gray")
auc2=roc_curve$auc
text(0.5,0.4,paste("AUC: ", round(auc2, digits=2)), col="blue")
#



========================================
R语言画生存期km plot
----------------------------------------
1.生存率：
    RFS: Relapse Free Survival; 无复发生存期
    OS: Overall Survival; 总生存期
    DMFS: Distant Metastasis Free Survival; 无远处转移生存期
    PPS: Post Progression Survival 进展后生存期

2.芯片数据该怎么做标准化：系统原理：两次标准化
第一次使用affy包MAS5算法标准化；第二次[文献14]把平均值调整为1000，消除批次效应。

1).After an initial quality control, redundant samples (n = 384) were excluded [12]. The raw CEL files were MAS5 normalized in the R statistical environment (www.r-project.org) using the affy Bioconductor library [13]. MAS5 can be applied to individual chips, making future extensions of the database uncomplicated.
2).At this stage, we performed a second scaling normalization to set the average expression on each chip to 1,000 to avoid batch effects [14].

13). Gautier L, Cope L, Bolstad BM et al (2004) affy—analysis of Affymetrix GeneChip data at the probe level. Bioinformatics 20:307–315
14). Sims AH, Smethurst GJ, Hey Y et al (2008) The removal of multiplicative, systematic bias allows integration of breast cancer gene expression datasets—improving meta-analysis and prediction of prognosis. BMC Med Genomics 1:42

探针序列：http://kmplot.com/analysis/index.php?p=download


3.使用网站画mRNA、miRNA画十年生存期的KM plot
http://kmplot.com/analysis/





4.R语言画km plot:
共三列数据有效：
# tx是分组
# status是是否删失（0：右删失，1：死亡）
# time表示存活时间

## 生存期曲线的画法
#原始数据 
# case	inst	tx	grade	cond	site	t.stage	n.stage	entry.dt	status	time
# 1	a1	1	1	1	1	1	1	1	1	1
# 2	a1	1	1	1	1	1	1	1	1	100
# 3	a1	1	1	1	1	1	1	1	1	12
# 4	a1	1	1	1	1	1	1	1	1	12
# 5	a1	1	1	1	1	1	1	1	1	13
# 6	a1	1	1	1	1	1	1	1	1	13
# 7	a1	1	1	1	1	1	1	1	1	13
# 8	a1	1	1	1	1	1	1	1	1	20
# 9	a1	1	1	1	1	1	1	1	1	30
# 10	a1	2	1	1	1	1	1	1	1	31
# 11	a1	2	1	1	1	1	1	1	1	32
# 12	a1	2	1	1	1	1	1	1	1	33
# 13	a1	2	1	1	1	1	1	1	1	34
# 14	a1	2	1	1	1	1	1	1	1	35
# 15	a1	2	1	1	1	1	1	1	1	35
# 16	a1	2	1	1	1	1	1	1	1	35
# 17	a1	2	1	1	1	1	1	1	1	40
# 18	a1	2	1	1	1	1	1	1	1	40
# 19	a1	2	1	1	1	1	1	1	1	60
# 20	a1	2	1	1	1	1	1	1	1	80
# 其中
# caese表示编号
# tx是分组
# status是是否删失（0：右删失，1：死亡）
# time表示存活时间

# 载入包
library('survival')

# 读取数据
setwd('D:/R_code')
my=read.csv('death.csv',header=T);my

# 按照tx分组对time和statuss拟合生存曲线，
sd=survfit(Surv(time,as.numeric(status))~tx,data=my,se.fit=FALSE, conf.int=.95)

# 画出生存曲线
plot(sd,lty=1,col=c("red","purple"),
      xlab="time(year)",ylab="Overall Survival",main="survival", #生存曲线
    ) 
#加上图例
legend(60, 1, c("Group1", "Group2"), col = c("red","purple"),
       text.col = c("red","purple"), lty = c(1, 1), pch = c(1, 2),
       merge = TRUE, bg ="#efeeef" );
#rgb(0.99,0.99,0.99) grey


# 这个没懂
with(lung, Surv(time, status))
Surv(heart$start, heart$stop, heart$event)

作图结果如下：





4.2.怎么计算P值（log rank p-value）呢？[解决]
[1]http://bcb.dfci.harvard.edu/~aedin/courses/Bioconductor/survival.pdf
[2]https://cran.r-project.org/web/packages/HSAUR/vignettes/Ch_survival_analysis.pdf   Survival Analysis:
在pdf中搜索log-rank 等关键词。
[3]http://stats.stackexchange.com/questions/114304/log-rank-test-in-r


#p值怎么算？

# http://bbs.pinggu.org/thread-2178930-1-1.html


#请使用survdiff函数做log rank检验或建立Cox模型(coxph)来比较两条生存曲线,

#KM曲线只是一种可视化手段，不是正经的统计推断分析工具。

# 应该用 suvdiff 函数做 log-rank test

#
# 读取数据
setwd('D:/R_code/')
my=read.csv('suvive.csv',header=T);my

#############################
#计算p值：
#http://stats.stackexchange.com/questions/114304/log-rank-test-in-r
# install.packages("survival")
library("survival")
sdf=survdiff(Surv(time, as.numeric(status))~tx, data=my)
sdf
pvalue=1-pchisq(sdf$chisq, df=1)
#pvalue=round(pvalue,2)
# There is also an option for ‘rho’. Rho = 0 (default)
# gives the log-rank test, rho=1 gives the Wilcoxon test.

#install.packages("coin")
#library("coin")
#st=logrank_test(Surv(time, as.numeric(status)) ~ tx, data=my, distribution = "exact")
#st
#########################


# 按照tx分组对time和statuss拟合生存曲线，
km=survfit(Surv(time,as.numeric(status))~tx,data=my,se.fit=FALSE, conf.int=.95)
km

# 画出生存曲线
plot(km,lty=1,col=c("red","purple"), 
     xlab="OS MONTHS",ylab="Percent Survival",main="survival", #生存曲线
) 
#加上图例
legend(130, 1,  legend=c(paste("WT(n=",km$n[1],")"), paste("R132H(n=",km$n[2],")")), col = c("red","purple"),
       text.col = c("red","purple"), lty = c(1, 1), title = paste("logrank p = ",pvalue) )
#       merge = FALSE, bg ="#ffffff"





4.3.怎么标出来截尾数据？

http://www.biostatistic.net/thread-87214-1-1.html
mark.time=T  生存分析显示截尾数据点.

## S3 method for class 'survfit'
plot(x, conf.int=, mark.time=TRUE, 
mark=3, col=1, lty=1, lwd=1, cex=1, log=FALSE, xscale=1, yscale=1,  
firstx=0, firsty=1, xmax, ymin=0, fun, 
xlab="", ylab="", xaxs="S", ...)






4.4.从TCGA下载数据，怎么匹配样品名和生存时间？[半解决]
http://www.cbioportal.org/
（1）用excel的vookup函数[勉强可用，但是还是不自动化]：
VLOOKUP函数是Excel中的一个纵向查找函数，它与LOOKUP函数和HLOOKUP函数属于一类函数，在工作中都有广泛应用。VLOOKUP是按列查找，最终返回该列所需查询列序所对应的值；与之对应的HLOOKUP是按行查找的。
VLOOKUP(lookup_value,table_array,col_index_num,range_lookup)
当前页面样品name，寻找的表格范围，返回的列编号，不精确查找吗？
大概是这样： =VLOOKUP(A2,Sheet2!A:F,5,FALSE)

（2）使用R语言的sql模块【推荐】
...




4.5.怎么从marker选择最合适的cutoff值？
看文献是使用x-tile软件选择最优的cutoff值。
软件：http://medicine.yale.edu/lab/rimm/research/software.aspx
教程：http://wenku.baidu.com/link?url=GmVqIKZ41r1av7wzYDxZYquKk8Am6MuLyq5AD0NGicIa9t5bUvgokQqpgTGCHpIS-Oh-Vd24j1-wJpjY1b25UKXrpPLmZ2W9tlzUrJPqqV7

也有人说用ROC曲线的。





4.6 问题

How to calculate the HR and 95%CI using the log-rank test in R?
怎么用R的log-rank检验分析HR和95%置信区间

The R survival package is very useful to do survival analysis. And I know the survdiff function can be used to compare the difference of survival time in two or more groups. And the p-value number can also be calculated as below. However, how can I calculate the HR and 95% CI using the log-rank test.
R的生存期包对生存分析很有用。我知道survdiff函数可以被用于比较2组或多组生存时间差异。p值可以如下方法计算。然而，我不知道怎么使用log-rank检验计算HR和95%置信区间(CI)。

And I also know I can use the coxph() function to calculate the HR and 95% CI using the Cox regression. However, as the assumption of both the Cox model and log-rank test are that the hazard ratio stay constant over time, so I think I can also calculate the HR and 95% CI using the log-rank test.
我知道怎么使用Cox回归的coxph()函数计算HR和95%CI。然而，Cox模型和log-rank检验的假设都是风险比随着时间稳定，所以我认为我可以使用log-rank检验计算 HR and 95% CI。

According to the book Survival Analysis: A Practical Approach, I got two formulas on Page 62 and 66 to do this (as shown below). So I wrote the R code as below, is there anybody know whether I'm right?
按照《生存分析：实用方法》一书，我在P62和P66看到2个做这个的函数（如下）。所以，我写了如下R代码，有人知道我这么写对吗？

library(survival)
data.survdiff <- survdiff(Surv(time, status) ~ group)
p.val = 1 - pchisq(data.survdiff$chisq, length(data.survdiff$n) - 1)
HR = (data.survdiff$obs[2]/data.survdiff$exp[2])/(data.survdiff$obs[1]/data.survdiff$exp[1])
up95 = exp(log(HR) + qnorm(0.975)*sqrt(1/data.survdiff$exp[2]+1/data.survdiff$exp[1]))
low95 = exp(log(HR) - qnorm(0.975)*sqrt(1/data.survdiff$exp[2]+1/data.survdiff$exp[1]))


输出：
> data.survdiff
Call:
survdiff(formula = Surv(data[, "os_whw"], data[, "status_whw"] == 
    1) ~ data[, "pcascore"] >= median(data[, "pcascore"]))

                                                       N Observed Expected (O-E)^2/E (O-E)^2/V
data[, "pcascore"] >= median(data[, "pcascore"])=FALSE 4        3     4.33     0.411     0.974
data[, "pcascore"] >= median(data[, "pcascore"])=TRUE  5        5     3.67     0.486     0.974

 Chisq= 1  on 1 degrees of freedom, p= 0.324 
> p.val
[1] 0.3235935
> HR
[1] 1.970484
> up95
[1] 7.917248
> low95
[1] 0.4904239





4.6.1 回答1
#############
如果你的问题是两组生存是否有差异，不管其他特性，你应该使用KM估计并获得P值。这是用log-rank检验获得的。仅仅考虑组间生存期差异。如果你（1）分组时随机的（2）仅仅想知道组间生存时间差异（3）想看生存曲线，则这个方法很合适。

然而，如果分组不是随机的而是基本特性差异很大（比如年龄、性别差异），简单的生存分析用处就不大了（比如可能生存差异是年龄因素造成的）；这就是需要引入Cox回归。你使用Cox回归比较生存同时调整混杂因素。如果Cox回归没有包含“分组”之外的预测因素，得到的p值和KM 估计中的log rank检验结果是一样的。但是Cox回归能让你获得每个预测因素的HR和CI，以及p值。

使用Sruvival包中的coxph函数，或者rms包中的cph函数。

通常不用为此纠结。使用Rms包中的函数就很好，还有一本很棒的与包同名的书(Springer; FE Harrell)。包中有可视化和表格化的函数。

可能你有些理论我不理解，但是这时有一个专家会指导我们。其他场景中，你会感觉这个途径很好。

The package: http://cran.r-project.org/web/packages/rms/rms.pdf

The book: http://www.springer.com/mathematics/probability/book/978-0-387-95232-1

(Btw: you can obtain Cox adjusted survival curves; 
google it or check out RMS package documentation).





4.6.2 回答2

你的实现看样子是对的，你认为log-rank可以用于计算HR和95%CI，但我有点问题：
1.你重新计算p值有特殊的理由吗？（因为survdiff()函数已经提供了，重算就像是额外工作）
2.你函数中的HR和95%，和使用coxph()函数计算出来的一致吗？使用coxph()函数验证你的实现。

注意：coxph()函数已经计算过，你为什么用log-rank检验又算了一次HR和95%CI？

I agree with your idea that log-rank test is used to compare the survival of two groups.









5. R语言超详细绘制生存曲线并说明各参数选择 //需要新版本的R，还没有测试。//todo
https://www.jianshu.com/p/34cc76221883

#生存曲线绘制

library(survminer) # 加载包
library(survival) # 加载包

data(lung) # 加载lung(lung) # 查看数据集
str(lung)

#'data.frame':  228 obs. of  10 variables:
#$ inst     : num  3 3 3 5 1 12 7 11 1 7 ...
#$ time     : num  306 455 1010 210 883 ...生存时间
#$ status   : num  2 2 1 2 2 1 2 2 2 2 ...生存状态，2为死亡，1位生存
#$ age      : num  74 68 56 57 60 74 68 71 53 61 ...
#$ sex      : num  1 1 1 1 1 1 2 2 1 1 ...
#$ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...
#$ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...
#$ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...
#$ meal.cal : num  1175 1225 NA 1150 NA ...进食时消耗的卡路里
#$ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...最近6个月内的体重下降

attach(lung)
Surv(time,status) # 创建生存对象

fit <- survfit(Surv(time,status) ~ sex,  # 创建生存对象 
               data = lung) # 数据集来源
fit # 查看拟合曲线信息

summary(fit)

ggsurvplot(fit, data = lung)

ggsurvplot(fit, # 创建的拟合对象
           data = lung,  # 指定变量数据来源
           conf.int = TRUE, # 显示置信区间
           pval = TRUE, # 添加P值
           surv.median.line = "hv",  # 添加中位生存时间线
           risk.table = TRUE, # 添加风险表
           xlab = "Follow up time(d)", # 指定x轴标签
           legend = c(0.8,0.75), # 指定图例位置
           legend.title = "", # 设置图例标题
           legend.labs = c("Male", "Female"), # 指定图例分组标签
           break.x.by = 100)  # 设置x轴刻度间距
## end








refer
1.[推荐]R的生存期包
https://cran.r-project.org/web/packages/survival/survival.pdf
2. http://stats.stackexchange.com/questions/124489/how-to-calculate-the-hr-and-95ci-using-the-log-rank-test-in-r

3.Use Software R to do Survival Analysis and Simulation. A tutorial
http://www.ms.uky.edu/~mai/Rsurv.pdf

10.英文图书 https://www.amazon.com/dp/0470870400/?tag=stackoverfl08-20

R语言的with和by http://statmethods.net/stats/withby.html

生存分析完整论述：https://cran.r-project.org/web/views/Survival.html  太复杂了



http://wenku.baidu.com/link?url=Ld9FAX6pFUfLet5YtTToTtyzhQ2GA2mD9IkmhBqHjjDEb3e3S2BAI8C3hxZ3UxhGKUhWD4aT_uh8rWIJmEvY0fFJtdmU1VWwQzlF9LolNU7
如何用graphpad制作生存曲线？ http://www.dxy.cn/bbs/topic/25619320
http://blog.csdn.net/shmilyringpull/article/details/17529637










========================================
R语言k折交叉验证 k fold C.V.
----------------------------------------
# 原理分析 https://blog.csdn.net/Tanya_girl/article/details/50221797

library(caret)
#caret包中有createFolds函数，如
training=iris
set.seed(7)
folds<-createFolds(y=training$Species,k=10)
#这里参数y是训练数据集label,k是几折交叉验证
#我们可以看看这里输出是什么
folds
#可以看出来folds有10个元素

#可见folds里每个元素是原来label长度的十分之一，存的是label的id随机的十分之一
#可以写个for循环进行交叉验证
for(i in 1:10){
  train_cv<-training[-folds[[i]],]
  test_cv<-training[folds[[i]],]
  
  print(i)
  print(train_cv)
  print(test_cv)
}


# 完整代码这里：https://blog.csdn.net/tiaaaaa/article/details/58116346




========================================
|-- 用R对数据进行随机抽样
----------------------------------------
#去掉一类，变成两类
x=iris[which(iris$Species!="virginica"),];
x$Species=factor(x$Species)
str(x);dim(x)

#生成抽样序列
set.seed(100)
ind=sample(2,nrow(x),replace=TRUE,prob=c(0.8,0.2));ind

#取80%作为训练集，20为测试集
x_train=x[ind==1,];dim(x_train)
x_test=x[ind==2,];dim(x_test) #去掉标签





========================================
SVM 分类
----------------------------------------







========================================
SVR: 支持向量回归
----------------------------------------






========================================
主成分分析 (PCA, principal component analysis)
----------------------------------------
主成分分析 (PCA, principal component analysis)是一种数学降维方法, 利用正交变换 (orthogonal transformation)把一系列可能线性相关的变量转换为一组线性不相关的新变量，也称为主成分，从而利用新变量在更小的维度下展示数据的特征。


主成分是原有变量的线性组合，其数目不多于原始变量。组合之后，相当于我们获得了一批新的观测数据，这些数据的含义不同于原有数据，但包含了之前数据的大部分特征，并且有着较低的维度，便于进一步的分析。

在空间上，PCA可以理解为把原始数据投射到一个新的坐标系统，第一主成分为第一坐标轴，它的含义代表了原始数据中多个变量经过某种变换得到的新变量的变化区间；第二成分为第二坐标轴，代表了原始数据中多个变量经过某种变换得到的第二个新变量的变化区间。这样我们把利用原始数据解释样品的差异转变为利用新变量解释样品的差异。

这种投射方式会有很多，为了最大限度保留对原始数据的解释，一般会用最大方差理论或最小损失理论，使得第一主成分有着最大的方差或变异数 (就是说其能尽量多的解释原始数据的差异)；随后的每一个主成分都与前面的主成分正交，且有着仅次于前一主成分的最大方差 (正交简单的理解就是两个主成分空间夹角为90°，两者之间无线性关联，从而完成去冗余操作)。


1. 主成分分析的意义
(1) 简化运算。
假如某些基因如持家基因在所有样本中表达都一样，它们对于解释样本的差异也没有意义。

应用PCA就可以在尽量多的保持变量所包含的信息又能维持尽量少的变量数目，帮助简化运算和结果解释。


(2)去除数据噪音。

比如说我们在样品的制备过程中，由于不完全一致的操作，导致样品的状态有细微的改变，从而造成一些持家基因也发生了相应的变化，但变化幅度远小于核心基因 (一般认为噪音的方差小于信息的方差）。而PCA在降维的过程中滤去了这些变化幅度较小的噪音变化，增大了数据的信噪比。

(3)利用散点图实现多维数据可视化。

在上面的表达谱分析中，假如我们有1个基因，可以在线性层面对样本进行分类；如果我们有2个基因，可以在一个平面对样本进行分类；如果我们有3个基因，可以在一个立体空间对样本进行分类；如果有更多的基因，比如说n个，那么每个样品就是n维空间的一个点，则很难在图形上展示样品的分类关系。利用PCA分析，我们可以选取贡献最大的2个或3个主成分作为数据代表用以可视化。这比直接选取三个表达变化最大的基因更能反映样品之间的差异。（利用Pearson相关系数对样品进行聚类在样品数目比较少时是一个解决办法）


(4) 发现隐性相关变量。

我们在合并冗余原始变量得到主成分过程中，会发现某些原始变量对同一主成分有着相似的贡献，也就是说这些变量之间存在着某种相关性，为相关变量。同时也可以获得这些变量对主成分的贡献程度。对基因表达数据可以理解为发现了存在协同或拮抗关系的基因。






2. PCA 实现原理 及注意事项
(1)
PCA分析不是简单的选取2-3个变化最大的基因，而是先把原始的变量做一个评估，计算各个变量各自的变异度（方差）和两两变量的相关度（协方差），得到一个协方差矩阵。

在这个协方差矩阵中，对角线的值为每个变量的方差，其他值为每两个变量的协方差。随后对原变量的协方差矩阵对角化处理，即求解其特征值和特征向量。原变量与特征向量的乘积（对原始变量的线性组合）即为新变量（回顾下线性代数中的矩阵乘法）；新变量的协方差矩阵为对角协方差矩阵且对角线上的方差由大到小排列；然后从新变量中选择信息量最丰富的也就是方差最大的前2个或3个新变量，也即是主成分用于可视化。


(2) 不同预处理对PCA结果的影响
1) 不同标准化的本质在于研究者认为，是数值的量度本身重要，还是数值的变化重要。
2) 数值的量度重要，则选择原始数据或log转换；
3) 数值的变化重要，则选择scale。




3. PCA注意事项

(1) 一般说来，在PCA之前原始数据需要中心化（centering，数值减去平均值）。中心化的方法很多，除了平均值中心化（mean-centering）外，还包括其它更稳健的方法，比如中位数中心化等。

(2)除了中心化以外，定标 (Scale, 数值除以标准差) 也是数据前处理中需要考虑的一点。如果数据没有定标，则原始数据中方差大的变量对主成分的贡献会很大。数据的方差与其量级成指数关系，比如一组数据(1,2,3,4)的方差是1.67，而(10,20,30,40)的方差就是167,数据变大10倍，方差放大了100倍。

(3)但是定标(scale)可能会有一些负面效果，因为定标后变量之间的权重就是变得相同。如果我们的变量中有噪音的话，我们就在无形中把噪音和信息的权重变得相同，但PCA本身无法区分信号和噪音。在这样的情形下，我们就不必做定标。

(4)一般而言，对于度量单位不同的指标或是取值范围彼此差异非常大的指标不直接由其协方差矩阵出发进行主成分分析，而应该考虑对数据的标准化。比如度量单位不同，有万人、万吨、万元、亿元，而数据间的差异性也非常大，小则几十大则几万，因此在用协方差矩阵求解主成分时存在协方差矩阵中数据的差异性很大。

在后面提取主成分时发现，只提取了一个主成分，而此时并不能将所有的变量都解释到，这就没有真正起到降维的作用。此时就需要对数据进行定标(scale)，这样提取的主成分可以覆盖更多的变量，这就实现主成分分析的最终目的。

但是对原始数据进行标准化后更倾向于使得各个指标的作用在主成分分析构成中相等。对于数据取值范围不大或是度量单位相同的指标进行标准化处理后，其主成分分析的结果与仍由协方差矩阵出发求得的结果有较大区别。这是因为对数据标准化的过程实际上就是抹杀原有变量离散程度差异的过程，标准化后方差均为1，而实际上方差是对数据信息的重要概括形式，也就是说，对原始数据进行标准化后抹杀了一部分重要信息，因此才使得标准化后各变量在主成分构成中的作用趋于相等。因此，对同度量或是取值范围在同量级的数据还是直接使用非定标数据求解主成分为宜。


(5)中心化和定标都会受数据中离群值（outliers）或者数据不均匀（比如数据被分为若干个小组）的影响，应该用更稳健的中心化和定标方法。


(6)PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。








3. PCA 分析和可视化 示例

########### step1 PCA
library(caret)

# By default, prcomp will centralized the data using mean.
# Normalize data for PCA by dividing each data by column standard deviation.
# Often, we would normalize data.
# Only when we care about the real number changes other than the trends,
# `scale` can be set to TRUE. 
# We will show the differences of scaling and un-scaling effects.

iris.pca=prcomp(iris[,1:4])
iris.pca2=prcomp(iris[,1:4], scale=T)

iris.pca
#Standard deviations (1, .., p=4):
#[1] 2.0562689 0.4926162 0.2796596 0.1543862
#Rotation (n x k) = (4 x 4):
#                 PC1         PC2         PC3        PC4
#Sepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872
#Sepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231
#Petal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390
#Petal.Width   0.35828920  0.07548102 -0.54583143  0.7536574

print(str(iris.pca))





########### step2 PCA结果提取和可视化神器

library(factoextra)

#1. 碎石图展示每个主成分的贡献
## 如果需要保持，去掉下面第1,3行的注释
#pdf("1.pdf")
fviz_eig(iris.pca, addlabels = TRUE)
#dev.off()

# 2. PCA样品聚类信息展示
# repel=T，自动调整文本位置
fviz_pca_ind(iris.pca, repel=T) +coord_fixed(1) # 关键的增加


# 3. 根据样品分组上色
fviz_pca_ind( iris.pca, col.ind=iris[,5], 
             mean.point=F, addEllipses = T, legend.title="Groups")

# 4. 增加不同属性的椭圆
# “convex”: plot convex hull of a set o points.
# “confidence”: plot confidence ellipses around group mean points as 
#     the function coord.ellipse() [in FactoMineR].
# “t”: assumes a multivariate t-distribution.
# “norm”: assumes a multivariate normal distribution.
# “euclid”: draws a circle with the radius equal to level, 
#    representing the euclidean distance from the center. 
#    This ellipse probably won’t appear circular unless coord_fixed() is applied.

# 根据分组上色并绘制95%置信区间
fviz_pca_ind(iris.pca, col.ind=iris[,5], 
             mean.point=F, addEllipses = T, legend.title="Groups", 
             ellipse.type="confidence", ellipse.level=0.95)
#

#5. 展示贡献最大的变量 (基因)
# 1） 展示与主坐标轴的相关性大于0.99的变量 (具体数字自己调整)
# Visualize variable with cos2 >= 0.99
fviz_pca_var(iris.pca, select.var = list(cos2 = 0.60), 
             repel=T, 
             col.var = "cos2", 
             geom.var = c("arrow", "text") )

#2)展示与主坐标轴最相关的10个变量
# Top 10 active variables with the highest cos2
fviz_pca_var(iris.pca, select.var= list(cos2 = 10), repel=T, col.var = "contrib")

# 3）展示自己关心的变量（基因）与主坐标轴的相关性分布
# Select by names
# 这里选择的是MAD值最大的几个基因
#name <- list(name = c("FN1", "DCN", "CEMIP","CCDC80","IGFBP5","COL1A1","GREM1"))
#fviz_pca_var(pca, select.var = name)


#6. biPLot同时展示样本分组和关键基因
# top 5 contributing individuals and variable
fviz_pca_biplot(iris.pca,
                fill.ind=iris[,5],
                palette="joo",
                addEllipses = T, 
                ellipse.type="confidence",
                ellipse.level=0.95,
                mean.point=F,
                col.var="contrib",
                gradient.cols = "RdYlBu",
                select.var = list(contrib = 10),
                ggtheme = theme_minimal())
#




========================================
|-- PCA 分析与可视化 示例
----------------------------------------
自己封装的函数

my.doCPA=function(rna_logcpm, celltype, center = F){
    test.pr<-prcomp( t(rna_logcpm), center = center) 

    library(factoextra)
    print(fviz_eig(test.pr, addlabels = TRUE))

    ind <- get_pca_ind(test.pr) #这是看观察值，也就是行。
    
    df=as.data.frame(ind$coord)
    df$celltype=celltype
    return(df)
}

df=my.doCPA(rnaM.logcpm, cellInfo[colnames(rnaM.logcpm), ]$cellType)

# 画出每个点的位置，使用前几个PC
library(ggplot2)
ggplot(df, aes(Dim.1, Dim.2, color=celltype) )+geom_point()+theme_bw()
ggplot(df, aes(Dim.1, Dim.3, color=celltype) )+geom_point()+theme_bw()
ggplot(df, aes(Dim.2, Dim.3, color=celltype) )+geom_point()+theme_bw()






1. 精简版
################
#PCA对列进行重组，重组成最多和原来一样多个PC成分。所以输入df竖着是基因，一行一个细胞。
test2=iris[,1:4] 
test=t(apply(test2, 1, function(x){x/sum(x)})) #按行标准化
test=as.data.frame(test)
head(test)

#做主成分分析
test.pr<-prcomp(test, center = F) 
test.pr
# 查看荷载
# 如果变量列数多于观察行数，则需要使用Q-PCA类型，函数时prcomp(), 载荷 test.pr$rotation
##  'princomp' can only be used with more units than variables

test.pr$rotation #PC1=-0.77sL-0.42sW-0.44pL-0.13pW, 后面以此类推
#princomp(test,cor=TRUE)时
#summary(test.pr,loadings=TRUE) 
#loading是逻辑变量，当 loading=TRUE 时表示显示 loading 的内容

biplot(test.pr)  #画出数据关于主成分的散点图和原坐标在主成分下的方向

#碎石图，显示每个PC占了多少百分比
screeplot(test.pr,type="lines")
#每个PC占了多少比例，另一种展示
library(factoextra)
fviz_eig(test.pr, addlabels = TRUE)

# 预测
p <- predict(test.pr) # 就是每行的PC维度上的值，可用于可视化观测值的位置
head(p)

# 查看每个PC的坐标
##var <- get_pca_var(test.pr)
ind <- get_pca_ind(test.pr) #这是看观察值，也就是行。
head(ind$coord) #和上文的head(p)结果一样

# 画出每个点的位置，使用前几个PC
library(ggplot2)

df=as.data.frame(ind$coord)
df$type=iris[,5]
head(df)

ggplot(df, aes(Dim.1, Dim.2, color=type) )+geom_point()+theme_bw()
ggplot(df, aes(Dim.1, Dim.3, color=type) )+geom_point()+theme_bw()
ggplot(df, aes(Dim.2, Dim.3, color=type) )+geom_point()+theme_bw()

# 三维可视化 https://www.jianshu.com/p/d248b7b54a4b


ref:
关于 R 中 princomp 和 prcomp 的 区别 https://blog.csdn.net/lfz_carlos/article/details/48442091






2.
http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/


library("FactoMineR")
res.pca <- PCA(iris[, 1:4], graph = FALSE)
res.pca

## **Results for the Principal Component Analysis (PCA)**
## The analysis was performed on 23 individuals, described by 10 variables
## *The results are available in the following objects:
## 
##    name               description                          
## 1  "$eig"             "eigenvalues"                        
## 2  "$var"             "results for the variables"          
## 3  "$var$coord"       "coord. for the variables"           
## 4  "$var$cor"         "correlations variables - dimensions"
## 5  "$var$cos2"        "cos2 for the variables"             
## 6  "$var$contrib"     "contributions of the variables"     
## 7  "$ind"             "results for the individuals"        
## 8  "$ind$coord"       "coord. for the individuals"         
## 9  "$ind$cos2"        "cos2 for the individuals"           
## 10 "$ind$contrib"     "contributions of the individuals"   
## 11 "$call"            "summary statistics"                 
## 12 "$call$centre"     "mean of the variables"              
## 13 "$call$ecart.type" "standard error of the variables"    
## 14 "$call$row.w"      "weights for the individuals"        
## 15 "$call$col.w"      "weights for the variables"


plot(res.pca)
summary(res.pca)



(2) PCA的可视化
No matter what function you decide to use [stats::prcomp(), FactoMiner::PCA(), ade4::dudi.pca(), ExPosition::epPCA()], you can easily extract and visualize the results of PCA using R functions provided in the factoextra R package.



These functions include:

- get_eigenvalue(res.pca): Extract the eigenvalues/variances of principal components
- fviz_eig(res.pca): Visualize the eigenvalues
- get_pca_ind(res.pca), get_pca_var(res.pca): Extract the results for individuals and variables, respectively.
- fviz_pca_ind(res.pca), fviz_pca_var(res.pca): Visualize the results individuals and variables, respectively.
- fviz_pca_biplot(res.pca): Make a biplot of individuals and variables.

In the next sections, we’ll illustrate each of these functions.


##1. Eigenvalues / Variances
library("factoextra")
eig.val <- get_eigenvalue(res.pca)
eig.val

#       eigenvalue variance.percent cumulative.variance.percent
# Dim.1 2.91849782       72.9624454                    72.96245
# Dim.2 0.91403047       22.8507618                    95.81321
# Dim.3 0.14675688        3.6689219                    99.48213
# Dim.4 0.02071484        0.5178709                   100.00000


## 2. 每个PC解释多少百分比，柱状图
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))



var <- get_pca_var(res.pca)
var

## Principal Component Analysis Results for variables
##  ======== ========== ========= ========= ======== =======
##   Name       Description                                    
## 1 "$coord"   "Coordinates for the variables"                
## 2 "$cor"     "Correlations between variables and dimensions"
## 3 "$cos2"    "Cos2 for the variables"                       
## 4 "$contrib" "contributions of the variables"



# Coordinates
head(var$coord)
# Cos2: quality on the factore map
head(var$cos2)
# Contributions to the principal components
head(var$contrib)


## Quality of representation
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)


# Total cos2 of variables on Dim.1 and Dim.2
fviz_cos2(res.pca, choice = "var", axes = 1:2)



## 
# Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )
#

# Change the transparency by cos2 values
fviz_pca_var(res.pca, alpha.var = "cos2")


##################
# PC 的贡献程度
library("corrplot")
corrplot(var$contrib, is.corr=FALSE)    



# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)


#The total contribution to PC1 and PC2 is obtained with the following R code:
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)



fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)
#

# Change the transparency by contrib values
fviz_pca_var(res.pca, alpha.var = "contrib")



############
# 可视化每个元素
fviz_pca_ind(res.pca)

#Like variables, it’s also possible to color individuals by their cos2 values:
fviz_pca_ind(res.pca, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping (slow if many points)
)



#You can also change the point size according the cos2 of the corresponding individuals:
fviz_pca_ind(res.pca, pointsize = "cos2", 
             pointshape = 21, fill = "#E7B800",
             repel = TRUE # Avoid text overlapping (slow if many points)
             )

#

#ind.p <- fviz_pca_ind(iris.pca, geom = "point", col.ind = iris$Species)
ggpubr::ggpar(ind.p,
              title = "Principal Component Analysis",
              subtitle = "Iris data set",
              caption = "Source: factoextra",
              xlab = "PC1", ylab = "PC2",
              legend.title = "Species", legend.position = "top",
              ggtheme = theme_gray(), palette = "jco"
)


##
ref:
https://www.jianshu.com/p/6e413420407a





========================================
|-- PCA 的原理和手工实现 //todo
----------------------------------------
1. 输入数据描述
行为gene，列为sample。
   s1 s2 s3 s4 s5 s6
g1  
g2
g3

一个基因时，一维图。
2个基因时，2-D图。
3个基因时，3-D图。
再多，就超出人类直觉了。
可以使用降维方法，把高维数据降低到2维，方便可视化查看。


(1) 考虑2个基因的情况，
先中心化: 按行计算每个基因的mean，每行减去该行的mean。

然后有一条过原点的直线，角度是变量，0-360。每个点到投射到该直线上。然后计算投射点到原点的距离和sum of squared distances (SSD).
旋转直线，直到SSD最大的方向，这个折线就叫做第一个主成分(简称PC 1)。

对于数据中的某一个点A(x,y)，在直线y=tan(Theta)*x上的投影为B(a,b).
B在直线上:  b=k*x
OB垂直于AB: (OB, AB)=0, (a,b)*(x-a, y-b)=0
解方程得
a=(x+k*y)/(1+k^2)
b=a*k


使用R实现这一过程: 找到2维中使SSD最大的直线夹角，并可视化PC1及各点在PC1上的的投影。
dt=t(iris[, c(1,3)])
dim(dt) #行为2个基因，列为样品名
# 中心化
dt=as.data.frame(t(apply(dt, 1, function(x){
  x-mean(x)
})))
head(dt[,1]) #-0.7433333 -2.3580000
result=list(angle=0,ssd=0)
ssds=c()
for(angle in seq(0,180, 0.1) ){
  #线的角度
  st=tan(angle/180*3.14159265358979)
  ssd=0
  for(i in 1:ncol(dt)){
    x=dt[1,i];  y=dt[2,i]; #数据点
    a=(x+st*y)/(1+st^2); b=a*st; #映射到直线上
    ssd=ssd+(a^2+b^2)
  }
  ssds=c(ssds, ssd)
  if(result[['ssd']]<ssd){
    result[['ssd']]=ssd
    result[['angle']]=angle
  }
}
plot(seq(0,180, 0.1), ssds)
result
# $angle
#[1] 66.8
#
#$ssd
#[1] 545.6228

# 可视化第PC 1的方向
st=tan(result[['angle']]/180*3.14159265358979) #斜率
plot(t(dt), col=iris[,5], pch=20, xlim=c(-3,3.5), ylim=c(-3,3.5))
x1=seq(-4,4,0.5); lines(x1, x1*st,type='l') #add line
#
ssd=0
outer=c() #残差
for(i in 1:ncol(dt)){
  x=dt[1,i];  y=dt[2,i]; #数据点
  a=(x+st*y)/(1+st^2); b=a*st; #映射到直线上
  # 画映射的点和垂线
  points(a,b,type='p',col='#F600FF55', pch=20)
  lines(c(x,a), c(y,b), type='l',col='grey',lty=2)
  outer=c(outer, sum( c(x-a,y-b)*c(1,st) ) )
  ssd=ssd+(a^2+b^2)
}
hist(outer, n=50) # 确实都垂直
ssd #545.6228


> st
[1] 2.333175
也就意味着在PC1上，gene1每增加1，gene2就增加2.33，也就是说基因2贡献的比较多。
也可以按照做菜的角度理解PC1，1份gene1, 2.33份gene2。

数学上，我们说PC1是gene1和gene2的线性组合。
而且数学上 PC1^2=1^2 + (2.33)^2
(1^2 + st^2)**0.5=2.538445

而使用SVD分解时，PC会被标准化到1。所以，只需要每个分量都除以斜边
gene1份数: 1/2.538445=0.393942
gene2份数: st/2.538445=0.9191355

> 0.393942**2+0.9191355**2
[1] 1


新名词: 这个单位向量(0.393942, 0.9191355)又叫做 Singular Vector or the "Eigenvector" for PC1;

SS(distance for PC1)=Eigenvalue for PC1
sqrt(Eigenvalue for PC1)=Sigular Value for PC1

PCA calls the ss(distances) for the best fit line the Eigenvalue for PC1, 
and the square root of the Eigenvalue for PC1 is called the Sigular Value for PC1.




2) 第二个PC
由于这是二维的，第二个PC要在PC1垂直的方向上找，所以，只有一个选择。
PC2方向 (-0.9191355, 0.393942)，这是PC2的奇异向量或特征向量。
系数也叫PC2的loading Scores: PC2需要 -0.9191355 份gene1, 和 0.393942 份 gene2，


b1=c(0.393942, 0.9191355);b1
b2=c(-0.9191355, 0.393942);b2
sum(b1*b2) #[1] 0
# 正交向量，乘积为0
x1=1; plot(NULL, xlim=c(-x1,x1), ylim=c(-x1,x1))
lines( c(0,b1[1]), c(0,b1[2]), col='red')
lines( c(0,b2[1]), c(0,b2[2]), col='blue')





3) 画最后的PC图时，旋转PC1至水平。相当于对每个点做坐标变换。
坐标轴的旋转矩阵: https://blog.csdn.net/TOM_00001/article/details/62054572
一个坐标(x,y)逆时针旋转alpha角度到(x',y'), 满足[x',y']T=A.[x,y]T时，坐标变换矩阵 A=
|cosA, -sinA|
|sinA, cosA|


## R 代码实现: 对坐标轴旋转到x和PC1一致，各个点的位置和在x轴(PC1)上的投影。
PI=3.1415926535897932
dt=t(iris[, c(1,3)])
dim(dt) #行为2个基因，列为样品名
# 中心化
dt=as.data.frame(t(apply(dt, 1, function(x){
  x-mean(x)
})))
head(dt[,1])
dim(dt)
# 旋转坐标轴
x0=0.3939420
y0=0.9191355
angle=-atan(y0/x0) #弧度制的角度，相当于顺时针旋转，是角度的负方向
dt2=apply(t(dt), 1, function(xy){
  c( sum(c(cos(angle), -sin(angle)) *xy), 
     sum(c(sin(angle), cos(angle))*xy ) )
})
head(dt2[,1])
dim(dt2)
# 检验旋转前后，点到原点的距离。应该是不变。
apply(dt[, 1:10],2,function(x){ sum(x**2) } )
apply(dt2[, 1:10],2,function(x){ sum(x**2)} )
plot(t(dt2), col=iris[,5], xlim=c(-3, 4), ylim=c(-3, 4))
# 可视化第PC 1的方向
st=tan(0/180*3.14159265358979) #线的角度
x1=seq(-3,5,0.5); lines(x1, x1*st,type='l') #add line
#
ssd=0
for(i in 1:ncol(dt)){
  x=dt2[1,i];  y=dt2[2,i]; #数据点
  a=(x+st*y)/(1+st^2); b=a*st; #映射到直线上
  # 画映射的点和垂线
  points(a,b,type='p',col='#F600FF55', pch=20)
  lines(c(x,a), c(y,b), type='l',col='grey',lty=2)
  ssd=ssd+(a^2+b^2)
}
ssd #[1] 545.6228 和旋转前一致。



备注: this is how PCA is done using Sinular Value Decomposition(SVD).






4) 变异程度和碎石图
还记得特征值吗？
SS(distances for PC1) = Eigenvalue for PC1
SS(distances for PC2) = Eigenvalue for PC2

除以样本数n-1，得到variation:
SS(distances for PC1)/(n-1) = variation for PC1
SS(distances for PC2)/(n-1) = variation for PC2



## R: 计算variation for PC1-2
ssd/(ncol(dt)-1) #3.661898 

# 可视化第PC 2的方向
st=tan(90/180*3.14159265358979) #线的角度
x1=seq(-3,15,0.5); lines(x1, x1*st,type='l') #add line
ssd2=0
for(i in 1:ncol(dt)){
  x=dt2[1,i];  y=dt2[2,i]; #数据点
  a=(x+st*y)/(1+st^2); b=a*st; #映射到直线上
  # 画映射的点和垂线
  points(a,b,type='p',col='#F600FF22', pch=20)
  lines(c(x,a), c(y,b), type='l',col='grey',lty=2)
  ssd2=ssd2+(a^2+b^2)
}
ssd2
ssd2/(ncol(dt)-1) #0.1400731 


这也就是说2个PC总的变异是 3.661898 +0.1400731=3.801971
也就是说
PC1占了变异的 3.661898/3.801971=0.9631578=96.3%
PC2占了变异的 0.1400731/3.801971=0.03684223=3.6%

碎石图 Scree plot 是描述每个PC解释多少变异的:
barplot(c(0.9631578, 0.03684223))







(3) 考虑3个基因的情况
过原点，找ssd最大的方向，为PC1。
然后过原点，在PC1的垂直平面上寻找ssd最大的方向，得到PC2；
然后过原点，和PC1和PC2都垂直的方向就是PC3;

如果有更多基因，就有同样数目的PC。一般来说，PC个数=min(变量数，观测值数);

每个PC都是3个基因的线性组合，基因前的3个系数是单位向量。


碎石图 Scree plot: 找到所有的PC之后，就可以使用特征值(比如，ss(distances))来计算每个PC解释的变异百分比，画碎石图。

如果前2个PC已经解释了>90%，就可以使用2个PC画图了。
旋转坐标轴，使PC1水平，PC2竖直，画出新坐标系统下的各个点。




(4) 4个基因时，已经无法直接画图了。
但是还可以找到4个PC，以及画碎石图。

注: 如果最后结果差不多，











ref:
https://www.cnblogs.com/leezx/p/6120302.html
https://www.bilibili.com/video/av35447404?from=search&seid=1532616759578227057
https://www.jianshu.com/p/101064609904

关于PCA的解释 https://blog.csdn.net/lanyuelvyun/article/details/82384179




========================================
logistic 回归，及odd ratio
----------------------------------------
1.
(1) Odds Ratio的定义
https://psychscenehub.com/psychpedia/odds-ratio-2/

一个模型，录取率和性别
     Admit, not admit
Male   a=7  b=3
Femail c=3  d=7

则录取频率 p=录取的/总人数 
p(M)=7/(7+3)=0.7
p(F)=3/(3+7)=0.3


发生比 odds=成功概率/失败概率
odds(M)=p/(1-p)=0.7/0.3=7/3
odds(F)=p/(1-p)=0.3/0.7=3/7


odds ratio=优势比=odds(M)/odds(F)=7/3 / (3/7)=49/9=5.4;
ad/bc=49/9




2) An odds ratio (OR) is a statistic that quantifies the strength of the association between two events, A and B. 
Odds Ratio (OR) is a measure of association between exposure and an outcome. The OR represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure.
有该暴露时的某结果发生的odds，比上没有该暴露时的结果发生的odds。

上文就是男性的录取odds，比上非男性的录取odds。

OR >1 indicates increased occurrence of event
OR <1 indicates decreased occurrence of event (protective exposure)
Look at CI and P value for statistical significance of value


3)解读
Interpretation
According to the tablet above, individuals with admited are 5.4 times more likely to be exposed to Male than those without admited.
如何解读OR: https://psychscenehub.com/psychpedia/odds-ratio-2/






(2)在研究两元分类响应变量和诸多自变量间的相互关系时，常选用logistic回归模型。

两元分类变量Y的一个结果记为“成功”，另一个结果记为“失败”，分别用1和0表示。
n个自变量(解释变量)记为X1,X2,...,Xn，在n个自变量作用下出现“成功”的条件概率记为
p=P(Y=1|X1,X2,...,Xn)，那么 logistic回归模型表示为


Z=B0+B1x1+B2x2+...+BnXn，无论Xi取值范围如何，Z总是-无穷大到+无穷大。
p=exp(Z)/( 1+exp(Z) ) ，导致p的取值范围总是0到1之间，这是logistic回归模型的合理性所在。

其中B0称为常数项或截距，B1,B2,...,Bn称为 logistic 回归模型的回归系数。
对上式做 logit 变换，logistic 回归模型可以写成下列线性形式:
logit(p)=ln( p / (1-p))=ln( exp(Z) )=Z = B0+B1x1+...+BnXn; ## p是事件发生的概率。

这样我们就可以使用线性回归模型对参数Bi进行估计了。



For example, in logistic regression the odds ratio represents the constant effect of a predictor X, on the likelihood that one outcome will occur.


推导 OR和系数的关系:
Z=ln( p / (1-p))=ln(odds);
logit(p1)-logit(p2)=ln( p1/(1-p1) / ( p2/(1-p2) ) )=ln( odds ratio )

对于logistic模型 Y~B0+B1X，X每变动1时 Odds 的比值都应该是一样的，这个比值就是 Odds Ratio;
odds=p/(1-p)=exp(Z)=exp( B0+B1X )
则 odds2/odds1=exp(B1*(X2-X1))=exp(B1), 因为X2-X1=1就是每次变动的单位1;
所以，logistic回归的各个变量的系数就是log(Odds Ratio)，求Odds Ratio只需要求exp(Bi)即可。


就像存款一样，有两种方式描述收益：一个是利率(不变)，一个是利息(随着本金变动)。
存一年，利率3%，本金100元，则利息3元；本金2000元，则利息60元；

logistic的 p 值会随着变量Xi而变动，但是 Odds Ratio 则保持恒定。








2. LOGIT REGRESSION 及odd ratio怎么求
https://stats.idre.ucla.edu/r/dae/logit-regression/

Examples

Example 1. Suppose that we are interested in the factors that influence whether a political candidate wins an election. The outcome (response) variable is binary (0/1); win or lose. The predictor variables of interest are the amount of money spent on the campaign, the amount of time spent campaigning negatively and whether or not the candidate is an incumbent.
实例1: 假设结果是输赢(0/1二分类变量)，变量包括很多...

Example 2. A researcher is interested in how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. The response variable, admit/don’t admit, is a binary variable.
实例2: 录取与否受到哪些因素影响？


(1) 读入数据
mydata <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv") #400 4
## view the first few rows of the data
head(mydata)
##   admit gre  gpa rank
## 1     0 380 3.61    3
## 2     1 660 3.67    3
## 3     1 800 4.00    1
## 4     1 640 3.19    4
## 5     0 520 2.93    4
## 6     1 760 3.00    2

# 这个可以省略
mydata$rank <- factor(mydata$rank)
str(mydata)


(2) 广义模型
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
# mylogit <- glm(admit ~ ., data = mydata, family = "binomial") #变量太多时，可以简写为.表示其他所有变量。

summary(mylogit) 
summary(mylogit)$coefficients
#                Estimate  Std. Error   z value     Pr(>|z|) ## 最后一列给出了p值
# (Intercept) -3.44954840 1.132846009 -3.045029 2.326583e-03
# gre          0.00229396 0.001091839  2.101005 3.564052e-02
# gpa          0.77701357 0.327483878  2.372677 1.765968e-02
# rank        -0.56003139 0.127136989 -4.404945 1.058109e-05

coef(mylogit)
# (Intercept)         gre         gpa        rank 
# -3.44954840  0.00229396  0.77701357 -0.56003139

#
## CIs using profiled log-likelihood
confint(mylogit) #给出置信区间

## CIs using standard errors
confint.default(mylogit)


#BiocManager::install('aod')
library(aod)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 4:6)


### odds ratios only
exp(coef(mylogit))


## odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit,level = 0.95))) # 给出OR和置信区间
#                    OR       2.5 %    97.5 %
# (Intercept) 0.03175998 0.003309497 0.2835650
# gre         1.00229659 1.000171559 1.0044714
# gpa         2.17496718 1.152082367 4.1717746
# rank        0.57119114 0.442656492 0.7294389



### 
logit(p)=ln( p / (1-p))=Z=-3.44954840 + 0.00229396*gre + 0.77701357*gpa -0.56003139*rank;
p=exp(Z)/( 1+exp(Z) )


(1)https://www.statsdirect.com/help/regression_and_correlation/logistic.htm
也认为 exp(Intercept)=Odds Ratio;
(2)https://rstudio-pubs-static.s3.amazonaws.com/182726_aef0a3092d4240f3830c2a7a9546916a.html
log odds=B0;
(3)How to interpret odds ratio in logistic regression?
https://www.researchgate.net/post/How_to_interpret_odds_ratio_in_logistic_regression
Odds Ratio=exp(Coefficient)







3. Logistic Regression
http://r-statistics.co/Logistic-Regression-With-R.html








========================================
一文读懂回归分析:概述Cox回归、岭回归、Lasso回归、ElasticNet 回归
----------------------------------------
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml

生物学中的基因很多是有共线性的，所以，后三种方法经常使用。
回归正则化方法（套索，岭和ElasticNet）在高维数据和数据集变量之间存在多重共线性的情况下运行良好。

3）Cox回归
Cox回归的因变量就有些特殊，它不经考虑结果而且考虑结果出现时间的回归模型。它用一个或多个自变量预测一个事件（死亡、失败或旧病复发）发生的时间。Cox回归的主要作用发现风险因素并用于探讨风险因素的强弱。但它的因变量必须同时有2个，一个代表状态，必须是分类变量，一个代表时间，应该是连续变量。只有同时具有这两个变量，才能用Cox回归分析。Cox回归主要用于生存资料的分析，生存资料至少有两个结局变量，一是死亡状态，是活着还是死亡；二是死亡时间，如果死亡，什么时间死亡？如果活着，从开始观察到结束时有多久了？所以有了这两个变量，就可以考虑用Cox回归分析。





9）岭回归
当数据之间存在多重共线性（自变量高度相关）时，就需要使用岭回归分析。在存在多重共线性时，尽管最小二乘法（OLS）测得的估计值不存在偏差，它们的方差也会很大，从而使得观测值与真实值相差甚远。岭回归通过给回归估计值添加一个偏差值，来降低标准误差。

岭回归要点：
1）除常数项以外，岭回归的假设与最小二乘回归相同；
2） 它收缩了相关系数的值，但没有达到零，这表明它不具有特征选择功能；
3）这是一个正则化方法，并且使用的是 L2 正则化。





13）套索回归 LASSO
与岭回归类似，套索也会对回归系数的绝对值添加一个罚值。此外，它能降低偏差并提高线性回归模型的精度。看看下面的等式：

套索回归要点：
1）除常数项以外，这种回归的假设与最小二乘回归类似；
2）它将收缩系数缩减至零（等于零），这确实有助于特征选择；
3）这是一个正则化方法，使用的是 L1 正则化；
4）如果一组预测因子是高度相关的，套索回归会选出其中一个因子并且将其它因子收缩为零。


lasso 回归就是这个意思，就是让回归系数不要太大，以免造成过度拟合（overfitting）。所以呢，lasso regression是个啥呢，就是一个回归，并且回归系数不要太大。

具体的实现方式是加了一个L1正则的惩罚项。





14）ElasticNet 回归
ElasticNet 回归是套索回归和岭回归的组合体。它会事先使用 L1 和 L2 作为正则化矩阵进行训练。当存在多个相关的特征时，Elastic-net 会很有用。岭回归一般会随机选择其中一个特征，而 Elastic-net 则会选择其中的两个。同时包含岭回归和套索回归的一个切实的优点是，ElasticNet 回归可以在循环状态下继承岭回归的一些稳定性。

ElasticNet 回归要点：
1）在高度相关变量的情况下，它会产生群体效应；
2）选择变量的数目没有限制；
3）它可以承受双重收缩。







ref:
https://www.bilibili.com/video/BV1LE411D7SW
Lasso回归（L1正则，MAP+拉普拉斯先验） https://blog.csdn.net/qq_32742009/article/details/81674021
Lasso回归总结 https://www.cnblogs.com/wmx24/p/9555219.html




========================================
|-- 实例: glmnet包做 Ridge回归、Lasso 回归、Elastic-Net Regression
----------------------------------------
1. 使用 glmnet 包

alpha=0时，只有岭回归;
alpha=1时，只有lasso回归;
lamda{ alpha(累加|variable1|) + (1-alpha)(累加|variable2|^2) }




2. 代码及详解
(1) 三种算法的结果与比较
##########
library(glmnet)
set.seed(42)

### step1 获取数据
#模拟数据
n=1000 #行
p=5000 #列
real_p=15 #真实有效的15列，其余都是背景噪音

x=matrix(rnorm(n*p), nrow=n, ncol=p)
y=apply(x[,1:real_p], 1, sum) + rnorm(n) #x的前15列，按行求和。加入噪音。
## 现在有一个向量y，需要用数据x预测向量y
# 预测方法: Ridge, Lasso, Elastic-Net Regression


### step2 分配训练集，测试集
train_rows=sample(1:n, 0.66*n) #选哪些行？随机的2/3行

x.train=x[train_rows,] #训练集
x.test=x[-train_rows,] #测试集
#
y.train=y[train_rows] #训练集结果
y.test=y[-train_rows] #测试集结果
#

### step3 开始岭归回
alpha0.fit=cv.glmnet(x.train, y.train, type.measure = 'mse',
                     alpha=0, family='gaussian')
### 参数解释:
#(1) 函数名字中的 cv 表示使用cross validation 获取最优的lambda; 默认使用 10-Fold Cross Validation;
# 和 lm(), glm() 不同，cv.glmnet()不接受 formula notation, x和y必须分别传入。
#(2) mse表示 mean squared error: 残差平方和，除以样本容量。
#  If we were applying Elastic_Net Regression to Logistic Regression, we would set this to deviance.
#(3) 因为使用Ridge Regression，所以设置 alpha=0;
#(4) family 设置为 gaussian; 如果使用Logistic Regression, 则使用 binomial;
#
## 整体解释：
# 该方程，应用岭回归罚分，使用10x交叉验证，找到最优化的lambda

### step4 开始预测
alpha0.predicted=predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx = x.test)
# 使用 predict() 函数，应用 alpha0.fit 到 测试数据集上。
# 参数解释:
# 第一个是模型
## 第二个参数s，可能是size，保存到alpha0.fit 中的最优化的lambda值对应的"the size of the penalty"
#   本例中，设置为 lambda.1se，保存在alpha0.fit中的，这导致最简单的模型(比如，模型有最少的非零参数)，
#   and within 1 standard error of the lambda that had the smallest sum.
## s也可以设置为 lambda.min，就是产生最小误差和的lambda值。
## 本例选择 lambda.1se, 因为统计学上，它和lambda.min没有差异，但是有更少的参数。
## Tips: 我认为只有 Lasso 和 Elastic Net Regression 可以去除参数... 然后呢？
## 本文为了比较的统一，都是用 lambda.1se 了。
# 第三个参数 newx 就是新数据。


## step5 计算损失函数: mean squared error
mean( (y.test - alpha0.predicted)^2 ) #14.88459
# (这里使用 预测与真实值差的平方均值)

################
# 尝试 Lasso Regression(alpha=1时)
# step3 B
alpha1.fit=cv.glmnet(x.train, y.train, type.measure = 'mse',
                     alpha=1, family='gaussian')
#step4 B
alpha1.predicted=predict(alpha1.fit, s=alpha1.fit$lambda.1se, newx = x.test)
#step5 B
mean( (y.test - alpha1.predicted)^2 ) #1.184701
# 误差比Ridge小，模型预测效果好于Ridge。


################
# 尝试 Elastic-Net Regression(alpha=(0,1)时)
# step3 C
alpha0.5.fit=cv.glmnet(x.train, y.train, type.measure = 'mse',
                     alpha=0.5, family='gaussian')
#step4 C
alpha0.5.predicted=predict(alpha0.5.fit, s=alpha0.5.fit$lambda.1se, newx = x.test)
#step5 C
mean( (y.test - alpha0.5.predicted)^2 ) #1.23797
# 误差比Lasso大，这个比较中 Lasso 赢了。

################
# 可以尝试一系列的 alpha，
list.of.fits=list()
for(i in 0:10){
  print(i) #进度条
  fit.name=paste0('alpha', i/10)
  
  list.of.fits[[fit.name]]=cv.glmnet(x.train, y.train, type.measure = 'mse',
                                     alpha=i/10, family='gaussian')
}
# 计算 mean squared errors for each fit with the testing dataset
results=data.frame()
for(i in 0:10){
  print(i) #进度条
  fit.name=paste0('alpha', i/10)
  fit=list.of.fits[[fit.name]]
  #
  predicted=predict(fit, s=fit$lambda.1se, newx = x.test)
  mse=mean( (y.test - predicted)^2 )
  
  temp=data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
  results=rbind(results, temp)
}
dim(results) #11 3
head(results)
#   alpha       mse fit.name
#1    0.0 14.918840   alpha0
#2    0.1  2.256924 alpha0.1
#3    0.2  1.472927 alpha0.2
plot(mse~alpha, results, type="o")
#结论 对这组数据，alpha=1时，Lasso回归的结果还是最好的。




(2) 确定好拟合参数，接下来解决问题
#####
# part1
fit2<-glmnet(x.train, y.train, alpha=1,family='gaussian')
plot(fit2, xvar = "lambda", label = TRUE)
print(fit2)


# part2
cv.fit <- cv.glmnet( x.train, y.train, alpha=1);

# pic1 可视化拟合效果，看纵坐标MSE什么时候最小(红色虚线标出)
plot(cv.fit)
abline(v=log(c(cv.fit$lambda.min)),lty=2, col="red")

# pic2 看有几个截距，就有几个非0参数
plot(fit2, xvar = "lambda", label = TRUE)
abline(v=log(c(cv.fit$lambda.min,cv.fit$lambda.1se)),lty=2)
abline(v=log(c(cv.fit$lambda.min)),lty=2, col="red")

# 查看每个参数的系数
coef(fit2, s = cv.fit$lambda.1se )



# part 3 预测结果图
pfit = predict(fit2, x.test, s = cv.fit$lambda.1se, type = "response")
plot(pfit, y.test)




#如果取最小值时
cv.fit$lambda.min #0.0908066
Coefficients <- coef(fit2, s = cv.fit$lambda.min)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
row.names(Coefficients)[Active.Index] #69个非0参数

minlassopred <-predict(cv.fit,newx=x.test,s=cv.fit$lambda.min) #,type="response"
minlassopred


#如果取1倍标准误内(大小和min无统计学差异)，参数最少的lambda值。【建议使用这个】
cv.fit$lambda.1se #0.1257568
Coefficients <- coef(fit2, s = cv.fit$lambda.1se)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
row.names(Coefficients)[Active.Index] #27个非0参数

selassopred <-predict(cv.fit,type="response",newx=x.test,s=cv.fit$lambda.1se)
selassopred



(3) 怎么解释2张图? 
特别是顶部数字是什么意思？我推测，是非0变量的个数。

贴吧 https://tieba.baidu.com/p/6301813616?red_tag=2818960114











========================================
|-- 实例: glmnet包做 Cox 回归 筛选变量
----------------------------------------
1. 与预后有关的文章，传统的做法一般会选择多变量cox回归，高级做法自然就是我们今天的lasso分析。
https://www.dxy.cn/bbs/newweb/pc/post/42430476

线性回归采用一个高维的线性函数来尽可能的拟合所有的数据点，最简单的想法就是最小化函数值与真实值误差的平方。

Lasso回归则是在一般线性回归基础上加入了正则项，在保证最佳拟合误差的同时，使得参数尽可能的“简单”，使得模型的泛化能力强。正则项一般采用一，二范数，使得模型更具有泛化性，同时可以解决线性回归中不可逆情况。这个时候你可能不淡定了，你是魔鬼吗？什么是正则项？？？

正则项：正则化就是通过对模型参数进行调整（数量和大小），降低模型的复杂度，以达到可以避免过拟合的效果。正则化是机器学习中的一种叫法，其它领域内叫法各不相同，统计学领域叫惩罚项，数学领域叫范数。而正则项又包括两种，即一范数和二范数，就是L1和L2范数。

重点来了：采用L1范数则是lasso 回归，L2范数则是岭回归了。那么函数有啥区别呢？如下：
	公式略。本博客暂时不支持公式。
- L1范数是所有参数绝对值之和，对应的回归方法叫做Lasso回归。
- L2范数是所有参数的平方和，对应的回归方法叫做Ridge回归，岭回归需要注意的是，正则项中的回归系数为每个自变量对应的回归系数，不包含回归常数项。


L1和L2各有优劣，
- L1是基于特征选择的方式，有多种求解方法，更加具有鲁棒性；
- L2则鲁棒性稍差，只有一种求解方式，而且不是基于特征选择的方式。

在GWAS分析中，当用多个SNP位点作为自变量时，采用基于特征选择的L1范式，不仅可以解决过拟合的问题，还可以筛选重要的SNP位点，所以lasso回归在GWAS中应用的更多一点。

我们在大多数signature文章中主要是基因挑选，自然就是今天的主题lasso cox回归，接下来我们看一下，如何采用R语言glmnet来实现。



2. 代码
library(glmnet)
#载入数据。
#包含30个基因在1000个病人样本中的表达，另一个是每个患者的生存状态和生存时间，生存时间以年为单位，如下：
data('CoxExample')
# 1000行(病人)，30列(基因)
dim(x)
x[1:10,1:5]

# 1000行(病人) 2列(生存时间、终点)
dim(y)
head(y)



# step1 整理数据(30个基因在1000个病人)
x=data.frame(x)
y=data.frame(y)

row.names(x)=paste0("patient",1:1000)
colnames(x)=paste0('gene',1:30)

row.names(y)=paste0("patient",1:1000)
colnames(y)=c('time','status')



# step2 构建生存分析
library(survival)
fit_sur=Surv(y$time, y$status)
dim(fit_sur) #1000 2
head(fit_sur) #[1] 1.76877757  0.54528404  0.04485918+ 0.85032298+ 0.61488426  0.29860939+
str(fit_sur)


# step3 通过glmnet函数中的设置family参数定义采用的算法模型，比如设置cox
fit=glmnet(as.matrix(x), fit_sur, family='cox')
fit

## 画图: 各个基因的系数，大体能看出正负
plot(fit, label=T)

# step4 Lasso回归最重要的就是选择合适的λ值，可以通过cv.glmnet函数实现
cv.fit=cv.glmnet(as.matrix(x), fit_sur, 
                #nfold=10,
                family='cox')
cv.fit
# Call:  cv.glmnet(x = as.matrix(x), y = fit_sur, family = "cox") 
# 
# Measure: Partial Likelihood Deviance 
# 
#      Lambda Measure      SE Nonzero
# min 0.01750   13.07 0.07251      15
# 1se 0.04869   13.13 0.06238      10

plot(cv.fit) #画图，能找到误差最小的lambda值的位置
# 基于该图选择最佳的λ，一般可以采用两个内置函数实现cvfit$lambda.min和 cvfit$lambda.1se 。


# step5 基因筛选，采用coef函数即可，有相应参数的gene则被保留，采用λ使用的是lambda.min
# 我感觉采用1se更好。大小和min没有统计学差异，同时做到了参数最少化
coef.1se=coef(cv.fit, s="lambda.1se")
coef.1se
# 30 x 1 sparse Matrix of class "dgCMatrix"
#                  1
# gene1   0.38108115
# gene2  -0.09838545
# gene3  -0.13898708
# gene4   0.10107014
# gene5  -0.11703684
# gene6  -0.39278773
# gene7   0.24631270
# gene8   0.03861551
# gene9   0.35114295
# gene10  0.04167588
# gene11  .        
# 第二列有数值是非点号的则代表被选择的基因。

# step6 美化lasso图，怎么标出来每个基因？
plot(fit, label=T)



#如果取1倍标准误内(大小和min无统计学差异)，参数最少的lambda值。【建议使用这个】
cv.fit$lambda.1se
Coefficients <- coef(fit, s = cv.fit$lambda.1se)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
row.names(Coefficients)[Active.Index] #10个非0参数





========================================
PCA 及 t-SNE ( t-Distributed Stochastic Neighbor Embedding (t-SNE) )
----------------------------------------
1933 PCA
1952 MDS
2000 Isomap
2002 SNE
2008 t-SNE
2014 BHtSNE
? UMAP

非线性降维方法
1969 Sammon mapping
1997 CCA
2000 Isomap
2000 LLE: locally linear embedding
2002 SNE 
2002 Laplacian Eigenmaps
2004 MVU: Maximum Variance Unfolding


如果是scRNAseq数据，还是用成熟的包比较好:  seurat in R or ScanPy in python.
They include everything from data-processing, clustering and generating plots such as these within them. It will make your life a lot easier.
- https://satijalab.org/seurat/
- https://scanpy.readthedocs.io/en/stable/


The t-SNE step may take a while depending on the size of your dataset. There are quicker options like UMAP which is also slightly better in other ways in maintaining the global architecture of the clusters. 






1. PCA 是对协方差矩阵求特征向量。最大的特征值对应的特征向量，被用来重构原始数据最重要的部分。
It does so by calculating the eigenvectors from the covariance matrix. The eigenvectors that correspond to the largest eigenvalues (the principal components) are used to reconstruct a significant fraction of the variance of the original data.

PCA之后获取的新特征或组分都是互相独立的。

PCA是线性降维。不能准确表示多维数据。
Recall the PCA objective: Project data onto a lower dimensional subspace, such that the variance is maximized.
Conclusion: Mostly preserves distances between dissimilar points;
But is that really what we want for the purpose of visualization?


实例: 高维弯曲的围巾图，投射到二维。原本弯曲而距离很近的点还能距离很近吗？






2. TNE 随机近邻插入
In contrast to PCA, SNE focused on maintaining the nearest neighbors in the lower dimensional map.

Aim is to match distributions of distances between points in high and low dimensional space via conditional probabilities;

Assume distrances in both high and low dimensional space are Gaussian-distributed;
SNE focuses on preserving the local structure of the data.


SNE是先将欧几里得距离转换为条件概率来表达点与点之间的相似度。

py 代码实现: https://github.com/DawnEve/ML_MachineLearning/blob/master/t-SNE/tSNE_demo.ipynb


- 如果一个N高维空间 xi表示第i个对象, x1,x2,...,xn;
- yi表示低维空间中的第i个对象, y1,y2,...,yn;
- 建立一个近邻矩阵，反应高维数据点之间的相似关系;
	把高维空间欧氏距离，转为反应相似度的条件概率。
- 高维空间的距离转为条件概率 Pj|i=exp(-||xi-xj||^2/ (2*sigmai^2) ) / 求和(k!i, exp(-||xi-xk||^2/ (2*sigmai^2) ) );
	距离越远，算出来的p越小; 距离越近，算出来的p越大;
	
	sigmai 和一个叫做perplexity的参数相关，perplexity可以大概的解释为每个点有几个近邻。
		perplexity与算法中使用的最近邻居的数量有关。不同的perplexity可能会导致最终结果发生巨大变化。
	sigmai 是二分法对距离矩阵搜索获得的???
	
	def p_joint(X, target_perplexity):
		"""Given a data matrix X, gives joint probabilities matrix.
		# Arguments
			X: Input data matrix.
		# Returns:
			P: Matrix with entries p_ij = joint probabilities.
		"""
		#(1)距离 Get the negative euclidian distances matrix for our data
		distances = neg_squared_euc_dists(X)
		#(2)根据距离、Perp求最优的sigmai. Find optimal sigma for each row of this distances matrix
		sigmas = find_optimal_sigmas(distances, target_perplexity)
		#(3)根据距离、sigma求相似矩阵. Calculate the probabilities based on these optimal sigmas
		p_conditional = calc_prob_matrix(distances, sigmas)
		#(4)对称化 Go from conditional to joint probabilities matrix
		P = p_conditional_to_joint(p_conditional)
		return P
	
- 低维空间的距离 Qj|i=exp(-||yi-yj||^2 ) / 求和(k!i, exp(-||yi-yk||^2 ) );
	规定 pi|i=qi|i=0;


- cost function: 移动Q点，使P和Q的KL散度(熵)最小化: C=累加(i, KL(Pi||Qi))=累加(i, 累加(j, Pj|i*log(pj|i/qj/i)));
	高维空间算出来的p是固定的，而低维空间算出来的q则每移动一次重新计算一次，怎么知道q的运动方向呢？
	if p==q, then log(1)=0;
	Penalize when p!=q:
		Large p & small q: Big penalty; 倾向于保持原始相似度大的关系。
		Small p & large q: Small penaly;
	所以，SNE能保持数据的局部结构。
	
- 梯度:
	求偏导数 dC/dyi= 2*求和(j, (yi-yj)*(pj|i-qj|i + pi|j-qi|j)) #----> 这个求导看不懂?????
		前面的 (yi-yj) 相当于弹簧 spring，后面的(pj|i-qj|i + pi|j-qi|j) 提供伸展性 Attraction/Repulsion
	有了cost function, SNE 就可以使用梯度方向做优化了。
	
- momentum term alpha(t): 除了cost function 的梯度，还有一项momentum term，来加速优化并避免局部最优条件。
- SNE 有2个问题:
	Cost function is difficult to optimize
	Crowding problem: overlapping
		How to solve crowding problems?
		Goal: dissimilar points have to be modeled as far apart as possible in the map;
#









3. t-SNE 解决了 crowding problem.
优势: t-SNE tends to preserve local structure at the same time preserving the global structure as much as possible;
而PCA保持global structure（变异最大的方向），却损失了 local structure;

tSNE 的cost函数有2个不同特性:
	- Cost function 是对称的(相对于 SNE): pi|j=pj|i, and qi|j=qj|i;
	- 低维空间的相似矩阵是使用t分布来求的。
#



具体算法细节: http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
主要步骤
- 计算相似概率：计算高低维度中该点相似性。
	计算方法就是按照高斯分布，以某点为中心，计算到其他点的距离，映射到正态分布获得一个可能性p值。
		距离越近，p越大； 距离越远，p越小。
		之后要标准化，就是防止选取的中心点不一样时导致的高斯分布sigma不同而p值差不同。
		(0.24,0.05) (0.12,0.024) 标准化后都是 (0.827, 0.172): 比较松散的类和比较紧凑的类，点的相似度一样。perplex 参数什么意思?
	然后换一个点，重新计算它到其他点的距离；使用高斯分布折算出p值。
	直到每个点为中心都计算一遍。
	两个方向的p值求平均，作为2点的最终相似度p。最后就获得了一个相似矩阵。每一行、每一列表示那个点和其他点的最终相似度p。
	由于自身与自身无关，所以自己到自己的最终相似度定义为0；
	
- 最小化高低纬点的条件概率差异，获得一个最优的低纬度空间内的点的表示。
	然后把高维空间的点随机映射到直线上;
	然后计算线性空间内的点之间的相似度，不过，这一次距离映射到相似度p时，使用t分布。
		t分布很像normal分布，但是最高点比较矮，尾巴比较高。能解决拥挤的问题。
	使用t分布，计算点两两之间的相似分数，然后scale，得到相似矩阵。
	
- 为了使条件概率和的最小化，使用梯度下降法，最小化 所有数据点的 Kullback-Leibler divergence。
	逐步移动低维空间的点，使其相似矩阵和高维空间越来越相似。
	

注: Kullback-Leibler divergence or KL divergence 是衡量一个分布和另一个期望分布的差异的方法。


简单说，tSNE 最小化两种分布的差异: 一个分布是衡量输入对象在高维空间的相似性的，一个分布是衡量低维空间插入点之间的相似性的。


(1) t分布比着正态分布，最高点更低，而尾巴更高。x在尾巴处对应着更细腻的y值变化。 
f=function(x){ 1/(3.1415926*(1+x^2)) }
x=seq(-8, 8, 0.1)
y=f(x)
plot(x, dnorm(x), type='l', main="Distribution") #normal distribution
lines(x,y, type='l', lty=3, col='red') #t-distribution
legend('topleft', lty=c(1,3), bty='n',
       col=c('black', 'red'), legend=c('Normal', "t"))
#

(2)
低维使用自由度为1的t分布(就是柯西分布): P(x)=1/[pi*(1+x^2)]
Qij=(1+||yi-yj||^2)^-1 / 累加(k!=l, (1+||yl-yk||^2)^-1)
这样，就把不相似的点最大程度的分开了。

偏导数 dC/dyi= 4*求和(j, (pij-qij)*(yi-yj)*(1+ ||yi-yj||^2 )^-1)  ## ----> 不知道这个偏导数怎么求的。原论文附录有推导。

定义 pij=(pj|i+pi|j)/(2*n)，会有更简单的梯度函数 dC/dyi=4*累加(j, (pij-qij)*(yi-yj))




(3) 局限性
1) tSNE很适合高维数据的可视化，但是不能用于降维。点之间的距离没有意义。
2) 数据有很高内在维度时不那么成功; ??
3) cost function不是凸函数! 要仔细选择最优的参数。



(4) 一个重要的参数 Perplexity 困惑度。
不同的Perp会导致不同的聚类
	Perp too small -> local variations dominate,
	Perp too large -> global change dominate.
	#
	Loosely speaking, one could say that a larger / denser dataset requires a larger perplexity.
	Typical values for the perplexity range between 5 and 50. 原始论文认为Perp[5,50]比较稳健。
#



(5) tSNE 失败的情形？
tSNE assumes local linearity(Euclidean distances between neighbors); when does this assumption fail?
1) 当数据噪音很大时，tSNE 会失败，怎么办？
Solution: 先用PCA做平滑化。然后对前几(5-50)个PC进行tSNE可视化。

2) Data with high intrinsic dimension(on a highly varying manifold)
Solution: learn a "code" - of lower dimension using an Auto-encoder;

什么是 intrinsic dimension? 
	https://eng.uber.com/intrinsic-dimension/
	https://cran.r-project.org/web/packages/intrinsicDimension/vignettes/intrinsic-dimension-estimation.html
		The intrinsic dimension of a data set is a measure of its complexity. Data sets that can be accurately described with a few parameters have low intrinsic dimension. It is expected that the performance of many machine learning algorithms is dependent on the intrinsic dimension of the data. 
#






#########
看不懂的部分:
cost 函数是什么？KL 散度，求和(p*log(p/q));
怎么移动低维空间的点？按照梯度移动; ==> 怎么求梯度/偏导数?
对矩阵求偏导数是什么概念？
	http://math.fudan.edu.cn/gdsx/KEJIAN/方向导数和梯度.pdf
#








4. 答疑Question
(1) R package "tsne" and "Rtsne" give different cell clustering results?
"tsne" is a "pure R" implementation of the t-SNE algorithm, while package "Rtsne" is an R wrapper around the fast t-SNE implementation by Van der Maaten.

A:
Apparently, there may have been a bug in the tsne package, which may or may not be related to what you are seeing. Check the discussion here to see if it applies: https://gist.github.com/mikelove/74bbf5c41010ae1dc94281cface90d32


(2) scale(perplexity)有什么作用？
大部分人在使用t-SNE时，一般都直接使用默认参数图个方便(一般perplexity的默认值是30)，如果忽视了perplexity带来的影响，有的时候遇到t-SNE可视化效果不好时，根本就不知道哪里出了问题，优化起来也就无从下手了。

那么perplexity到底是啥呢？我们可以回顾t-SNE的数学表达式，主要是和sigma这一项相关

perplexity表示了近邻的数量，例如设perplexity为2，那么就很有可能得到很多两个一对的小集群。



(3) t-SNE 分类可信吗？ 为什么多重复几次 t-SNE 结果不一样，而且有些类一会分开一会聚合？
A: t-SNE更关心的是学习维持局部结构，群间的距离并不能说明什么，而且每次跑t-SNE的结果并不完全一致。所以解决这个问题，我们只需要跑多次找出效果最好的就可以了。引起这个问题的本质原因是，t-SNE是在优化一个非凸的目标函数，我们每次得到的只不过是一个局部最小。所以会出现同一集群被分为两半的情况

分类要依靠其他方法，t-SNE 仅仅是一个可视化方法: 
t-SNE is a valuable tool in generating hypotheses and understanding, but does not produce conclusive evidence

可以用t-SNE来提出假设 不要用t-SNE得出结论。



(4) t-SNE 中cluster之间的距离有意义吗？t-SNE能用于寻找离群点outlier吗？
t-SNE中集群之间的距离并不表示相似度。
对t分布来说，超出一定距离范围以后，其相似度都是很小的。也就是说，只要不在一个集群范围内，其相似度都是一个很小的值，我们所看到的集群之间的呈现出来的距离并不能说明什么，这是由t-SNE的内在所决定的。

t-SNE不能用于寻找离群点outlier。




第一作者(machine learning and computer vision): https://lvdmaaten.github.io/
	更多答疑看这里: https://lvdmaaten.github.io/tsne/
	Accelerating t-SNE using Tree-Based Algorithms. https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf
#





ref:
t-SNE使用过程中的一些坑 https://www.jianshu.com/p/631d6529e0df?from=singlemessage (有一个原理图)
视频 https://www.bilibili.com/video/BV1Z7411v7EL?p=1
https://www.datacamp.com/community/tutorials/introduction-t-sne
https://vimsky.com/article/4400.html
实战: https://ajitjohnson.com/tsne-for-biologist-tutorial/

出版社的专题 https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/

t-SNE visualization of CNN codes: http://cs.stanford.edu/people/karpathy/cnnembed/













========================================
|-- t-SNE的实现: R, python, js
----------------------------------------

https://github.com/DawnEve/ML_MachineLearning/blob/master/t-SNE/tSNE_demo.ipynb



1. 在R中实现 PCA 和 tSNE

真实scRNAseq数据
RNA: https://github.com/ajitjohnson/ajitjohnson.github.io/blob/master/assets/data/tsne_tutorial/exp.csv
cell type: https://github.com/ajitjohnson/ajitjohnson.github.io/blob/master/assets/data/tsne_tutorial/meta.csv


(1) 使用R包
## Load the t-SNE library
# install.packages("Rtsne")
library(Rtsne)

######## (1) input data
#1) input raw counts
IR_data0 =iris[1:4]
#2) input cpm: 对obs进行归一化，这里是row
IR_data1 <- as.data.frame(t(apply(iris[,1:4], 1, function(x){ x/sum(x)*1e4 })))
#3) input log2cpm:
IR_data2 <- as.data.frame(t(apply(iris[,1:4], 1, function(x){ log2(1+ x/sum(x)*1e4) })))
IR_species <- iris[ ,5]
dim(IR_data2) #150 4

## (2)do PCA
library(FactoMineR)
do_PCA=function(dt){
  # PCA 默认是对列计算的。而我们要对obs聚类，就要先转置
  dt.pca=PCA( t(dt) )
  #dt.pca$ind
  #str(dt.pca$ind)
  dim(dt.pca$var$coord) #150 3
  head(dt.pca$var$coord)
  #          Dim.1     Dim.2         Dim.3
  #cell1 0.8381696 0.5454088 -0.0009952193
  #cell2 0.8758969 0.4800065 -0.0489729933
  par(mfrow=c(1,3))
  plot(dt.pca$var$coord[,1] , dt.pca$var$coord[,2], col=iris[,5])
  plot(dt.pca$var$coord[,1] , dt.pca$var$coord[,3], col=iris[,5])
  plot(dt.pca$var$coord[,2] , dt.pca$var$coord[,3], col=iris[,5])
  return(dt.pca)
}
#PCA结果很稳定。如果输入相同，则每次运行结果都一样
dt.pca0=do_PCA(IR_data0)
dt.pca1=do_PCA(IR_data1)
dt.pca2=do_PCA(IR_data2)
dt.pca2$eig #每个PC解释了百分之多少的变异
str(dt.pca2$var)


## (3)do tSNE
do_tsne=function(IR_data){
  ## Run the t-SNE algorithm and store the results into an object called tsne_results
  tsne_results <- Rtsne(IR_data, perplexity=30, check_duplicates = FALSE) 
  # You can change the value of perplexity and see how the plot changes
  #str(tsne_results)
  
  ## Generate the t_SNE plot
  par(mfrow=c(1,2)) # To plot two images side-by-side
  plot(tsne_results$Y, col = "blue", pch = 19, cex = 1.5) # Plotting the first image
  plot(tsne_results$Y, col = "black", bg= IR_species, pch = 21, cex = 1.5) 
  # Second plot: Color the plot by the real species type (bg= IR_species)
  return(tsne_results)
}
set.seed(2020)
tsne_results0=do_tsne(IR_data0) #每次执行t-SNE，结果都是随机的
tsne_results1=do_tsne(IR_data1)
tsne_results2=do_tsne(IR_data2)
#
dim(tsne_results2$Y) #150 2
head(tsne_results2$Y)
plot(tsne_results2$Y[,1], col=iris[,5])
plot(tsne_results2$Y[,2], col=iris[,5])
plot(tsne_results2$Y[,1],tsne_results2$Y[,2], col=iris[,5])

#
# (4)带入前几个PC做tSNE
tsne_results2_2=do_tsne(dt.pca2$var$coord[,1:3])
plot(tsne_results2_2$Y[,1],tsne_results2_2$Y[,2], col=iris[,5])
#





(2) 使用纯R实现t-SNE (来自同名R包)
tsne=function (X, initial_config = NULL, k = 2, initial_dims = 30, 
  perplexity = 30, max_iter = 1000, min_cost = 0, epoch_callback = NULL, 
  whiten = TRUE, epoch = 100) 
{
  if ("dist" %in% class(X)) {
    n = attr(X, "Size")
  }
  else {
    X = as.matrix(X)
    X = X - min(X)
    X = X/max(X)
    initial_dims = min(initial_dims, ncol(X))
    if (whiten) 
      X <- .whiten(as.matrix(X), n.comp = initial_dims)
    n = nrow(X)
  }
  momentum = 0.5
  final_momentum = 0.8
  mom_switch_iter = 250
  epsilon = 500
  min_gain = 0.01
  initial_P_gain = 4
  eps = 2^(-52)
  if (!is.null(initial_config) && is.matrix(initial_config)) {
    if (nrow(initial_config) != n | ncol(initial_config) != 
      k) {
      stop("initial_config argument does not match necessary configuration for X")
    }
    ydata = initial_config
    initial_P_gain = 1
  }
  else {
    ydata = matrix(rnorm(k * n), n)
  }
  P = .x2p(X, perplexity, 1e-05)$P
  P = 0.5 * (P + t(P))
  P[P < eps] <- eps
  P = P/sum(P)
  P = P * initial_P_gain
  grads = matrix(0, nrow(ydata), ncol(ydata))
  incs = matrix(0, nrow(ydata), ncol(ydata))
  gains = matrix(1, nrow(ydata), ncol(ydata))
  for (iter in 1:max_iter) {
    if (iter%%epoch == 0) {
      cost = sum(apply(P * log((P + eps)/(Q + eps)), 1, 
        sum))
      message("Epoch: Iteration #", iter, " error is: ", 
        cost)
      if (cost < min_cost) 
        break
      if (!is.null(epoch_callback)) 
        epoch_callback(ydata)
    }
    sum_ydata = apply(ydata^2, 1, sum)
    num = 1/(1 + sum_ydata + sweep(-2 * ydata %*% t(ydata), 
      2, -t(sum_ydata)))
    diag(num) = 0
    Q = num/sum(num)
    if (any(is.nan(num))) 
      message("NaN in grad. descent")
    Q[Q < eps] = eps
    stiffnesses = 4 * (P - Q) * num
    for (i in 1:n) {
      grads[i, ] = apply(sweep(-ydata, 2, -ydata[i, ]) * 
        stiffnesses[, i], 2, sum)
    }
    gains = ((gains + 0.2) * abs(sign(grads) != sign(incs)) + 
      gains * 0.8 * abs(sign(grads) == sign(incs)))
    gains[gains < min_gain] = min_gain
    incs = momentum * incs - epsilon * (gains * grads)
    ydata = ydata + incs
    ydata = sweep(ydata, 2, apply(ydata, 2, mean))
    if (iter == mom_switch_iter) 
      momentum = final_momentum
    if (iter == 100 && is.null(initial_config)) 
      P = P/4
  }
  ydata
}














2. 手写 python版的t-SNE 
https://www.plob.org/article/15978.html
https://nlml.github.io/in-raw-numpy/in-raw-numpy-t-sne/
tsne.py代码: https://github.com/nlml/tsne_raw/blob/master/src/tsne.py


(2) python包: sklearn
https://www.deeplearn.me/2137.html









3.js 版本的t-SNE
(1) 项目实例 https://cs.stanford.edu/people/karpathy/tsnejs/csvdemo.html
代码 https://github.com/karpathy/tsnejs







========================================
UMAP 简介
----------------------------------------
1. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction
https://arxiv.org/abs/1802.03426

UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.

(1) 原始论文
py参考实现: https://github.com/lmcinnes/umap
文档: https://umap-learn.readthedocs.io/en/latest/


视频讲解: https://www.bilibili.com/video/BV1a4411R76S?from=search&seid=12484312964074038519
	RP-trees + NN-descent
	SGD + negative sampling
	UMAP speeed up over t-SNE 
	UMAP可以用于有监督的聚类
#





(2) 生物学应用
Published: 03 December 2018
Dimensionality reduction for visualizing single-cell data using UMAP
https://www.nature.com/articles/nbt.4314


2)Yeast: rticle|Open Access|Published: 24 March 2020
Dimensionality reduction by UMAP to visualize physical and genetic interactions










========================================
****** 广告 ******
----------------------------------------




========================================
临床预测模型 速成班
----------------------------------------
#################
###day1
#################

一、快速掌握预测模型构建神器STATA\R软件
1、熟练应用SPSS,手把手让你快速学会STATA\R，无需相关统计软件的前期知识。
2、全程实战数据现场演示特征筛选、变量选择中三大法宝（逐步法、最优子集、Lasso）；模型评价中的三大法宝（AIC\BIC\Adjr2）。
3、发放STATA和R软件全套代码（形成2张Table，5张Figure），学员无需编程基础，对号入座，极大降低统计软件的学习成本。

二、反复练习掌握Liner\Logistic\Cox回归构建预测模型的策略
1、实战经验告知你不同类别预测模型构建的关键点、侧重点、闪光点、以问题为导向的案例式教学，帮你快速厘清建模时的哑变量、交互作用、调整作用、混杂因素、独立作用、阈值作用……




#################
### day2
#################
一、实战临床预测模型-验证部分实战课
1、快速实战体会什么是内部验证和外部验证？模型的可重复性Reproducibility，Cross-Validation、Bootstrap 验证等方法？如何体现模型的外推性（generalizability）？
2、2个以上模型比较的套路流程与模型改进思路

二、实战经验分析二分类结局（logistic回归）与time-to-event数据（COX回归）的similar与dissimilar
1、模型改进验证特点
2、快速理解NRI\IDI等最新评价指标




#################
### day3
#################
一、来自实战的数据，手把手的教你结构化的研究方式，利用STATA、R编程，每一个学员都能亲自独立完成基本的临床预测模型研究。（DCA分析结果、Nomogram图的制作代码等）

二、临床预测模型国际指南（TRIPOD）详细解读

三、既往学员的案例示范讨论，经典预测模型高分文章解析，回复审稿人问题的技巧，国际上喜欢刊登临床预测模型文章的期刊分析，开拓你的眼界……




ref:
https://mp.weixin.qq.com/s?__biz=MjM5MDc0NDU2NQ==&mid=2650376120&idx=1&sn=bac5fdce4914355189e2dadd4b239360




========================================
|-- 文章实例：解决的问题，及用到的机器学习方法
----------------------------------------



1.[4.856, 2019] 核磁图像提高宫颈癌淋巴结转移的诊断效果

Radiother Oncol. 2019 Jun 25;138:141-148. doi: 10.1016/j.radonc.2019.04.035. [Epub ahead of print]
Radiomics analysis of magnetic resonance imaging improves diagnostic performance of lymph node metastasis in patients with cervical cancer.

1. Department of Medical Imaging, Henan Provincial People's Hospital, Zhengzhou, China; Zhengzhou University People's Hospital, Zhengzhou, China; Henan University People's Hospital, Zhengzhou, China.


A total of 189 cervical cancer patients were divided into a training cohort (n = 126) and a validation cohort (n = 63). For each patient, we extracted radiomic features from intratumoral and peritumoral tissues on sagittal T2WI and axial apparent diffusion coefficient (ADC) maps. Afterward, the radiomic features associated with LNM status were selected by univariate ROC testing and logistic regression with the least absolute shrinkage and selection operator (LASSO) penalty in the training cohort. Based on the selected features, a support vector machine (SVM) model was established to predict LNM status. To further improve the diagnostic performance, a decision tree which combines the radiomics model with clinical factors was built.

主要方法: ROC testing + logistic regression+LASSO;
SVM;
decision tree;





2. [3.303,2019] 甲状腺超声，预测滤泡性甲状腺癌的远端转移
J Clin Med. 2020 Jul 8;9(7). pii: E2156. doi: 10.3390/jcm9072156.
Radiomics Based on Thyroid Ultrasound Can Predict Distant Metastasis of Follicular Thyroid Carcinoma.

1. Department of Radiology, Kangbuk Samsung Hospital, Sungkyunkwan University School of Medicine, Seoul 03181, Korea.

A radiomics signature was generated using the least absolute shrinkage and selection operator and was used to train a support vector machine (SVM) classifier in five-fold cross-validation. The SVM classifier showed an area under the curve (AUC) of 0.90 on average on the test folds. 

主要方法： LASSO + SVM; AUC;






3. [2.687,2019IF] 对早期宫颈癌盆腔淋巴结转移的术前预测
Eur J Radiol. 2019 May;114:128-135. doi: 10.1016/j.ejrad.2019.01.003. Epub 2019 Mar 20.
Preoperative prediction of pelvic lymph nodes metastasis in early-stage cervical cancer using radiomics nomogram developed based on T2-weighted MRI and diffusion-weighted imaging.

1. Department of Medical Imaging, First Affiliated Hospital of Xi'an Jiaotong University, No.277, West Yanta Road, Xi'an, 710061, Shaanxi, People's Republic of China; Department of Radiology, Shaanxi Provincial People's Hospital, Xi'an, Shaanxi, 710068, People's Republic of China.


//Radiomics features extracted from T2WI and DWI were selected by least absolute shrinkage and selection operation regression for further radimoics signature calculation. The discrimination of this radiomics signature for PLN metastasis was then assessed using a support vector machine (SVM) model. 

主要方法： LASSO + SVM;






4.[2.327,2019IF] 蛋白的细胞亚定位

J Theor Biol. 2018 Nov 16. pii: S0022-5193(18)30565-4. doi: 10.1016/j.jtbi.2018.11.012. [Epub ahead of print]
Identification of protein subcellular localization via integrating evolutionary and physicochemical information into Chou's general PseAAC.
Shen Y1, Tang J2, Guo F3.
Author information:
1. School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Yaguan Road, Jinnan District, Tianjin, P.R.China. Electronic address: shenyinan@tju.edu.cn.

// In this paper, we propose a multi-kernel SVM to predict subcellular localization of both multi-location and single-location proteins. 

First, we make use of the evolutionary information extracted from position specific scoring matrix (PSSM) and physicochemical properties of proteins, by Chou's general PseAAC and other efficient functions. 

Then, we propose a multi-kernel support vector machine (SVM) model to identify multi-label protein subcellular localization. As a result, our method has a good performance on predicting subcellular localization of proteins. It achieves an average precision of 0.7065 and 0.6889 on two human datasets, respectively.

主要方法: 多核SVM;







5. [4.101,2019IF] 非侵袭直肠癌的核磁成像，评估5个术后指标

Eur Radiol. 2018 Nov 9. doi: 10.1007/s00330-018-5763-x. [Epub ahead of print]
Preoperative radiomic signature based on multiparametric magnetic resonance imaging for noninvasive evaluation of biological characteristics in rectal cancer.
Meng X1, Xia W2, Xie P1, Zhang R 2, Li W1, Wang M2, Xiong F1, Liu Y2, Fan X3, Xie Y1, Wan X4, Zhu K5, Shan H6, Wang L7, Gao X8.
Author information:
1. Department of Radiology, Sixth Affiliated Hospital of Sun Yat-sen University, Guangzhou, 510655, China.

//Radiomic features were extracted from MP-MRI and then refined for reproducibility and redundancy. The refined features were investigated for usefulness in building radiomic signatures by using two feature-ranking methods (MRMR and WLCX) and three classifiers (RF, SVM, and LASSO). Multivariable logistic regression was used to build an integrated evaluation model combining radiomic signatures and clinical characteristics. The performance was evaluated using an independent validation dataset comprising 148 patients.

//The MRMR and LASSO regression produced the best-performing radiomic signatures for 

主要方法: MRMR + LASSO;





6. [2.826,2019IF] 转移性肾细胞肾癌中的VEGF和TKI

Pathol Oncol Res. 2017 Sep 29. doi: 10.1007/s12253-017-0323-2. [Epub ahead of print]
Development of Response Classifier for Vascular Endothelial Growth Factor Receptor (VEGFR)-Tyrosine Kinase Inhibitor (TKI) in Metastatic Renal Cell Carcinoma.
Go H1, Kang MJ1, Kim PJ2, Lee JL 3, Park JY4, Park JM1, Ro JY5, Cho YM6.
Author information:

1
Department of Pathology, Asan Medical Center, University of Ulsan College of Medicine, 88, Olympic-ro 43-gil, Songpa-gu, Seoul, 05505, Republic of Korea.

//Clinicolaboratory-histopathological data, 41 gene mutations, 20 protein expression levels and 1733 miRNA expression levels were compared between clinical benefit and non-benefit groups. The classifier was built using support vector machine (SVM). Seventy-three patients were clinical benefit group, and 28 patients were clinical non-benefit group. 

主要方法: SVM; simple decision tree





7. [3.632,2019IF]
	
Developing a new radiomics-based CT image marker to detect lymph node metastasis among cervical cancer patients
Comput Methods Programs Biomed. 2020 Sep 16;197:105759. doi: 10.1016/j.cmpb.2020.105759. Online ahead of print.

1 School of Electrical and Computer Engineering, University of Oklahoma, Norman, OK, 73019, USA.





8.[7.971, 2019IF] （临床预测）整合3’UTR来建立模型，6基因构成，识别可手术的三阴性乳腺癌的腋窝淋巴结转移风险。

Wang, L., et al. (2019). "Integrative 3' Untranslated Region-Based Model to Identify Patients with Low Risk of Axillary Lymph Node Metastasis in Operable Triple-Negative Breast Cancer." Oncologist 24(1): 22-30.

We evaluated 3' untranslated region (3'UTR) profiles using microarray data of TNBC from two Gene Expression Omnibus datasets. Samples from GSE31519 were divided into training set (n = 164) and validation set (n = 163), and GSE76275 was used to construct testing set (n = 164). 

We built a six-member 3'UTR panel (ADD2, COL1A1, APOL2, IL21R, PKP2, and EIF4G3) using an elastic net model to estimate the risk of lymph node metastasis (LNM). 

A combinatorial analysis of the 3'UTR panel and tumor size yielded an accuracy of 97.2%, 100%, and 100% in training, validation, and testing set, respectively.






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------




