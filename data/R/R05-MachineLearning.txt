R与机器学习

目标: 
	掌握回归: 线性回归(lm)、多元回归，非线性回归
	聚类: K-means, KNN, 层次聚类, SOM 聚类;
        单细胞聚类: Louvain, Leiden, 
	分类: SVM, lm, 
	降维等机器学习的基本算法: 线性降维(PCA, NMF, LDA?)，非线性降维(tNSE, UMAP)
	研究深度学习算法: CNN, RNN, 


我的机器学习代码库: 
https://github.com/DawnEve/ML_MachineLearning/
http://ml.biomooc.com


R 算法/机器学习: https://github.com/TheAlgorithms/R
	https://github.com/TheAlgorithms
R NN 神经网络: https://github.com/ncornwell/R



看不懂: pyMC vs. stan  贝叶斯深度学习框架 + 马尔科夫链蒙特卡洛采样工具拟合算法库。




========================================
常用R脚本 及 R ML优秀参考资料
----------------------------------------
GC含量统计：http://www.biotrainee.com/forum.php?mod=viewthread&tid=625&page=2#pid1904


1. pdf经典书籍





2. 网站资料和课程

R机器学习教程 https://bradleyboehmke.github.io/HOML/

关于统计和算法的网站: 
UCLA 统计学院: https://stats.idre.ucla.edu/r/dae/logit-regression/


sklearn: https://qinqianshan.com/machine_learning/sklearn/


Dr. Shirin Elsinghorst: Biologist turned Bioinformatician turned Data Scientist
https://shirinsplayground.netlify.app/categories/machine-learning/



机器学习算法的基本原理-附Python和R语言代码 https://juejin.cn/post/6844903688520073223



R机器学习(电子书) https://bradleyboehmke.github.io/HOML/





3. 视频资料








========================================
|-- 模型评估与选择：敏感性（Sensitivity）/真阳性率，特异性（Specificity）/真阴性率，及ROC曲线
----------------------------------------
https://blog.csdn.net/quiet_girl/article/details/70830796
见 周志华老师的西瓜书的第2章：模型评估与选择


1. 基础定义
(1) 描述
真 true, T, 1;
假 false, F, 0;

对于一系列检测，已知的真假是真实值，预测/检测给出真假分类为预测值。
那么统计已知值与预测值的四个象限中的观测数，填下表。

默认使用R语言的table()函数画四线表时，False是0，排在前面。
从左到右：
true negative: 检测为F，真实也为F。
false positive: 检测为T，而真实为F。
false negative: 检测为F，而真实为T。
true positive: 检测为T，真实也为T。

                预测分类 
		     假F/0     真T/1      
真实 假F/0   TN(A)     FP(B)
真实 真T/1   FN(C)     TP(D)

注: 这里的ABCD可能和其他地方的不一致，但是TN等是一致的。


混淆矩阵
True Positive(真正，TP)：将正类预测为正类数
True Negative(真负，TN)：将负类预测为负类数
False Positive(假正，FP)：将负类预测为正类数误报 (Type I error)
False Negative(假负，FN)：将正类预测为负类数→漏报 (Type II error)





(2) 
敏感性（Sensitivity）实际上就是预测正确的1占真正的1的百分比；又叫真阳性率。
特异性（Specificity）实际上就是预测正确的0占真正的0的百分比；又叫真阴性率。


公式

敏感性，真值中预测出真值的百分比。(相对于的总体是 真实值class=1的部分)
sens = TP / (TP + FN) =D/(C+D)

特异性，假值中预测出为假值的百分比。()
spec = TN / (TN + FP) =A/(A+B)


(3) 还经常用到的几个指标为：
漏诊率 又称为假阴性率 = 1-灵敏性
误诊率 又称为假阳性率 = 1-特异性


阴性预测能力，NPV=A/(A+C)，预测正确的0占预测为0的百分比；
阳性预测能力，PPV=D/(B+D)，预测正确的1占预测为1的百分比；
准确性，accuracy=(A+D)/(A+B+C+D)，不管1还是0，只要预测正确的占全部的百分比。



https://zhuanlan.zhihu.com/p/49895905
http://www.creighton.edu/fileadmin/user/HSL/docs/ref/Searching_-_Recall_Precision.pdf

Precision：被我们的算法选为positive的数据中，有多少真的是positive的？
	precision = TP /(TP+FP) =D/(B+D) //就是阳性预测能力；相对于所有预测为真的总体(class=1的列)。

Recall: 实际应该为Positive的数据中，多少被我们选为了Positive？
	recall=TP/(TP+FN)=D/(D+C)=sens //就是 敏感性sens。相对于真实值为真的总体(class=1的行)。
	召回率 与 灵敏度 是一样的。

Accuracy: 所有数据中，我们正确分类了多少？
	accuracy=(TP+TN)/(TP+FN+TN+FP)=(D+A)/(A+B+C+D)

F1 score 又是干什么的？
	F1 score= 2*TP / (2*TP +FP +FN) = 2*precision*recall/(precision + recall)

F1 scores就是模型的准确率和召回率的调和平均数。






(4) 实例
任何诊断试验都必须具备灵敏度、特异度这两大基本特性，缺一不可。

对于某一诊断试剂而言，可以通过调整cut-off值来提高灵敏度或特异度，但是两者不能同时都提高，灵敏度提高，必然会导致特异度降低；反之提高特异度，必然会导致灵敏度降低。


这其实可以很形象地理解：以临床检测血糖为一个很粗糙的例子，血糖值越大当然就是代表糖尿病可能性越高，cut-off值可理解为多少数值的血糖浓度为糖尿病的判定标准（在实际工作中，还需考虑上临床实际情况，在这个例子中暂且不涉及临床意义，以便于理解）。当cut-off值增大的时候，真阴性率（特异度）肯定会增大，但同时假阴性率（漏诊率）的提高，临床意义就是容易把糖尿病患者疏漏判断为正常人，并且真阳性率（灵敏度）也会下降；当cut-off值减小的时候，真阳性率（灵敏度）会增大，但同时假阳性率（误诊率）也会提高，临床意义就是容易把正常人错判为糖尿病患者，并且真阴性率（特异度）也会下降。

由此可见，确定合适的cut-off值，是与临床实际情况、产品的灵敏度与特异度，息息相关。通常cut-off值的确定与验证，会采用ROC曲线得出。



1) 测试 table():
> table(mtcars$cyl)
 4  6  8 
11  7 14 
> table(mtcars$cyl, mtcars$carb)
  1 2 3 4 6 8
4 5 6 0 0 0 0
6 2 0 0 4 1 0
8 0 4 3 6 0 1
二维时，第一个参数竖着写。
按习惯，真实值竖着写，就是第一个参数。



2) 糖尿病尿糖诊断模型 - 模拟数据
set.seed(2021)
normal=rnorm(500, 100,20)
treat=rnorm(1500, 170,20)

dat1=data.frame(
  group=c(rep("normal", length(normal) ), rep("treat", length(treat)) ),
  val=c(normal, treat)
)
#dat1


test=function(cutoff, verbose=T){
  if(verbose==T){
    # 频率才表示曲线下面积为1
    hist(normal, n=50, freq=F, col="#FF000088", xlim=c(40, 250),
         mgp=c(2,1,0), xlab="Readout", main="Histogram")
    #add=T 叠加到上一个hist图上
    hist(treat, n=50, freq=F, col="#0000FF88", add=T)
    # 图例
    legend("topleft", legend = c("normal", "patient"), fill=c("#FF000088", "#0000FF88"), bty="n")
    # cut off 竖线
    abline(v=cutoff, col="red", lty=2, lwd=2)
  }
  
  # 四格表
  tb=table( 
    c( rep(F, length(normal)), rep(T, length(treat)) ),  #真实值，第一组都是没病，第二组都有病
    c(normal, treat)>cutoff  #诊断认为，超过这个值就是得病
  );
  if(verbose==T){ print(tb) }
  #敏感性，真阳性率
  sens=tb[2,2] / (tb[2,2] + tb[2,1]); sens #0.6946667
  
  #特异性，真阴性率
  spec=tb[1,1] / (tb[1,1] +tb[1,2]); spec #0.998
  
  return(c(cutoff, sens, spec))
}
# 真阳性(是阳性，诊断出来是阳性)， 真阴性(是阴性，诊断出来是阴性)。
test(60) #60 1.000 0.016
test(100, F) #100 0.9993333 0.49 
test(120) #120 0.9946667 0.8360000
# 病人组(紫色峰)几乎都大于这条线，紫色峰在线左侧的百分比是99.4%，是灵敏度sens/真阳性。
# 健康人(红色峰)一大半小于这条线，红色峰在左侧的百分比是83.6%，是特异性spec/真阴性。

test(150) #150 0.850 0.994 
test(200) #200 0.074 1.000
test(230) #230 0.00266 1.00



3) 画 ROC 曲线，求 auc

# ROC 曲线
df2=NULL;
for(i in seq(min( c(normal, treat) ), max( c(normal, treat) ), 0.5) ){
  rs=test(i, F)
  df2=rbind(df2, data.frame(
    cutoff=rs[1],
    sens=rs[2],
    spec=rs[3]
  ))
}
dim(df2)
head(df2)
plot(x=1-df2$spec, df2$sens, type="l", col="red",
     xlab="1-spec", ylab="sens", main="ROC curve")
abline(a=0, b=1, col="grey")



区域大于0.7就是比较好了，当然太大就要怀疑一下了，比如0.9以上，过于优秀了。
曲线下面积怎么计算呢？
类似积分的方式，计算曲线下每2个相邻点之间小矩形的面积和。

# 曲线下面积
plot(x=1-df2$spec, df2$sens, type="o", col="red", cex=0.4,
     xlab="1-spec", ylab="sens", main="ROC curve")
abline(a=0, b=1, col="grey")

df2$x=1-df2$spec
df2=df2[order(df2$x),]

auc=0
for(i in 2:nrow(df2)){
  auc = auc + (df2[i,"x"] - df2[i-1,"x"]) * df2$sens[i]
}
auc #0.9916507






4) 通过 ROC 曲线确定最佳 cutoff 值

怎么计算cutoff值呢？
- ROC曲线中最靠近左上角的点就是cut-off值。
- 根据各个点的灵敏度和特异性，计算 (灵敏度+特异性-1)取值最大的点作为cut-off值。

> df2$val=df2$sens+df2$spec
> df2[which(df2$val==max(df2$val)),]
    cutoff  sens  spec     x  val
186 138.4857 0.948 0.972 0.028 1.92

复查一下图形，这几个值确实位于两个峰最中间交错的位置。
> test(138.4857)







========================================
|-- ROC曲线的意义及绘制: ROCR包和pROC包
----------------------------------------

AUC为ROC曲线下方的面积。一般AUC大于0.75就能够说明模型是比较合理的了。


1. ROC曲线的意义
http://www.cnblogs.com/emanlee/archive/2011/05/29/2062280.html

　　ROC曲线指受试者工作特征曲线(receiver operating characteristic curve), 是反映敏感性和特异性连续变量的综合指标,是用构图法揭示敏感性和特异性的相互关系，它通过将连续变量设定出多个不同的临界值，从而计算出一系列敏感性和特异性，再以敏感性为纵坐标、（1-特异性）为横坐标绘制成曲线，曲线下面积越大，诊断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为敏感性和特异性均较高的临界值. 　　
SPSS统计软件包的10.0版本有ROC曲线的统计功能。ROC曲线真阳性率为纵坐标,假阳性率为横坐标,在座标上由无数个临界值求出的无数对真阳性率和假阳性率作图构成,计算ROC曲线下面积AUCROC来评价诊断效率。




2. 示例

可以使用ROC曲线的定义，直接用原生R画，例子见上一章节糖尿病模拟数据示例； 


(1)library(ROCR)
data(ROCR.simple)
pred <- prediction(ROCR.simple$predictions, ROCR.simple$labels)
perf <- performance(pred,"tpr","fpr")
str(perf)

##AUC值,ROC曲线下面积为AUC，用来评价分类器的综合性能，该数值取0-1之间，越大越好。
#https://blog.csdn.net/Hellolijunshy/article/details/79991385
auc <- performance(pred,'auc');auc_value=auc@y.values[[1]]
auc_value=round(auc_value,2)
auc_value #0.84

plot(perf,colorize=TRUE,main=paste0('AUC=',auc_value))
abline(a=c(0,0),b=c(1,1),col="grey")


(2)#用ggplot2画图
plotdata=data.frame(
  x=unlist(perf@x.values),
  y=unlist(perf@y.values)
)
g=ggplot(plotdata, aes(x,y, color=x))+
  geom_path(size=1)+
  labs(x="False positive rate",y="True positive rate",title="ROC Curve")+
  scale_color_gradient(name="False positive rate", low="blue", high="red")+
  theme(plot.title=element_text(face="bold",size=10))
g
#



(3).
library(pROC)
roc_curve=roc(pre_test$Y~probability)
str(roc_curve)
plot(1-roc_curve$specificities, roc_curve$sensitivities,type="o",
     xlab="1-specificities",ylab="sensitivities",main="ROC Curve")
abline(a=0,b=1,col="gray")
auc2=roc_curve$auc
text(0.5,0.4,paste("AUC: ", round(auc2, digits=2)), col="blue")
#









========================================
|-- R语言k折交叉验证 k fold C.V.
----------------------------------------
# 原理分析 https://blog.csdn.net/Tanya_girl/article/details/50221797

library(caret)
#caret包中有createFolds函数，如
training=iris
set.seed(7)
folds<-createFolds(y=training$Species,k=10)
#这里参数y是训练数据集label,k是几折交叉验证
#我们可以看看这里输出是什么
folds
#可以看出来folds有10个元素

#可见folds里每个元素是原来label长度的十分之一，存的是label的id随机的十分之一
#可以写个for循环进行交叉验证
for(i in 1:10){
  train_cv<-training[-folds[[i]],]
  test_cv<-training[folds[[i]],]
  
  print(i)
  print(train_cv)
  print(test_cv)
}


# 完整代码这里：https://blog.csdn.net/tiaaaaa/article/details/58116346




========================================
|-- 用R对数据进行随机抽样
----------------------------------------
#去掉一类，变成两类
x=iris[which(iris$Species!="virginica"),];
x$Species=factor(x$Species)
str(x);dim(x)

#生成抽样序列
set.seed(100)
ind=sample(2,nrow(x),replace=TRUE,prob=c(0.8,0.2));ind

#取80%作为训练集，20为测试集
x_train=x[ind==1,];dim(x_train)
x_test=x[ind==2,];dim(x_test) #去掉标签




========================================
AIC（赤池信息准则）和BIC（贝叶斯信息准则）是常用的模型选择和评估方法 //todo
----------------------------------------

1.最优模型选择准则：AIC和BIC，用于 logistic 模型变量筛选

aic和bic经常被用作为逻辑回归模型通过逐步回归的方法来筛选变量的标准，
当然广义上来说，标准也可以是ks，用了ks的话可能也不算是逐步回归了；

(1) 定义
赤池信息准则（Akaike Information Criterion，AIC）和贝叶斯信息准则（Bayesian Information Criterion，BIC）

(2) 适用情况
统计学里（应该是）这两个标准有不同的假设，AIC的假设是不存在最优模型，BIC的假设是存在最优模型。
根据假设，可以发现这两个标准的原理。AIC是为了寻找现模型和最优模型的最短距离，BIC是求得到最优模型的最大后验概率。

AIC用于判断哪个模型最适合用于预测(best used for prediction)，AIC也是越低越好。
BIC用于判断哪个模型拟合最好(best fit)，BIC越低越好。

要点：
    best predict → AIC
    best fit → BIC
    二者越小越好




2. 计算方法
计算每个模型的均方根误差（RMSE），然后根据RMSE计算AIC和BIC的值。

(1) 先回顾一下R2和调整R2。

首先是 R2 = RSS / TSS 

在多元线性回归中，R2不太适合作为模型拟合优度的衡量标准。这是因为当模型中加入自变量时，R2会增加或保持不变，但不会减少。

在多元回归中使用R2的问题包括：
- R2不能提供关于这些系数是否具有统计学意义的信息。
- R2不能提供关于估计系数和预测是否存在偏差的信息。
- R2无法判断模型拟合是否良好。一个好的模型可能具有较低的R2。

其次是 调整 R2 = 1 - [RSS/(n-k-1)] / [TSS/(n-1)]

二者的关系可以写为：
    adj-R2 = 1 - (n-1)/(n-k-1) * (1-R2)

对于k≥1，则 R2严格大于 调整R2。此外，调整R2可能是负数，而R2的最小值为零。

- 若新增加的变量的t值的绝对值大于1，则调整R2会增加
- 若新增加的变量的t值的绝对值小于1，则调整R2会减少

注意，这里的表述并没有说是1%或5%的水平显著之类。

当加入自变量时，由于R2和调整R2都可能增加，面临模型过拟合风险。但AIC和BIC可以帮助比较模型质量并确定最精简的模型。




(2) AIC用来评估解释相同因变量的模型集合
表达式为：AIC = n*ln(SSE/n) + 2(k-1)

AIC统计量取决于样本量(n)、模型中自变量的个数(k)和模型的平方和误差(SSE)。多元回归的一个目标是在不添加无关的自变量的情况下推导出最佳拟合模型。AIC是模型简约性的度量，因此AIC越低表示模型拟合越好。2(k + 1)是在模型中添加自变量的惩罚。


(3) 而BIC（或SBC）允许具有相同因变量的模型进行比较
表达式为：BIC = n*ln(SSE/n) + ln(n)(k+1)

与AIC相比，BIC评估模型中有更多参数的惩罚更大，所以它倾向于选择小而简洁的模型。这是因为ln(n)大于2，即使样本容量很小。


(4) 用途
从实践上讲，如果将模型用于预测，则采用AIC方法，但如果要求最佳拟合优度则采用BIC方法。单独考虑这些衡量的价值是没有意义的，在一组模型中AIC或BIC的相对值才重要。












2. 疑似AI生成的答案

在统计学和机器学习中，AIC（赤池信息准则）和BIC（贝叶斯信息准则）是常用的模型选择和评估方法。它们的主要思想是根据模型拟合数据的程度来评估模型的优劣。通常情况下，AIC和BIC的值越小，模型的性能就越好。本文将详细介绍AIC和BIC的原理、应用以及注意事项。

首先，让我们了解一下AIC和BIC的背景。AIC和BIC是由日本统计学家赤池弘次提出的一种模型选择和评估方法。AIC的目标是在假设模型拟合数据的过程中找到最优模型，而BIC则是在假设数据来自独立同分布的样本中，根据贝叶斯定理来选择最优模型。虽然两者在计算上有一些差异，但它们的基本思想是一致的，即值越小，模型性能越好。

接下来，我们将详细解释AIC和BIC的原理。AIC是通过比较不同模型对数据的拟合程度来评估模型的优劣。具体来说，它计算了模型对数据的预测误差，并将这个误差加权平均以考虑模型复杂度。BIC的计算方式与AIC类似，但它是基于贝叶斯定理的，考虑了模型复杂度和样本容量对模型的影响。在实际应用中，我们通常会根据AIC或BIC的值来选择最优模型。

为了更好地理解AIC和BIC的应用，我们可以通过一个案例进行分析。假设我们有三个模型，分别是线性回归模型、二次回归模型和三次回归模型。我们可以通过在训练集上进行交叉验证来计算每个模型的均方根误差（RMSE），然后根据RMSE计算AIC和BIC的值。根据这些值，我们可以选择最优模型。

在使用AIC和BIC时，我们需要注意一些事项。首先，我们需要确保数据是独立同分布的，否则计算结果将不准确。其次，我们需要考虑模型的复杂度。如果模型过于复杂，可能会导致过拟合，而如果模型过于简单，则可能会导致欠拟合。因此，我们需要根据实际情况来选择最优模型。

总之，AIC和BIC是评估模型性能的重要方法。在实际应用中，我们通常会根据AIC和BIC的值来选择最优模型。但是，我们需要确保数据是独立同分布的，并根据实际情况来选择最优模型。















ref:
https://zhuanlan.zhihu.com/p/366895163
https://zhuanlan.zhihu.com/p/293315874
https://zhuanlan.zhihu.com/p/572708522 公式




========================================
SVM 分类 //todo
----------------------------------------







========================================
SVR: 支持向量回归 //todo 
----------------------------------------




========================================
R包计算各种距离：vegan::vegdist()函数 计算距离矩阵/相异性(Dissimilarity Indices for Community Ecologists)
----------------------------------------
1、基本介绍
vegan包中的vegdist()函数是用于计算生态学研究中样本间距离或相异性的函数。该函数主要用于计算样本之间的距离矩阵，以便进行后续的聚类分析(Cluster Analysis)、非参数多样性测试(Non-parametric Diversity Testing)、主坐标分析(PCoA分析)等。

* 目的：计算样本之间的距离矩阵，用于比较它们之间的相似性或差异性。
* 输入：通常是一个数据矩阵，其中行代表样本，列代表各个特征或物种的相对丰度。
* 输出：距离矩阵，其中每个元素表示两个样本之间的距离或相异性。






2、基本语法及参数解释
vegdist(x, method="bray", binary=FALSE, diag=FALSE, upper=FALSE,
        na.rm = FALSE, ...) 

参数解释：
x：数据矩阵，其中行代表样本，列代表各个特征或物种的相对丰度。

method：距离计算方法。这是一个字符串参数，用于指定计算距离的方法。常见的方法包括："manhattan", "euclidean", "canberra", "clark", "bray", "kulczynski", "jaccard", "gower", "altGower", "morisita", "horn", "mountford", "raup", "binomial", "chao", "cao", "mahalanobis", "chisq", "chord", "hellinger", "aitchison", 或 "robust.aitchison"

binary：逻辑值，用于指定是否计算二元指数。
	如果为TRUE，则使用二元数据计算距离；
	如果为FALSE，则使用定量数据计算距离。默认值为FALSE。

diag：逻辑值，用于指定是否计算对角线上的距离。
	如果为TRUE，则对角线上的距离将被计算并返回到结果中；
	如果为FALSE，则对角线上的距离将被忽略。默认值为FALSE。

upper：逻辑值，用于指定是否返回距离矩阵的上三角部分。
	如果为TRUE，则返回距离矩阵的上三角部分（不包括对角线）；
	如果为FALSE，则返回完整的距离矩阵。默认值为FALSE。

na.rm：逻辑值，用于指定是否删除包含NA值的行或列。
	如果为TRUE，则在计算距离之前将删除包含NA值的行或列；
	如果为FALSE，则保留包含NA值的行或列。默认值为FALSE。

...：其他参数，用于传递给底层的距离计算函数。可以根据具体需要传递其他参数。





3、举例
library(vegan)

# 计算欧氏距离
dist_matrix <- vegdist(data_matrix, method = "euclidean")

# 计算Bray-Curtis距离
dist_matrix <- vegdist(data_matrix, method = "bray")

# 计算Jaccard距离
dist_matrix <- vegdist(data_matrix, method = "jaccard")

# 其他距离方法参考文档以获得更多选项
注：后面几个参数不常用，data_matrix和 method为必填项。

注意事项：

使用前确保已经安装并加载了vegan包。
不同的距离方法适用于不同的数据类型和分析目的，选择适合你研究的距离方法。
可以通过查看函数文档或使用?vegdist命令来获取更多信息和参数选项。





4、method部分补充 
这些是在生态学和生物统计学中常用的不同距离或相似性指数，用于衡量样本或群落之间的差异或相似性。
以下是对这些指数的简要解释：

1.Manhattan Distance：曼哈顿距离，也称为城市街区距离，是两个点之间的绝对距离。
2.Euclidean Distance：欧氏距离，是两个点之间的直线距离。
3.Canberra Distance：坎贝拉距离，用于衡量两个样本之间的相似性，特别适用于高维数据。
4.Clark Distance：克拉克距离，一种特定的相似性指数，用于衡量样本之间的差异。
5.Bray-Curtis Distance：Bray-Curtis距离，用于衡量两个样本之间的相对差异，通常用于生态学中的物种组成数据。
6.Kulczynski Distance：Kulczynski距离，用于比较两个样本之间的相似性或差异。
7.Jaccard Distance：Jaccard距离，用于衡量两个样本之间的相似性，通常用于存在-缺失数据。
8.Gower Distance：Gower距离，用于比较样本之间的相似性，适用于混合数据类型。
9.AltGower Distance：另一种形式的Gower距离，适用于混合数据类型。
10.Morisita Distance：Morisita距离，用于衡量样本之间的相似性，特别适用于物种丰富度数据。
11.Horn Distance：Horn距离，用于比较两个样本之间的相似性或差异。
12.Mountford Distance：Mountford距离，一种用于存在-缺失数据的相似性指数。
13.Raup Distance：Raup距离，用于比较两个样本之间的相似性或差异，特别适用于存在-缺失数据。
14.Binomial Distance：二项式距离，用于衡量两个样本之间的相似性。
15.Chao Distance：Chao距离，一种用于处理样本大小不同的相似性指数。
16.Cao Distance：Cao距离，用于比较两个样本之间的相似性或差异。
17.Mahalanobis Distance：马氏距离，用于衡量多维数据中两个样本之间的距离。
18.Chisq Distance：卡方距离，用于比较两个样本之间的相似性或差异。
19.Chord Distance：Chord距离，用于计算两个样本之间的相似性。
20.Hellinger Distance：Hellinger距离，用于比较两个样本之间的相似性，通常用于频率数据。
21.Aitchison Distance：Aitchison距离，用于处理正组成数据的距离度量。
22.Robust Aitchison Distance：鲁棒Aitchison距离，用于处理非负数据（包括零值）的距离度量，相比标准Aitchison距离更鲁棒。

这些距离或相似性指数根据不同的数据类型、性质和分析目的，选择合适的指数进行数据分析和解释。



ref:
https://blog.csdn.net/qq_55230229/article/details/137965196
https://rdrr.io/cran/vegan/man/vegdist.html








========================================
主成分分析 (PCA, principal component analysis)
----------------------------------------
主成分分析 (PCA, principal component analysis)是一种数学降维方法, 利用正交变换 (orthogonal transformation)把一系列可能线性相关的变量转换为一组线性不相关的新变量，也称为主成分，从而利用新变量在更小的维度下展示数据的特征。


主成分是原有变量的线性组合，其数目不多于原始变量。组合之后，相当于我们获得了一批新的观测数据，这些数据的含义不同于原有数据，但包含了之前数据的大部分特征，并且有着较低的维度，便于进一步的分析。

在空间上，PCA可以理解为把原始数据投射到一个新的坐标系统，第一主成分为第一坐标轴，它的含义代表了原始数据中多个变量经过某种变换得到的新变量的变化区间；第二成分为第二坐标轴，代表了原始数据中多个变量经过某种变换得到的第二个新变量的变化区间。这样我们把利用原始数据解释样品的差异转变为利用新变量解释样品的差异。

这种投射方式会有很多，为了最大限度保留对原始数据的解释，一般会用最大方差理论或最小损失理论，使得第一主成分有着最大的方差或变异数 (就是说其能尽量多的解释原始数据的差异)；随后的每一个主成分都与前面的主成分正交，且有着仅次于前一主成分的最大方差 (正交简单的理解就是两个主成分空间夹角为90°，两者之间无线性关联，从而完成去冗余操作)。


1. 主成分分析的意义
(1) 简化运算。
假如某些基因如持家基因在所有样本中表达都一样，它们对于解释样本的差异也没有意义。

应用PCA就可以在尽量多的保持变量所包含的信息又能维持尽量少的变量数目，帮助简化运算和结果解释。


(2)去除数据噪音。

比如说我们在样品的制备过程中，由于不完全一致的操作，导致样品的状态有细微的改变，从而造成一些持家基因也发生了相应的变化，但变化幅度远小于核心基因 (一般认为噪音的方差小于信息的方差）。而PCA在降维的过程中滤去了这些变化幅度较小的噪音变化，增大了数据的信噪比。

(3)利用散点图实现多维数据可视化。

在上面的表达谱分析中，假如我们有1个基因，可以在线性层面对样本进行分类；如果我们有2个基因，可以在一个平面对样本进行分类；如果我们有3个基因，可以在一个立体空间对样本进行分类；如果有更多的基因，比如说n个，那么每个样品就是n维空间的一个点，则很难在图形上展示样品的分类关系。利用PCA分析，我们可以选取贡献最大的2个或3个主成分作为数据代表用以可视化。这比直接选取三个表达变化最大的基因更能反映样品之间的差异。（利用Pearson相关系数对样品进行聚类在样品数目比较少时是一个解决办法）


(4) 发现隐性相关变量。

我们在合并冗余原始变量得到主成分过程中，会发现某些原始变量对同一主成分有着相似的贡献，也就是说这些变量之间存在着某种相关性，为相关变量。同时也可以获得这些变量对主成分的贡献程度。对基因表达数据可以理解为发现了存在协同或拮抗关系的基因。






2. PCA 实现原理 及注意事项
(1)
PCA分析不是简单的选取2-3个变化最大的基因，而是先把原始的变量做一个评估，计算各个变量各自的变异度（方差）和两两变量的相关度（协方差），得到一个协方差矩阵。

在这个协方差矩阵中，对角线的值为每个变量的方差，其他值为每两个变量的协方差。随后对原变量的协方差矩阵对角化处理，即求解其特征值和特征向量。原变量与特征向量的乘积（对原始变量的线性组合）即为新变量（回顾下线性代数中的矩阵乘法）；新变量的协方差矩阵为对角协方差矩阵且对角线上的方差由大到小排列；然后从新变量中选择信息量最丰富的也就是方差最大的前2个或3个新变量，也即是主成分用于可视化。


(2) 不同预处理对PCA结果的影响
1) 不同标准化的本质在于研究者认为，是数值的量度本身重要，还是数值的变化重要。
2) 数值的量度重要，则选择原始数据或log转换；
3) 数值的变化重要，则选择scale。




3. PCA注意事项

(1) 一般说来，在PCA之前原始数据需要中心化（centering，数值减去平均值）。中心化的方法很多，除了平均值中心化（mean-centering）外，还包括其它更稳健的方法，比如中位数中心化等。

(2)除了中心化以外，定标 (Scale, 数值除以标准差) 也是数据前处理中需要考虑的一点。如果数据没有定标，则原始数据中方差大的变量对主成分的贡献会很大。数据的方差与其量级成指数关系，比如一组数据(1,2,3,4)的方差是1.67，而(10,20,30,40)的方差就是167,数据变大10倍，方差放大了100倍。

(3)但是定标(scale)可能会有一些负面效果，因为定标后变量之间的权重就是变得相同。如果我们的变量中有噪音的话，我们就在无形中把噪音和信息的权重变得相同，但PCA本身无法区分信号和噪音。在这样的情形下，我们就不必做定标。

(4)一般而言，对于度量单位不同的指标或是取值范围彼此差异非常大的指标不直接由其协方差矩阵出发进行主成分分析，而应该考虑对数据的标准化。比如度量单位不同，有万人、万吨、万元、亿元，而数据间的差异性也非常大，小则几十大则几万，因此在用协方差矩阵求解主成分时存在协方差矩阵中数据的差异性很大。

在后面提取主成分时发现，只提取了一个主成分，而此时并不能将所有的变量都解释到，这就没有真正起到降维的作用。此时就需要对数据进行定标(scale)，这样提取的主成分可以覆盖更多的变量，这就实现主成分分析的最终目的。

但是对原始数据进行标准化后更倾向于使得各个指标的作用在主成分分析构成中相等。对于数据取值范围不大或是度量单位相同的指标进行标准化处理后，其主成分分析的结果与仍由协方差矩阵出发求得的结果有较大区别。这是因为对数据标准化的过程实际上就是抹杀原有变量离散程度差异的过程，标准化后方差均为1，而实际上方差是对数据信息的重要概括形式，也就是说，对原始数据进行标准化后抹杀了一部分重要信息，因此才使得标准化后各变量在主成分构成中的作用趋于相等。因此，对同度量或是取值范围在同量级的数据还是直接使用非定标数据求解主成分为宜。


(5)中心化和定标都会受数据中离群值（outliers）或者数据不均匀（比如数据被分为若干个小组）的影响，应该用更稳健的中心化和定标方法。


(6)PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。








3. PCA 分析和可视化 示例

########### step1 PCA
library(caret)

# By default, prcomp will centralized the data using mean.
# Normalize data for PCA by dividing each data by column standard deviation.
# Often, we would normalize data.
# Only when we care about the real number changes other than the trends,
# `scale` can be set to TRUE. 
# We will show the differences of scaling and un-scaling effects.

iris.pca=prcomp(iris[,1:4])
iris.pca2=prcomp(iris[,1:4], scale=T)

iris.pca
#Standard deviations (1, .., p=4):
#[1] 2.0562689 0.4926162 0.2796596 0.1543862
#Rotation (n x k) = (4 x 4):
#                 PC1         PC2         PC3        PC4
#Sepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872
#Sepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231
#Petal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390
#Petal.Width   0.35828920  0.07548102 -0.54583143  0.7536574

print(str(iris.pca))





########### step2 PCA结果提取和可视化神器

library(factoextra)

#1. 碎石图展示每个主成分的贡献
## 如果需要保持，去掉下面第1,3行的注释
#pdf("1.pdf")
fviz_eig(iris.pca, addlabels = TRUE)
#dev.off()

# 2. PCA样品聚类信息展示
# repel=T，自动调整文本位置
fviz_pca_ind(iris.pca, repel=T) +coord_fixed(1) # 关键的增加


# 3. 根据样品分组上色
fviz_pca_ind( iris.pca, col.ind=iris[,5], 
             mean.point=F, addEllipses = T, legend.title="Groups")

# 4. 增加不同属性的椭圆
# “convex”: plot convex hull of a set o points.
# “confidence”: plot confidence ellipses around group mean points as 
#     the function coord.ellipse() [in FactoMineR].
# “t”: assumes a multivariate t-distribution.
# “norm”: assumes a multivariate normal distribution.
# “euclid”: draws a circle with the radius equal to level, 
#    representing the euclidean distance from the center. 
#    This ellipse probably won’t appear circular unless coord_fixed() is applied.

# 根据分组上色并绘制95%置信区间
fviz_pca_ind(iris.pca, col.ind=iris[,5], 
             mean.point=F, addEllipses = T, legend.title="Groups", 
             ellipse.type="confidence", ellipse.level=0.95)
#

#5. 展示贡献最大的变量 (基因)
# 1） 展示与主坐标轴的相关性大于0.99的变量 (具体数字自己调整)
# Visualize variable with cos2 >= 0.99
fviz_pca_var(iris.pca, select.var = list(cos2 = 0.60), 
             repel=T, 
             col.var = "cos2", 
             geom.var = c("arrow", "text") )

#2)展示与主坐标轴最相关的10个变量
# Top 10 active variables with the highest cos2
fviz_pca_var(iris.pca, select.var= list(cos2 = 10), repel=T, col.var = "contrib")

# 3）展示自己关心的变量（基因）与主坐标轴的相关性分布
# Select by names
# 这里选择的是MAD值最大的几个基因
#name <- list(name = c("FN1", "DCN", "CEMIP","CCDC80","IGFBP5","COL1A1","GREM1"))
#fviz_pca_var(pca, select.var = name)


#6. biPLot同时展示样本分组和关键基因
# top 5 contributing individuals and variable
fviz_pca_biplot(iris.pca,
                fill.ind=iris[,5],
                palette="joo",
                addEllipses = T, 
                ellipse.type="confidence",
                ellipse.level=0.95,
                mean.point=F,
                col.var="contrib",
                gradient.cols = "RdYlBu",
                select.var = list(contrib = 10),
                ggtheme = theme_minimal())
#




========================================
|-- PCA 分析与可视化 示例
----------------------------------------
自己封装的函数

my.doCPA=function(rna_logcpm, celltype, center = F){
    test.pr<-prcomp( t(rna_logcpm), center = center) 

    library(factoextra)
    print(fviz_eig(test.pr, addlabels = TRUE))

    ind <- get_pca_ind(test.pr) #这是看观察值，也就是行。
    
    df=as.data.frame(ind$coord)
    df$celltype=celltype
    return(df)
}

df=my.doCPA(rnaM.logcpm, cellInfo[colnames(rnaM.logcpm), ]$cellType)

# 画出每个点的位置，使用前几个PC
library(ggplot2)
ggplot(df, aes(Dim.1, Dim.2, color=celltype) )+geom_point()+theme_bw()
ggplot(df, aes(Dim.1, Dim.3, color=celltype) )+geom_point()+theme_bw()
ggplot(df, aes(Dim.2, Dim.3, color=celltype) )+geom_point()+theme_bw()






1. 精简版
################
#PCA对列进行重组，重组成最多和原来一样多个PC成分。所以输入df竖着是基因，一行一个细胞。
test2=iris[,1:4] 
test=t(apply(test2, 1, function(x){x/sum(x)})) #按行标准化
test=as.data.frame(test)
head(test)

#做主成分分析
test.pr<-prcomp(test, center = F) 
test.pr
# 查看荷载
# 如果变量列数多于观察行数，则需要使用Q-PCA类型，函数时prcomp(), 载荷 test.pr$rotation
##  'princomp' can only be used with more units than variables

test.pr$rotation #PC1=-0.77sL-0.42sW-0.44pL-0.13pW, 后面以此类推
#princomp(test,cor=TRUE)时
#summary(test.pr,loadings=TRUE) 
#loading是逻辑变量，当 loading=TRUE 时表示显示 loading 的内容

biplot(test.pr)  #画出数据关于主成分的散点图和原坐标在主成分下的方向

#碎石图，显示每个PC占了多少百分比
screeplot(test.pr,type="lines")
#每个PC占了多少比例，另一种展示
library(factoextra)
fviz_eig(test.pr, addlabels = TRUE)

# 预测
p <- predict(test.pr) # 就是每行的PC维度上的值，可用于可视化观测值的位置
head(p)

# 查看每个PC的坐标
##var <- get_pca_var(test.pr)
ind <- get_pca_ind(test.pr) #这是看观察值，也就是行。
head(ind$coord) #和上文的head(p)结果一样

# 画出每个点的位置，使用前几个PC
library(ggplot2)

df=as.data.frame(ind$coord)
df$type=iris[,5]
head(df)

ggplot(df, aes(Dim.1, Dim.2, color=type) )+geom_point()+theme_bw()
ggplot(df, aes(Dim.1, Dim.3, color=type) )+geom_point()+theme_bw()
ggplot(df, aes(Dim.2, Dim.3, color=type) )+geom_point()+theme_bw()

# 三维可视化 https://www.jianshu.com/p/d248b7b54a4b


ref:
关于 R 中 princomp 和 prcomp 的 区别 https://blog.csdn.net/lfz_carlos/article/details/48442091






2.
http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/


library("FactoMineR")
res.pca <- PCA(iris[, 1:4], graph = FALSE)
res.pca

## **Results for the Principal Component Analysis (PCA)**
## The analysis was performed on 23 individuals, described by 10 variables
## *The results are available in the following objects:
## 
##    name               description                          
## 1  "$eig"             "eigenvalues"                        
## 2  "$var"             "results for the variables"          
## 3  "$var$coord"       "coord. for the variables"           
## 4  "$var$cor"         "correlations variables - dimensions"
## 5  "$var$cos2"        "cos2 for the variables"             
## 6  "$var$contrib"     "contributions of the variables"     
## 7  "$ind"             "results for the individuals"        
## 8  "$ind$coord"       "coord. for the individuals"         
## 9  "$ind$cos2"        "cos2 for the individuals"           
## 10 "$ind$contrib"     "contributions of the individuals"   
## 11 "$call"            "summary statistics"                 
## 12 "$call$centre"     "mean of the variables"              
## 13 "$call$ecart.type" "standard error of the variables"    
## 14 "$call$row.w"      "weights for the individuals"        
## 15 "$call$col.w"      "weights for the variables"


plot(res.pca)
summary(res.pca)



(2) PCA的可视化
No matter what function you decide to use [stats::prcomp(), FactoMiner::PCA(), ade4::dudi.pca(), ExPosition::epPCA()], you can easily extract and visualize the results of PCA using R functions provided in the factoextra R package.



These functions include:

- get_eigenvalue(res.pca): Extract the eigenvalues/variances of principal components
- fviz_eig(res.pca): Visualize the eigenvalues
- get_pca_ind(res.pca), get_pca_var(res.pca): Extract the results for individuals and variables, respectively.
- fviz_pca_ind(res.pca), fviz_pca_var(res.pca): Visualize the results individuals and variables, respectively.
- fviz_pca_biplot(res.pca): Make a biplot of individuals and variables.

In the next sections, we’ll illustrate each of these functions.


##1. Eigenvalues / Variances
library("factoextra")
eig.val <- get_eigenvalue(res.pca)
eig.val

#       eigenvalue variance.percent cumulative.variance.percent
# Dim.1 2.91849782       72.9624454                    72.96245
# Dim.2 0.91403047       22.8507618                    95.81321
# Dim.3 0.14675688        3.6689219                    99.48213
# Dim.4 0.02071484        0.5178709                   100.00000


## 2. 每个PC解释多少百分比，柱状图
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))



var <- get_pca_var(res.pca)
var

## Principal Component Analysis Results for variables
##  ======== ========== ========= ========= ======== =======
##   Name       Description                                    
## 1 "$coord"   "Coordinates for the variables"                
## 2 "$cor"     "Correlations between variables and dimensions"
## 3 "$cos2"    "Cos2 for the variables"                       
## 4 "$contrib" "contributions of the variables"



# Coordinates
head(var$coord)
# Cos2: quality on the factore map
head(var$cos2)
# Contributions to the principal components
head(var$contrib)


## Quality of representation
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)


# Total cos2 of variables on Dim.1 and Dim.2
fviz_cos2(res.pca, choice = "var", axes = 1:2)



## 
# Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )
#

# Change the transparency by cos2 values
fviz_pca_var(res.pca, alpha.var = "cos2")


##################
# PC 的贡献程度
library("corrplot")
corrplot(var$contrib, is.corr=FALSE)    



# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)


#The total contribution to PC1 and PC2 is obtained with the following R code:
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)



fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)
#

# Change the transparency by contrib values
fviz_pca_var(res.pca, alpha.var = "contrib")



############
# 可视化每个元素
fviz_pca_ind(res.pca)

#Like variables, it’s also possible to color individuals by their cos2 values:
fviz_pca_ind(res.pca, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping (slow if many points)
)



#You can also change the point size according the cos2 of the corresponding individuals:
fviz_pca_ind(res.pca, pointsize = "cos2", 
             pointshape = 21, fill = "#E7B800",
             repel = TRUE # Avoid text overlapping (slow if many points)
             )

#

#ind.p <- fviz_pca_ind(iris.pca, geom = "point", col.ind = iris$Species)
ggpubr::ggpar(ind.p,
              title = "Principal Component Analysis",
              subtitle = "Iris data set",
              caption = "Source: factoextra",
              xlab = "PC1", ylab = "PC2",
              legend.title = "Species", legend.position = "top",
              ggtheme = theme_gray(), palette = "jco"
)


##
ref:
https://www.jianshu.com/p/6e413420407a





========================================
|-- PCA 的原理和手工实现 //todo
----------------------------------------
1. 输入数据描述
行为gene，列为sample。
   s1 s2 s3 s4 s5 s6
g1  
g2
g3

一个基因时，一维图。
2个基因时，2-D图。
3个基因时，3-D图。
再多，就超出人类直觉了。
可以使用降维方法，把高维数据降低到2维，方便可视化查看。


(1) 考虑2个基因的情况，
先中心化: 按行计算每个基因的mean，每行减去该行的mean。

然后有一条过原点的直线，角度是变量，0-360。每个点到投射到该直线上。然后计算投射点到原点的距离和sum of squared distances (SSD).
旋转直线，直到SSD最大的方向，这个折线就叫做第一个主成分(简称PC 1)。

对于数据中的某一个点A(x,y)，在直线y=tan(Theta)*x上的投影为B(a,b).
B在直线上:  b=k*x
OB垂直于AB: (OB, AB)=0, (a,b)*(x-a, y-b)=0
解方程得
a=(x+k*y)/(1+k^2)
b=a*k


使用R实现这一过程: 找到2维中使SSD最大的直线夹角，并可视化PC1及各点在PC1上的的投影。
dt=t(iris[, c(1,3)])
dim(dt) #行为2个基因，列为样品名
# 中心化
dt=as.data.frame(t(apply(dt, 1, function(x){
  x-mean(x)
})))
head(dt[,1]) #-0.7433333 -2.3580000
result=list(angle=0,ssd=0)
ssds=c()
for(angle in seq(0,180, 0.1) ){
  #线的角度
  st=tan(angle/180*3.14159265358979)
  ssd=0
  for(i in 1:ncol(dt)){
    x=dt[1,i];  y=dt[2,i]; #数据点
    a=(x+st*y)/(1+st^2); b=a*st; #映射到直线上
    ssd=ssd+(a^2+b^2)
  }
  ssds=c(ssds, ssd)
  if(result[['ssd']]<ssd){
    result[['ssd']]=ssd
    result[['angle']]=angle
  }
}
plot(seq(0,180, 0.1), ssds)
result
# $angle
#[1] 66.8
#
#$ssd
#[1] 545.6228

# 可视化第PC 1的方向
st=tan(result[['angle']]/180*3.14159265358979) #斜率
plot(t(dt), col=iris[,5], pch=20, xlim=c(-3,3.5), ylim=c(-3,3.5))
x1=seq(-4,4,0.5); lines(x1, x1*st,type='l') #add line
#
ssd=0
outer=c() #残差
for(i in 1:ncol(dt)){
  x=dt[1,i];  y=dt[2,i]; #数据点
  a=(x+st*y)/(1+st^2); b=a*st; #映射到直线上
  # 画映射的点和垂线
  points(a,b,type='p',col='#F600FF55', pch=20)
  lines(c(x,a), c(y,b), type='l',col='grey',lty=2)
  outer=c(outer, sum( c(x-a,y-b)*c(1,st) ) )
  ssd=ssd+(a^2+b^2)
}
hist(outer, n=50) # 确实都垂直
ssd #545.6228


> st
[1] 2.333175
也就意味着在PC1上，gene1每增加1，gene2就增加2.33，也就是说基因2贡献的比较多。
也可以按照做菜的角度理解PC1，1份gene1, 2.33份gene2。

数学上，我们说PC1是gene1和gene2的线性组合。
而且数学上 PC1^2=1^2 + (2.33)^2
(1^2 + st^2)**0.5=2.538445

而使用SVD分解时，PC会被标准化到1。所以，只需要每个分量都除以斜边
gene1份数: 1/2.538445=0.393942
gene2份数: st/2.538445=0.9191355

> 0.393942**2+0.9191355**2
[1] 1


新名词: 这个单位向量(0.393942, 0.9191355)又叫做 Singular Vector or the "Eigenvector" for PC1;

SS(distance for PC1)=Eigenvalue for PC1
sqrt(Eigenvalue for PC1)=Sigular Value for PC1

PCA calls the ss(distances) for the best fit line the Eigenvalue for PC1, 
and the square root of the Eigenvalue for PC1 is called the Sigular Value for PC1.




2) 第二个PC
由于这是二维的，第二个PC要在PC1垂直的方向上找，所以，只有一个选择。
PC2方向 (-0.9191355, 0.393942)，这是PC2的奇异向量或特征向量。
系数也叫PC2的loading Scores: PC2需要 -0.9191355 份gene1, 和 0.393942 份 gene2，


b1=c(0.393942, 0.9191355);b1
b2=c(-0.9191355, 0.393942);b2
sum(b1*b2) #[1] 0
# 正交向量，乘积为0
x1=1; plot(NULL, xlim=c(-x1,x1), ylim=c(-x1,x1))
lines( c(0,b1[1]), c(0,b1[2]), col='red')
lines( c(0,b2[1]), c(0,b2[2]), col='blue')





3) 画最后的PC图时，旋转PC1至水平。相当于对每个点做坐标变换。
坐标轴的旋转矩阵: https://blog.csdn.net/TOM_00001/article/details/62054572
一个坐标(x,y)逆时针旋转alpha角度到(x',y'), 满足[x',y']T=A.[x,y]T时，坐标变换矩阵 A=
|cosA, -sinA|
|sinA, cosA|


## R 代码实现: 对坐标轴旋转到x和PC1一致，各个点的位置和在x轴(PC1)上的投影。
PI=3.1415926535897932
dt=t(iris[, c(1,3)])
dim(dt) #行为2个基因，列为样品名
# 中心化
dt=as.data.frame(t(apply(dt, 1, function(x){
  x-mean(x)
})))
head(dt[,1])
dim(dt)
# 旋转坐标轴
x0=0.3939420
y0=0.9191355
angle=-atan(y0/x0) #弧度制的角度，相当于顺时针旋转，是角度的负方向
dt2=apply(t(dt), 1, function(xy){
  c( sum(c(cos(angle), -sin(angle)) *xy), 
     sum(c(sin(angle), cos(angle))*xy ) )
})
head(dt2[,1])
dim(dt2)
# 检验旋转前后，点到原点的距离。应该是不变。
apply(dt[, 1:10],2,function(x){ sum(x**2) } )
apply(dt2[, 1:10],2,function(x){ sum(x**2)} )
plot(t(dt2), col=iris[,5], xlim=c(-3, 4), ylim=c(-3, 4))
# 可视化第PC 1的方向
st=tan(0/180*3.14159265358979) #线的角度
x1=seq(-3,5,0.5); lines(x1, x1*st,type='l') #add line
#
ssd=0
for(i in 1:ncol(dt)){
  x=dt2[1,i];  y=dt2[2,i]; #数据点
  a=(x+st*y)/(1+st^2); b=a*st; #映射到直线上
  # 画映射的点和垂线
  points(a,b,type='p',col='#F600FF55', pch=20)
  lines(c(x,a), c(y,b), type='l',col='grey',lty=2)
  ssd=ssd+(a^2+b^2)
}
ssd #[1] 545.6228 和旋转前一致。



备注: this is how PCA is done using Sinular Value Decomposition(SVD).






4) 变异程度和碎石图
还记得特征值吗？
SS(distances for PC1) = Eigenvalue for PC1
SS(distances for PC2) = Eigenvalue for PC2

除以样本数n-1，得到variation:
SS(distances for PC1)/(n-1) = variation for PC1
SS(distances for PC2)/(n-1) = variation for PC2



## R: 计算variation for PC1-2
ssd/(ncol(dt)-1) #3.661898 

# 可视化第PC 2的方向
st=tan(90/180*3.14159265358979) #线的角度
x1=seq(-3,15,0.5); lines(x1, x1*st,type='l') #add line
ssd2=0
for(i in 1:ncol(dt)){
  x=dt2[1,i];  y=dt2[2,i]; #数据点
  a=(x+st*y)/(1+st^2); b=a*st; #映射到直线上
  # 画映射的点和垂线
  points(a,b,type='p',col='#F600FF22', pch=20)
  lines(c(x,a), c(y,b), type='l',col='grey',lty=2)
  ssd2=ssd2+(a^2+b^2)
}
ssd2
ssd2/(ncol(dt)-1) #0.1400731 


这也就是说2个PC总的变异是 3.661898 +0.1400731=3.801971
也就是说
PC1占了变异的 3.661898/3.801971=0.9631578=96.3%
PC2占了变异的 0.1400731/3.801971=0.03684223=3.6%

碎石图 Scree plot 是描述每个PC解释多少变异的:
barplot(c(0.9631578, 0.03684223))







(3) 考虑3个基因的情况
过原点，找ssd最大的方向，为PC1。
然后过原点，在PC1的垂直平面上寻找ssd最大的方向，得到PC2；
然后过原点，和PC1和PC2都垂直的方向就是PC3;

如果有更多基因，就有同样数目的PC。一般来说，PC个数=min(变量数，观测值数);

每个PC都是3个基因的线性组合，基因前的3个系数是单位向量。


碎石图 Scree plot: 找到所有的PC之后，就可以使用特征值(比如，ss(distances))来计算每个PC解释的变异百分比，画碎石图。

如果前2个PC已经解释了>90%，就可以使用2个PC画图了。
旋转坐标轴，使PC1水平，PC2竖直，画出新坐标系统下的各个点。




(4) 4个基因时，已经无法直接画图了。
但是还可以找到4个PC，以及画碎石图。

注: 如果最后结果差不多，











ref:
https://www.cnblogs.com/leezx/p/6120302.html
https://www.bilibili.com/video/av35447404?from=search&seid=1532616759578227057
https://www.jianshu.com/p/101064609904

关于PCA的解释 https://blog.csdn.net/lanyuelvyun/article/details/82384179




========================================
|-- PCA与PCoA
----------------------------------------
其实不论是PCoA还是PCA图均是用散点图来展示结果PCoA和PCA的结果，PCoA和PCA准确来讲是数据降维分析方法。


1. 区别
(1)主成分分析（Principal components analysis，PCA）是一种统计分析、简化数据集的方法。

它利用正交变换来对一系列可能相关的变量的观测值进行线性变换，从而投影为一系列线性不相关变量的值，这些不相关变量称为主成分（Principal Components）。

具体地，主成分可以看做一个线性方程，其包含一系列线性系数来指示投影方向（如图）。PCA对原始数据的正则化或预处理敏感（相对缩放）。PCA是最简单的以特征量分析多元统计分布的方法。通常情况下，这种运算可以被看作是揭露数据的内部结构，从而更好的解释数据的变量的方法。



(2) 主坐标分析（Principal Coordinates Analysis，PCoA），即经典多维标度（Classical multidimensional scaling），用于研究数据间的相似性。

PCoA与PCA都是降低数据维度的方法，但是差异在在于PCA是基于原始矩阵，而PCoA是基于通过原始矩阵计算出的距离矩阵。

因此，PCA是尽力保留数据中的变异让点的位置不改动，而PCoA是尽力保证原本的距离关系不发生改变，也就是使得原始数据间点的距离与投影中即结果中各点之间的距离尽可能相关（如图）。

我的理解：
- PCA线性降维，可以看各个亚群的距离；
- 而PCoA则可以看各个点之间的距离。



2.如何进行PCA和PCoA分析
R中有很多包都提供了PCA和PCoA,比如常用的ade4包。本文将基于该包进行PCA和PCoA的分析，数据是自带的deug，该数据提供了104个学生9门课程的成绩（见截图）和综合评定。综合评定有以下几个等级：A+,A,B,B-,C-,D。
让我们通过PCA和PCoA来看一看这样的综合评定是否合理，是否确实依据这9门课把这104个学生合理分配到不同组（每个等级一个组）。


(1) PCA分析及作图
前文已经介绍了PCA是基于原始数据，所以直接进行PCA分析即可。

library(ade4)
library(ggplot2)
library(RColorBrewer)
data(deug)

#PCA分析
pca<- dudi.pca(deug$tab, scal = FALSE, center = deug$cent, scan = FALSE)

#坐标轴解释量（前两轴）
pca_eig <- (pca$eig)[1:2] / sum(pca$eig)

#提取样本点坐标（前两轴）
sample_site <- data.frame({pca$li})[1:2]
sample_site$names <- rownames(sample_site)
names(sample_site)[1:2] <- c('PCA1', 'PCA2')

#以最终成绩作为分组
sample_site$level<-factor(deug$result,levels=c('A+','A','B','B-','C-','D'))

library(ggplot2)

pca_plot <- ggplot(sample_site, aes(PCA1, PCA2,color=level)) +
  theme_classic()+#去掉背景框
  geom_vline(xintercept = 0, color = 'gray', size = 0.4) + 
  geom_hline(yintercept = 0, color = 'gray', size = 0.4) +
  geom_point(size = 1.5)+  #可在这里修改点的透明度、大小
  scale_color_manual(values = brewer.pal(6,"Set2")) + #可在这里修改点的颜色
  theme(panel.grid = element_line(color = 'gray', linetype = 2, size = 0.1), 
        panel.background = element_rect(color = 'black', fill = 'transparent'), 
        legend.title=element_blank()
  )+
  labs(x = paste('PCA1: ', round(100 * pca_eig[1], 2), '%'), y = paste('PCA2: ', round(100 * pca_eig[2], 2), '%')) 

pca_plot

整体看起来还不错，就是B-和C-的学生似乎难以区分。






(2) PCoA分析及作图

library(ade4)
library(ggplot2)
library(RColorBrewer)
library(vegan)#用于计算距离
data(deug)
tab<-deug$tab
tab.dist<-vegdist(tab,method='euclidean')#基于euclidean距离
pcoa<- dudi.pco(tab.dist, scan = FALSE,nf=3)

#坐标轴解释量（前两轴）
pcoa_eig <- (pcoa$eig)[1:2] / sum(pcoa$eig)

#提取样本点坐标（前两轴）
sample_site <- data.frame({pcoa$li})[1:2]
sample_site$names <- rownames(sample_site)
names(sample_site)[1:2] <- c('PCoA1', 'PCoA2')

#以最终成绩作为分组
sample_site$level<-factor(deug$result,levels=c('A+','A','B','B-','C-','D'))

library(ggplot2)

pcoa_plot <- ggplot(sample_site, aes(PCoA1, PCoA2,color=level)) +
  theme_classic()+#去掉背景框
  geom_vline(xintercept = 0, color = 'gray', size = 0.4) + 
  geom_hline(yintercept = 0, color = 'gray', size = 0.4) +
  geom_point(size = 1.5)+  #可在这里修改点的透明度、大小
  scale_color_manual(values = brewer.pal(6,"Set2")) + #可在这里修改点的颜色
  theme(panel.grid = element_line(color = 'gray', linetype = 2, size = 0.1), 
        panel.background = element_rect(color = 'black', fill = 'transparent'), 
        legend.title=element_blank()
  )+
  labs(x = paste('PCoA1: ', round(100 * pcoa_eig[1], 2), '%'), y = paste('PCoA2: ', round(100 * pcoa_eig[2], 2), '%')) 

pcoa_plot

有时候PCA和PCoA的结果差不多，有时候某种方法能够把样本有效分开而另一种可能效果不佳，这些都要看样本数据的特性。


因为没有现成可供分享的微生物组数据，所以用了这个成绩的数据集。通常来说在微生物组的研究中，我们会根据物种丰度的文件对数据进行PCA或者PCoA分析，也是我们所说的beta-diveristy分析，根据PCA或者PCoA的结果看疾病组和对照组能否分开，以了解微生物组的总体变化情况。





ref: https://www.jianshu.com/p/a40715fa9b04




========================================
|-- 可视化实例: PCA的3d作图
----------------------------------------
#step1: 对iris做PCA分析，按列做
df.pca<-prcomp(iris[,1:4])
summary(df.pca)
str(df.pca)

#check: PC sdev, percentage
barplot(df.pca$sdev)


#check: 旋转矩阵 ?? 怎么理解? 
# 4个原变量怎么变成新变量的，线性组合系数
# PC1=0.36*SL-0.08*SW+0.85PL+0.35PW
df.pca$rotation
library(pheatmap)
pheatmap( df.pca$rotation, scale="none",
         border_color = NA,
         cluster_rows = F, cluster_cols = F,
         main="rotation matrix")


#step2: 获取PC
pca.result<-df.pca$x 
pca.result<-data.frame(pca.result)
head(pca.result)
pca.result$Species<-iris$Species

#step3: 总共数据是150，准备150个颜色和150个形状
colors0 <- c("#999999", "#E69F00", "#56B4E9")
colors <- colors0[as.numeric(pca.result$Species)]
shapes0<-16:18
shapes<-shapes0[as.numeric(pca.result$Species)]

#step4: 2d plot
library(ggplot2)
ggplot(pca.result, aes(PC1, PC2, color=Species))+
  geom_point()+theme_bw()

#step4: 3d plot
library("scatterplot3d")
s3d <- scatterplot3d(pca.result[,1:3],
                     pch = shapes,
                     color=colors,
                     angle=80, #x和y轴的夹角
                     cex.symbols = 1)
usrP=par("usr");usrP
legend(x=usrP[1],y=usrP[4]+2, #"top", 
       legend = levels(pca.result$Species),
       col = c("#999999", "#E69F00", "#56B4E9"),
       pch = c(16, 17, 18), box.col = NA,
       inset = -0.1, xpd = TRUE, horiz = TRUE)

# ref: https://zhuanlan.zhihu.com/p/375110294




========================================
logistic 回归，及odd ratio
----------------------------------------
1.
(1) Odds Ratio的定义
https://psychscenehub.com/psychpedia/odds-ratio-2/

一个模型，录取率和性别
     Admit, not admit
Male   a=7  b=3
Femail c=3  d=7

则录取频率 p=录取的/总人数 
p(M)=7/(7+3)=0.7
p(F)=3/(3+7)=0.3


发生比 odds=成功概率/失败概率
odds(M)=p/(1-p)=0.7/0.3=7/3
odds(F)=p/(1-p)=0.3/0.7=3/7


odds ratio=优势比=odds(M)/odds(F)=7/3 / (3/7)=49/9=5.4;
ad/bc=49/9




2) An odds ratio (OR) is a statistic that quantifies the strength of the association between two events, A and B. 
Odds Ratio (OR) is a measure of association between exposure and an outcome. The OR represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure.
有该暴露时的某结果发生的odds，比上没有该暴露时的结果发生的odds。

上文就是男性的录取odds，比上非男性的录取odds。

OR >1 indicates increased occurrence of event
OR <1 indicates decreased occurrence of event (protective exposure)
Look at CI and P value for statistical significance of value


3)解读
Interpretation
According to the tablet above, individuals with admited are 5.4 times more likely to be exposed to Male than those without admited.
如何解读OR: https://psychscenehub.com/psychpedia/odds-ratio-2/






(2)在研究两元分类响应变量和诸多自变量间的相互关系时，常选用logistic回归模型。

两元分类变量Y的一个结果记为“成功”，另一个结果记为“失败”，分别用1和0表示。
n个自变量(解释变量)记为X1,X2,...,Xn，在n个自变量作用下出现“成功”的条件概率记为
p=P(Y=1|X1,X2,...,Xn)，那么 logistic回归模型表示为


Z=B0+B1x1+B2x2+...+BnXn，无论Xi取值范围如何，Z总是-无穷大到+无穷大。
p=exp(Z)/( 1+exp(Z) ) ，导致p的取值范围总是0到1之间，这是logistic回归模型的合理性所在。

其中B0称为常数项或截距，B1,B2,...,Bn称为 logistic 回归模型的回归系数。
对上式做 logit 变换，logistic 回归模型可以写成下列线性形式:
logit(p)=ln( p / (1-p))=ln( exp(Z) )=Z = B0+B1x1+...+BnXn; ## p是事件发生的概率。

这样我们就可以使用线性回归模型对参数Bi进行估计了。



For example, in logistic regression the odds ratio represents the constant effect of a predictor X, on the likelihood that one outcome will occur.


推导 OR和系数的关系:
Z=ln( p / (1-p))=ln(odds);
logit(p1)-logit(p2)=ln( p1/(1-p1) / ( p2/(1-p2) ) )=ln( odds ratio )

对于logistic模型 Y~B0+B1X，X每变动1时 Odds 的比值都应该是一样的，这个比值就是 Odds Ratio;
odds=p/(1-p)=exp(Z)=exp( B0+B1X )
则 odds2/odds1=exp(B1*(X2-X1))=exp(B1), 因为X2-X1=1就是每次变动的单位1;
所以，logistic回归的各个变量的系数就是log(Odds Ratio)，求Odds Ratio只需要求exp(Bi)即可。


就像存款一样，有两种方式描述收益：一个是利率(不变)，一个是利息(随着本金变动)。
存一年，利率3%，本金100元，则利息3元；本金2000元，则利息60元；

logistic的 p 值会随着变量Xi而变动，但是 Odds Ratio 则保持恒定。








2. LOGIT REGRESSION 及odd ratio怎么求
https://stats.idre.ucla.edu/r/dae/logit-regression/

Examples

Example 1. Suppose that we are interested in the factors that influence whether a political candidate wins an election. The outcome (response) variable is binary (0/1); win or lose. The predictor variables of interest are the amount of money spent on the campaign, the amount of time spent campaigning negatively and whether or not the candidate is an incumbent.
实例1: 假设结果是输赢(0/1二分类变量)，变量包括很多...

Example 2. A researcher is interested in how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. The response variable, admit/don’t admit, is a binary variable.
实例2: 录取与否受到哪些因素影响？


(1) 读入数据
mydata <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv") #400 4
## view the first few rows of the data
head(mydata)
##   admit gre  gpa rank
## 1     0 380 3.61    3
## 2     1 660 3.67    3
## 3     1 800 4.00    1
## 4     1 640 3.19    4
## 5     0 520 2.93    4
## 6     1 760 3.00    2

# 这个可以省略
mydata$rank <- factor(mydata$rank)
str(mydata)


(2) 广义模型
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
# mylogit <- glm(admit ~ ., data = mydata, family = "binomial") #变量太多时，可以简写为.表示其他所有变量。

summary(mylogit) 
summary(mylogit)$coefficients
#                Estimate  Std. Error   z value     Pr(>|z|) ## 最后一列给出了p值
# (Intercept) -3.44954840 1.132846009 -3.045029 2.326583e-03
# gre          0.00229396 0.001091839  2.101005 3.564052e-02
# gpa          0.77701357 0.327483878  2.372677 1.765968e-02
# rank        -0.56003139 0.127136989 -4.404945 1.058109e-05

coef(mylogit)
# (Intercept)         gre         gpa        rank 
# -3.44954840  0.00229396  0.77701357 -0.56003139

#
## CIs using profiled log-likelihood
confint(mylogit) #给出置信区间

## CIs using standard errors
confint.default(mylogit)


#BiocManager::install('aod')
library(aod)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 4:6)


### odds ratios only
exp(coef(mylogit))


## odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit,level = 0.95))) # 给出OR和置信区间
#                    OR       2.5 %    97.5 %
# (Intercept) 0.03175998 0.003309497 0.2835650
# gre         1.00229659 1.000171559 1.0044714
# gpa         2.17496718 1.152082367 4.1717746
# rank        0.57119114 0.442656492 0.7294389



### 
logit(p)=ln( p / (1-p))=Z=-3.44954840 + 0.00229396*gre + 0.77701357*gpa -0.56003139*rank;
p=exp(Z)/( 1+exp(Z) )


(1)https://www.statsdirect.com/help/regression_and_correlation/logistic.htm
也认为 exp(Intercept)=Odds Ratio;
(2)https://rstudio-pubs-static.s3.amazonaws.com/182726_aef0a3092d4240f3830c2a7a9546916a.html
log odds=B0;
(3)How to interpret odds ratio in logistic regression?
https://www.researchgate.net/post/How_to_interpret_odds_ratio_in_logistic_regression
Odds Ratio=exp(Coefficient)







3. Logistic Regression
http://r-statistics.co/Logistic-Regression-With-R.html










========================================
一文读懂回归分析:概述Cox回归、岭回归、Lasso回归、ElasticNet 回归
----------------------------------------
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml

生物学中的基因很多是有共线性的，所以，后三种方法经常使用。
回归正则化方法（套索，岭和ElasticNet）在高维数据和数据集变量之间存在多重共线性的情况下运行良好。

3）Cox回归
Cox回归的因变量就有些特殊，它不经考虑结果而且考虑结果出现时间的回归模型。它用一个或多个自变量预测一个事件（死亡、失败或旧病复发）发生的时间。Cox回归的主要作用发现风险因素并用于探讨风险因素的强弱。但它的因变量必须同时有2个，一个代表状态，必须是分类变量，一个代表时间，应该是连续变量。只有同时具有这两个变量，才能用Cox回归分析。Cox回归主要用于生存资料的分析，生存资料至少有两个结局变量，一是死亡状态，是活着还是死亡；二是死亡时间，如果死亡，什么时间死亡？如果活着，从开始观察到结束时有多久了？所以有了这两个变量，就可以考虑用Cox回归分析。





9）岭回归
当数据之间存在多重共线性（自变量高度相关）时，就需要使用岭回归分析。在存在多重共线性时，尽管最小二乘法（OLS）测得的估计值不存在偏差，它们的方差也会很大，从而使得观测值与真实值相差甚远。岭回归通过给回归估计值添加一个偏差值，来降低标准误差。

岭回归要点：
1）除常数项以外，岭回归的假设与最小二乘回归相同；
2） 它收缩了相关系数的值，但没有达到零，这表明它不具有特征选择功能；
3）这是一个正则化方法，并且使用的是 L2 正则化。





13）套索回归 LASSO
与岭回归类似，套索也会对回归系数的绝对值添加一个罚值。此外，它能降低偏差并提高线性回归模型的精度。看看下面的等式：

套索回归要点：
1）除常数项以外，这种回归的假设与最小二乘回归类似；
2）它将收缩系数缩减至零（等于零），这确实有助于特征选择；
3）这是一个正则化方法，使用的是 L1 正则化；
4）如果一组预测因子是高度相关的，套索回归会选出其中一个因子并且将其它因子收缩为零。


lasso 回归就是这个意思，就是让回归系数不要太大，以免造成过度拟合（overfitting）。所以呢，lasso regression是个啥呢，就是一个回归，并且回归系数不要太大。

具体的实现方式是加了一个L1正则的惩罚项。





14）ElasticNet 回归
ElasticNet 回归是套索回归和岭回归的组合体。它会事先使用 L1 和 L2 作为正则化矩阵进行训练。当存在多个相关的特征时，Elastic-net 会很有用。岭回归一般会随机选择其中一个特征，而 Elastic-net 则会选择其中的两个。同时包含岭回归和套索回归的一个切实的优点是，ElasticNet 回归可以在循环状态下继承岭回归的一些稳定性。

ElasticNet 回归要点：
1）在高度相关变量的情况下，它会产生群体效应；
2）选择变量的数目没有限制；
3）它可以承受双重收缩。







ref:
https://www.bilibili.com/video/BV1LE411D7SW
Lasso回归（L1正则，MAP+拉普拉斯先验） https://blog.csdn.net/qq_32742009/article/details/81674021
Lasso回归总结 https://www.cnblogs.com/wmx24/p/9555219.html




========================================
|-- 实例: glmnet包做 Ridge回归、Lasso 回归、Elastic-Net Regression
----------------------------------------
1. 使用 glmnet 包

alpha=0时，只有岭回归;
alpha=1时，只有lasso回归;
lamda{ alpha(累加|variable1|) + (1-alpha)(累加|variable2|^2) }




2. 代码及详解
(1) 三种算法的结果与比较
##########
library(glmnet)
set.seed(42)

### step1 获取数据
#模拟数据
n=1000 #行
p=5000 #列
real_p=15 #真实有效的15列，其余都是背景噪音

x=matrix(rnorm(n*p), nrow=n, ncol=p)
y=apply(x[,1:real_p], 1, sum) + rnorm(n) #x的前15列，按行求和。加入噪音。
## 现在有一个向量y，需要用数据x预测向量y
# 预测方法: Ridge, Lasso, Elastic-Net Regression


### step2 分配训练集，测试集
train_rows=sample(1:n, 0.66*n) #选哪些行？随机的2/3行

x.train=x[train_rows,] #训练集
x.test=x[-train_rows,] #测试集
#
y.train=y[train_rows] #训练集结果
y.test=y[-train_rows] #测试集结果
#

### step3 开始岭归回
alpha0.fit=cv.glmnet(x.train, y.train, type.measure = 'mse',
                     alpha=0, family='gaussian')
### 参数解释:
#(1) 函数名字中的 cv 表示使用cross validation 获取最优的lambda; 默认使用 10-Fold Cross Validation;
# 和 lm(), glm() 不同，cv.glmnet()不接受 formula notation, x和y必须分别传入。
#(2) mse表示 mean squared error: 残差平方和，除以样本容量。
#  If we were applying Elastic_Net Regression to Logistic Regression, we would set this to deviance.
#(3) 因为使用Ridge Regression，所以设置 alpha=0;
#(4) family 设置为 gaussian; 如果使用Logistic Regression, 则使用 binomial;
#
## 整体解释：
# 该方程，应用岭回归罚分，使用10x交叉验证，找到最优化的lambda

### step4 开始预测
alpha0.predicted=predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx = x.test)
# 使用 predict() 函数，应用 alpha0.fit 到 测试数据集上。
# 参数解释:
# 第一个是模型
## 第二个参数s，可能是size，保存到alpha0.fit 中的最优化的lambda值对应的"the size of the penalty"
#   本例中，设置为 lambda.1se，保存在alpha0.fit中的，这导致最简单的模型(比如，模型有最少的非零参数)，
#   and within 1 standard error of the lambda that had the smallest sum.
## s也可以设置为 lambda.min，就是产生最小误差和的lambda值。
## 本例选择 lambda.1se, 因为统计学上，它和lambda.min没有差异，但是有更少的参数。
## Tips: 我认为只有 Lasso 和 Elastic Net Regression 可以去除参数... 然后呢？
## 本文为了比较的统一，都是用 lambda.1se 了。
# 第三个参数 newx 就是新数据。


## step5 计算损失函数: mean squared error
mean( (y.test - alpha0.predicted)^2 ) #14.88459
# (这里使用 预测与真实值差的平方均值)

################
# 尝试 Lasso Regression(alpha=1时)
# step3 B
alpha1.fit=cv.glmnet(x.train, y.train, type.measure = 'mse',
                     alpha=1, family='gaussian')
#step4 B
alpha1.predicted=predict(alpha1.fit, s=alpha1.fit$lambda.1se, newx = x.test)
#step5 B
mean( (y.test - alpha1.predicted)^2 ) #1.184701
# 误差比Ridge小，模型预测效果好于Ridge。


################
# 尝试 Elastic-Net Regression(alpha=(0,1)时)
# step3 C
alpha0.5.fit=cv.glmnet(x.train, y.train, type.measure = 'mse',
                     alpha=0.5, family='gaussian')
#step4 C
alpha0.5.predicted=predict(alpha0.5.fit, s=alpha0.5.fit$lambda.1se, newx = x.test)
#step5 C
mean( (y.test - alpha0.5.predicted)^2 ) #1.23797
# 误差比Lasso大，这个比较中 Lasso 赢了。

################
# 可以尝试一系列的 alpha，
list.of.fits=list()
for(i in 0:10){
  print(i) #进度条
  fit.name=paste0('alpha', i/10)
  
  list.of.fits[[fit.name]]=cv.glmnet(x.train, y.train, type.measure = 'mse',
                                     alpha=i/10, family='gaussian')
}
# 计算 mean squared errors for each fit with the testing dataset
results=data.frame()
for(i in 0:10){
  print(i) #进度条
  fit.name=paste0('alpha', i/10)
  fit=list.of.fits[[fit.name]]
  #
  predicted=predict(fit, s=fit$lambda.1se, newx = x.test)
  mse=mean( (y.test - predicted)^2 )
  
  temp=data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
  results=rbind(results, temp)
}
dim(results) #11 3
head(results)
#   alpha       mse fit.name
#1    0.0 14.918840   alpha0
#2    0.1  2.256924 alpha0.1
#3    0.2  1.472927 alpha0.2
plot(mse~alpha, results, type="o")
#结论 对这组数据，alpha=1时，Lasso回归的结果还是最好的。




(2) 确定好拟合参数，接下来解决问题
#####
# part1
fit2<-glmnet(x.train, y.train, alpha=1,family='gaussian')
plot(fit2, xvar = "lambda", label = TRUE)
print(fit2)


# part2
cv.fit <- cv.glmnet( x.train, y.train, alpha=1);

# pic1 可视化拟合效果，看纵坐标MSE什么时候最小(红色虚线标出)
plot(cv.fit)
abline(v=log(c(cv.fit$lambda.min)),lty=2, col="red")

# pic2 看有几个截距，就有几个非0参数
plot(fit2, xvar = "lambda", label = TRUE)
abline(v=log(c(cv.fit$lambda.min,cv.fit$lambda.1se)),lty=2)
abline(v=log(c(cv.fit$lambda.min)),lty=2, col="red")

# 查看每个参数的系数
coef(fit2, s = cv.fit$lambda.1se )



# part 3 预测结果图
pfit = predict(fit2, x.test, s = cv.fit$lambda.1se, type = "response")
plot(pfit, y.test)




#如果取最小值时
cv.fit$lambda.min #0.0908066
Coefficients <- coef(fit2, s = cv.fit$lambda.min)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
row.names(Coefficients)[Active.Index] #69个非0参数

minlassopred <-predict(cv.fit,newx=x.test,s=cv.fit$lambda.min) #,type="response"
minlassopred


#如果取1倍标准误内(大小和min无统计学差异)，参数最少的lambda值。【建议使用这个】
cv.fit$lambda.1se #0.1257568
Coefficients <- coef(fit2, s = cv.fit$lambda.1se)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
row.names(Coefficients)[Active.Index] #27个非0参数

selassopred <-predict(cv.fit,type="response",newx=x.test,s=cv.fit$lambda.1se)
selassopred



(3) 怎么解释2张图? 
特别是顶部数字是什么意思？我推测，是非0变量的个数。

贴吧 https://tieba.baidu.com/p/6301813616?red_tag=2818960114











========================================
|-- 实例: glmnet包做 Cox 回归 筛选变量
----------------------------------------
1. 与预后有关的文章，传统的做法一般会选择多变量cox回归，高级做法自然就是我们今天的lasso分析。
https://www.dxy.cn/bbs/newweb/pc/post/42430476

线性回归采用一个高维的线性函数来尽可能的拟合所有的数据点，最简单的想法就是最小化函数值与真实值误差的平方。

Lasso回归则是在一般线性回归基础上加入了正则项，在保证最佳拟合误差的同时，使得参数尽可能的“简单”，使得模型的泛化能力强。正则项一般采用一，二范数，使得模型更具有泛化性，同时可以解决线性回归中不可逆情况。这个时候你可能不淡定了，你是魔鬼吗？什么是正则项？？？

正则项：正则化就是通过对模型参数进行调整（数量和大小），降低模型的复杂度，以达到可以避免过拟合的效果。正则化是机器学习中的一种叫法，其它领域内叫法各不相同，统计学领域叫惩罚项，数学领域叫范数。而正则项又包括两种，即一范数和二范数，就是L1和L2范数。

重点来了：采用L1范数则是lasso 回归，L2范数则是岭回归了。那么函数有啥区别呢？如下：
	公式略。本博客暂时不支持公式。
- L1范数是所有参数绝对值之和，对应的回归方法叫做Lasso回归。
- L2范数是所有参数的平方和，对应的回归方法叫做Ridge回归，岭回归需要注意的是，正则项中的回归系数为每个自变量对应的回归系数，不包含回归常数项。


L1和L2各有优劣，
- L1是基于特征选择的方式，有多种求解方法，更加具有鲁棒性；
- L2则鲁棒性稍差，只有一种求解方式，而且不是基于特征选择的方式。

在GWAS分析中，当用多个SNP位点作为自变量时，采用基于特征选择的L1范式，不仅可以解决过拟合的问题，还可以筛选重要的SNP位点，所以lasso回归在GWAS中应用的更多一点。

我们在大多数signature文章中主要是基因挑选，自然就是今天的主题lasso cox回归，接下来我们看一下，如何采用R语言glmnet来实现。



2. 代码
library(glmnet)
#载入数据。
#包含30个基因在1000个病人样本中的表达，另一个是每个患者的生存状态和生存时间，生存时间以年为单位，如下：
data('CoxExample')
# 1000行(病人)，30列(基因)
dim(x)
x[1:10,1:5]

# 1000行(病人) 2列(生存时间、终点)
dim(y)
head(y)



# step1 整理数据(30个基因在1000个病人)
x=data.frame(x)
y=data.frame(y)

row.names(x)=paste0("patient",1:1000)
colnames(x)=paste0('gene',1:30)

row.names(y)=paste0("patient",1:1000)
colnames(y)=c('time','status')



# step2 构建生存分析
library(survival)
fit_sur=Surv(y$time, y$status)
dim(fit_sur) #1000 2
head(fit_sur) #[1] 1.76877757  0.54528404  0.04485918+ 0.85032298+ 0.61488426  0.29860939+
str(fit_sur)


# step3 通过glmnet函数中的设置family参数定义采用的算法模型，比如设置cox
fit=glmnet(as.matrix(x), fit_sur, family='cox')
fit

## 画图: 各个基因的系数，大体能看出正负
plot(fit, label=T)

# step4 Lasso回归最重要的就是选择合适的λ值，可以通过cv.glmnet函数实现
cv.fit=cv.glmnet(as.matrix(x), fit_sur, 
                #nfold=10,
                family='cox')
cv.fit
# Call:  cv.glmnet(x = as.matrix(x), y = fit_sur, family = "cox") 
# 
# Measure: Partial Likelihood Deviance 
# 
#      Lambda Measure      SE Nonzero
# min 0.01750   13.07 0.07251      15
# 1se 0.04869   13.13 0.06238      10

plot(cv.fit) #画图，能找到误差最小的lambda值的位置
# 基于该图选择最佳的λ，一般可以采用两个内置函数实现cvfit$lambda.min和 cvfit$lambda.1se 。


# step5 基因筛选，采用coef函数即可，有相应参数的gene则被保留，采用λ使用的是lambda.min
# 我感觉采用1se更好。大小和min没有统计学差异，同时做到了参数最少化
coef.1se=coef(cv.fit, s="lambda.1se")
coef.1se
# 30 x 1 sparse Matrix of class "dgCMatrix"
#                  1
# gene1   0.38108115
# gene2  -0.09838545
# gene3  -0.13898708
# gene4   0.10107014
# gene5  -0.11703684
# gene6  -0.39278773
# gene7   0.24631270
# gene8   0.03861551
# gene9   0.35114295
# gene10  0.04167588
# gene11  .        
# 第二列有数值是非点号的则代表被选择的基因。

# step6 美化lasso图，怎么标出来每个基因？
plot(fit, label=T)



#如果取1倍标准误内(大小和min无统计学差异)，参数最少的lambda值。【建议使用这个】
cv.fit$lambda.1se
Coefficients <- coef(fit, s = cv.fit$lambda.1se)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
row.names(Coefficients)[Active.Index] #10个非0参数





========================================
PCA 及 t-SNE ( t-Distributed Stochastic Neighbor Embedding (t-SNE) )
----------------------------------------
1933 PCA
1952 MDS
2000 Isomap
2002 SNE
2008 t-SNE
2014 BHtSNE
? UMAP

非线性降维方法
1969 Sammon mapping
1997 CCA
2000 Isomap
2000 LLE: locally linear embedding
2002 SNE 
2002 Laplacian Eigenmaps
2004 MVU: Maximum Variance Unfolding


如果是scRNAseq数据，还是用成熟的包比较好:  seurat in R or ScanPy in python.
They include everything from data-processing, clustering and generating plots such as these within them. It will make your life a lot easier.
- https://satijalab.org/seurat/
- https://scanpy.readthedocs.io/en/stable/


The t-SNE step may take a while depending on the size of your dataset. There are quicker options like UMAP which is also slightly better in other ways in maintaining the global architecture of the clusters. 






1. PCA 是对协方差矩阵求特征向量。最大的特征值对应的特征向量，被用来重构原始数据最重要的部分。
It does so by calculating the eigenvectors from the covariance matrix. The eigenvectors that correspond to the largest eigenvalues (the principal components) are used to reconstruct a significant fraction of the variance of the original data.

PCA之后获取的新特征或组分都是互相独立的。

PCA是线性降维。不能准确表示多维数据。
Recall the PCA objective: Project data onto a lower dimensional subspace, such that the variance is maximized.
Conclusion: Mostly preserves distances between dissimilar points;
But is that really what we want for the purpose of visualization?


实例: 高维弯曲的围巾图，投射到二维。原本弯曲而距离很近的点还能距离很近吗？






2. TNE 随机近邻插入
In contrast to PCA, SNE focused on maintaining the nearest neighbors in the lower dimensional map.

Aim is to match distributions of distances between points in high and low dimensional space via conditional probabilities;

Assume distrances in both high and low dimensional space are Gaussian-distributed;
SNE focuses on preserving the local structure of the data.


SNE是先将欧几里得距离转换为条件概率来表达点与点之间的相似度。

py 代码实现: https://github.com/DawnEve/ML_MachineLearning/blob/master/t-SNE/tSNE_demo.ipynb


- 如果一个N高维空间 xi表示第i个对象, x1,x2,...,xn;
- yi表示低维空间中的第i个对象, y1,y2,...,yn;
- 建立一个近邻矩阵，反应高维数据点之间的相似关系;
	把高维空间欧氏距离，转为反应相似度的条件概率。
- 高维空间的距离转为条件概率 Pj|i=exp(-||xi-xj||^2/ (2*sigmai^2) ) / 求和(k!i, exp(-||xi-xk||^2/ (2*sigmai^2) ) );
	距离越远，算出来的p越小; 距离越近，算出来的p越大;
	
	sigmai 和一个叫做perplexity的参数相关，perplexity可以大概的解释为每个点有几个近邻。
		perplexity与算法中使用的最近邻居的数量有关。不同的perplexity可能会导致最终结果发生巨大变化。
	sigmai 是二分法对距离矩阵搜索获得的???
	
	def p_joint(X, target_perplexity):
		"""Given a data matrix X, gives joint probabilities matrix.
		# Arguments
			X: Input data matrix.
		# Returns:
			P: Matrix with entries p_ij = joint probabilities.
		"""
		#(1)距离 Get the negative euclidian distances matrix for our data
		distances = neg_squared_euc_dists(X)
		#(2)根据距离、Perp求最优的sigmai. Find optimal sigma for each row of this distances matrix
		sigmas = find_optimal_sigmas(distances, target_perplexity)
		#(3)根据距离、sigma求相似矩阵. Calculate the probabilities based on these optimal sigmas
		p_conditional = calc_prob_matrix(distances, sigmas)
		#(4)对称化 Go from conditional to joint probabilities matrix
		P = p_conditional_to_joint(p_conditional)
		return P
	
- 低维空间的距离 Qj|i=exp(-||yi-yj||^2 ) / 求和(k!i, exp(-||yi-yk||^2 ) );
	规定 pi|i=qi|i=0;


- cost function: 移动Q点，使P和Q的KL散度(熵)最小化: C=累加(i, KL(Pi||Qi))=累加(i, 累加(j, Pj|i*log(pj|i/qj/i)));
	高维空间算出来的p是固定的，而低维空间算出来的q则每移动一次重新计算一次，怎么知道q的运动方向呢？
	if p==q, then log(1)=0;
	Penalize when p!=q:
		Large p & small q: Big penalty; 倾向于保持原始相似度大的关系。
		Small p & large q: Small penaly;
	所以，SNE能保持数据的局部结构。
	
- 梯度:
	求偏导数 dC/dyi= 2*求和(j, (yi-yj)*(pj|i-qj|i + pi|j-qi|j)) #----> 这个求导看不懂?????
		前面的 (yi-yj) 相当于弹簧 spring，后面的(pj|i-qj|i + pi|j-qi|j) 提供伸展性 Attraction/Repulsion
	有了cost function, SNE 就可以使用梯度方向做优化了。
	
- momentum term alpha(t): 除了cost function 的梯度，还有一项momentum term，来加速优化并避免局部最优条件。
- SNE 有2个问题:
	Cost function is difficult to optimize
	Crowding problem: overlapping
		How to solve crowding problems?
		Goal: dissimilar points have to be modeled as far apart as possible in the map;
#









3. t-SNE 解决了 crowding problem.
优势: t-SNE tends to preserve local structure at the same time preserving the global structure as much as possible;
而PCA保持global structure（变异最大的方向），却损失了 local structure;

tSNE 的cost函数有2个不同特性:
	- Cost function 是对称的(相对于 SNE): pi|j=pj|i, and qi|j=qj|i;
	- 低维空间的相似矩阵是使用t分布来求的。
#



具体算法细节: http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
主要步骤
- 计算相似概率：计算高低维度中该点相似性。
	计算方法就是按照高斯分布，以某点为中心，计算到其他点的距离，映射到正态分布获得一个可能性p值。
		距离越近，p越大； 距离越远，p越小。
		之后要标准化，就是防止选取的中心点不一样时导致的高斯分布sigma不同而p值差不同。
		(0.24,0.05) (0.12,0.024) 标准化后都是 (0.827, 0.172): 比较松散的类和比较紧凑的类，点的相似度一样。perplex 参数什么意思?
	然后换一个点，重新计算它到其他点的距离；使用高斯分布折算出p值。
	直到每个点为中心都计算一遍。
	两个方向的p值求平均，作为2点的最终相似度p。最后就获得了一个相似矩阵。每一行、每一列表示那个点和其他点的最终相似度p。
	由于自身与自身无关，所以自己到自己的最终相似度定义为0；
	
- 最小化高低纬点的条件概率差异，获得一个最优的低纬度空间内的点的表示。
	然后把高维空间的点随机映射到直线上;
	然后计算线性空间内的点之间的相似度，不过，这一次距离映射到相似度p时，使用t分布。
		t分布很像normal分布，但是最高点比较矮，尾巴比较高。能解决拥挤的问题。
	使用t分布，计算点两两之间的相似分数，然后scale，得到相似矩阵。
	
- 为了使条件概率和的最小化，使用梯度下降法，最小化 所有数据点的 Kullback-Leibler divergence。
	逐步移动低维空间的点，使其相似矩阵和高维空间越来越相似。
	

注: Kullback-Leibler divergence or KL divergence 是衡量一个分布和另一个期望分布的差异的方法。


简单说，tSNE 最小化两种分布的差异: 一个分布是衡量输入对象在高维空间的相似性的，一个分布是衡量低维空间插入点之间的相似性的。


(1) t分布比着正态分布，最高点更低，而尾巴更高。x在尾巴处对应着更细腻的y值变化。 
f=function(x){ 1/(3.1415926*(1+x^2)) }
x=seq(-8, 8, 0.1)
y=f(x)
plot(x, dnorm(x), type='l', main="Distribution") #normal distribution
lines(x,y, type='l', lty=3, col='red') #t-distribution
legend('topleft', lty=c(1,3), bty='n',
       col=c('black', 'red'), legend=c('Normal', "t"))
#

(2)
低维使用自由度为1的t分布(就是柯西分布): P(x)=1/[pi*(1+x^2)]
Qij=(1+||yi-yj||^2)^-1 / 累加(k!=l, (1+||yl-yk||^2)^-1)
这样，就把不相似的点最大程度的分开了。

偏导数 dC/dyi= 4*求和(j, (pij-qij)*(yi-yj)*(1+ ||yi-yj||^2 )^-1)  ## ----> 不知道这个偏导数怎么求的。原论文附录有推导。

定义 pij=(pj|i+pi|j)/(2*n)，会有更简单的梯度函数 dC/dyi=4*累加(j, (pij-qij)*(yi-yj))




(3) 局限性
1) tSNE很适合高维数据的可视化，但是不能用于降维。点之间的距离没有意义。
2) 数据有很高内在维度时不那么成功; ??
3) cost function不是凸函数! 要仔细选择最优的参数。



(4) 一个重要的参数 Perplexity 困惑度。
不同的Perp会导致不同的聚类
	Perp too small -> local variations dominate,
	Perp too large -> global change dominate.
	#
	Loosely speaking, one could say that a larger / denser dataset requires a larger perplexity.
	Typical values for the perplexity range between 5 and 50. 原始论文认为Perp[5,50]比较稳健。
#



(5) tSNE 失败的情形？
tSNE assumes local linearity(Euclidean distances between neighbors); when does this assumption fail?
1) 当数据噪音很大时，tSNE 会失败，怎么办？
Solution: 先用PCA做平滑化。然后对前几(5-50)个PC进行tSNE可视化。

2) Data with high intrinsic dimension(on a highly varying manifold)
Solution: learn a "code" - of lower dimension using an Auto-encoder;

什么是 intrinsic dimension? 
	https://eng.uber.com/intrinsic-dimension/
	https://cran.r-project.org/web/packages/intrinsicDimension/vignettes/intrinsic-dimension-estimation.html
		The intrinsic dimension of a data set is a measure of its complexity. Data sets that can be accurately described with a few parameters have low intrinsic dimension. It is expected that the performance of many machine learning algorithms is dependent on the intrinsic dimension of the data. 
#






#########
看不懂的部分:
cost 函数是什么？KL 散度，求和(p*log(p/q));
怎么移动低维空间的点？按照梯度移动; ==> 怎么求梯度/偏导数?
对矩阵求偏导数是什么概念？
	http://math.fudan.edu.cn/gdsx/KEJIAN/方向导数和梯度.pdf
#








4. 答疑Question
(1) R package "tsne" and "Rtsne" give different cell clustering results?
"tsne" is a "pure R" implementation of the t-SNE algorithm, while package "Rtsne" is an R wrapper around the fast t-SNE implementation by Van der Maaten.

A:
Apparently, there may have been a bug in the tsne package, which may or may not be related to what you are seeing. Check the discussion here to see if it applies: https://gist.github.com/mikelove/74bbf5c41010ae1dc94281cface90d32


(2) scale(perplexity)有什么作用？
大部分人在使用t-SNE时，一般都直接使用默认参数图个方便(一般perplexity的默认值是30)，如果忽视了perplexity带来的影响，有的时候遇到t-SNE可视化效果不好时，根本就不知道哪里出了问题，优化起来也就无从下手了。

那么perplexity到底是啥呢？我们可以回顾t-SNE的数学表达式，主要是和sigma这一项相关

perplexity表示了近邻的数量，例如设perplexity为2，那么就很有可能得到很多两个一对的小集群。



(3) t-SNE 分类可信吗？ 为什么多重复几次 t-SNE 结果不一样，而且有些类一会分开一会聚合？
A: t-SNE更关心的是学习维持局部结构，群间的距离并不能说明什么，而且每次跑t-SNE的结果并不完全一致。所以解决这个问题，我们只需要跑多次找出效果最好的就可以了。引起这个问题的本质原因是，t-SNE是在优化一个非凸的目标函数，我们每次得到的只不过是一个局部最小。所以会出现同一集群被分为两半的情况

分类要依靠其他方法，t-SNE 仅仅是一个可视化方法: 
t-SNE is a valuable tool in generating hypotheses and understanding, but does not produce conclusive evidence

可以用t-SNE来提出假设 不要用t-SNE得出结论。



(4) t-SNE 中cluster之间的距离有意义吗？t-SNE能用于寻找离群点outlier吗？
t-SNE中集群之间的距离并不表示相似度。
对t分布来说，超出一定距离范围以后，其相似度都是很小的。也就是说，只要不在一个集群范围内，其相似度都是一个很小的值，我们所看到的集群之间的呈现出来的距离并不能说明什么，这是由t-SNE的内在所决定的。

t-SNE不能用于寻找离群点outlier。




第一作者(machine learning and computer vision): https://lvdmaaten.github.io/
	更多答疑看这里: https://lvdmaaten.github.io/tsne/
	Accelerating t-SNE using Tree-Based Algorithms. https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf
#





ref:
t-SNE使用过程中的一些坑 https://www.jianshu.com/p/631d6529e0df?from=singlemessage (有一个原理图)
视频 https://www.bilibili.com/video/BV1Z7411v7EL?p=1
https://www.datacamp.com/community/tutorials/introduction-t-sne
https://vimsky.com/article/4400.html
实战: https://ajitjohnson.com/tsne-for-biologist-tutorial/

出版社的专题 https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/

t-SNE visualization of CNN codes: http://cs.stanford.edu/people/karpathy/cnnembed/













========================================
|-- t-SNE的实现: R, python, js
----------------------------------------

https://github.com/DawnEve/ML_MachineLearning/blob/master/t-SNE/tSNE_demo.ipynb



1. 在R中实现 PCA 和 tSNE

真实scRNAseq数据
RNA: https://github.com/ajitjohnson/ajitjohnson.github.io/blob/master/assets/data/tsne_tutorial/exp.csv
cell type: https://github.com/ajitjohnson/ajitjohnson.github.io/blob/master/assets/data/tsne_tutorial/meta.csv


(1) 使用R包
## Load the t-SNE library
# install.packages("Rtsne")
library(Rtsne)

######## (1) input data
#1) input raw counts
IR_data0 =iris[1:4]
#2) input cpm: 对obs进行归一化，这里是row
IR_data1 <- as.data.frame(t(apply(iris[,1:4], 1, function(x){ x/sum(x)*1e4 })))
#3) input log2cpm:
IR_data2 <- as.data.frame(t(apply(iris[,1:4], 1, function(x){ log2(1+ x/sum(x)*1e4) })))
IR_species <- iris[ ,5]
dim(IR_data2) #150 4

## (2)do PCA
library(FactoMineR)
do_PCA=function(dt){
  # PCA 默认是对列计算的。而我们要对obs聚类，就要先转置
  dt.pca=PCA( t(dt) )
  #dt.pca$ind
  #str(dt.pca$ind)
  dim(dt.pca$var$coord) #150 3
  head(dt.pca$var$coord)
  #          Dim.1     Dim.2         Dim.3
  #cell1 0.8381696 0.5454088 -0.0009952193
  #cell2 0.8758969 0.4800065 -0.0489729933
  par(mfrow=c(1,3))
  plot(dt.pca$var$coord[,1] , dt.pca$var$coord[,2], col=iris[,5])
  plot(dt.pca$var$coord[,1] , dt.pca$var$coord[,3], col=iris[,5])
  plot(dt.pca$var$coord[,2] , dt.pca$var$coord[,3], col=iris[,5])
  return(dt.pca)
}
#PCA结果很稳定。如果输入相同，则每次运行结果都一样
dt.pca0=do_PCA(IR_data0)
dt.pca1=do_PCA(IR_data1)
dt.pca2=do_PCA(IR_data2)
dt.pca2$eig #每个PC解释了百分之多少的变异
str(dt.pca2$var)


## (3)do tSNE
do_tsne=function(IR_data){
  ## Run the t-SNE algorithm and store the results into an object called tsne_results
  tsne_results <- Rtsne(IR_data, perplexity=30, check_duplicates = FALSE) 
  # You can change the value of perplexity and see how the plot changes
  #str(tsne_results)
  
  ## Generate the t_SNE plot
  par(mfrow=c(1,2)) # To plot two images side-by-side
  plot(tsne_results$Y, col = "blue", pch = 19, cex = 1.5) # Plotting the first image
  plot(tsne_results$Y, col = "black", bg= IR_species, pch = 21, cex = 1.5) 
  # Second plot: Color the plot by the real species type (bg= IR_species)
  return(tsne_results)
}
set.seed(2020)
tsne_results0=do_tsne(IR_data0) #每次执行t-SNE，结果都是随机的
tsne_results1=do_tsne(IR_data1)
tsne_results2=do_tsne(IR_data2)
#
dim(tsne_results2$Y) #150 2
head(tsne_results2$Y)
plot(tsne_results2$Y[,1], col=iris[,5])
plot(tsne_results2$Y[,2], col=iris[,5])
plot(tsne_results2$Y[,1],tsne_results2$Y[,2], col=iris[,5])

#
# (4)带入前几个PC做tSNE
tsne_results2_2=do_tsne(dt.pca2$var$coord[,1:3])
plot(tsne_results2_2$Y[,1],tsne_results2_2$Y[,2], col=iris[,5])
#





(2) 使用纯R实现t-SNE (来自同名R包)
tsne=function (X, initial_config = NULL, k = 2, initial_dims = 30, 
  perplexity = 30, max_iter = 1000, min_cost = 0, epoch_callback = NULL, 
  whiten = TRUE, epoch = 100) 
{
  if ("dist" %in% class(X)) {
    n = attr(X, "Size")
  }
  else {
    X = as.matrix(X)
    X = X - min(X)
    X = X/max(X)
    initial_dims = min(initial_dims, ncol(X))
    if (whiten) 
      X <- .whiten(as.matrix(X), n.comp = initial_dims)
    n = nrow(X)
  }
  momentum = 0.5
  final_momentum = 0.8
  mom_switch_iter = 250
  epsilon = 500
  min_gain = 0.01
  initial_P_gain = 4
  eps = 2^(-52)
  if (!is.null(initial_config) && is.matrix(initial_config)) {
    if (nrow(initial_config) != n | ncol(initial_config) != 
      k) {
      stop("initial_config argument does not match necessary configuration for X")
    }
    ydata = initial_config
    initial_P_gain = 1
  }
  else {
    ydata = matrix(rnorm(k * n), n)
  }
  P = .x2p(X, perplexity, 1e-05)$P
  P = 0.5 * (P + t(P))
  P[P < eps] <- eps
  P = P/sum(P)
  P = P * initial_P_gain
  grads = matrix(0, nrow(ydata), ncol(ydata))
  incs = matrix(0, nrow(ydata), ncol(ydata))
  gains = matrix(1, nrow(ydata), ncol(ydata))
  for (iter in 1:max_iter) {
    if (iter%%epoch == 0) {
      cost = sum(apply(P * log((P + eps)/(Q + eps)), 1, 
        sum))
      message("Epoch: Iteration #", iter, " error is: ", 
        cost)
      if (cost < min_cost) 
        break
      if (!is.null(epoch_callback)) 
        epoch_callback(ydata)
    }
    sum_ydata = apply(ydata^2, 1, sum)
    num = 1/(1 + sum_ydata + sweep(-2 * ydata %*% t(ydata), 
      2, -t(sum_ydata)))
    diag(num) = 0
    Q = num/sum(num)
    if (any(is.nan(num))) 
      message("NaN in grad. descent")
    Q[Q < eps] = eps
    stiffnesses = 4 * (P - Q) * num
    for (i in 1:n) {
      grads[i, ] = apply(sweep(-ydata, 2, -ydata[i, ]) * 
        stiffnesses[, i], 2, sum)
    }
    gains = ((gains + 0.2) * abs(sign(grads) != sign(incs)) + 
      gains * 0.8 * abs(sign(grads) == sign(incs)))
    gains[gains < min_gain] = min_gain
    incs = momentum * incs - epsilon * (gains * grads)
    ydata = ydata + incs
    ydata = sweep(ydata, 2, apply(ydata, 2, mean))
    if (iter == mom_switch_iter) 
      momentum = final_momentum
    if (iter == 100 && is.null(initial_config)) 
      P = P/4
  }
  ydata
}














2. 手写 python版的t-SNE 
https://www.plob.org/article/15978.html
https://nlml.github.io/in-raw-numpy/in-raw-numpy-t-sne/
tsne.py代码: https://github.com/nlml/tsne_raw/blob/master/src/tsne.py


(2) python包: sklearn
https://www.deeplearn.me/2137.html









3.js 版本的t-SNE
(1) 项目实例 https://cs.stanford.edu/people/karpathy/tsnejs/csvdemo.html
代码 https://github.com/karpathy/tsnejs







========================================
|-- UMAP 简介 与R实例
----------------------------------------
1. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction
https://arxiv.org/abs/1802.03426

UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.

(1) 原始论文
py参考实现: https://github.com/lmcinnes/umap
文档: https://umap-learn.readthedocs.io/en/latest/


视频讲解: https://www.bilibili.com/video/BV1a4411R76S?from=search&seid=12484312964074038519
	RP-trees + NN-descent
	SGD + negative sampling
	UMAP speeed up over t-SNE 
	UMAP可以用于有监督的聚类
#





(2) 生物学应用
Published: 03 December 2018
Dimensionality reduction for visualizing single-cell data using UMAP
https://www.nature.com/articles/nbt.4314


2)Yeast: rticle|Open Access|Published: 24 March 2020
Dimensionality reduction by UMAP to visualize physical and genetic interactions






2. R语言实例
# install.packages('umap')

(1). 直接使用原始值进行umap降维
library(umap)

# embedd iris dataset using default settings
iris.umap = umap(iris[,1:4])
str(iris.umap)
head(iris)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1          5.1         3.5          1.4         0.2  setosa
#2          4.9         3.0          1.4         0.2  setosa
#3          4.7         3.2          1.3         0.2  setosa
out=iris.umap$layout
head(out)

## UMAP 二维图
plot(out[,1], out[,2], col=iris[,5], xlab="UMAP_1", ylab="UMAP_2", pch=16,
     main="pkg umap ")
#
plot(iris[,4], iris[,3], col=iris[,5], pch=16)






(2). 输入PCA，进行umap降维

# input: PCA of raw
iris.pca=princomp( iris[,1:4] )
str(iris.pca)
str(iris.pca$loadings)
iris.pca$loadings
#Loadings:
#             Comp.1 Comp.2 Comp.3 Comp.4
#Sepal.Length  0.361  0.657  0.582  0.315
#Sepal.Width          0.730 -0.598 -0.320
#Petal.Length  0.857 -0.173        -0.480
#Petal.Width   0.358        -0.546  0.754

#                Comp.1 Comp.2 Comp.3 Comp.4
#SS loadings      1.00   1.00   1.00   1.00
#Proportion Var   0.25   0.25   0.25   0.25
#Cumulative Var   0.25   0.50   0.75   1.00

iris.pcs=t(apply(iris[,1:4], 1, function(x){
  c(sum(iris.pca$loadings[,1]*x),
    sum(iris.pca$loadings[,2]*x),
    sum(iris.pca$loadings[,3]*x),
    sum(iris.pca$loadings[,4]*x)
  )
}))
head(iris.pcs)
plot(iris.pcs[,1], iris.pcs[,2], col=iris[,5], main="PCA of raw counts")

#
#######
# input: umap of raw counts pca
iris.pc.umap=umap(iris.pcs)
pc.umap=iris.pc.umap$layout

plot(pc.umap[,1], pc.umap[,2], col=iris[,5], xlab="UMAP_1", ylab="UMAP_2", pch=16,
     main="umap: input raw counts PC", cex=0.5)
legend('topright', pch=16, col=unique(iris[,5]), legend =unique(iris[,5]) )











========================================
典型相关分析法(Canonical Correlation Analysis, CCA)及其变种：稀疏典型相关分析法(sCCA)，正则化 rCCA
----------------------------------------
1. CCA 简介

(1)典型关联分析(CCA)原理总结（转自刘建平Pinard）
https://www.jianshu.com/p/7486e2b426dd
https://www.cnblogs.com/pinard/p/6288716.html

典型关联分析(Canonical Correlation Analysis，以下简称CCA)是最常用的挖掘数据关联关系的算法之一。比如我们拿到两组数据，第一组是人身高和体重的数据，第二组是对应的跑步能力和跳远能力的数据。那么我们能不能说这两组数据是相关的呢？CCA可以帮助我们分析这个问题。



(2) 综述中的 CCA
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8114078/

Canonical correlation analysis (CCA) is a correlation-based multivariate analysis method to examine the linear relationship between two datasets [99], [100]. 

A set of linear combinations of all variables in each of the two datasets is determined so that it maximizes the correlation between them and best explains both within and between dataset variability. 

The high dimensionality, sparsity and variable feature spaces across the different omics layers pose constraints for the linear combinations limiting the biological applicability of CCA. 
Generally, to solve these issues variants of CCA including sparse CCA [100] and penalized matrix decomposition (PMD) method [101] have been proposed. 
为了解决稀疏性，提出了CCA变种:
> 100. Hardoon D.R., Shawe-Taylor J. Sparse canonical correlation analysis. Mach Learn. 2011;83(3):331–353. doi: 10.1007/s10994-010-5222-7.
> 101. Witten D.M., Tibshirani R., Hastie T. A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis. Biostatistics. 2009;10(3):515–534. doi: 10.1093/biostatistics/kxp008.


1)Seurat3 实现的对角化CCA方法：整合不同细胞的数据
For instance Seurat3 [87] implements CCA in order to integrate two single-cell omics datasets. 
It first jointly reduces the dimensionality of two datasets using the diagonalized CCA 
followed by a search for a mutual nearest neighbor in lower dimensional space, 
and then establishes the cellular relationship across the datasets as an anchor.
- 对2个数据集适用 对角化CCA 联合降维
- 对典型相关向量应用L2正则化：then apply L2-normalization to the canonical correlation vectors
- 低纬度寻找 相互最近临： search for MNNs in this shared low-dimensional representation. 
- 在不同数据集建立细胞之间的联系，作为锚点

用于整合 小鼠视觉皮层的 scRNA-seq + scATAC-seq，骨髓的 scRNA-seq + 膜蛋白。
This has been used, for example, to integrate scRNA-seq and scATAC-seq data from the mouse visual cortex and scRNA-seq and surface protein expression from bone marrow [87]. 

> 87. Stuart T., Butler A., Hoffman P., Hafemeister C., Papalexi E., Mauck W.M. Comprehensive Integration of Single-Cell Data. Cell. 2019;177(7):1888–1902.e21. doi: 10.1016/j.cell.2019.05.031.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6687398/


2) 双阶典型相关分析 bi-order canonical correlation analysis (bi-CCA)
Another recent adaptation of CCA for single-cell multi-omics clustering is bindSC [80] which utilizes bi-order canonical correlation analysis (bi-CCA) that captures the correlated variables from both cells and features between two modalities to formulate the canonical correlation vectors in a latent space. 
分别捕获细胞和基因，到一个潜在空间中。
> 80. Dou J., Liang S., Mohanty V., Cheng X., Kim S., Choi J. Unbiased integration of single cell multi-omics data. BioRxiv. 2020 doi: 10.1101/2020.12.11.422014.
已发表：Bi-order multimodal integration of single-cell data https://pubmed.ncbi.nlm.nih.gov/35534898/
	看Fig1. 是对不同的细胞: Cell set1 + Cell set2.

3) 多于2个的模态整合
While Seurat3 or bindSC can only be applied to two datasets at a time, multiset CCA [102] aims to simultaneously find multivariate associations between more than two modalities. In multiset CCA, the canonical coefficients of all variables are optimized to maximize the pairwise canonical correlations [103]. Currently, we are not aware of multiset CCA being applied to single-cell multi-omics.
> 102. KETTENRING J.R. Canonical analysis of several sets of variables. Biometrika. 1971;58(3):433–451. doi: 10.1093/biomet/58.3.433.
> 103. Zhuang X., Yang Z., Cordes D. A technical review of canonical correlation analysis for neuroscience applications. Hum Brain Mapp. 2020;41(13):3807–3833. doi: 10.1002/hbm.v41.1310.1002/hbm.25090. 







(3) Seurat CCA 的数学原理
Seurat CCA? It's just a simple extension of PCA! https://xinmingtu.cn/blog/2022/CCA_dual_PCA/
博客认为：Difference between “Seurat CCA” and Real CCA
The “Seurat CCA” is taking the projection vector from the traditional CCA directly as cell embeddings. But in fact, the classical definition of CCA would imply projecting genes into a common space rather than cells.






(4) Seurat CCA 的过程
https://hbctraining.github.io/scRNA-seq_online/lessons/06_integration.html

i) Perform canonical correlation analysis (CCA)

CCA identifies shared sources of variation between the conditions/groups. It is a form of PCA, in that it identifies the greatest sources of variation in the data, but only if it is shared or conserved across the conditions/groups (using the 3000 most variant genes from each sample).
使用每个条件中3k高变基因，找共有的最大变异。

This step roughly aligns the cells using the greatest shared sources of variation.

* NOTE: The shared highly variable genes are used because they are the most likely to represent those genes distinguishing the different cell types present.


ii) Identify anchors or mutual nearest neighbors (MNNs) across datasets (sometimes incorrect anchors are identified)
通过数据集相互最近临，寻找锚点

MNNs can be thought of as ‘best buddies’. For each cell in one condition:

* The cell’s closest neighbor in the other condition is identified based on gene expression values - its ‘best buddy’.
* The reciprocal analysis is performed, and if the two cells are ‘best buddies’ in both directions, then those cells will be marked as anchors to ‘anchor’ the two datasets together.
基于基因表达，寻找细胞的最近临(MNN)
对另一套数据集也这么做，如果两个细胞相互都是对方的最近临，则作为锚点。

* “The difference in expression values between cells in an MNN pair provides an estimate of the batch effect, which is made more precise by averaging across many such pairs. A correction vector is obtained and applied to the expression values to perform batch correction.” [Stuart and Bulter et al. (2018)].
MNN对的表达差异，可以用于估计批次效应。使用多对就更精确的估计批次效应。
获取修正向量，应用到表达值上，修正批次效应。


iii) Filter anchors to remove incorrect anchors:

Assess the similarity between anchor pairs by the overlap in their local neighborhoods (incorrect anchors will have low scores) - do the adjacent cells have ‘best buddies’ that are adjacent to each other?
局部重叠，查看配对锚点细胞的相似性(错误锚点打分低)：近距离的细胞的最佳伙伴是否也是最近？


iv) Integrate the conditions/datasets:

Use anchors and corresponding scores to transform the cell expression values, allowing for the integration of the conditions/datasets (different samples, conditions, datasets, modalities)
适用锚点和对应的打分，转化细胞表达值，来整合不同条件/数据集(不同样本，条件，数据集，模态)。

NOTE: Transformation of each cell uses a weighted average of the two cells of each anchor across anchors of the datasets. Weights determined by cell similarity score (distance between cell and k nearest anchors) and anchor scores, so cells in the same neighborhood should have similar correction values.
转化每个细胞，使用两个数据集锚点的权重平均值。权重就是细胞相似打分(细胞间距离，k最临近锚点)和锚点打分，所以相近的细胞有相似的修正值。

If cell types are present in one dataset, but not the other, then the cells will still appear as a separate sample-specific cluster.





(5) CCA 不适用的情况
https://satijalab.org/seurat/articles/integration_rpca
However, CCA-based integration may also lead to overcorrection, especially when a large proportion of cells are non-overlapping across datasets.









2. CCA及其变种的适用情况
(1) 一般形式
典型相关分析法（CCA）是分析两个数据矩阵的经典方法，不需要降维提取主成分。但普通CCA只适用于行比列多的矩阵，即样本比基因多。
其中用CCA()函数可得到每个元素（比如每个EBV基因）的典型系数（canonical coefficient），它表示该元素对整个矩阵典型相关系数的贡献，如果某个元素的典型系数非零，则表示它对全局相关性有很大贡献，被选为核心（essential）基因。


问题：但是，对于不超过2万细胞的单细胞数据，通常是样本(细胞)少、基因多。

当数据中样本数量少于特征数量时，也就是在高维设置中，经典的 Canonical Correlation Analysis (CCA) 可能会遇到困难，因为计算相关矩阵的逆在数学上可能是不稳定的或不可能的。


(2) 正则化 rCCA
为了解决这个问题，可以采用一些改进的方法，特别是使用正则化技术。

正则化 CCA 是解决样本数量少于特征数量的一种有效方法。它通过在CCA的优化问题中添加正则化项来实现。这种方法不仅可以处理高维数据，还可以提高模型的泛化能力。

步骤和方法：
i)引入正则化参数：
在求解 CCA 的标准方程时，对特征的协方差矩阵 X^T.X 和Y^T.Y 加入正则化项。具体来说，修改为 
X^T.X + λ[x].I 和 Y^T.Y + λ[y].I，其中 λ[x] 和 λ[y] 是正则化参数，I 是单位矩阵。
^是上标
[]是下标

ii)选择正则化参数：
正则化参数的选择通常依赖于交叉验证或其他模型选择方法，目的是找到一个平衡模型复杂度和拟合度的最佳值。

=> 问题：怎么选择 正则化参数？评价指标是什么？

iii)计算正则化的 CCA：
使用修改后的协方差矩阵计算 CCA，求解得到的规范相关变量将考虑到正则化的影响，从而避免了过拟合并增强了数学稳定性。





(3) 稀疏 sCCA

Sparse CCA 也是一种可行的选择，尤其是当需要在特征非常多的情况下从中选择最有影响力的特征。Sparse CCA 通过在模型中引入稀疏性约束，只保留对解释两组变量关系最重要的特征，从而简化了问题并减少了计算负担。

优点
	在高维数据分析中非常有效，能够识别和利用最有影响力的变量。
	帮助简化模型，提高模型的解释性和泛化能力。
缺点
	选择合适的正则化参数可以是一个挑战。
	稀疏性可能会导致重要信息的遗漏，特别是当所有变量都相对重要时。




(4) Partial CCA
Partial CCA 是在传统的 CCA 框架内引入了偏差（partialing out）的概念，旨在剔除一组或多组干扰变量的影响，从而更准确地捕捉两组主要变量之间的相关性。这种方法在存在潜在干扰因素或混杂变量时特别有用。


优点
	能够剔除不感兴趣的变量的影响，从而更准确地估计主要变量之间的关联。
	在多变量分析中提高了结果的解释性。
缺点
	需要预先确定哪些变量是干扰变量。
	计算复杂度较高，尤其是在变量多的情况下。






(10) 其他方法
* 核CCA（Kernel CCA）：通过使用核技术，将数据映射到高维空间，可以有效处理非线性关系和高维问题。

* 随机投影/降维：在进行 CCA 前，可以先使用随机投影或其他降维技术减少特征的数量。


对于样本数少于特征数的问题，通过引入正则化和考虑稀疏性的方法可以有效解决经典CCA的局限性。选择合适的方法取决于具体的数据特征和研究目标。在实际应用中，这些方法往往需要结合交叉验证等技术来确定最优的模型参数。




========================================
|-- CCA 典型相关分析及其变种，R包分析 pbmc3k 按特征拆分的2个矩阵
----------------------------------------
0. 步骤：
1)对两个矩阵，分别求高变基因
	# Select the most variable features to use for integration
2)取data 或 scale.data，按照高变基因取子集
3)调用R包，进行CCA分解
4)获取u向量，代替PCA，进行UMAP和细胞分群


R包
stats::cancor() #经典CCA，报错：观测数少于特征数
vegan::cca() partial constrained correspondence analysis.
CCA::rcc()  Regularized Canonical Correlation Analysis
PMA::CCA() 稀疏典型相关分析 sparse canonical correlation analysis using the penalized matrix decomposition.
nscancor::nscancor() 用于非负和稀疏CCA的R包[not run]





1. 准备数据
# A. 准备pbmc3k数据集 ----

## (1) Cluster cell ====
library(Seurat)
# file_2="D:\\code_R\\filtered_gene_bc_matrices\\hg19\\"
pbmc.data <- Read10X(data.dir = "D:\\code_R\\filtered_gene_bc_matrices\\hg19\\")
pbmc <- CreateSeuratObject(counts = pbmc.data, project = "pbmc3k", min.cells = 3, min.features = 200)
pbmc
pbmc[["percent.mt"]] <- PercentageFeatureSet(pbmc, pattern = "^MT-")
pbmc <- NormalizeData(pbmc, normalization.method = "LogNormalize", scale.factor = 10000)
pbmc <- FindVariableFeatures(pbmc, selection.method = "vst", nfeatures = 2000)
pbmc <- ScaleData(pbmc, features = rownames(pbmc))

pbmc <- RunPCA(pbmc, features = VariableFeatures(object = pbmc))
DimPlot(pbmc, reduction = "pca")
ElbowPlot(pbmc)

pbmc <- FindNeighbors(pbmc, dims = 1:10)
pbmc <- FindClusters(pbmc, resolution = 0.5)

pbmc <- RunUMAP(pbmc, dims = 1:10)
DimPlot(pbmc, reduction = "umap", label=T)

# save
saveRDS(pbmc, file="D:\\code_R\\filtered_gene_bc_matrices\\pbmc3k.final.Rds")
#scObj=readRDS(file="D:\\code_R\\filtered_gene_bc_matrices\\pbmc3k.final.Rds")


## (2) split features ====
scObj=pbmc
#
set.seed(202405)
features_index = sample(1:3, nrow(scObj), replace = T )
table(features_index)

scObj1=scObj[which(features_index==1), ]
# find HVG
scObj1=FindVariableFeatures(scObj1, selection.method = "vst", nfeatures = 3000)
scObj1 #4643 2700


scObj2=scObj[which(features_index!=1), ]
# find HVG
scObj2=FindVariableFeatures(scObj2, selection.method = "vst", nfeatures = 4000)
scObj2 #9071 2700





2. stats::cancor[NOT suitable]





3. CCA::rcc() 正则化的CCA

适用于 变量(列) 超过 行(样品)的情况：2万基因 > 3千细胞
The function performs the Regularized extension of the Canonical Correlation Analysis to seek correlations between two data matrices when the number of columns (variables) exceeds the number of rows (observations)

> cca.rs1 = CCA::rcc(X=X_vector , Y=Y_vector)
错误于CCA::rcc(X = X_vector, Y = Y_vector): 
  缺少参数"lambda1",也缺失默认值

必须提供参数3和4，正则化参数：Regularization parameter
	不知道怎么设置？

示例：
	data(nutrimouse)
	X=as.matrix(nutrimouse$gene)
	Y=as.matrix(nutrimouse$lipid)
	res.cc=rcc(X,Y,0.1,0.2)
	plt.cc(res.cc)


示例2：
X_vector = t(scObj1@assays$RNA@data[VariableFeatures(scObj1), ]|> as.matrix())
Y_vector = t(scObj2@assays$RNA@data[VariableFeatures(scObj2), ]|>as.matrix())

cca.rs1 = CCA::rcc(X=X_vector , Y=Y_vector, lambda1 = 0.1, lambda2 = 0.2)
u_vector =  cca.rs1$scores$xscores
u_vector[1:2,1:2]
#                        [,1]       [,2]
#AAACATACAACCAC-1 -0.83568478 -0.1218233
#AAACATTGAGCTAC-1 -0.02981178  1.4936991

# another calculation (similar to u_vector)
u_vector2=(as.matrix(X_vector) %*% cca.rs1$xcoef)

## (2) UMAP based on CCA ====
library(uwot)
umap_results = umap(u_vector[,1:10], n_neighbors = 15, n_components = 2, metric = "cosine") #2min
umap_results = umap(u_vector[,1:10], n_neighbors = 15, n_components = 2) #2min

colnames(umap_results)=paste0("UMAP_",1:2)
umap_results=as.data.frame(umap_results)
plot(umap_results$UMAP_1, umap_results$UMAP_2, pch=19, col="#88888822", main="UMAP based on u", cex=0.5)
head(umap_results)

# plot
dat2=umap_results
dat2$cluster = scObj@meta.data[rownames(dat2),]$seurat_clusters
library(ggplot2)
ggplot(dat2, aes(UMAP_1, UMAP_2, color=cluster))+
  geom_point(size=0.5)+theme_classic()+ggtitle("CCA-based cluster")+
  LargeLegend(2)


# Seurat object
p0=DimPlot(scObj, reduction = "umap", label=T)+ggtitle("Classical Seurat")
scObj_re2=scObj
scObj_re2@reductions$cca=scObj_re2@reductions$pca
scObj_re2@reductions$cca@feature.loadings=cca.rs1$xcoef
scObj_re2@reductions$cca@cell.embeddings=u_vector
#scObj_re2@reductions$cca@stdev = cca.rs1$CCA$eig
scObj_re2@reductions$cca@key="CCA_"

scObj_re2=Seurat::RunUMAP(scObj_re2, reduction = "cca", dims = 1:10)
p2=DimPlot(scObj_re2, reduction = "umap", label=T)+ggtitle("CCA-based UMAP")
p0+p2






4. vegan::cca() partial constrained correspondence analysis.

https://cran.r-project.org/web/packages/vegan/index.html
Ordination methods, diversity analysis and other functions for community and vegetation ecologists.

vegan::cca(), 
Function cca performs correspondence analysis, or optionally constrained correspondence analysis (a.k.a. canonical correspondence analysis), or optionally partial constrained correspondence analysis.


X_vector = t(scObj1@assays$RNA@data[VariableFeatures(scObj1),] |> as.matrix())
Y_vector = t(scObj2@assays$RNA@data[VariableFeatures(scObj2),] |>as.matrix())

library(vegan)
cca.rs3 <- vegan::cca(X_vector, Y_vector )

# get U vector
u_vector=cca.rs3$CCA$u
dim(u_vector) #[1] 2700 2699
u_vector[1:2, 1:3]
#                       CCA1        CCA2       CCA3
#AAACATACAACCAC-1 0.5914861 -0.7230028  0.3977991
#AAACATTGAGCTAC-1 0.1675775 -0.1583371 -1.4463575
cca.cells=rownames(u_vector)
setdiff(colnames(scObj), cca.cells) #all cell included

# get V vector
v_vector=cca.rs3$CCA$v
dim(v_vector) #[1] 3000 2699
v_vector[1:2, 1:3]
#               CCA1       CCA2       CCA3
#PF4     -20.8000835 -10.635738 -0.4798296
#HLA-DRA  -0.4860016   1.079377 -2.5370283
cca.genes=rownames(v_vector)
setdiff(rownames(scObj), cca.genes) |> length()


# cells: u
#like PCA
#plot(u_vector[,1], u_vector[,2])
plot(u_vector[,1], u_vector[,2], pch=19, col="#88888822", main="u vector: cells")
plot(u_vector[,1], u_vector[,3], pch=19, col="#88888822", main="u vector: cells")


## (2) umap based on v vector ====
library(uwot)
umap_results = umap(u_vector[,1:10], n_neighbors = 15, n_components = 2, metric = "cosine") #2min

colnames(umap_results)=paste0("UMAP_",1:2)
umap_results=as.data.frame(umap_results)
plot(umap_results$UMAP_1, umap_results$UMAP_2, pch=19, col="#88888822", main="UMAP based on u", cex=0.5)
head(umap_results)

# plot
dat2=umap_results
dat2$cluster = scObj@meta.data[rownames(dat2),]$seurat_clusters
library(ggplot2)
ggplot(dat2, aes(UMAP_1, UMAP_2, color=cluster))+
  geom_point(size=0.5)+theme_classic()+ggtitle("Based on CCA U vector")+
  LargeLegend(2)


# Seurat object
p0=DimPlot(scObj, reduction = "umap", label=T)+ggtitle("Classical Seurat")
scObj_re2=scObj
scObj_re2@reductions$cca=scObj_re2@reductions$pca
scObj_re2@reductions$cca@feature.loadings=v_vector
scObj_re2@reductions$cca@cell.embeddings=u_vector
scObj_re2@reductions$cca@stdev = cca.rs3$CCA$eig
scObj_re2@reductions$cca@key="CCA_"

#
library(factoextra)
#factoextra::fviz_eig(scObj_re2@reductions$cca@stdev)
(ElbowPlot(scObj_re2, reduction = "pca", ndims = 50)+ggtitle("PCA")) +
(ElbowPlot(scObj_re2, reduction = "cca", ndims = 50)+ggtitle("CCA"))


# cca plot
plot(x=Embeddings(scObj_re2, reduction = "cca")[,1],
     y=Embeddings(scObj_re2, reduction = "pca")[,1],
     pch=19, col="#88888844", cex=0.5)

# UMAP
scObj_re2=RunUMAP(scObj_re2, reduction = "cca", dims = 1:10, reduction.name = "ccaUMAP")
p1=DimPlot(scObj_re2, reduction = "ccaUMAP", label=T)+ggtitle("CCA-based UMAP")
p0+p1

# cell cluster
scObj_re2=FindNeighbors(scObj_re2, reduction = "cca", dims = 1:10, graph.name=c("cca_nn", "cca_snn") )
scObj_re2@graphs |> names()
scObj_re2=FindClusters(scObj_re2, resolution = 0.9, graph.name="cca_snn")

p2=DimPlot(scObj_re2, reduction = "umap", label=T)+ggtitle("CCA-based Cluster")
p3=DimPlot(scObj_re2, reduction = "ccaUMAP", label=T)+ggtitle("CCA-based UMAP+Cluster")

(p0+p1)/(p2+p3)



## (3) markers & save ====
library(dplyr)
scObj_re2.markers <- FindAllMarkers(scObj_re2, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)
scObj_re2@misc[["markers"]]=scObj_re2.markers
scObj_re2.markers %>% group_by(cluster) %>% top_n(n = 2, wt = avg_log2FC)

DotPlot(scObj_re2, features = unique(c(
  "PTPRC", "CD3D", "CD3E", "CD4", "CD8A", "CD8B", "FOXP3", "IL2RA", 'GZMA', "GZMK", "SELL", 'CD274',
  "MS4A1", "GNLY", "CD3E", "CD14", "FCER1A", "FCGR3A", "LYZ", "PPBP", 
  "CD8A",
  scObj_re2.markers %>% group_by(cluster) %>% top_n(n = 7, wt = avg_log2FC) %>% pull(gene)
)), cluster.idents = T) + RotatedAxis()

# save
saveRDS(scObj_re2, paste0(outputRoot, keyword, "_1_use_data.vegan_cca.Seurat.Rds"))









5. PMA::CCA() Perform sparse canonical correlation analysis using the penalized matrix decomposition.

https://cran.r-project.org/web/packages/PMA/index.html
Performs Penalized Multivariate Analysis: a penalized matrix decomposition, sparse principal components analysis, and sparse canonical correlation analysis, described in Witten, Tibshirani and Hastie (2009) <doi:10.1093/biostatistics/kxp008> and Witten and Tibshirani (2009) Extensions of sparse canonical correlation analysis, with applications to genomic data <doi:10.2202/1544-6115.1470>.


X_vector = t(scObj1@assays$RNA@data[VariableFeatures(scObj1),] |> as.matrix())
Y_vector = t(scObj2@assays$RNA@data[VariableFeatures(scObj2),] |>as.matrix())

cca.rs4 = PMA::CCA(X_vector, Y_vector, typex="standard",typez="standard", K=100) #17:05->17:10
str(cca.rs4)


## (1) get u vectors ====
u_vector=X_vector %*% cca.rs4$u
dim(u_vector)
u_vector[1:2,1:2]

plot(u_vector[,1], u_vector[,2])
plot(u_vector[,1], u_vector[,3])

## (2) umap based on u vector ====
library(uwot)
umap_results = umap(u_vector[,1:10], n_neighbors = 15, n_components = 2) #2min

colnames(umap_results)=paste0("UMAP_",1:2)
umap_results=as.data.frame(umap_results)
plot(umap_results$UMAP_1, umap_results$UMAP_2, pch=19, col="#88888822", main="UMAP based on u", cex=0.5)
head(umap_results)

# plot
dat2=umap_results
dat2$cluster = scObj@meta.data[rownames(dat2),]$seurat_clusters
library(ggplot2)
pG=ggplot(dat2, aes(UMAP_1, UMAP_2, color=cluster))+
  geom_point(size=0.5)+theme_classic()+ggtitle("Based on CCA U vector")+
  LargeLegend(2)

# Seurat object
p0=DimPlot(scObj, reduction = "umap", label=T)+ggtitle("Classical Seurat")
scObj_re2=scObj
scObj_re2@reductions$cca=scObj_re2@reductions$pca
scObj_re2@reductions$cca@feature.loadings=cca.rs4$u
scObj_re2@reductions$cca@cell.embeddings=u_vector
#scObj_re2@reductions$cca@stdev = cca.rs4$CCA$eig
scObj_re2@reductions$cca@key="CCA_"

scObj_re2=RunUMAP(scObj_re2, reduction = "cca", dims = 1:10 )
#DimPlot(scObj_re2, reduction = "cca", label=T)
p1=DimPlot(scObj_re2, reduction = "umap", label=T)+ggtitle("UMAP based CCA")
pG+p0+p1









6. nscancor::nscancor() Non-Negative and Sparse CCA

https://cran.r-project.org/web/packages/nscancor/
Two implementations of canonical correlation analysis (CCA) that are based on iterated regression.

X_vector = t(scObj1@assays$RNA@data[VariableFeatures(scObj1),] |> as.matrix())
Y_vector = t(scObj2@assays$RNA@data[VariableFeatures(scObj2),] |>as.matrix())

## (1) partial cca ====
library(nscancor)
cca.rs6 <- nscancor::nscancor(X_vector, Y_vector)
#错误于nscancor::nscancor(X_vector, Y_vector): 
#缺少参数"ypredict",也缺失默认值
	怎么设置 predict ？？












========================================
NMF 非负矩阵分解 与 R 包实现
----------------------------------------
更多原理介绍见 Math/ 线代2
对单细胞的分群见 scSeq/adv_method.txt


1. NMF在单细胞研究中的优势

单细胞研究避免不了要回答两个问题：组织中有哪些细胞类型，每个细胞类型又有哪些表达模式？NMF解决这类问题具有天然的优势，因为它分解的因子很容易与细胞类型或表达模式对应起来。Github上有很多基于NMF和其变种算法的单细胞分析工具，我比较喜欢的有单细胞整合分析工具liger和空间转录组去卷积工具SPOTlight。应用NMF分析方法发表的高分文章也有很多，我给大家介绍一篇，更多的文章请自己搜索。

Chen, YP., Yin, JH., Li, WF. et al. Single-cell transcriptomics reveals regulators underlying immune cell diversity and immune subtypes associated with prognosis in nasopharyngeal carcinoma. Cell Res 30, 1024–1042 (2020). https://doi.org/10.1038/s41422-020-0374-x


(1) 安装NMF基础包
BiocManager::install('Biobase')
install.packages('NMF')

(2) nmf函数简介
NMF包通过nmf()函数实现矩阵分解，它的用法及重要参数如下：
> nmf(x, rank, method, seed, nrun, ...)
    x：待分解非负矩阵，数据格式可以是matrix，data.frame， ExpressionSet
    rank：分解的基数量，对于单细胞数据，可以设置为期望的细胞类型数量或表达模式数量
    method：因式分解的常用方法，这里介绍三种常用的
      1、基于KL 散度进行度量目标函数的多重迭代梯度下降算法——brunet(默认算法)
      2、基于欧几里得距离度量目标函数的多重迭代梯度下降算法——lee
      3、交替最小二乘法(Alternating Least Squares(ALS))——snmf/r
    seed：因式分解的初始化种子
    nrun：运行次数

rank值一般是要通过测试评估后确定的，但是分析单细胞数据这是一个很难完成的工作，5000个细胞的测试时间可能超过10个小时。
替代办法是使用经验或先验知识指定，可以尝试略多于细胞类型或细胞状态（细胞亚群再聚类时）的一个数值，例如我在本帖的PBMC数据分解中就指定为rank=10。

因为NMF一般是从随机数开始，通过迭代算法收敛误差的方法求出最优W和H矩阵，所以seed不同最后的结果也不同。为了减少seed的影响求得最优解，常规的办法是通过nrun参数设置运行100-200次矩阵分解选取最优值，也可以使用特殊的算法选择一个最佳的seed（设置seed='nndsvd'或seed='ica'），这样运行一次也能得到最优解。


重要参数翻译：
1) method
specification of the NMF algorithm. The most common way of specifying the algorithm is to pass the access key (i.e. a character string) of an algorithm stored in the package's dedicated registry, but methods exists that handle other types of values, such as function or list object. See their descriptions in section Methods.

If method is missing the algorithm to use is obtained from the option nmf.getOption('default.algorithm'), unless it can be infer from the type of NMF model to fit, if this later is available from other arguments. Factory fresh default value is ‘brunet’, which corresponds to the standard NMF algorithm from Brunet2004 (see section Algorithms).

Cases where the algorithm is inferred from the call are when an NMF model is passed in arguments rank or seed (see description for nmf,matrix,numeric,NULL in section Methods).


> nmf.getOption('default.algorithm') #默认方法
[1] "brunet"



==> ‘nsNMF’
Nonsmooth NMF from Pascual-Montano et al. (2006). It uses a modified version of Lee and Seung's multiplicative updates for the Kullback-Leibler divergence Lee et al. (2001), to fit a extension of the standard NMF model, that includes an intermediate smoothing matrix, meant meant to produce sparser factors.

Default stopping criterion: invariance of the connectivity matrix (see nmf.stop.connectivity).



2)nrun	
默认是1，除非 rank 是一个多于1个元素的向量，这时每个rank值默认运行30个run，用于计算 consensus matrix 来选择最优rank。
number of runs to perform. It specifies the number of runs to perform. By default only one run is performed, except if rank is a numeric vector with more than one element, in which case a default of 30 runs per value of the rank are performed, allowing the computation of a consensus matrix that is used in selecting the appropriate rank (see consensus).

当使用随机种子方法时，通常必须运行多个run，达到稳定、避免局部最优解。
When using a random seeding method, multiple runs are generally required to achieve stability and avoid bad local minima.






(3) 下面我们测试一下不同方法的运行时间

library(Biobase)
library(NMF)
## 参数测试
data("esGolub")
#esGolub <- esGolub[1:500,]
t1 <- nmf(esGolub, 3, method = "brunet", seed = 219)
runtime(t1)   # elapsed: 1.239 
t2 <- nmf(esGolub, 3, method = "lee", seed = 219)
runtime(t2)   # elapsed: 1.471 
t3 <- nmf(esGolub, 3, method = "snmf/r", seed = 219)
runtime(t3)   # elapsed: 0.995 
t4 <- nmf(esGolub, 3, method = "brunet", seed = 'nndsvd')
runtime(t4)   # elapsed: 3.156
t5 <- nmf(esGolub, 3, method = "brunet", nrun = 100)
runtime(t5)   # elapsed: 2.363






2. 文献复现
使用 NMF 的文献：https://pubmed.ncbi.nlm.nih.gov/32929364/
Fig2(C) Cophenetic and dispersion metrics for NMF across 2 to 10 clusters with 50 runs suggest 4 stable subtypes.
cophenetic 共表型性的
dispersion [dɪˈspɜːʃn] n. 传播，散布；分布

(1) 最佳分类的确定
nmf_res <- nmf(tcga_log2fpkm, ranks=2:10, nrun=50)
plot(nmf_res)


(2) 轮廓图
res_3 <- nmf(tcga_log2fpkm, ranks=2:10, nrun=50)
pdf(paste0(outputRoot, keyword, "_03_rank_3.silhouette.pdf"), width=5.5, height=5.5)
# no color
#sil_3=silhouette(res_3)
#plot(sil_3)

#set color
sil_3C=sortSilhouette( silhouette(res_3) )
df1=sil_3C
df1=head(df1, n=nrow(df1)) |> as.data.frame()
plot(sil_3C, 
     col=(df1$cluster+1),
     main=attr(sil_3C, "call") |> deparse() )
dev.off()






========================================
|-- NMF 非负矩阵分解 聚类，及效果评价: 一致性聚类（consensus）
----------------------------------------
1、NMF方法简介
NMF（Non-negative Matrix Factorization，非负矩阵分解）是一种矩阵分解方法，最早是在1999年Nature杂志刊登的由D.D.Lee和H.S.Seung两位科学家提出的一个对非负矩阵研究的成果。

NMF的目标：
给定一个所有元素均为非负的矩阵V，维数为n*m。要寻找到两个非负矩阵W和H，使得尽可能满足V=WH。其中W维数为n*r，H维数为r*m。

NMF的算法思想：
NMF的目标不是找到使得V=WH严格成立的矩阵分解，而是使得V和WH尽可能接近。这就需要构造一个代价函数J(V,W,H)，满足V和WH越接近，J越小。然后可以根据J本身的连续性、凹凸性等特征，使用恰当的优化方法，最终得到符合条件的W和H。

其中，代价函数J可以取很多种，这里介绍两种：


(1) 2范数距离：J=||V-WH||。
2范数距离定义如下：
使用拉格朗日KKT方法来寻找最优解，每次迭代公式如下：
当W和H是一个稳定点时，迭代收敛。

(2) KL距离：J=D(V||WH)。
KL距离定义如下：
使用拉格朗日KKT方法来寻找最优解，每次迭代公式如下：
当W和H是一个稳定点时，迭代收敛。





2、NMF聚类应用

有一个数据集，共m个样本，每个样本维度为n，构成了矩阵X，大小为n*m，即每一列为一个样本。

使用NMF方法，寻找到了W和H，使得X=WH。其中，X的第i列，就等于W乘以H的第i列，可以这样理解，H的第i列的第j个元素，相当于W的第j列的权重，X的第i列就是W的每一列与权重的乘积的求和。或者说，W的每一列相当于一个基向量，H的每一列相当于坐标向量。这样，就将原本样本的n维，转换成了r 维，r 维的每个元素对应一个基向量的坐标。因此，将原本对X聚类的问题，通过降维，转化为了对H聚类的问题。接下来就可以使用经典的聚类方法，比如k均值等。



(1) 问题1：最佳点的选取
https://cloud.tencent.com/developer/article/1936854

library(NMF)
coad.log2fpkm.enengy <- enengyTurExp##
ranks <- 2:10
estim.coad <- nmf(coad.log2fpkm.enengy,ranks, nrun=50)
duplicated(colnames(coad.log2fpkm.enengy))

#Estimation of the rank: Quality measures computed from 10 runs for each value of r.
plot(estim.coad) ## 重要的图

具体怎么选择rank，官方文档是这样建议的：

(Brunet2004) proposed to take the first value of r for which the cophenetic coefficient starts decreasing, (Hutchins2008) suggested to choose the first value where the RSS curve presents aninflection point, and (Frigyesi2008) considered the smallest value at which the decrease in the RSS is lower than the decrease of the RSS obtained from random data.

文章用了三个指标：cophenetic, dispersion 和silhouette。判断最佳rank值的准则就是，cophenetic值随K变化的最大变动的前点，上面结果中cophenetic值在rank为4-5时是第一个变化最大的拐点，所以选择最佳rank值为4。文章中也是4。



#再次NMF,rank=4
seed = 2020820
nmf.rank4 <- nmf(coad.log2fpkm.enengy, 
                 rank = 4, 
                 nrun=50,
                 seed = seed, 
                 method = "brunet")

#设置颜色
jco <- c("#2874C5","#EABF00","#C6524A","#868686")
index <- extractFeatures(nmf.rank4,"max") 
sig.order <- unlist(index)
NMF.Exp.rank4 <- coad.log2fpkm.enengy[sig.order,]
NMF.Exp.rank4 <- na.omit(NMF.Exp.rank4) #sig.order有时候会有缺失值
group <- predict(nmf.rank4) # 提出亚型
table(group)
consensusmap(nmf.rank4,
             labRow = NA,
             labCol = NA,
             annCol = data.frame("cluster"=group[colnames(NMF.Exp.rank4)]),
             annColors = list(cluster=c("1"=jco[1],"2"=jco[2],"3"=jco[3],"4"=jco[4])))

从结果来看，分4个亚群还是可以的。

文章中只给出了consensus matrix这个图（如下）。





(2)问题2：所以说NMF本身只是特征压缩的算法，想要聚类还要再KMEANS是吗？
在R中，你可以使用predict函数来获取NMF模型的分类结果。但是，需要注意的是，NMF本身并不是一个分类方法，而是一个数据压缩和可视化的工具。如果你想用NMF进行分类，你可能需要先将数据转换为适合NMF处理的形式，然后再将NMF的输出作为分类算法的输入。

1A) 使用predict函数获取分类结果
group <- predict(nmf.rank4) # 提出亚型 **


1B) R中使用NMF进行数据压缩，然后用其他分类算法（例如k-means）来获取分类结果
library(NMF)
nmf_model <- NMF(df, num.basis = 5, seed = 123)
 
# 提取分解得到的特征矩阵
features <- nmf.basis(nmf_model)
 
# 使用k-means对特征矩阵进行分类
library(cluster)
km_model <- kmeans(features, centers = 3) # 假设我们分成3类
 
# 获取分类结果
predictions <- km_model$cluster
 
# 输出分类结果
print(predictions)











========================================
|-- NMF 对 TCGA AML 聚类实例
----------------------------------------
3. 聚类实例
(0) 输入矩阵: 如TCGA表达数据框 log2(FPKM + 1)矩阵
# load fpkm
fpkm=read.table(paste0(outputRoot, "../TCGA-LAML.htseq_fpkm.tsv"), 
                row.names = 1, header = T, check.names = F )
fpkm[1:3,1:4]
dim(fpkm) # 60483   151

# rm last A or B, to be the same as clin
colnames(fpkm) = sapply( colnames(fpkm), function(x){
  substr(x, 1, nchar(x)-1)
} ) |> as.character()


# load gene anno
anno=read.table(paste0(outputRoot, "../gencode.v22.annotation.gene.probeMap"), row.names = 1, header = T )
anno[1:3,]
dim(anno) # 60483     5

## load clinic
clin=read.table(paste0(outputRoot, "../survival_LAML_survival.txt"), 
                row.names = 1, 
                header = T, sep="\t", check.names = F )
clin[1:3, ]
dim(clin) #200 10


## connect
patient = intersect( colnames(fpkm), rownames(clin))
length(patient) # 151

fpkm = fpkm[,patient]
dim(fpkm) #60483   151

clin = clin[patient,]
dim(clin) #151  10

fpkm = as.matrix(fpkm)
identical(rownames(clin), colnames(fpkm)) #T

#colnames(meta)[c(1,3)] = c("event","time") #OS, OS.time




(0B) 单因素筛选，缩小基因集合
# tableS3.txt是这篇文章的附表三，里面是免疫相关的基因，有2006个。
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8377979/#MOESM3
# 为了缩小一下基因数量，我选了免疫相关的基因，并做了批量单因素cox

# 下文仅做了单因素cox筛选

## 1) 基因筛选 ----
# rm genes whose 0 acounts for >= 1/4 of all cells
dim(fpkm) #60483   151
fpkm = fpkm[apply(fpkm, 1, function(x){ sum(x==0) < 0.25 *ncol(fpkm)} ), ]
dim(fpkm) # 32273   151

#tmp = read.table("tables3.txt",header = T)[,1]
#exp = exp[rownames(exp)%in%tmp,]


## 2)批量单因素cox ----
#cs = rownames(surv_cox(fpkm, clin, pvalue_cutoff = 0.001))

library(survival)
# res.cox<-coxph(Surv(time,status)~age+sex+ph.ecog,data=lung)

symbol="SCCPDH"
dif=clin[,c("OS", "OS.time")]
dif$val= fpkm[ anno[which(anno$gene==symbol), ] |> rownames(), , drop=F] |> colSums()
head(dif)
#res.cox <- coxph(Surv(time, status) ~ age, data = )
res.cox <- coxph(Surv(OS.time, OS) ~ val, data =dif )
res.cox
summary(res.cox)
summary(res.cox)$logtest['pvalue']
summary(res.cox)$waldtest['pvalue']



## 批量单因素cox回归分析
# 总结做单因素cox回归分析的特征
#covariates <- c("age", "sex", "bili", "albumin", "copper", "alk.phos", "ast", "trig", "platelet", "protime", "stage")
covariates <- unique(anno$gene)
covariates |> head()

# 分别对每一个变量，构建生存分析的公式
#univ_formulas <- sapply(covariates, function(x) as.formula(paste('Surv(OS.time, OS)~', x)))
formulas_cox=as.formula( paste('Surv(OS.time, OS)~val') )

# 循环对每一个特征做cox回归分析
univ_models <- sapply( covariates, function(symbol){
  #message("symbol:", symbol)
  dif=clin[,c("OS", "OS.time")]
  if(!(symbol %in% anno$gene)){
    return(-1)
  }else{
    enId=anno[which(anno$gene==symbol), ] |> rownames()
    if( any(enId %in% rownames(fpkm)) ){
      enId=enId[ enId %in% rownames(fpkm) ]
      dif$val= fpkm[enId, , drop=F] |> colSums()
      #print(head(dif, n=2))
      res.cox = coxph( formulas_cox, data = dif )
      return( summary(res.cox)$waldtest['pvalue'] |> signif(digits=2) |> as.numeric() )
      #return(summary(res.cox)$logtest['pvalue'] |> as.numeric())
    }
    return(-1)
  }
} ) # 16:39-->16:41
# filter -1
length(univ_models) #58387
univ_models=univ_models[univ_models!= -1]
length(univ_models) #31721
hist(univ_models, n=1000)
#
# rm non-sig
table(univ_models<0.05)
#FALSE  TRUE 
#24807  6914
#
table(univ_models<0.01)
#FALSE  TRUE 
#28489  3232
#
cs=univ_models[univ_models<0.01]
length(cs) #3232
tail(cs, n=3)
#SLC10A3    G6PD   CLIC2 
#3.5e-03 6.8e-06 6.4e-03 

# subset fpkm, change emsemble to gene symbol
nmf.input <- t(sapply(names(cs), function(symbol){
  enId=anno[which(anno$gene==symbol), ] |> rownames()
  enId = enId[enId %in% rownames(fpkm) ]
  fpkm[enId , , drop=F] |> colSums()
})) |> as.data.frame()
#
dim(nmf.input) #3232  151
nmf.input[1:3, 1:3] #row as gene, col as cell
#             TCGA-AB-2949-03 TCGA-AB-2943-03 TCGA-AB-2851-03
#U6                  23.706904       22.027168      22.7322383










(1) data filter
cell_info = data.frame(cname=colnames(nmf.input),
           time=clin$OS.time,
           status=clin$OS,
           row.names = 1)
dim(cell_info) #151   2
head(cell_info)

# sub time NA as -1
cell_info$time=ifelse(is.na(cell_info$time), -1, cell_info$time)

#
#gene_info = data.frame(gene=rownames(X), gid=1:4, row.names = 1)
#head(gene_info)

#
pdf(paste0(outputRoot, keyword, "_03_A00_fpkm.hist.pdf"), width=5, height=4)
hist(nmf.input |> unlist(), n=1000)
hist(log2(nmf.input+1) |> unlist(), n=1000, ylim=c(0, 2000))
abline(v=4, col="red", lty=2)
#
nmf.input.log2=log2(nmf.input+1)
nmf.input.log2[nmf.input.log2>4]=4
hist(nmf.input.log2 |> unlist(), n=1000, ylim=c(0, 2000))
dev.off()

#
library(NMF)
pdf(paste0(outputRoot, keyword, "_03_A01_log2fpkmP1.heatmap.pdf"), width=12, height=7)
opar=par(mfrow = c(1, 2))
#aheatmap(X, annCol = cell_info, annRow = gene_info)
aheatmap(nmf.input.log2, annCol = cell_info)
#aheatmap(nmf.input, scale = "column")
aheatmap(nmf.input.log2, scale = "row")
par(opar)
dev.off()




(2) 假设聚成5类
注意：1)输入变量不能有负数，2)行的和不能为0。
# vm[is.na(vm)]=0
vm[vm<0]=0
vm= vm[rowSums(vm) > 0, ]

library(NMF)
library(pheatmap)
res = nmf(nmf.input.log2, rank=5, nrun = 10) # 1min
res



(3) coefmap: 混合系数矩阵(cell)
# NMF结果的混合系数矩阵可以使用coefmap()函数进行绘制。
# 该函数默认添加2个注释通道用来展示从最佳拟合结果中获得的簇（聚类数）和一致性矩阵的层次聚类。

pdf(paste0(outputRoot, keyword, "_03_A02_res.coefmap.pdf"), width=12, height=4.5)
opar = par(mfrow = c(1, 2))
# coefmap from multiple run fit: includes a consensus track
coefmap(res)
# coefmap of a single run fit: no consensus track
coefmap(minfit(res))
par(opar)
dev.off()


# 自动注释的通道可以使用tracks=NA进行隐藏
pdf(paste0(outputRoot, keyword, "_03_A03_res.tracks_EQ_NA.coefmap.pdf"), width=12, height=5)
opar = par(mfrow = c(1,2))
# removing all automatic annotation tracks
coefmap(res, tracks = NA)
# customized plot
coefmap(res, Colv = 'euclidean',
        main = "Metagene contributions in each sample", labCol = NULL,
        annRow = list(Metagene = ":basis"), 
        #annCol = list(':basis', Class = a, Index = c),
        annCol = list(':basis', #status = clin$OS, 
                      year=clin$OS.time/365 ),
        annColors = list(Metagene = 'Set2'), 
        info = TRUE)
par(opar)
dev.off()





(4) basismap: 基底矩阵(gene)
pdf(paste0(outputRoot, keyword, "_03_A04_res.basismap.pdf"), width=8, height=6)
opar <- par(mfrow=c(1,2))
# default plot
basismap(res)
# customized plot: only use row special annotation track.
basismap(res, main="Metagenes", 
         #annRow=list(d, e), 
         tracks=c(Metagene=':basis'))
par(opar)
dev.off()




(5) consensusmap:一致性矩阵
pdf(paste0(outputRoot, keyword, "_03_A05_res.consensusmap.pdf"), width=12, height=6)
opar <- par(mfrow=c(1,2))
# default plot
consensusmap(res)
# customized plot
consensusmap(res, 
             annCol=cell_info, 
             annColors=list(status='orange'), 
             labCol='sample ', 
             main='Cluster stability', 
             sub='Consensus matrix and all covariates')
dev.off()



=> 报错：Error: C stack usage  7969332 is too close to the limit
https://stackoverflow.com/questions/14719349/error-c-stack-usage-is-too-close-to-the-limit

$ ulimit -s # print default
8192
$ R --slave -e 'Cstack_info()["size"]'
   size 
8388608

> Cstack_info()
      size    current  direction eval_depth 
   7969177      29216          1          2 

$ ulimit -s
8192

修改:
$ ulimit -s 16384 # enlarge stack limit to 16 megs
$ R --slave -e 'Cstack_info()["size"]'
    size 
16777216 

$ ulimit -s 102400 #设置为 105512M=105G?
$ ulimit -s 5512384 #设置为 5512M=5G
$ ulimit -s unlimited #采用

$ /usr/local/bin/R --slave -e 'Cstack_info()["size"]'
    size 
62632755




==> 权宜的解决方法：调整完毕后需要在终端中执行R脚本，这样才会应用到刚刚修改的配置信息
$ ulimit -s unlimited
$ R or Rscript xx.R #run in shell R then










4. 同一方法，计算多个rank的结果
res2_7 <- nmf(nmf.input.log2, 2:7, nrun=10, .options='v') #17:43->17:48

pdf(paste0(outputRoot, keyword, "_04_res.rank2_7.consensusmap.pdf"), width=18, height=11)
consensusmap(res2_7)
plot(res2_7) #绘制7个折线图，判断最佳分类数
dev.off()








5. 单个rank，多种方法 ----
res_methods <- nmf(nmf.input.log2, rank = 6, method = list('lee', 'brunet', 'nsNMF'), nrun=10) #17:52->17:55
class(res_methods)
# rank=6, good but 5 cluster
# rank=5, very bad;

#res.multi.method <- nmf(dat, 3, list('brunet', 'lee', 'ns'), seed=222)
compare(res.multi.method) #输出表格


pdf(paste0(outputRoot, keyword, "_05_res.methods3.consensusmap.pdf"), width=12, height=11)
consensusmap(res_methods)
dev.off()

#
#demo('aheatmap')







6. 判断最佳分组：TCGA 样本作为输入

dim(nmf.input.log2)
nmf.input.log2[1:4, 1:4]


## (1)第一步：尝试多个rank ----
library(NMF)
ranks <- 2:10
estim <- lapply(ranks, function(r){
  message("ranks=", r)
  # nrun设置为5以免运行时间过长
  fit <- nmf(nmf.input.log2, r, nrun = 5, seed = 4, method = "lee") 
  
  list(fit = fit, consensus = consensus(fit), .opt = "vp",coph = cophcor(fit))
})

names(estim) <- paste('rank', ranks)
sapply(estim, '[[', 'coph')

# 绘制随rank变化的cophenetic系数图
#png("Cophenetic coefficient for seleting optimal nmf rank.png")
pdf(paste0(outputRoot, keyword, "_06_A01_Cophenetic_coefficient.ranks.line.pdf"), width=5, height=4)
par(cex.axis=1.2)
plot(ranks, sapply(estim, '[[', 'coph'), 
     xlab="", ylab="", type="b", 
     col="red", lwd=3, xaxt="n")
axis(side = 1, at=1:10)
title(xlab="number of clusters", ylab="Cophenetic coefficient", 
      mgp=c(2.2,1,0),
      cex.lab=1.3)
dev.off()
# 理论上，根据 cophenetic 得分选择rank=N，因为它是拐点，但是常常也选择N-1作为最佳。




## (2)第二步：筛选signature ----
# 对样本聚类分组的基因
rank <- 6
seed <- 2019620
#rownames(nmf.input) <- gsub("Signature","Sig",rownames(nmf.input)) # 行名简化
mut.nmf <- nmf(nmf.input.log2, 
               rank = rank, 
               seed = seed, 
               method = "lee")
mut.nmf
index <- extractFeatures(mut.nmf, "max")
sig.order <- unlist(index) #只有第n个基因用于后续分析





## (3)第三步：使用挑选出的signature再次NMF ----
sig.order=sig.order[!is.na(sig.order)] #rm NA
nmf.input.log2_b=nmf.input.log2[sig.order, ]
dim(nmf.input.log2_b) #69 151
#
library(pheatmap)
pdf(paste0(outputRoot, keyword, "_06_A03_select_signature.pheatmap.pdf"), width=5, height=9)
pheatmap(nmf.input.log2_b, cluster_rows = T, cluster_cols = T,
         #scale = "row",
         show_colnames = F,
         clustering_method = "ward.D2",
         main="")
dev.off()

#
mut.nmf2 <- nmf(nmf.input.log2_b, 
                rank = rank, 
                seed = seed, 
                method = "lee") 
group <- predict(mut.nmf2) # 提出亚型 **====>>> 对样本的分类结果，可用于后续分析
table(group)
length(group) #151 cells

pdf(paste0(outputRoot, keyword, "_06_A04_select_signature.consensusmap.pdf"), width=4.5, height=5)
consensusmap(mut.nmf2)
consensusmap(mut.nmf2, 
             annCol=cell_info, 
             annColors=list(status='orange'), 
             labCol='sample ', 
             main='Cluster stability', 
             sub='Consensus matrix and all covariates')
dev.off()





ref:
https://blog.csdn.net/HappyRocking/article/details/42212155





========================================
聚类 及 最佳聚类数的确定 //toto
----------------------------------------
无监督聚类最常见的三种方法：K-means, 层次聚类, SOM 聚类。

自组织映射(SOM clustering)



silhouette [ˌsɪluˈet] n. （浅色背景衬托出的）暗色轮廓；剪影，（尤指人脸的）侧影；（人的）体形，（事物的）形状 v. 把……画成黑色轮廓像，使现出影像（或轮廓）


一致性聚类(consensus clustering)



Ref: 
https://zhuanlan.zhihu.com/p/24546995




========================================
|-- 无监督聚类：K-means 及其可视化
----------------------------------------
无监督聚类最常见的三种方法：K-means, 层次聚类, SOM 聚类。

1. 示例：对鸢尾花分类
(1) 执行就一行
dat=t(iris[,1:4]) #一列一个样本
colnames(dat)=paste0(iris$Species, "_", 1:ncol(dat))
dat[, 1:4]
#             setosa_1 setosa_2 setosa_3 setosa_4
#Sepal.Length      5.1      4.9      4.7      4.6
#Sepal.Width       3.5      3.0      3.2      3.1
#Petal.Length      1.4      1.4      1.3      1.5
#Petal.Width       0.2      0.2      0.2      0.2


# 选作：最佳分类个数
factoextra::fviz_nbclust( t(dat), kmeans, method = "silhouette")


# 聚类：对传入矩阵的行进行聚类
kclu=kmeans(t(dat),centers=3)


(2)查看和保存结果
str(kclu)
kclu$cluster |> head()

plot(kclu$withinss, type="o")

kclu.df=data.frame(
	type=kclu$cluster |> names(),
	cluster=kclu$cluster |> as.numeric(),
	row.names = 1
)
head(kclu.df)
write.table(kclu.df, paste0(outputRoot, keyword, "_01_0.Kmeans_cluster.txt"))



(3) 可视化
library("factoextra")
fviz_cluster(kclu, data = t(dat))



(4) 和真实标签比较
> table(kclu$cluster, iris$Species)

    setosa versicolor virginica
  1     50          0         0
  2      0          2        36
  3      0         48        14


(5) 最佳分类数的确定：轮廓系数法

# 辅助确定最佳聚类数 4.7*2.6
factoextra::fviz_nbclust( t(DPAU_2), kmeans, method = "silhouette")

library(cluster)
set.seed(101)
pamclu=cluster::pam(t(DPAU_2), k=7)

{
	pdf( paste0(outputRoot, keyword, "_01_2.K_means.7.silhouette.pdf"), width=6, height=5)
	df1=silhouette(pamclu)
	df1=head(df1, n=nrow(df1)) |> as.data.frame()
	plot(silhouette(pamclu), 
		 col = df1$cluster +1,
		 #xlim=c(min(df1$sil_width)-0.2, max(df1$sil_width))+0.2,
		 main=NULL)
	dev.off()
}

解释：
> ?cluster::pam
基于划分的聚类算法主要有K均值和K中心点算法，其他的方法都是这两种算法的变种。
Partitioning Around Medoids
Description: Partitioning (clustering) of the data into k clusters “around medoids”, a more robust version of K-means.






========================================
|-- 无监督聚类：层次聚类
----------------------------------------
无监督聚类最常见的三种方法：K-means, 层次聚类, SOM 聚类。

1. 单细胞的聚类

(1) 求距离

1) dist() 函数 求距离，求行之间的距离

在R基础包stat中已经包含了一个可以计算所有样本对之间的距离的函数dist(),dist()函数可用的距离有，“euclidean”、“maximum”、“manhattan”、“canberra”、“binary”或“minkowski”等。。

dist(x, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)


因为我们需要的是细胞之间的距离，所以，需要对原始的表达矩阵进行转置。
d <- dist(t(alldata@reductions[["pca"]]@cell.embeddings), method = "euclidean")


=> 实例: 求行之间的距离
dat=iris;
rownames(dat) = paste0(dat$Species, 1:nrow(dat))
head(dat)

d2=dist(dat[, 1:4]); d2

##
clst= hclust(d2, method = "ward.D2")
plot(clst)




2) cor() 函数 求相关

除了dist()计算的距离以外，我们还可以利用 cor() 计算细胞之间的相关性从而评估细胞之间的联系，但是相关性系数是从-1到1，其中小于0的数值在距离上并没有意义，所以，这个时候，我们需要做一些转换，把相关性系数转换为距离对象，这时候我们通过一个公式 
adj = (1 − cor(x) ) / 2
将其转换为邻接矩阵。

# 计算相关性
sample_cor <- cor(Matrix::t(alldata@reductions[["pca"]]@cell.embeddings))
# 转换为邻接矩阵
sample_cor <- (1 - sample_cor)/2
# 转换为距离对象
d2 <- as.dist(sample_cor)


=> 实例：
dat=iris;
rownames(dat) = paste0(dat$Species, 1:nrow(dat))

dat_cor = cor( t(dat[,1:4]) ) #按列求相关
#max(dat_cor); min(dat_cor)

dat_cor <- (1 - dat_cor)/2

d2b=as.dist(dat_cor)

##
clst2=hclust(d2b, method="ward.D2")
plot(clst2)




(2) 层次聚类
在计算完距离之后，就可以使用hclust函数进行层次聚类了，其中可以使用的方法有：“ward.D”、“ward.D2”、“single”、“complete”、“average”、“mcquitty”、“median”或“centroid”。这里仅展示"ward.D2"方法。


#1) euclidean，欧氏距离
h_euclidean <- hclust(d2, method = "ward.D2")
> table(cutree(h_euclidean,3), iris$Species )
    setosa versicolor virginica
  1     50          0         0
  2      0         49        15
  3      0          1        35

绘图，圈出来可能的分类：
plot(h_euclidean)
rect.hclust(h_euclidean, k = 3, border =c("#E41E25", "#FBD800", "#208A41") )



#2) correlation，相关性距离
h_correlation <- hclust(d2b, method = "ward.D2")
plot(h_correlation) #效果不好
> table(cutree(h_correlation,3), iris$Species )
    setosa versicolor virginica
  1     50          0         0
  2      0         33         0
  3      0         17        50



(3) 确定类型数
创建完树状图后，我们紧接着就是去定义哪些样本属于哪些集群，即在不同的级别以固定的阈值（cuttree）切割树以定义集群，从而定义集群的数量和高度，也可以尝试不同的集群。

> rs = cutree(h_euclidean,k = 3)
> str(rs)
 Named int [1:150] 1 1 1 1 1 1 1 1 1 1 ...
 - attr(*, "names")= chr [1:150] "setosa1" "setosa2" "setosa3" "setosa4" ...
> head(rs)
setosa1 setosa2 setosa3 setosa4 setosa5 setosa6 
      1       1       1       1       1       1 
> table(rs, iris$Species)
rs  setosa versicolor virginica
  1     50          0         0
  2      0         49        15
  3      0          1        35 
#





用于单细胞数据：
#euclidean distance
alldata$hc_euclidean_5 <- cutree(h_euclidean,k = 5)
alldata$hc_euclidean_10 <- cutree(h_euclidean,k = 10)
alldata$hc_euclidean_15 <- cutree(h_euclidean,k = 15)

#correlation distance
alldata$hc_corelation_5 <- cutree(h_correlation,k = 5)
alldata$hc_corelation_10 <- cutree(h_correlation,k = 10)
alldata$hc_corelation_15 <- cutree(h_correlation,k = 15)

library(ggplot2)
library(cowplot)
plot_grid(ncol = 3,
  DimPlot(alldata, reduction = "umap", group.by = "hc_euclidean_5")+ggtitle("hc_euc_5"),
  DimPlot(alldata, reduction = "umap", group.by = "hc_euclidean_10")+ggtitle("hc_euc_10"),
  DimPlot(alldata, reduction = "umap", group.by = "hc_euclidean_15")+ggtitle("hc_euc_15"),

  DimPlot(alldata, reduction = "umap", group.by = "hc_corelation_5")+ggtitle("hc_cor_5"),
  DimPlot(alldata, reduction = "umap", group.by = "hc_corelation_10")+ggtitle("hc_cor_10"),
  DimPlot(alldata, reduction = "umap", group.by = "hc_corelation_15")+ggtitle("hc_cor_15")




Ref:
https://blog.csdn.net/songyi10/article/details/126536239



========================================
|-- 无监督聚类：Self Organizing Maps (SOM) 一种基于神经网络的聚类算法 //todo
----------------------------------------
无监督聚类最常见的三种方法：K-means, 层次聚类, SOM 聚类。

另见 python/ 机器学习






========================================
|** 聚类算法常用内部评价指标：轮廓系数、肘部法（Elbow Method）、Gap statistic、Calinski-Harabasz Index（CH）、Davies-Bouldin Index（DB）
----------------------------------------
https://blog.csdn.net/xiaolong124/article/details/126345406


Determining optimal number of clusters (k)
* Elbow Method
* Silhouette Method
* Gap Static Method





========================================
|-- 无监督聚类数的评价: 肘部法（Elbow Method）
----------------------------------------
肘部法（Elbow Method）：该方法通过绘制聚类簇数与聚类内部平方和（Within-cluster Sum of Squares，WCSS）之间的关系图来确定最佳簇数。通过观察图形的肘部弯曲点，选择该点对应的簇数作为最佳簇数。


x=t(iris[,1:4])

library(cluster)
wss = sapply(1:10, function(k){kmeans( t(x), k)$tot.withinss})
plot(1:10, wss, type="b", xlab="Number of clusters", ylab="WCSS")





========================================
|-- 无监督聚类数的评价: Gap statistic
----------------------------------------

library(cluster)
set.seed(101)
# define the clustering function
pam1 <- function(x,k){
  list(cluster = pam(x,k, cluster.only=TRUE))
}

# calculate the gap statistic
mat=t(iris[, 1:4])
pam.gap= clusGap(t(mat), FUN = pam1, K.max = 8, B=50)

# plot the gap statistic accross k values
plot(pam.gap, main = "Gap statistic for the 'Leukemia' data")


选择方法：达到最大值的最小k。

Formally written, we would pick the smallest k satisfying the following condition:  
Gap(k)≥Gap(k+1)−s(k+1), where  s(k+1)  is the simulation error for Gap(k+1).




ref:
https://compgenomr.github.io/book/clustering-grouping-samples-based-on-their-similarity.html#how-to-choose-k-the-number-of-clusters




========================================
|-- 无监督聚类数的评价: 轮廓系数(silhouette Coefficient)
----------------------------------------
==> 文章实例
https://pubmed.ncbi.nlm.nih.gov/32929364/ Fig2
(A) Consensus heatmaps showing the robustness of sample classification using non-negative matrix factorization (NMF) clustering. 
(B) Silhouette plot displaying the composition (n = number of samples) and stability (average width) of clustering. 



1.轮廓系数 （Silhouette Coefficient）
对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值。
轮廓系数的取值范围是[-1,1]，同类别样本距离越相近，不同类别样本距离越远，分数越高。

(1) 定义
https://baike.baidu.com/item/轮廓系数/17361607

针对某个样本的轮廓系数s为：
	s=(b-a) / max(a,b)
	其中,a表示某个样本与其所在簇内其他样本的平均距离，
	b表示某个样本与其他簇样本的平均距离。

聚类总的轮廓系数SC为：
	SC=累加(i=1,N, si) / N;




(2)R包cluster::silhouette 函数 计算方法
n个样本分成 defined if 2 <= k <= n-1 才可以计算轮廓系数。

For each observation i, the silhouette width s(i) is defined as follows:
Put a(i) = average dissimilarity between i and all other points of the cluster to which i belongs (if i is the only observation in its cluster, s(i) := 0 without further calculations). For all other clusters C, put d(i,C) = average dissimilarity of i to all observations of C. The smallest of these d(i,C) is b(i) := \min_C d(i,C), and can be seen as the dissimilarity between i and its “neighbor” cluster, i.e., the nearest one to which it does not belong. Finally,

s(i) := ( b(i) - a(i) ) / max( a(i), b(i) ).

silhouette.default() is now based on C code donated by Romain Francois (the R version being still available as cluster:::silhouette.default.R).

Observations with a large s(i) (almost 1) are very well clustered, a small s(i) (around 0) means that the observation lies between two clusters, and observations with a negative s(i) are probably placed in the wrong cluster.


a)可见轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。
将所有点的轮廓系数求平均，就是该聚类结果总的轮廓系数：Average silhouette
a(i) ：i向量到同一簇内其他点不相似程度的平均值
b(i) ：i向量到其他簇的平均不相似程度的最小值

注：不相似程度 == (某种)距离

b)最佳聚类数的确定
轮廓系数是类的密集与分散程度的评价指标。

s(i) = [b(i) -a(i)] / max(a(i), b(i))

a(i)是测量i与组内的相似度，b(i)是测量i与组间的相似度。
s(i)范围从-1到1，值越大说明组内吻合越高，组间距离越远——也就是说，轮廓系数值越大，聚类效果越好。

注意：
* 上部分中所说的“距离”，指的是不相似度（区别于相似度）。“距离“值越大，代表不相似度程度越高。
* 欧氏距离就满足这个条件，而Tanimoto Measure 则用做相似度度量。
* 当簇内只有一点时，我们定义轮廓系数s(i)为0。








2. 代码实现: python 核心代码
    def silhouette_coefficient(self, data_num, cluster, dis_matrix):
        sum = 0
        for index in range(data_num):
            cluster_in = 0
            cluster_in_num = 0
            cluster_out = 0
            cluster_out_num = 0
            for class_index in range(len(cluster)):
                if index == class_index:
                    continue
                elif cluster[index] == cluster[class_index]:
                    cluster_in_num += 1
                    cluster_in += dis_matrix[index][class_index]
                else:
                    cluster_out_num += 1
                    cluster_out += dis_matrix[index][class_index]
            sum += (cluster_out / cluster_out_num - cluster_in / cluster_in_num) / max(cluster_out / cluster_out_num,
                                                                                       cluster_in / cluster_in_num)
        sc = sum / data_num
        return sc

轮廓系数为-1时表示聚类结果不好，
为+1时表示簇内实例之间紧凑，
为0时表示有簇重叠。

轮廓系数越大，表示簇内实例之间紧凑，簇间距离大，这正是聚类的标准概念。
- 簇内的样本应该尽可能相似。
- 不同簇之间应该尽可能不相似。


对于簇结构为凸的数据轮廓系数值高，而对于簇结构非凸需要使用DBSCAN进行聚类的数据，轮廓系数值低。
因此，轮廓系数不应该用来评估不同聚类算法之间的优劣，比如Kmeans聚类结果与DBSCAN聚类结果之间的比较。


应用
轮廓系数广泛应用于各种聚类算法的性能评估和比较，如K均值聚类、层次聚类、DBSCAN等。它也被用于确定最佳的聚类数目和帮助解释聚类结果。


https://blog.csdn.net/xiaolong124/article/details/126345406






3. R代码实现
(1) 
a)Average silhouette values for k-medoids clustering for k values between 2 and 7
library(cluster)
set.seed(101)
mat= t(iris[,1:4])
pamclu=cluster::pam(t(mat),k=3)
plot(silhouette(pamclu),main=NULL)



b)Silhouette values for k-medoids with k=2 to 7

mat= t(iris[,1:4])
Ks=sapply(2:7, function(i){
  summary(silhouette(pam(t(mat),k=i)))$avg.width
})
plot(2:7,Ks,xlab="k",ylab="av. silhouette",type="b", pch=19)
下跌前的最后一个最高点对应的k。





(2)示例: pam, k=7

library(factoextra)
fviz_nbclust(dataset, kmeans, method = "silhouette")


library(cluster)
set.seed(101)
pamclu=cluster::pam(t(DPAU_2), k=7)

{
	pdf( paste0(outputRoot, keyword, "_01_2.K_means.7.silhouette.pdf"), width=6, height=5)
	df1=head(df1, n=nrow(df1)) |> as.data.frame()
	plot(silhouette(pamclu), 
		 col = df1$cluster +1,
		 #xlim=c(min(df1$sil_width)-0.2, max(df1$sil_width))+0.2,
		 main=NULL)
	dev.off()
}



(3) kmeans 的轮廓系数图, kmeans, k=5
dat=DPAU_2
kclu=kmeans(t(dat), centers=5)

#kclu$clustering=kclu$cluster #add this list element: clustering
distance=dist( t(dat) )  #10min
kclu.sil=sortSilhouette( silhouette(kclu$cluster, dist = distance ) )
#rownames(kclu.sil)=colnames(dat)
head(kclu.sil)
#     cluster neighbor sil_width
#1226       1        2 0.1124117
#991        1        2 0.1113240
pdf( paste0(outputRoot, keyword, "_01_2.K_means.5.silhouette.pdf"), width=6, height=5)
df1=kclu.sil
df1=head(df1, n=nrow(df1)) |> as.data.frame()
plot(kclu.sil, 
     col = rev(df1$cluster+1),
     do.col.sort=F,
     main=NULL)

factoextra::fviz_silhouette(kclu.sil)
dev.off()



# 方法2: 
library(cluster)
library(factoextra)
sil = silhouette(kmeans(x,3))
summary(sil)
fviz_silhouette(sil)


Silhouette Method of kmeans: https://medium.com/codesmart/r-series-k-means-clustering-silhouette-794774b46586
例1 x:number of cluster, y:Average Silhouette Scores
silhouette_score <- function(k){
  km <- kmeans(df, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(df))
  mean(ss[, 3])
}
k <- 2:10
avg_sil <- sapply(k, silhouette_score)
plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)

# 另一种画法，图同上
factoextra::fviz_nbclust(df, kmeans, method='silhouette')


# Actual Clustering
km.final <- kmeans(df, 2)
## Total Within cluster sum of square

km.final$tot.withinss
[1] 113217528521
## Cluster sizes

km.final$size
[1] 375  65

data$cluster <- km.final$cluster
head(data, 6)














(4) 层次聚类的轮廓系数图, hclust, k=5

dat=DPAU_2
distance=dist( t(dat) ) 
out.hclust=hclust(distance, method = "ward.D2")

# visual
pdf( paste0(outputRoot, keyword, "_01_2.hclust.4.silhouette.pdf"), width=6, height=5)
plot(out.hclust,
     #hang = -1,
     #hang=0.1,
     hang=0,
     ann=F, axes=F, 
     labels = F, #no labels
     cex = 0.7,
     col = "grey20")
rect.hclust( out.hclust, k=4, border = c("#FF6B6B", "#4ECDC4", "#556270", "deeppink") )
# sil plot
out.hclust.D2=cutree(out.hclust, k=4)
sil_hclust=sortSilhouette(silhouette(out.hclust.D2, distance))
rownames(sil_hclust) = rownames(as.matrix(distance))[attr(sil_hclust, 'iOrd')]
#
plot(sil_hclust, 
     col=out.hclust.D2[rownames( head(sil_hclust, n=nrow(sil_hclust)) )]+1,
     main=attr(sil_hclust, "call") |> deparse() )
dev.off()



不绘制样本名：
mat=t(iris[,1:4])
hcl=hclust(dist(t(mat)))
plot(hcl,labels = FALSE, hang= -1)
#rect.hclust(hcl, h = 80, border = "red")
rect.hclust(hcl, k=3, border = c("red", "orange", "cyan") )







(5) NMF 的轮廓图
http://www.idata8.com/rpackage/NMF/silhouette.NMF.html
plot(silhouette(res))

示例: 
pdf(paste0(outputRoot, keyword, "_03_rank_3.silhouette.pdf"), width=5.5, height=5.5)
# no color
#sil_3=silhouette(res_3)
#plot(sil_3)

#set color
sil_3C=sortSilhouette( silhouette(res_3) )
df1=sil_3C
df1=head(df1, n=nrow(df1)) |> as.data.frame()
plot(sil_3C, 
     col=(df1$cluster+1),
     main=attr(sil_3C, "call") |> deparse() )
dev.off()





ref:
[1] Rousseeuw, P.J. (1987) Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. J. Comput. Appl. Math., 20, 53-65.
[2] chapter 2 of Kaufman and Rousseeuw (1990), see the references in plot.agnes.










========================================
|-- 无监督聚类数的评价：一致性聚类（consensus），通俗理解： 随机抽样后，两两样品在一个分类的概率
----------------------------------------
1. 初次遇到

https://pubmed.ncbi.nlm.nih.gov/32929364/ Fig.2A

原始文献：
Monti,S., et al. (2003) Consensus clustering: A resampling-based method for class discovery and visualization of gene microarray, Mach Learn,52, 91-118








2. 作用
https://blog.csdn.net/nixiang_888/article/details/122224201

当我们听到一致性聚类的时候，有时候我们会认为这是一种聚类方法。如果您也这样想的话，那就完全入坑了。下面我们来说一下，什么是一致性聚类（consensus）?

一致性聚类是为不同的聚类算法，选择最优的聚类数量（K）。其具体的原理是：通过改变聚类的数据集（里面的数据全部从原始数据中抽取，也可以理解为是原始数据的子集），考量任意一个数据在不同样本中聚类表现的一致性来确定聚类的参数是否合适。


第一步：从原始数据中随机抽取子集，当然子集的规模不能太小，最好是原始数据集的半数以上（这是我自己理解的，数据太少聚类的话没有意义），子集要尽量多，以确保里面的每一个数据都多次被取到（100次以上），然后，我们选择任意一种聚类方法，可以使K-means或者层次聚类，对所有的数据子集分别聚类。

第二步：这一步的关键在于建立一个新的矩阵：consensus matrix， 我们之前说聚类的输入通常是一个distance matrix。 那么consensus matrix怎么建呢？假设有D_1，D_2...D_n 这N个数据，那么consensus matrix是 N × N N \times NN×N 的方阵。
D1 D2 D3… Dn
D1 C11 C12 C13… C1n
D2 C21 C22 C23… C2n
… Cij
Dn Cn1 Cn2 Cn3… Cnn

其中，Cij 代表的是在多次的聚类过程中，数据 Di 和数据 Dj 被聚到同一类里面的概率（该值在0和1之间），
等于1代表100次聚类这两个数据点全部在同一个类里面，
等于0代表代表100次聚类全部不在同一个类里面。

我们取不同的k值时，显然聚类效果不同，越是干净，越是效果好；不好的聚类参数则表现出越是有很多“噪音”。

有些情况下，仅仅通过看不同参数下consensus matrix聚类出来的热图就基本可以判断怎么选择了。当然还有根据consensus matrix得来的数学参数用于评估聚类的稳定性的








3. 步骤
https://blog.csdn.net/qq_30057549/article/details/88351895

for i in 1:k 
    for each iter:
        - 随机取子集
        - 运行聚类算法
        - 保存聚类标签
    创建一致性矩阵 D.k 
依赖一致性分布，返回 最佳 D.k


https://blog.csdn.net/nixiang_888/article/details/122224201

对原始数据抽样 100 次，聚类。
计算 一致性矩阵 consensus matrix：
    行列都是样本名，
    行列交叉的值是：每次聚类 两两样品 聚到一个类中的概率。
        （该值在0和1之间），等于1代表100次聚类这两个数据点全部在同一个类里面，等于0代表代表100次聚类全部不在同一个类里面。
使用 层次聚类 画出 consensus matrix。


好的聚类方法会得到怎么样的consensus matrix呢？
对了，全部由0或1组成的方阵，代表着那些很像的数据总在一类，而不像的数据则总是不在一类，这正符合了聚类的初衷是吧。
再对consensus matrix做一次聚类(这里用层次聚类方便可视化）
- 我们取不同的k值时，显然聚类效果不同，越是干净，越是效果好；
- 不好的聚类参数则表现出越是有很多“噪音”。





4. NMF 拟合实例 
一致性矩阵：consensusmap
当使用NMF进行矩阵的时候，一种评估基于指定rank评估聚类稳定性的方法是考虑由多个独立NMF运行结果计算得到的连接矩阵。

(1) 单个拟合
我们所用数据res设定的参数是nrun=10，因此包含了10次运行得到的最佳结果以及基于所有运行的一致性矩阵。

opar <- par(mfrow=c(1,2))
# default plot
consensusmap(res)

# customized plot
consensusmap(res, annCol=covariates, annColors=list(c='blue')
    , labCol='sample ', main='Cluster stability'
    , sub='Consensus matrix and all covariates')
par(opar)


(2) 同一个方法，计算多个rank的结果
函数nmf可以接受一组rank序列用来拟合多个不同的rank的结果。
res2_7 <- nmf(X, 2:7, nrun=10, .options='v')

然后可以同样的画图
consensusmap(res2_7)


(3) 单个rank，多种方法
可以比较同一rank不同方法的结果。

res_methods <- nmf(X, 3, list('lee', 'brunet', 'nsNMF'), nrun=10)
class(res_methods)
consensusmap(res_methods)


通用热图引擎：aheatmap
还有很多自定义画热图的例子，使用下面的命令查看。

demo('aheatmap')
# or
demo('heatmaps')


ref:
https://www.jianshu.com/p/f5cbbe771ce2












========================================
R语言与神经网络 //todo
----------------------------------------





ref:
R语言 建立一个简单的神经网络 https://geek-docs.com/r-language/r-tutorials/g_building-a-simple-neural-network-in-r-programming.html






========================================
****** 广告 ******
----------------------------------------




========================================
临床预测模型 速成班
----------------------------------------
#################
###day1
#################

一、快速掌握预测模型构建神器STATA\R软件
1、熟练应用SPSS,手把手让你快速学会STATA\R，无需相关统计软件的前期知识。
2、全程实战数据现场演示特征筛选、变量选择中三大法宝（逐步法、最优子集、Lasso）；模型评价中的三大法宝（AIC\BIC\Adjr2）。
3、发放STATA和R软件全套代码（形成2张Table，5张Figure），学员无需编程基础，对号入座，极大降低统计软件的学习成本。

二、反复练习掌握Liner\Logistic\Cox回归构建预测模型的策略
1、实战经验告知你不同类别预测模型构建的关键点、侧重点、闪光点、以问题为导向的案例式教学，帮你快速厘清建模时的哑变量、交互作用、调整作用、混杂因素、独立作用、阈值作用……




#################
### day2
#################
一、实战临床预测模型-验证部分实战课
1、快速实战体会什么是内部验证和外部验证？模型的可重复性Reproducibility，Cross-Validation、Bootstrap 验证等方法？如何体现模型的外推性（generalizability）？
2、2个以上模型比较的套路流程与模型改进思路

二、实战经验分析二分类结局（logistic回归）与time-to-event数据（COX回归）的similar与dissimilar
1、模型改进验证特点
2、快速理解NRI\IDI等最新评价指标




#################
### day3
#################
一、来自实战的数据，手把手的教你结构化的研究方式，利用STATA、R编程，每一个学员都能亲自独立完成基本的临床预测模型研究。（DCA分析结果、Nomogram图的制作代码等）

二、临床预测模型国际指南（TRIPOD）详细解读

三、既往学员的案例示范讨论，经典预测模型高分文章解析，回复审稿人问题的技巧，国际上喜欢刊登临床预测模型文章的期刊分析，开拓你的眼界……




ref:
https://mp.weixin.qq.com/s?__biz=MjM5MDc0NDU2NQ==&mid=2650376120&idx=1&sn=bac5fdce4914355189e2dadd4b239360




========================================
|-- 文章实例：解决的问题，及用到的机器学习方法
----------------------------------------
1.[4.856, 2019] 核磁图像提高宫颈癌淋巴结转移的诊断效果

Radiother Oncol. 2019 Jun 25;138:141-148. doi: 10.1016/j.radonc.2019.04.035. [Epub ahead of print]
Radiomics analysis of magnetic resonance imaging improves diagnostic performance of lymph node metastasis in patients with cervical cancer.

1. Department of Medical Imaging, Henan Provincial People's Hospital, Zhengzhou, China; Zhengzhou University People's Hospital, Zhengzhou, China; Henan University People's Hospital, Zhengzhou, China.


A total of 189 cervical cancer patients were divided into a training cohort (n = 126) and a validation cohort (n = 63). For each patient, we extracted radiomic features from intratumoral and peritumoral tissues on sagittal T2WI and axial apparent diffusion coefficient (ADC) maps. Afterward, the radiomic features associated with LNM status were selected by univariate ROC testing and logistic regression with the least absolute shrinkage and selection operator (LASSO) penalty in the training cohort. Based on the selected features, a support vector machine (SVM) model was established to predict LNM status. To further improve the diagnostic performance, a decision tree which combines the radiomics model with clinical factors was built.

主要方法: ROC testing + logistic regression+LASSO;
SVM;
decision tree;





2. [3.303,2019] 甲状腺超声，预测滤泡性甲状腺癌的远端转移
J Clin Med. 2020 Jul 8;9(7). pii: E2156. doi: 10.3390/jcm9072156.
Radiomics Based on Thyroid Ultrasound Can Predict Distant Metastasis of Follicular Thyroid Carcinoma.

1. Department of Radiology, Kangbuk Samsung Hospital, Sungkyunkwan University School of Medicine, Seoul 03181, Korea.

A radiomics signature was generated using the least absolute shrinkage and selection operator and was used to train a support vector machine (SVM) classifier in five-fold cross-validation. The SVM classifier showed an area under the curve (AUC) of 0.90 on average on the test folds. 

主要方法： LASSO + SVM; AUC;






3. [2.687,2019IF] 对早期宫颈癌盆腔淋巴结转移的术前预测
Eur J Radiol. 2019 May;114:128-135. doi: 10.1016/j.ejrad.2019.01.003. Epub 2019 Mar 20.
Preoperative prediction of pelvic lymph nodes metastasis in early-stage cervical cancer using radiomics nomogram developed based on T2-weighted MRI and diffusion-weighted imaging.

1. Department of Medical Imaging, First Affiliated Hospital of Xi'an Jiaotong University, No.277, West Yanta Road, Xi'an, 710061, Shaanxi, People's Republic of China; Department of Radiology, Shaanxi Provincial People's Hospital, Xi'an, Shaanxi, 710068, People's Republic of China.


//Radiomics features extracted from T2WI and DWI were selected by least absolute shrinkage and selection operation regression for further radimoics signature calculation. The discrimination of this radiomics signature for PLN metastasis was then assessed using a support vector machine (SVM) model. 

主要方法： LASSO + SVM;






4.[2.327,2019IF] 蛋白的细胞亚定位

J Theor Biol. 2018 Nov 16. pii: S0022-5193(18)30565-4. doi: 10.1016/j.jtbi.2018.11.012. [Epub ahead of print]
Identification of protein subcellular localization via integrating evolutionary and physicochemical information into Chou's general PseAAC.
Shen Y1, Tang J2, Guo F3.
Author information:
1. School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Yaguan Road, Jinnan District, Tianjin, P.R.China. Electronic address: shenyinan@tju.edu.cn.

// In this paper, we propose a multi-kernel SVM to predict subcellular localization of both multi-location and single-location proteins. 

First, we make use of the evolutionary information extracted from position specific scoring matrix (PSSM) and physicochemical properties of proteins, by Chou's general PseAAC and other efficient functions. 

Then, we propose a multi-kernel support vector machine (SVM) model to identify multi-label protein subcellular localization. As a result, our method has a good performance on predicting subcellular localization of proteins. It achieves an average precision of 0.7065 and 0.6889 on two human datasets, respectively.

主要方法: 多核SVM;







5. [4.101,2019IF] 非侵袭直肠癌的核磁成像，评估5个术后指标

Eur Radiol. 2018 Nov 9. doi: 10.1007/s00330-018-5763-x. [Epub ahead of print]
Preoperative radiomic signature based on multiparametric magnetic resonance imaging for noninvasive evaluation of biological characteristics in rectal cancer.
Meng X1, Xia W2, Xie P1, Zhang R 2, Li W1, Wang M2, Xiong F1, Liu Y2, Fan X3, Xie Y1, Wan X4, Zhu K5, Shan H6, Wang L7, Gao X8.
Author information:
1. Department of Radiology, Sixth Affiliated Hospital of Sun Yat-sen University, Guangzhou, 510655, China.

//Radiomic features were extracted from MP-MRI and then refined for reproducibility and redundancy. The refined features were investigated for usefulness in building radiomic signatures by using two feature-ranking methods (MRMR and WLCX) and three classifiers (RF, SVM, and LASSO). Multivariable logistic regression was used to build an integrated evaluation model combining radiomic signatures and clinical characteristics. The performance was evaluated using an independent validation dataset comprising 148 patients.

//The MRMR and LASSO regression produced the best-performing radiomic signatures for 

主要方法: MRMR + LASSO;





6. [2.826,2019IF] 转移性肾细胞肾癌中的VEGF和TKI

Pathol Oncol Res. 2017 Sep 29. doi: 10.1007/s12253-017-0323-2. [Epub ahead of print]
Development of Response Classifier for Vascular Endothelial Growth Factor Receptor (VEGFR)-Tyrosine Kinase Inhibitor (TKI) in Metastatic Renal Cell Carcinoma.
Go H1, Kang MJ1, Kim PJ2, Lee JL 3, Park JY4, Park JM1, Ro JY5, Cho YM6.
Author information:

1
Department of Pathology, Asan Medical Center, University of Ulsan College of Medicine, 88, Olympic-ro 43-gil, Songpa-gu, Seoul, 05505, Republic of Korea.

//Clinicolaboratory-histopathological data, 41 gene mutations, 20 protein expression levels and 1733 miRNA expression levels were compared between clinical benefit and non-benefit groups. The classifier was built using support vector machine (SVM). Seventy-three patients were clinical benefit group, and 28 patients were clinical non-benefit group. 

主要方法: SVM; simple decision tree





7. [3.632,2019IF]
	
Developing a new radiomics-based CT image marker to detect lymph node metastasis among cervical cancer patients
Comput Methods Programs Biomed. 2020 Sep 16;197:105759. doi: 10.1016/j.cmpb.2020.105759. Online ahead of print.

1 School of Electrical and Computer Engineering, University of Oklahoma, Norman, OK, 73019, USA.





8.[7.971, 2019IF] （临床预测）整合3’UTR来建立模型，6基因构成，识别可手术的三阴性乳腺癌的腋窝淋巴结转移风险。

Wang, L., et al. (2019). "Integrative 3' Untranslated Region-Based Model to Identify Patients with Low Risk of Axillary Lymph Node Metastasis in Operable Triple-Negative Breast Cancer." Oncologist 24(1): 22-30.

We evaluated 3' untranslated region (3'UTR) profiles using microarray data of TNBC from two Gene Expression Omnibus datasets. Samples from GSE31519 were divided into training set (n = 164) and validation set (n = 163), and GSE76275 was used to construct testing set (n = 164). 

We built a six-member 3'UTR panel (ADD2, COL1A1, APOL2, IL21R, PKP2, and EIF4G3) using an elastic net model to estimate the risk of lymph node metastasis (LNM). 

A combinatorial analysis of the 3'UTR panel and tumor size yielded an accuracy of 97.2%, 100%, and 100% in training, validation, and testing set, respectively.






========================================
|-- 单细胞 + 机器学习
----------------------------------------
1. Single-cell sequencing and establishment of an 8-gene prognostic model for pancreatic cancer patients
https://pubmed.ncbi.nlm.nih.gov/36237305/
Front Oncol. 2022 Sep 28


2.







========================================
Cibersort 免疫浸润的在线分析及R语言代码实现 //todo
----------------------------------------
https://zhuanlan.zhihu.com/p/433506218






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------






========================================
----------------------------------------




