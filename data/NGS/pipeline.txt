构建生信分析流程，能大大提高生产力！
这一篇主要是 snakemake.


本文从理论和技术上描述如何构建生信分析流程。
一个好的生物信息分析流程可以让你事倍功半，有效减负，同时也有利于他人重复你的数据分析结果。

难点是：脚本和编程语言之间的交互调用，及反馈。




警告: 不要花太多时间在非创新事务上！除非有理由这么做！
  还是跟小孩一样，喜欢玩，尤其是这种搭积木的游戏，其实真的没什么技术含量。
  但科研分析是反标准化的，如果你的paper只是pipeline出来的，那最多也就3分。
  
作为researcher，pipeline这种东西，看懂别人的就行，真没必要耗费大部分精力去自己搭建和完善，除非你是真的想转行做技工了。
找到自己的定位，明确自己的科学问题，协助发现有意义的生物学规律，发paper，申请funding，冲刺顶刊，不要玩物丧志。



todo:
	一个流程可视化py工具 https://github.com/spotify/luigi




========================================
生信分析流程pipeline概述
----------------------------------------
在生信公司里，有相当一部分人都是负责编写流程和维护流程。好的流程，能大大降低使用难度，进而节约人力成本。

流程pipeline，就是让用户（前端）在填写配置文件后能够一键运行的脚本，最终自动得出结果，并生成报告，有的公司还会将结果上传。


我觉得一个好的流程一定得模块化, 各个功能之间相互独立, 大问题划分为小问题, 这样在写代码、调试和以后流程修改时就非常简单和方便，效率大大的提高了。 

一个好的生物信息分析流程可以让你事倍功半，有效减负，同时也有利于他人重复你的数据分析结果。
别人要干一个季度的事，我一个月就可以搞定。



1.已有的生信分析流程开发方案，五花八门。
其中，使用统一的管道（pipeline）、工作流程（workflow）就是其中最重要的一环。
根据生信信息学数据分析流程（管道、工作流程序）构建的风格和方式，大致有以下几大流派

	脚本语言流
	Common Workflow language 语言流
	Makefile流
	配置文件流
	Jupyter notebook和R markdown流 //这个可以自动执行吗？
	……

A review of bioinformatics pipeline framework (https://www.ncbi.nlm.nih.gov/pubmed/27013646) 的作者对已有的工具进行很好的分类
awesome-pipeline: https://github.com/pditommaso/awesome-pipeline



Workflow systems turn raw data into scientific knowledge
https://www.nature.com/articles/d41586-019-02619-z
How workflow tools can make your computational methods portable, maintainable, reproducible and shareable.






(1)脚本语言流，主要是通过简单的脚本语言（如shell，R，Python，Perl）运行各类命令行脚本/程序。常见的几种工作模式：
	1)单个脚本就是一整个流程
	2)多个脚本组成一个流程
	3)封装成可以输入参数的命令行程序
	4)封装成函数/模块/包（包含示例文件、文档和测试）

前两种（1和2）是大多数生物信息学初学者（不具备封装和打包能力）最早开始接触生信分析流程的方式。
后两种（3和4）是专业人员开发新工具、新流程的必备技能。

使用和开发这类工具/流程的主要原因：
	只需要掌握原生编程语言的语法和命令行工具的用法就可以开始构建工具/流程
	其他流程化语言/框架也可以直接调用这些脚本/函数/模块/包/命令行程序
	封装和打包可以减少代码的冗余程度、降低维护难度
	通过使用各类编程语言自带的包管理器解决依赖问题，便于其他用户安装和调用

我目前主要是R语言、Python写命令行程序、函数、R包/模块。


基于shell的pipleline实例：
史上最快的转录组定量流程 https://mp.weixin.qq.com/s?__biz=MzAxMDkxODM1Ng==&mid=2247485134&idx=1&sn=396fb3826573fbc5a9b0ea33695ef764




(2)Common Workflow language（CWL）语言
Common Workflow language 语言流是近几年兴起的，专门用于数据分析流程构建的一类语言/工具。

这类语言/工具最核心的部分：定义每一个计算过程（脚本）的输入和输出，然后通过连接这些输入和输出，构成数据分析流程（图二，图三）（如 Galaxy, wdl，cromwell，nextflow，snakemake，bpipe 等）。

示例项目：

chip-seq-pipeline2：ENCODE Transcription Factor and Histone ChIP-Seq processing pipeline （chipseq_pipeline的WDL版）.

ngsjs-wkfl-wdl: A library of next-generation sequencing data analysis workflow (WDL).

biowdl: BioWDL is a collection of pipelines and workflows usable for a variety of sequencing related analyses. They are made using WDL and closely related to BIOPET.

snakemake-workflows: An experimental repo for common snakemake rules and workflows.




(3)Makefile 流

(4)配置文件流

(5) Jupyter notebook 和 R markdown 流









2. 生信分析流程开发方案选择策略
	模块化：方便调试，改进
	配置文件化：方便统一配置软件和库的路径。
	绝对路径：软件、输入、输出都使用绝对路径，方便管理，提高可移植性。
	有反馈可控：如果调用失败，要及时暂停脚本，并通知用户检查错误原因
	可视化报告：方便阅读、交付。
	可重复制图： 可视化之前要保存中间数据文件，便于快速美化图片；
	日志文件：记录执行状态，执行起止时间（年月日、时分秒）
	版本管理： 有版本管理，记录每次修改
	使用简单： 按照说明，配置好路径等，运行启动脚本，就可以等着收结果了。

高级性能：
	断点重启：断电，或者意外出问题，可以排错后继续
	并发执行：可配置并发执行，缩短时间
	打包：逐步抽象成函数、类、库、模块等。




3. 自动化pipeline必须具备的功能：
	读取配置文件，生成相应的待执行的脚本
	按照先后逻辑关系依次向集群投递任务
	能将大任务分割成小的任务，并行执行，缩短项目周期
 

4. 必须使用的工具：
	一门脚本语言，Shell、Perl 或 Python
	集群调度系统工具，SGE 或 monitor
	工具必须都使用绝对路径，因为这脚本是给同部门的人用的，如果你要提高可移植性，可以将工具路径写到配置文件里。
	写流程前，最好先画出 pipeline 的分析流程图
	配置好软件工具及库的路径




5.实现手段
留言： 我们采用argo来做workflow引擎，运行在k8s里面，可以做到系统高度弹性。



结论：
使用shell写流程，不方便记录日志，所以决定使用python，主要还是因为我对python相对比较熟悉吧，有安全感。




refer:
1.李剑峰的博客：
生信分析流程构建的几大流派  https://life2cloud.com/cn/2018/11/pipelines-styles/
影响生信分析流程开发方案选择的几大因素 https://life2cloud.com/cn/2019/02/how-to-choose-the-way-of-bioinformatics-pipeline/

2.部署一个简单的生信分析流程  https://www.cnblogs.com/leezx/p/6109911.html

3.一个大师级综述：Jeremy Leipzig, A review of bioinformatic pipeline frameworks, Briefings in Bioinformatics, Volume 18, Issue 3, May 2017, Pages 530–536, https://doi.org/10.1093/bib/bbw020
Keywords: pipeline, workflow, framework

https://mp.weixin.qq.com/s/L04wd4Ud3eQ5wP6EWtke5A











========================================
基于python的流程构建
----------------------------------------
用python调用python、R和shell，并传入参数，接收返回值。


========================================
|-- py调用系统命令 os.system()和os.popen()
----------------------------------------
1. 使用os模块
Python调用Shell，有两种方法：os.system(cmd)或os.popen(cmd)脚本执行过程中的输出内容。实际使用时视需求情况而选择。

两者的区别是：
	os.system(cmd)的返回值是脚本的退出状态码，只会有0(成功),1,2
	os.popen(cmd)返回脚本执行的输出内容作为返回值

比较：
	os.system() 是将执行结果在 Shell 中执行后，将执行后的状态码返回，0 表示为执行成功。
	os.popen()  是将执行结果直接返回，返回的是一个内存地址，用 read() 将内存地址中的数据解析。（PS：地址或者叫对象，C 程序员比较能理解地址这类说法）




(1)如果需要传参数，就用os.system()；
import os
os.system("python filename.py arg1")

示例：
>>> os.system('md5sum /root/all.sql')
7735d611ebce91ebb4c7acc4747a8b67  /root/all.sql
0      #返回的信号代码  0(成功)




(2)如果还想获得这个文件的输出，就用os.popen();
这种调用方式是通过管道的方式来实现，函数返回一个file-like的对象，里面的内容是脚本输出的内容（可简单理解为echo输出的内容）。

示例：
>>> md5_value = os.popen('md5sum /root/all.sql')    #将结果赋值给变量
>>> print(type(md5_value))          #查看类型
<class 'os._wrap_close'>
>>> print(md5_value.read().split()[0])         #取值
7735d611ebce91ebb4c7acc4747a8b67


明显地，像调用”ls”这样的shell命令，应该使用popen的方法来获得内容






(3) 不好用，不推荐：execfile('xx.py')，括号内为py文件路径；
python3 删去了 execfile()，代替方法如下：
with open('test1.py','r') as f:
    exec(f.read())
#能逐行执行，不能给被调用脚本传递参数。



https://blog.csdn.net/yilovexing/article/details/77981576
https://www.cnblogs.com/xhyan/p/8383814.html




========================================
|-- py使用os.system调用其他脚本(py, R 和shell)，并传入参数
----------------------------------------
1.py脚本中使用sys.argv接收参数
py下标从0开始，第0个是脚本文件名本身，第1个是第1个参数。

(1)$ cat b2.py
import sys
print("from b2.py, paras = ",sys.argv)

$ python b2.py "path/to/fastq/xx.fq"
from b2.py, paras =  ['b2.py', 'path/to/fastq/xx.fq']

#sys.argv是一个数组，内容是字符串。
#sys.argv[0] 类似于shell中的$0,是脚本名称  
#sys.argv[1] 表示传入的第一个参数

(2)py调用py，并传递参数
$ cat a3.py
import os
keyword="aKeyWord"
os.system("python b2.py paras from a3 "+keyword)
print("py3 end==")

$ python a3.py
from b2.py, paras =  ['b2.py', 'paras', 'from', 'a3', 'aKeyWord']
py3 end==




2.R脚本使用commandArgs(TRUE) 接收参数
R的下标从1开始，第一个就是第一个参数。

(1)$ cat my2.R 
#获取命令行参数
myArgs<-commandArgs(TRUE)

#myArgs是所有参数的特征向量
print(myArgs) 
print(class(myArgs))
myArgs[1]
myArgs[2]


直接执行
$ Rscript my2.R 10 25
[1] "10" "25"
[1] "character"
[1] "10"
[1] "25"


(2)py调用R，并传递参数
$ cat a4.py 
import os
keyword="aKeyWord"
os.system("Rscript my2.R paras from a3 "+keyword)
print("py3 end==")


$ python a4.py
[1] "paras"    "from"     "a3"       "aKeyWord"
[1] "character"
[1] "paras"
[1] "from"
py3 end==




3.shell脚本使用 $0(脚本文件名本身), $1(第一个参数), $2 接收参数
(1)$ cat my3.sh
echo "p0=$0, p1=$1, p2=$2";

$ bash my3.sh 1 2 good
p0=my3.sh, p1=1, p2=2


(2)py调用shell，并传递参数
$ cat a5.py 
import os
keyword="aKeyWord"
os.system("bash my3.sh paras from a3 "+keyword)
print("py3 end==")


$ python a5.py
p0=my3.sh, p1=paras, p2=from
py3 end==




========================================
|-- py使用os.popen()调用其他脚本、传入参数，并接收返回值
----------------------------------------
os.popen() 方法用于从一个命令打开一个管道。

语法格式：os.popen(command[, mode[, bufsize]])
参数
	command -- 使用的命令。
	mode -- 模式权限可以是 'r'(默认) 或 'w'。
	bufsize -- 指明了文件需要的缓冲大小：0意味着无缓冲；1意味着行缓冲；其它正值表示使用参数大小的缓冲（大概值，以字节为单位）。负的bufsize意味着使用系统的默认值，一般来说，对于tty设备，它是行缓冲；对于其它文件，它是全缓冲。如果没有改参数，使用系统的默认值。

#

仅就py调用shell举例
$ cat a1.py 
import os
rs1=os.system("ls -lth");
rs2=os.popen("ls -lth");

print("rs1=", rs1);
print("rs2=", rs2);

#
print(type(rs2) );

ss=rs2.read();
print(ss);
print( ss.split() );


运行后的输出结果：
$ python a1.py
total 12K
-rw-rw-r-- 1 wangjl wangjl 172 Jul 22 14:57 a1.py
-rw-rw-r-- 1 wangjl wangjl  97 Jul 22 14:17 a5.py
-rw-rw-r-- 1 wangjl wangjl  28 Jul 22 14:08 my3.sh
rs1= 0
rs2= <os._wrap_close object at 0x7fb24b4a5860>
<class 'os._wrap_close'>  #type
total 12K
-rw-rw-r-- 1 wangjl wangjl 172 Jul 22 14:57 a1.py
-rw-rw-r-- 1 wangjl wangjl  97 Jul 22 14:17 a5.py
-rw-rw-r-- 1 wangjl wangjl  28 Jul 22 14:08 my3.sh

['total', '12K', '-rw-rw-r--', '1', 'wangjl', 'wangjl', '172', 'Jul', '22', '14:57', 'a1.py', '-rw-rw-r--', '1', 'wangjl', 'wangjl', '97', 'Jul', '22', '14:17', 'a5.py', '-rw-rw-r--', '1', 'wangjl', 'wangjl', '28', 'Jul', '22', '14:08', 'my3.sh']








========================================
|-- Python执行shell脚本的几种方式（最好用 subprocess.Popen 来替代os.system）
----------------------------------------
1. Python3中subprocess.Popen与os.popen的区别 https://blog.csdn.net/Ls4034/article/details/89386857
os.popen其实只是对subprocess.Popen的封装。
多个os.popen共同执行，还不能保证谁先执行完。
具体看文章： 




2.subprocess模块

subprocess，译作“子进程”。在运行Python程序时，都是在创建并运行一个进程。在Python中，可通过标准库的subprocess模块 来fork一个子进程，并运行一个外部程序（如执行一条cmd命令）。

subprocess模块是Python 2.4新增的一个模块，它允许生成新的进程（子进程），连接到它们的input、output、error管道，并获取它们的返回（状态）码。subprocess模块的目的：替换几个旧的模块和方法（如os.system()、os.spawn*()，甚至os.popen，当然commands更是舍弃了）。

subprocess模块中定义了数个创建子进程的函数，这些函数分别以不同的方式创建子进程，可根据需要进行选取。subprocess还提供了管理标准流（standard stream）和管道（pipe）的工具，从而在进程之间使用文本通信。


结论：
The recommended approach to invoking subprocesses is to use the run() function for all use cases it can handle. For more advanced use cases, the underlying Popen interface can be used directly.

1).若使用的是Python >=3.5，Python官方给出的建议是尽量使用subprocess.run()函数。
2).当subprocess.call()、subprocess.check_call()、subprocess.check_output()、subprocess.run()这些高级函数无法满足需求时，可使用subprocess.Popen类来实现所需要的复杂功能。

3).个人感觉，还是 subprocess.getstatusoutput 方法使用比较便捷。

import subprocess
fastaPath="/home/wangjl/data/apa/190610APA/bed/all/bed/pas_bed/"

cmd1="bedtools getfasta -fi /home/wangjl/data/ref/hg19/hg19.fa -bed "+fastaPath+cid+".up101_down100.bed -s -fo "+fastaPath+cid+".201.fasta"
print(cmd1)

(status1, output1)=subprocess.getstatusoutput(cmd1)







refer:
https://docs.python.org/3.6/library/subprocess.html
https://blog.csdn.net/weixin_38256474/article/details/83117270





========================================
|-- Python3脚本中使用另一个py中的函数
----------------------------------------

1.同一个目录下的调用
(1). 定义函数库
$ cat fn_add.py 
def add(a,b):
        return a+b;

(2). 同一个目录下，直接import文件名，引用函数时要加上文件名前缀
$ cat a1.py 
import fn_add
print( fn_add.add(2,4) );

运行
$ python a1.py 
6

(3)如果想直接使用函数名，则需要重命名：
$ cat a2.py 
import fn_add
add=fn_add.add
print( add(2,4) );

或者使用 
$ cat a3.py 
from fn_add import add
print( add(2,4) );



2. 调用不同目录下文件
$ cat t.py
from pipeline.my_fn import getNow
print(getNow() );

在当前py脚本并列的文件夹pipeline内，有一个my_fn.py,其中定义了getNow()函数。 

./t.py
./pipeline/my_fn.py  #其中定义的函数


https://www.cnblogs.com/arkenstone/p/5609063.html















========================================
基于 Common Workflow language 语言流的流程 / Broad Institute's  WDL(workflow description language)
----------------------------------------
CWL（Common Workflow Language）普通工作流语言和WDL（Workflow Description Language）工作流描述语言。定义每一个计算过程（脚本）的输入和输出，然后通过连接这些输入和输出，构成数据分析流程。

CWL/WDL现已被Seven Bridges、Broad Institute等众多国际顶级研究机构使用。

可以在多个平台执行，比如本地服务器、SGE 集群，云计算平台等，可以做到一次编写多处执行。

CWL是一种描述分析管线和计算工具的方式——这类系统已经有超过250种了，其中比较流行的包括
 - Linux版本最有名的如Snakemake, Nextflow，bpipe等。
 - 图形界面版的如Galaxy，一些商业公司的云平台（拖拉图标即可）。

虽然它们使用了不同的语言，支持不同的特性，但这类系统有着共同的目标：让计算过程能够重现、移植、维护、共享。
CWL本质上是一种让研究人员用来共享管线的交换语言。对于研究者来说，这门语言让代码库变得正常了，减少了大约73%的代码。更重要的是，这让代码更容易测试、执行和共享新的算法，以及在云上运行。


其他好处？举个例子：
某计算平台的开发者，在该平台下编写了一个分析流程，但一段时间后要将分析流程迁移，到Seven Bridges/DNAnexus上... 此时，该开发者需要重新按照以上三个流程规范修改这个流程。

但是，如果开发者是按照CWL标准编写的分析流程，那么这个流程就可以分别在Seven Bridges/DNAnexus上正确运行，不需要做任何修改，因为这些平台都支持CWL。








1. CWL 
https://github.com/common-workflow-language/
https://www.commonwl.org/

CWL is designed to meet the needs of data-intensive science, such as Bioinformatics, Medical Imaging, Astronomy, Physics, and Chemistry.

## CWL的一个实现是snakemake。
configfile: "config.yaml"

rule all:
    input:
        "report.html"

rule bwa_map:
    input:
        "data/genome.fa",
        lambda wildcards: config["samples"][wildcards.sample]
    output:
        temp("mapped_reads/{sample}.bam")
    params:
        rg="@RG\tID:{sample}\tSM:{sample}"
    log:
        "logs/bwa_mem/{sample}.log"
    threads: 8
    shell:
        "(bwa mem -R '{params.rg}' -t {threads} {input} | "
        "samtools view -Sb - > {output}) 2> {log}"

rule report:
    input: "calls/all.vcf"
    output: "report.html"
    run:
        from snakemake.utils import report
        with open(input[0]) as vcf:
            n_calls = sum(1 for l in vcf if not l.startswith("#"))

        report("""
        An example variant calling workflow
        """, output[0], T1=input[0])






2. The Workflow Description Language (WDL) makes it straightforward to define analysis tasks, chain them together in workflows, and parallelize their execution.

(1) Cromwell 是 Broad Institute 开发的工作流管理引擎，支持 WDL 和 CWL 两种工作流描述语言。
Broad Institute可谓业内之翘楚，我们利用其开发的WDL和Cromwell来搭建自己的生物信息分析流程。
https://openwdl.org/

WDL语法结构：
	workflow
	task
	call
	command
	output


示例1 
workflow myWorkflow {
    call myTask
}
task myTask {
    command {
        echo "hello world"
    }
    output {
        String out = read_string(stdout())
    }
}


 
(2) 从 Hello world 开始编写WDL
像我们学习其他语言一样，先从经典的 hello world 开始，学习 WDL 的编写
## 定义task
task echo {
  String out

  command {
    echo Hello World! > ${out}
  }

  runtime {
    cluster: "OnDemand ecs.sn1.medium img-ubuntu-vpc"
    docker: "registry.cn-shanghai.aliyuncs.com/mynamespace/myubuntu:0.1"
  }

  output {
    File outFile = "${out}"
  }
}

## 定义工作流
workflow wf_echo {
  call echo
  output {
    echo.outFile
  }
}


上面的例子是一个简单的 WDL，作用是输出 Hello world 并保存在一个文件里面，输出文件名可以指定。一个完整的 WDL 一般由下面几个部分组成：

	workflow：工作流定义
	task：工作流包含的任务定义
	call：调用或触发工作流里面的 task 执行
	command：task在计算节点上要执行的命令行
	runtime：task在计算节点上的运行时参数，包括 CPU、内存、docker 镜像等
	output：task 或 workflow 的输出定义









3. 如何运行 wdl 脚本? 使用 Cromwell。
Cromwell是一个开源的由Java编写支持WDL的执行工具。
需要java 8。
支持 WDL 和 CWL 两种工作流描述语言
可以多平台支持支持WDL的执行：本地机器，本地服务器集群，云平台。

Cromwell(an execution engine that can run WDL scripts)  Broad Institute 公司开发的工作流管理系统
Cromwell is a Workflow Management System geared towards scientific workflows. 面向科学工作流。
https://github.com/broadinstitute/cromwell

Scientific workflow engine designed for simplicity & scalability. Trivially transition between one off use cases to massive scale production environments http://cromwell.readthedocs.io/


(1)下载和安装 Cromwell
Cromwell可执行文件可以从Cromwell GitHub存储库中以预编译的jar文件的形式获得 。它需要Java 8才能运行。
https://github.com/broadinstitute/cromwell/releases/

$ wget https://github.com/broadinstitute/cromwell/releases/download/65/cromwell-65.jar

$ java --version
openjdk 11.0.11 2021-04-20




(2) 使用 Cromwell 运行 WDL 有两种模式
==> Run 模式
用来执行单个 WDL，适用于调试初期，快速执行一个WDL。
$ java -jar cromwell.jar run echo.wdl --inputs input.json

==> Server 模式
用下面的命令启动一个 HTTP server
$ java -Dconfig.file=application.conf -jar cromwell.jar server

再使用 RESTful API 提交工作流到 server 执行：
$ java -jar cromwell.jar submit -t wdl -i input.json -o option.json -h http://localhost:8000



相比 Run 模式，Server 模式有以下优势：
- 可以并行处理多个 workflow，适用于生产环境
- 有 Call caching 等高级特性（下文会讲到），优化 workflow 的执行
- 提供丰富的 workflow metadata，来展示 workflow 的执行过程

注意：不管是使用Run 模式还是 Server模式，要使用批量计算作为后端运行 WDL，都需要对应的配置文件支持，配置文件详解请参考批量计算官方文档或Cromwell 官方文档。


语法检查工具
WDL 编写完成后，在真正执行之前，我们可以使用官方工具进行语法检查：
$ java -jar wdltool.jar validate myWorkflow.wdl




(4) 现成案例:
https://github.com/gatk-workflows
GATK Cromwell +WDL学习 https://blog.csdn.net/theomarker/article/details/79627804
gatk-tool-wdls https://github.com/broadinstitute/gatk-tool-wdls/tree/master

总体来说，WDL结构略复杂，槽点很多，容易出错，调试较麻烦，但依然不失为科学共同体杰出的流程化语言，值得学习、推广应用。




ref:
https://wenlongshen.github.io/2018/09/15/Pipelining-Solution-2/
https://help.aliyun.com/document_detail/110173.html
https://developer.aliyun.com/article/716546









========================================
|-- 使用 docker 分析高通量测序数据
----------------------------------------
docker也是一个很有发展前途的方向，可以作为轻量级生物信息工具的容器，它将帮助具有复杂依赖关系的流程变得简单。
docker不是一个流程方法，只是个封装的容器工具而已，能把软件及其运行环境一起打包，方便环境的迁移。可以作为流程中的一个工具使用。
docker，作为一个开源的应用容器引擎，小巧、可移植、可虚拟化、沙箱式的使用机制使其成为流程应用开发的不二之选。

容器是工作流程上特别有用的一类附加功能，因为它允许研究者将执行每个步骤所需的计算组件精准打包。

2018年，非洲生物信息网络H3ABioNet的研究者们构建了四条管线——两条使用了CWL，两条使用了Nextflow——并将它们存在了Docker容器中（S. Baichoo et al. BMC Bioinform. 19, 457; 2018）。这样，就能让计算结果可以重现，工作流程也可以在网上下载，尽管不同人用的计算资源各不相同。
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2446-1


使用docker要理解image和container的概念，镜像包含了一个完整的操作系统及所需要的应用程序，容器则是从镜像创建的一个实例，同一个镜像可以对应多个容器，而容器彼此之间隔离。



(1) 使用方式
可以去文章或 https://biocontainers.pro/ 找别人构建好的镜像。
BioContainers: 10.1K tools, 39.7K versions, 196.9K containers and packages
SOFTWARE --> CONTAINER --> WORKFLOW

https://biocontainers.pro/registry?all_fields_search=bowtie2
https://biocontainers.pro/tools/bioconductor-atacseqqc
https://hub.docker.com/r/biocontainers/bowtie2/

$ docker run --rm -v /home/wenlong/Data/:/data/ biocontainers/bowtie2 bowtie2 -x /data/hg19index/hg19 -1 /data/test.r1.gz -2 /data/test.r2.gz -S /data/test.sam

参数解释
--rm 使得该容器退出后自动删除。
-v 是文件映射，冒号前面是本机，后面是docker内部地址。相当于文件夹挂载。
	bowtie2生成的比对文件将出现在本地/home/wenlong/Data下。
镜像名是 biocontainers/bowtie2。
运行的命令是 bowtie2 -x /data/hg19index/hg19 -1 /data/test.r1.gz -2 /data/test.r2.gz -S 

注意biocontainers构建这个镜像时使用了CMD ["bowtie2"]，所以想要交互终端的话得使用docker run -it biocontainers/bowtie2 /bin/bash。




(2) 构建方式
以ChIP-seq等分析时常用的peak calling工具MACS2为例。

	下载官方的Ubuntu镜像作为起始镜像docker pull ubuntu，
	建立一个文件夹用于存放制作镜像过程中所用到的文件，下载MACS2的源码包并新建Dockerfile文件
	执行docker build -t macs2:ubuntu.v1 .（慢）
	构建成功后，查看并尝试使用该镜像
	将该镜像上传到自己的docker hub


$ docker run --rm -v /home/wenlong/Data/:/data/ macs2:ubuntu.v1 macs2 callpeak -t /data/test.sam -f SAM -n test --outdir /data/macs2_result







ref:
https://wenlongshen.github.io/2018/09/08/Pipelining-Solution-1/








========================================
生信流程工具-CWL (BD 单细胞平台分析流程是cwl写的: bdgenomics/rhapsody 1.9.1)
----------------------------------------
https://www.commonwl.org/user_guide/

1. 安装
依赖 node.js, Java compiler。

$ git clone https://github.com/common-workflow-language/cwltool.git
$ cd cwltool# Switch to source directory
$ pip3 install . -i https://pypi.douban.com/simple/ # Install `cwltool` from source
$ cwltool --version# Check if the installation works correctly
/home/wangjl/.local/bin/cwltool 3.1.20210825140344


2. 使用方法
user guide:https://www.commonwl.org/user_guide/
cwltool [tool-or-workflow-description] [input-job-settings]
	第一个cwl文件描述做什么，第二个yaml文件设置IO

cwl文件需要写成json或yaml格式，或者两者的混合。YAML文件编写格式参考YAML Guide。

需要两个cwl文件，一个进行流程说明，一个设置输入输出和参数。



(1) 最简版 https://www.commonwl.org/user_guide/02-1st-example/index.html
$ cat echo.cwl 
#!/usr/bin/env cwl-runner 

cwlVersion: v1.0        # cwl 版本
class: CommandLineTool  # 命令行工具
baseCommand: echo       # 运行的程序名
inputs:              #工具的输入
  message:          # 参数id
    type: string   #参数类型
    inputBinding:  #参数如何出现在命令行
      position: 1  # 参数出现在 命令行的位置
outputs: []

注意：
- 不能使用 \t 代替空格开头；

$ cat echo_input.yaml 
message: Hello world! from cwl


运行：第一个参数是cwl脚本，第二个参数是yaml配置文件。
$ cwl-runner echo.cwl echo_input.yaml
/usr/bin/cwl-runner 1.0.20180302231433
Resolved 'echo.cwl' to 'file:///home/wangjl/test/cwl_test/echo.cwl'  #解析配置文件
[job echo.cwl] /tmp/tmpOOdt7U$ echo \    #读取cwl，拼装命令，命令的参数是yaml提供的
    'Hello world! from cwl'
Hello world! from cwl    #输出
[job echo.cwl] completed success
{}
Final process status is success


(2) 深入学习
https://www.commonwl.org/user_guide/





3. IO 输出到文件
(1)输入 Input
在CWL描述文件的input部分描述input；
如果是输入是文件类型要加class: File
可以通过inputBinding设置在命令行中input出现时的前缀名，以及前缀和input中间是否有空格。


(2) Output
在CWL描述文件的output部分描述output；
可以通过outputBinding设置output;
通配符允许在glob字段使用。

获取标准输出流

$ cat io.yaml
#!/usr/bin/env cwl-runner
cwlVersion: v1.0
class: CommandLineTool
baseCommand: echo 
stdout: output.txt # 设置输出文件
inputs:
  message:
    type: string
    inputBinding:
      position: 1
outputs:
  example_out:
    type: stdout #设置输出方式
# end


用stdout来将输出内容指定到一个文件名；
相应的，在output参数中，必须要有type: stdout。

运行：
$ cwl-runner io.cwl echo_input.yaml
/usr/bin/cwl-runner 1.0.20180302231433
Resolved 'io.cwl' to 'file:///data/wangjl/test/testCWL/io.cwl'
[job io.cwl] /tmp/tmpZnBIPm$ echo \
    'Hello world! from cwl' > /tmp/tmpZnBIPm/output.txt
[job io.cwl] completed success
{
    "example_out": {
        "checksum": "sha1$8f24a97752ec555e86e165f8ad005ed389776dda", 
        "basename": "output.txt", 
        "location": "file:///data/wangjl/test/testCWL/output.txt", 
        "path": "/data/wangjl/test/testCWL/output.txt", 
        "class": "File", 
        "size": 22
    }
}
Final process status is success

$ cat output.txt #输出内容到新文件
Hello world! from cwl




4. 更多细节，请参考 biomooc.com/linux/cwl: https://www.biomooc.com/linux/cwl_guide.html

https://www.commonwl.org/user_guide/








ref:
https://www.jianshu.com/p/4b818498dfa9
https://blog.csdn.net/sinat_39288981/article/details/114990642











========================================
生信流程工具 -- workflow description language (wdl): GATK +Cromwell //todo
----------------------------------------
1. 简介

一个优秀的pipeline工具应该具备良好的组织性、复用性、支持多种集群架构，较低的上手及查错成本，而目前WDL就是这样一个具备这种潜力的工具。

WDL是Broad Institute开发的“human readable and writable”定义组织任务与工作流的一种语言。从开发者及开发者赋予的名字中，我们就能看出WDL是一个面向生物信息/基因组学领域的专业的工具。

官网: https://software.broadinstitute.org/wdl/
官方入门教程: https://software.broadinstitute.org/wdl/documentation/quickstart

该项目貌似要有单独网址: https://sciwiki.fredhutch.org/compdemos/Cromwell/
https://www.rc.virginia.edu/userinfo/howtos/rivanna/wdl-bioinformatics/


wdl 语言
https://github.com/openwdl/wdl/
https://www.openwdl.org/
https://github.com/biowdl

运行环境
https://github.com/broadinstitute/cromwell
http://cromwell.readthedocs.io/



(2) 基本的WDL包括五个重要的概念：workflow, task, call, command and output.
理解了就可以完成在本地运行的基本的脚本。

除此之外，还有 runtime、parameter_meta、meta等概念，它们可以给流程增加更多的特性，这里我们后面再介绍，先从基本概念开始。







ref:
https://www.jianshu.com/p/979fd06661cb
https://www.jianshu.com/p/41f377e20ff7

https://zhuanlan.zhihu.com/p/121599533

Cromwell +WDL学习 https://blog.csdn.net/qq_41551450/article/details/93486438
https://blog.csdn.net/weixin_43999327/article/details/104245691
https://blog.csdn.net/theomarker/article/details/79627804

干货 https://www.cnblogs.com/raisok/p/11151430.html




========================================
使用Snakemake(一个基于Python3写的DSL流程框架)搭建生信分析流程
----------------------------------------
领域专用语言 domain specific language
除了通用的C语言、Java等，很多领域还可以有一些“半吊子语言”，也就是DSL（domain-specific language，领域专用语言）。


Snakemake是Bioconda的作者——算法工程师Johannes Köster为了数据分析能够更有效的重复，用Python3写的工作流管理工具（Workflow Management System）。这个工具的特点是可以根据文件的依赖关系来智能管理命令的并行和串行。

Snakemake的优势：
  - 工作流通过基于Python的语言进行描述，容易理解。前后生成文件逻辑清晰，方便维护。
  - Snakemake工作流程可以无缝扩展到工作站、集群和云环境而无需修改工作流定义。作业调度方便控制，比如CPU内核、内存等。
  - Snakemake可以使用Conda或Singularity自动部署工作流所需的软件依赖性。
  - 如果任务中断，当前的文件状态保留，已成功生成的文件不会被覆盖，已运行完的命令不会重复运行。




我们都知道ATAC-seq是探索染色体开放程度的一项非常重要的技术，比如我们要分析ATAC-seq的数据（注：ATAC-seq的原理请移步 ATAC-seq - Wikipedia），根据ENCODE的建议，ATAT-seq数据分析往往会分成下面几个步骤：
raw FASTQ cut adapter
mapping to the reference with aligner like bowtie2
sort alignment result (BAM files)
remove BAM file duplications
peak-calling with MACS2

如果有50个样品需要跑这样重复的流程，使用shell去写循环提交任务当然可以。但是在提交的时候，我们需要考虑前后生成文件的逻辑，需要考虑整体使用的CPU核心数目，需要考虑如果任务从中间断掉之后怎么去恢复之前的文件状态。还需要考虑，如果生成的文件不完整怎么办等等。

此外，除去这些问题，下次我们再跑一个ChIP-seq的数据，虽然也是类似的流程，就需要再次重新构建一个pipeline，费事费力。那么这个时候就需要请出我们今天的主角——Snakemake！


什么是Snakemake？
Snakemake是一款基于Python3的软件，在它的帮助下，我们不但可以快速搭建流程，还可以实现包括并不限于下列功能的流程控制：
1支持并行运算；
2支持断点运行；
3支持流程控制；
4支持内存控制；
5支持CPU核心控制；
6支持运行时间控制；
7支持向大型计算机集群提交任务；
8…… ……


同时，在Snakemake的帮助下，我们可以生成数据运行的网络拓扑图，就比如我们前文提到的ATAC-seq的数据分析。假设我们有2个重复的ATAC-seq的数据需要分析，那么使用Snakemake搭建出的流程就类似于， 图略；
在运行的时候，我们还可以自动生成运行逻辑拓扑图，图略；





高水平的snakemake脚本, 见 bioToolKit/Python/snakemake/
https://mp.weixin.qq.com/s/qSIrXkJPfIuQQWNJA4YH0g









refer:
1.文本 https://www.jianshu.com/p/ab9b9c3370af 
  视频 https://www.bilibili.com/video/av45832590  ATAC-seq分析示例
https://www.bilibili.com/video/BV1jb411i76T?p=2


2.视频 https://www.bilibili.com/video/av15908415  以转录组分析为例

3. snakemake--我最喜欢的流程管理工具  https://www.jianshu.com/p/8e57fd2b81b2 变异检测流程
4.snakemake搭建生信分析流程  原创： 林行众  生信技能树  2017-11-17
5. snakemake使用笔记 https://www.jianshu.com/p/14b9eccc0c0e





========================================
|-- snakemake包简介与文档、安装与执行、官网测试数据
----------------------------------------
官方文档： https://snakemake.readthedocs.io/en/latest/

主要功能：快速搭建需要重复实现的分析流程。

支持并行运算、断点运行、流程控制、内存控制、CPU控制等。
自动生成流程拓扑图。
基于py3.

Snakemake支持并行处理任务，可以设定运行核心数或并行任务数，也可以将任务投递到集群运行。



1.安装 snakemake 包

(0) 如果有pip3，也可无需激活环境: $ pip3 install snakemake -i https://pypi.douban.com/simple/


(1)$ pip3 -V ##(安装失败)
pip 19.1.1 from /home/wangjl/software/anaconda3/lib/python3.7/site-packages/pip (python 3.7)

$ pip3 install --user snakemake
连接不上

或
$ conda create --name py3F python=3.7.3
## 需要安装很多包，时间取决于网速（使用中科大或清华的源）
#
# To activate this environment, use:
# > conda activate py3F
#
# To deactivate an active environment, use:
# > conda deactivate


#激活环境
$ source activate py3F

# 添加Anaconda的TUNA镜像
(py3F)$ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
(py3F)$ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/

# 设置搜索时显示通道地址
(py3F)$ conda config --set show_channel_urls yes

(py3F)$ conda search snakemake
搜索包

(py3F)$ conda install snakemake #时间取决于网速
还是失败：PackagesNotFoundError
查官网，建议使用bioconda频道。
(py3F)$ conda install -c bioconda snakemake 
还是失败： ConnectionError






(2)也可以从源文件安装:
$ git clone https://bitbucket.org/snakemake/snakemake.git
$ cd snakemake  ## cd ~/software/snakemake/

#创建虚拟环境
$ virtualenv -p python3 .venv
$ source .venv/bin/activate ##先到目录 cd ~/software/snakemake/
(.venv)$ python setup.py install  #带前缀的虚拟环境


#检查是否安装成功
$ snakemake -h
usage: snakemake [-h] [--dryrun] [--profile PROFILE] [--snakefile FILE]
                 [--cores [N]] [--local-cores N]
                 [--resources [NAME=INT [NAME=INT ...]]]
                 [--config [KEY=VALUE [KEY=VALUE ...]]] [--configfile FILE]

# 退出当前的venv环境，使用deactivate命令：
(.venv) [wangjl@bio_svr1 snakemake]$ deactivate
[wangjl@bio_svr1 snakemake]$ 



virtualenv是如何创建“独立”的Python运行环境的呢？原理很简单，就是把系统Python复制一份到virtualenv的环境，用命令source venv/bin/activate进入一个virtualenv环境时，virtualenv会修改相关环境变量，让命令python和pip均指向当前的virtualenv环境。







2. 简单执行shell命令

snakemake 的参数
-s  指定Snakefile，
-n  不真正执行。通过假运行，可以检查自己的文件是否正确
-p   输出要执行的shell命令
-r  输出每条rule执行的原因,默认FALSE
-j  指定运行的核数，若不指定，则使用最大的核数
-f 重新运行第一条rule或指定的rule
-F 重新运行所有的rule，不管是否已经有输出结果


(1)再次进入虚拟环境：(不是必须的，只要能调用 $ snakemake --version #6.4.1 即可)
$ source /home/wangjl/software/snakemake/.venv/bin/activate


(2)创建两个文本文件1.txt, 2.txt，内容分别是hello和world，作为输入文件。
$ cat Snakefile
rule test_cat:
    input:
        "1.txt",
        "2.txt"
    output:
        "hebing.txt"
    shell:
        "cat  {input}  >> {output}"
#
这里有四个参数:
rule: 是名称, 这里命名为test_cat
input: 输入文件, 这里是”1.txt”, “2.txt”。使用英文引号括着，逗号隔开。
output: 输出文件, 这里是”hebing.txt”
shell: 这里是要执行的 shell 命令, 用到输入文件是{input}, 输出文件是{output}.
	要加引号，一个长命令可以换行，且中间不需要加引号。

只有三个是必须的，input, output, shell或run或script
	后三者分别运行 
		shell: shell 命令
		run: py 代码
		script: 任何 脚本文件




实例2
$ cat sort.sf
rule sort:
	input: "words.txt"
	output: "words.sorted.txt"
	shell: 
		"sort {input} > {output}"
		" && ls -lth {output}"

## rules can use shell: Python (run:), and R (run: R() ) //不懂
$ snakemake -s sort.sf
-rw-rw-r-- 1 wangjl wangjl 31 Jun 14 15:45 words.sorted.txt



执行的一般流程：先检查脚本语法，生成图检查逻辑，再执行脚本。



(3)查看任务内容 dry-run
$ snakemake -np
这里不执行程序，而是把相关的脚本打印出来，同时还有任务统计信息


运行前检查潜在错误：
snakemake -n
snakemake -np
snakemake -nr
	# --dryrun/-n: 不真正执行
	# --printshellcmds/-p: 输出要执行的shell命令
	# --reason/-r: 输出每条rule执行的原因



可视化：
snakemake --dag  | dot -Tsvg > dag.svg
snakemake --dag  | dot -Tpdf > dag.pdf
# --dag: 生成依赖的有向图

snakemake --gui 0.0.0.0:2468
# --gui: 通过网页查看运行状态




(4)执行命令
$ snakemake -s sort.sf


$ snakemake  #如果文件名是默认的Snakemake, 不用加参数
$ snakemake -s Snakefile -j 4
# -s/--snakefile 指定Snakefile，否则是当前目录下的Snakefile
# --cores/--jobs/-j N: 指定并行数，如果不指定N，则使用当前最大可用的核心数




更多命令行运行方式：
# perfom dry-run
snakemake -n

# execute workflow locally with 16 CPU cores
snakemake --cores 16

# execute on cluster
snakemake --cluster qsub --jobs 100

# execute in the cloud
snakemake --kubernetes --jobs 1000 --default-remote-provider GS --default-remote-prefix mybucket












(5)查看结果:
(.venv) [wangjl@bio_svr1 snakemake]$ cat hebing.txt 
hello, 
from another file.

可以看到, 使用snakemake, 成功的将1.txt 和2.txt 合并为hebing.txt.




(6)运行成功, 重新运行时
显示Nothing to be done, 即不会执行.

(.venv) [wangjl@bio_svr1 snakemake]$ snakemake
Building DAG of jobs...
Nothing to be done.
Complete log: /data/jinwf/wangjl/software/snakemake/.snakemake/log/2019-07-20T075923.589548.snakemake.log

如果 hebing.txt文件被删掉了, 会重新执行.




(7)查看日志文件是啥
(.venv) [wangjl@bio_svr1 snakemake]$ cat /data/jinwf/wangjl/software/snakemake/.snakemake/log/2019-07-20T075923.589548.snakemake.log
Building DAG of jobs...
Nothing to be done.
Complete log: /data/jinwf/wangjl/software/snakemake/.snakemake/log/2019-07-20T075923.589548.snakemake.log

好吧，就是屏幕上输出的内容~




(8) 可视化： 生成流程图 --dag
$ sudo apt install graphviz #dot 命令所在的包

1)针对每个文件:
$ snakemake --dag -s s1.sf | dot -Tpdf > dag.pdf       # 虚线框
$ snakemake --forceall --dag -s s1.sf | dot -Tpdf > dag.pdf # 加forceall后是实线。

$ snakemake -s cat3.sf --dag {1,2}.cat.txt | dot -Tsvg > dag.svg

$ snakemake --dag 2> /dev/null | dot -T png > ../img/workflow_bysample.png


2)或针对每个步骤 Or as a single, global, output.
$ snakemake --rulegraph 2> /dev/null | dot -T png > ../../img/workflow.png



然后可以静态服务器查看图片 
$ http-server -p=9000













3. 官网提供的测试数据 
https://github.com/snakemake/snakemake-tutorial-data/tags

$ mkdir snakemake-demo && cd snakemake-demo
$ wget https://github.com/snakemake/snakemake-tutorial-data/archive/refs/tags/v5.24.1.2.tar.gz
## -rw-r--r--. 1 wangjl jinwf 7.0M Jun 16 11:29 v5.24.1.2.tar.gz

(1)
$ tar zxvf v5.24.1.2.tar.gz #标准解压

./snakemake-tutorial-data-5.24.1.2:
data  Dockerfile  environment.yaml  README.md  Snakefile  upload_google_storage.py

(2)
$ tar --wildcards -xf v5.24.1.2.tar.gz --strip 1 "*/data"  #仅保留 /data 文件夹

解压后得到一个 data 文件夹，目录如下：
data
├── genome.fa
├── genome.fa.amb
├── genome.fa.ann
├── genome.fa.bwt
├── genome.fa.fai
├── genome.fa.pac
├── genome.fa.sa
└── samples
    ├── A.fastq
    ├── B.fastq
    └── C.fastq





refer:
1.snakemake 教程： https://blog.csdn.net/u012110870/article/details/85330457
2. https://www.jianshu.com/p/14b9eccc0c0e
3.Resources
Documentation and change log: https://snakemake.readthedocs.io
Questions:http://stackoverflow.com/questions/tagged/snakemake
Gold standard workflows:https://github.com/snakemake-workflows/docs
Configuration profiles:https://github.com/snakemake-profiles/doc
Command line help:snakemake --help
4.Singularity 教程(用Go语言写的类docker镜像系统) https://sylabs.io/docs/







========================================
|-- snakefile 参数：给snakemake传参(通配符)、脚本顶部定义变量、run直接运行py/R代码、外部脚本接收snake传参/记日志log
----------------------------------------
1. 使用bwa比对到参考基因组，并保存bam文件
(1) 原版本
rule bwa_map:
    input:
        "data/genome.fa",
        "data/samples/A.fastq"
    output:
        "mapped_reads/A.bam"
    shell:
        "bwa mem {input} | samtools view -Sb - > {output}"
#

以上命令比较死板，甚至还不如手工打的爽快。
但是如果使用上通配符，则灵活性大大增加。



(2) 可以从 命令行 传入参数 (传入目标文件(就是output的文件名)，脚本内部按照 通配符 获取相应参数)
rule cat2:
    input:
        "header.txt",
        "{sample}.txt"
    output:
        "{sample}.cat.txt"
    shell:
        "cat {input} > {output}"

1)命令中指定目标文件，他会自动套用规则中的output，然后查找依赖并执行。
$ snakemake -s cat2.sf 2.cat.txt -j1 #最后是指定cpu核心数 1


2)命令指定多个目标文件，用花括号括着。
$ snakemake -s cat2.sf {1,2}.cat.txt -j1
结果只执行了1.cat.txt, 因为2.cat.txt已经生成过。省却了手动判断是否存在等操作。

如果删掉2个目标文件 $ rm *cat.txt, 再次执行该命令，则生成2个文件。

如果使用 $ touch 2.txt 改变输入文件的时间戳，它就会认为 2.txt 文件发生了改变，那么重复之前的命令就会再次生成 2.cat.txt。


$ head *cat.txt
==> 1.cat.txt <==
this is a header.
this is file 1

==> 2.cat.txt <==
this is a header.
this is file 2




输出的日志:
Job counts:
	count	jobs
	2	cat2 # 这个过程走了2遍。
	2

[Mon Jun 14 10:59:15 2021]
rule cat2:
    input: header.txt, 1.txt
    output: 1.cat.txt
    jobid: 0
    wildcards: sample=1  # 通配符1

[Mon Jun 14 10:59:15 2021]
Finished job 0.
1 of 2 steps (50%) done

[Mon Jun 14 10:59:15 2021]
rule cat2:
    input: header.txt, 2.txt
    output: 2.cat.txt
    jobid: 1
    wildcards: sample=2 # 通配符2

[Mon Jun 14 10:59:15 2021]
Finished job 1.
2 of 2 steps (100%) done




(3) 如果想在shell中引用通配符，形式是 {wildcards.sample}
rule cat3:
    input:
        "header.txt",
        "{sample}.txt"
    output:
        "{sample}.cat.txt"
    shell:
        "cat {input} > {output} 2>log/{wildcards.sample}.log"

$ snakemake -s cat3.sf {1,3}.cat.txt















2. 脚本顶部、规则外定义变量，方便批量组装，方便顶部自定义绝对路径

(1) expand() 函数: 
定义好变量名字，然后使用 列表推导式 批量组装
>>> ["sorted_reads/{}.bam".format(sample) for sample in ["A","B"]]
['sorted_reads/A.bam', 'sorted_reads/B.bam']

snakemake定义了 expand进行简化, 上面可以继续改写成 
expand("sorted_reads/{sample}.bam",sample=SAMPLES)


(2) 只需要在 rule all 中使用，或者尽量最后一个rule中使用，前面的相同的变量会自动赋值
input 和 output 中多个变量可以使用 名字="内容" 来定义，逗号隔开，获取时使用 点号. 获取。


$ cat snakefile
SAMPLES = ['Sample1', 'Sample2']
rule all:
    input:
        expand('{sample}.txt', sample=SAMPLES)

rule quantify_genes:
    input:
        genome = 'genome.fa',
        r1 = 'fastq/{sample}.R1.fastq.gz',
        r2 = 'fastq/{sample}.R2.fastq.gz'
    output:
        '{sample}.txt'
    shell:
        'echo {input.genome} {input.r1} {input.r2} > {output}'


==> 测试 
$ cat cat4.sf
Samples=["A", "B"]
rule make_page:
	input: 
		hd="header.txt",
		txt=expand("sorted_read/{sample}.txt", sample=Samples)
	output:
		"out/index.html"
	shell:
		"cat {input.hd} {input.txt} > {output}"
		"&& echo '==End==' {output} "

$ snakemake -s cat4.sf 
==End== out/index.html


检查输出
$ cat out/index.html 
<h1>this is header</h1>

this is a
this is B file



注意: 每个rule中的相同的变量可能是不同的值，根据上下文自动匹配。
$ cat paraDiff.sf
SI=["A","B"]
rule all:
	input: "stat.txt"

rule sort:
	input: "{sample}.bam"
	output: "{sample}.sort.bam"
	shell: "cp {input} {output} && echo 'in sort:' {wildcards.sample}"

rule index:
	input: "{sample}.bam"
	output: "{sample}.bam.bai"
	shell: "wc {input} > {output} && echo 'in index:' {wildcards.sample}"

rule stat:
	input: 
		expand("{sample}.sort.bam", sample=SI ), 
		expand("{sample}.sort.bam.bai", sample=SI)
	output: "stat.txt"
	shell: "echo {input} > {output}"

$ snakemake -s paraDiff.sf -j 4
我们看到输出中
in sort: A
in sort: B
in index: A.sort
in index: B.sort

可见，同样的变量 sample，再不同的rule中具有不同的值。 
snakemake 会根据自下而上的追溯中，为每个通配符确定最合适的值。实在无法解析，才报错。

可视化各个 rule
$ snakemake -s paraDiff.sf --rulegraph 2> /dev/null | dot -T svg >workflow.svg #各个rule为中心
$ snakemake -s paraDiff.sf --dag | dot -Tsvg > dag.svg #各个文件为中心


(3) 是用正则表达式，定义全局变量
https://slowkow.com/notes/snakemake-tutorial/

这种硬编码的，太不灵活。
SAMPLES = ['Sample1', 'Sample2']
而使用正则表达式则更灵活获取某文件夹下的所有文件。


$ cat glob.sf 
# Globals ---------
from os.path import join

# Full path to a FASTA file.
GENOME = 'genome.fa'

# Full path to a folder that holds all of your FASTQ files.
FASTQ_DIR = './fastq/'

# A Snakemake regular expression matching the forward mate FASTQ files.
SAMPLES, = glob_wildcards(join(FASTQ_DIR, '{sample,Samp[^/]+}.R1.fastq.gz'))

# Patterns for the 1st mate and the 2nd mate using the 'sample' wildcard.
PATTERN_R1 = '{sample}.R1.fastq.gz'
PATTERN_R2 = '{sample}.R2.fastq.gz'

# Rules ---------

rule all:
    input:
        'test.txt'

rule quantify_genes:
    input:
        genome = GENOME,
        r1 = join(FASTQ_DIR, PATTERN_R1),
        r2 = join(FASTQ_DIR, PATTERN_R2)
    output:
        '{sample}.txt'
    shell:
        'echo {input.genome} {input.r1} {input.r2} > {output}'

rule collate_outputs:
    input:
        expand('{sample}.txt', sample=SAMPLES)
    output:
        'test.txt'
    run:
        with open(output[0], 'w') as out:
            for i in input:
                sample = i.split('.')[0]
                for line in open(i):
                    out.write(sample + ' ' + line)
==> 解释
比较难懂的是
SAMPLES, = glob_wildcards(join(FASTQ_DIR, '{sample,Samp[^/]+}.R1.fastq.gz'))

glob_wildcards() 函数类似py函数 glob.glob()，但是需要满足语法：
	就是文件夹FASTQ_DIR下，
	满足正则表达式 Samp[^/]+.R1.fastq.gz
	然后括号{}内的部分将会记录下来。
		问题:{}内逗号前的是啥？好像没啥用，但是又不能省略
		所以变量 SAMPLES 就是 ['Sample1','Sample2'].
		最后返回一个由list组成的tuple，所以如果只有一个变量的话，别忘了添加逗号（具体可以参考例子）。
		注意: glob_wildcards只支持正则表达式，不支持bash中用的通配符（ ?, *)。
		
通过 join(FASTQ_DIR, PATTERN_R1), 获取 ./fastq/{sample}.R1.fastq.gz. 更灵活了


# Example 1
(SAMPLE_LIST,) = glob_wildcards("fastq/{sample}.fastq")  #需要注意下SAMPLE_LIST后面的逗号。
# Example 2
(genome_builds, samples, ...) = glob_wildcards("{pattern1}/{pattern2}.whatever")
例2说明，可以接收多个匹配模板。如果存在下面文件：
	grch37/A.bam
	grch37/B.bam
	grch38/A.bam
	grch38/B.bam
那 (targets, samples) = glob_wildcards("{target}/{sample}.bam")返回的结果将是：
targets = ['grch37', 'grch38']
samples = ['A', 'B']


==> 测试
$ cat glob.sf 
from os.path import join
GENOME = 'genome.fa'
FASTQ_DIR="/home/wangjl/test/snakemake/"
#SAMPLES, = glob_wildcards( join( FASTQ_DIR, '{sample,stu[^/]+}.R1.txt' ) )
SAMPLES, = glob_wildcards(join(FASTQ_DIR, '{x,stu[^/]+}.R1.txt')) #改成这个也OK

print(SAMPLES) #['stu2', 'stu3', 'stu4', 'stu5'] 自动识别出5个 stu开头 .R1.txt结尾的 文件名。

PATTERN_R1 = '{sample}.R1.txt'
PATTERN_R2 = '{sample}.R2.txt'

rule all:
    input:
        'stat.txt'

rule quantify_genes:
    input:
        genome = GENOME,
        r1 = join(FASTQ_DIR, PATTERN_R1),
        r2 = join(FASTQ_DIR, PATTERN_R2)
    output:
        '{sample}.s.txt'
    shell:
        'echo {input.genome} {input.r1} {input.r2} > {output}'

rule count:
    input:
        expand('{sample}.s.txt', sample=SAMPLES)
    output:
        'stat.txt'
    run:
        with open(output[0], 'w') as out:
            k=0
            for i in input:
                sample = i.split('.')[0]
                for line in open(i):
                    k+=1
                    out.write(str(k) + sample + ' ' + line)

提前制造文件: 一个 genome.fa, 几个 stuN.R1.txt, stuN.R2.txt
$ cat stat.txt 
1stu1 genome.fa /home/wangjl/test/snakemake/stu1.R1.txt /home/wangjl/test/snakemake/stu1.R2.txt
2stu2 genome.fa /home/wangjl/test/snakemake/stu2.R1.txt /home/wangjl/test/snakemake/stu2.R2.txt









3. run 直接运行 py/R 脚本。
实例: 输出报告
$ snakemake --version 
注意使用最新版: #6.4.1
而老版本无法运行: #4.3.1

(1). snakemake的一个优点就是可以在规则里面写Python脚本，只需要把shell改成run，此外还不需要用到引号。

例子: 输出报告：用run语句，不加引号。
$ cat cat5.sf 
rule report:
	input: "1.txt"
	output: "report.html"
	run:
		from snakemake.utils import report
		arr=[]
		with open(input[0]) as f:
			for line in f.readlines():
				arr.append( line.strip() )
		n_len=len(arr)
		
		report("""
		# An example of report
		
		Reads were mapped to hg19 
		refer genome and variants were called jointly with 
		SAMtools / BCFtools.
		
		This resulted in {n_len} variants(see Table T1_)
		""", output[0], T1=	input[0] )

$ snakemake -s cat5.sf -j1



==> 测试
$ cat xx.sf 
rule report:
	input: "1.txt"
	output: "report.html"
	run:
		fr=open(input[0], 'r')
		fw=open(output[0], "w")
		for lineR in fr.readlines():
			line=lineR.strip()
			print(line)
			fw.write(line+"<br>")
		fr.close()
		fw.close()

$ snakemake -s xx.sh -j1




(2) 使用 run: R(""" # R code  """) 运行R代码
$ pip3 install rpy2
$ cat runR.sf
rule diagnostic_plot:
        input: "results/counts/gene_counts_mini.txt"
        output: "results/diagnostic_plot/diagnostic.pdf"
        run: R("""
            # dir.create("results/diagnostic_plot")
            data <- read.table("{input}", 
                                sep="\t", 
                                header=T, 
                                row.names=1)
            data <- data[rowSums(data) > 0, ]
            data <- log2(data + 1)
            pdf("{output}")
				dev.null <- apply(data, 2, hist, border="white", col="blue")
				boxplot(data, color="blue", pch=16)
				pairs(data, pch=".", col="blue")
            dev.off()
            cat("etc...")
        """)

$ cat result/counts/gene_counts_mini.txt
        s1      s2
gene1   1       20
gene2   1       2
gene3   100     10

$ snakemake -s runR.sf -j1
报错: name 'R' is not defined
难道不支持直接运行R了，刚才测试是可以使用 script 指令 运行R脚本的。 


经测试，发现需要在脚本头部引入
from snakemake.utils import R
再次运行就正常画图了。

查看图片 $ python -m http.server --bind 0.0.0.0 9994






4. script 指令 运行外部脚本，snakemake向外部脚本中传递参数，外部脚本记录日志的方法
refer: https://slides.com/johanneskoester/ismb-snakemake-tutorial-2019#/16

(1) snakemake中的rule，调用外部R脚本
$ cat R.sf
rule mytask1:
	input:
		"path/to/{dataset}.txt"
	output:
		"result/{dataset}.txt"
	script:
		"scripts/myscript.R"
#
# 在外部R脚本中接收参数：
print("Enter R") #无用
data=read.table(snakemake@input[[1]])
data=data[order(data$id),]
write.table(data, file=snakemake@output[[1]])
print("Leave R") #无用


$ cat path/to/stu.txt 
	id name
1 200 Tom
2 20 Dim
3 1 Apple
4 4 Trp
 
$ snakemake -s R.sf -j1 result/stu.txt

$ cat result/stu.txt  #已经按照id排序了
"id" "name"
"3" 1 "Apple"
"4" 4 "Trp"
"2" 20 "Dim"
"1" 200 "Tom"



尝试多进程
$ cp stu.txt stu2.txt
$ cp stu.txt stu3.txt
$ cp stu.txt stu4.txt
$ cp stu.txt stu5.txt

$ snakemake -s R.sf result/{stu,stu2,stu3,stu4,stu5}.txt -j 10
$ snakemake -s R.sf result/{stu,stu2,stu3,stu4,stu5}.txt -j 2
需要清空输出文件夹 $ rm result/*
观察输出的区别。
- 10个核心，则5个命令并行执行，打印出5个 jobid，然后是脚本的输出。
- 2个核心，则2个并行执行，打印出2个 jobid，然后脚本输出，然后再2个 jobid，然后再脚本输出。





(2) 调用外部py脚本 
$ cat Py.sh
rule mytask2:
	input:
		"data/{sample}.txt"
	output:
		"result/{sample}.txt"
	script:
		"scripts/myscript.py"
#

# 在外部 py中接收参数
import pandas as pd
data=pd.read_table(snakemake.input[0])
data=data.sort_values("id")
data.to_csv(snakemake.output[0], sep="\t")


$ cat person.txt  #注意: 是tab分割的
id	name
200	Tom
20	Dim
1	Apple
4	Trp

$ snakemake -s Py.sf result/person.txt -j1

$ cat result/person.txt 
	id	name
2	1	Apple
3	4	Trp
1	20	Dim
0	200	Tom







(3) 还可以复用 reuseable wrappers from central repository
rule map_reads:
	input:
		"{sample}.bam"
	output:
		"{sample}.sorted.bam"
	wrapper:
		"0.22.0/bio/samtools/sort"
#

## use CWL tool definitions
rule map_reads:
    input:
        "{sample}.bam"
    output:
        "{sample}.sorted.bam"
    cwl:
        "https://github.com/common-workflow-language/"
        "workflows/blob/fb406c95/tools/samtools-sort.cwl"
#

## 输出处理
rule mytask:
    input:
        "data/{sample}.txt"
    output:
        temp("result/{sample}.txt")  ## 临时文件夹
    shell:
        "some-tool {input} > {output}"
#

rule mytask:
    input:
        "data/{sample}.txt"
    output:
        protected("result/{sample}.txt") ## 永久文件夹？
    shell:
        "some-tool {input} > {output}"
#

rule mytask:
    input:
        "data/{sample}.txt"
    output:
        pipe("result/{sample}.txt") ## 管道？
    shell:
        "some-tool {input} > {output}"
#





(4) 外部脚本怎么记录日志 log ？
https://stackoverflow.com/questions/64101921/snakemake-how-to-log-python-scripts-executed-by-script-directive

You could redirect all stdout and stderr to the logfile

i) python:
import sys
with open(snakemake.log[0], "w") as f:
    sys.stderr = sys.stdout = f
    [rest of the script]

感觉这个py的日志很不灵活，能让其他函数顶格写吗？
flog=open(snakemake.log[0], "w")
sys.stderr = sys.stdout = flog
# other script
flog.close() #end line;




ii)R:
log <- file(snakemake@log[[1]], open="wt")
sink(log)
[rest of script]









基础部分小结：
总结下学习过程，知识点如下：
	Snakemake基于规则执行命令，规则一般由 input, output, shell三部分组成。
	Snakemake可以自动确定不同规则的输入输出的依赖关系，根据时间戳来判断文件是否需要重新生成
	Snakemake 以{sample}.fa形式进行文件名通配，用 {wildcards.sample}获取sample的实际文件名
	Snakemake用 expand()生成多个文件名，本质是Python的列表推导式
	Snakemake可以在规则外直接写Python代码，在规则内的 run里也可以写Python代码。
	Snakefile的第一个规则通常是 rule all，因为默snakemake默认执行第一条规则

ref:
https://mp.weixin.qq.com/s?__biz=MzI1MjU5MjMzNA==&mid=2247486637&idx=1&sn=93035d68b68a178c803f3689fb937dec















========================================
|-- snakemake 进阶 - 对流程进一步修饰: 进程数、配置文件(输入函数)、规则参数params、日志文件、临时和受保护文件、消息
----------------------------------------
在基础部分中，我们完成了流程的框架，
下一步则是对这个框架进行不断完善，比如说编写配置文件，声明不同rule的消耗资源，记录运行日志等。



进阶拓展：如果你想要更强大的功能，可以在 rule 里面添加更多的元素，例如：
threads：设置核心数，来指定程序运行的线程
resources：指定程序运行的内存
log：指定生成日志文件
conda：指定专用的 conda 环境



1. 声明所需进程数

对于一些工具，比如说bwa，多进程或者多线程运行能够大大加速计算。snakemake使用 threads来定义当前规则所用的进程数，我们可以对之前的 bwa_map增加该指令。

rule bwa_map:
	input:
		"data/genome.fa",
		"data/samples/{sample}.fastq"
	output:
		"mapped_reads/{sample}.bam"
	threads:8
	shell:
		"bwa mem -t {threads} {input} | samtools view -Sb - > {output}"


声明 threads后，Snakemake任务调度器就会在程序运行的时候是否并行多个任务。这主要和参数中的 --cores相关。比如说


$ snakemake --cores 10

由于总体上就分配了10个核心，于是一次就只能运行一个需要消耗8个核心的 bwa_map。
但是当其中一个 bwa_map运行完毕，这个时候snakemaek就会同时运行一个消耗8个核心的 bwa_map和没有设置核心数的 samtools_sort,来保证效率最大化。

因此对于需要多线程或多进程运行的程序而言，将所需的进程单独编码，而不是硬编码到shell命令中，能够更有效的使用资源。









2. 配置文件
https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html
用法:
  configfile: "path/to/config.yaml" 然后变量都在 config 中
  貌似配置文件必须应用到 rule all 中，其他地方则报错。

rule all:
    input:
        expand("{sample}.{param}.output.pdf", sample=config["samples"], param=config["yourparam"])
在shell中语句引用配置文件的格式
shell:
    "mycommand {config[foo]} ..."

配置文件还能被 命令行传入参数 覆盖
$ snakemake --config yourparam=1.5




(0) 概述
之前的SAMPLES写在了snakefile，也就是意味这对于不同的项目，需要对snakefile进行修改，更好的方式是用一个配置文件。
配置文件可以用JSON或YAML语法进行写，然后用 configfile:"config.yaml"读取成字典，变量名为config。

config.yaml内容为:
samples:
    A: data/samples/A.fastq
    B: data/samples/B.fastq

YAML使用缩进表示层级关系，其中缩进必须用空格，但是空格数目不重要，重要的是缩进后左侧对齐。
上面的YAML被Pytho读取之后，以字典保存，形式为 {'samples':{'A':'data/samples/A.fastq','B':'data/samples/B.fastq'}}

而snakefile也可以改写成
configfile: "config.yaml"
...
rule bcftools_call:
    input:
		fa="data/genome.fa",
		bam=expand("sorted_reads/{sample}.bam", sample=config["samples"]),
		bai=expand("sorted_reads/{sample}.bam.bai", sample=config["smaples])
    output:
		"calls/all.vcf"
    shell:
		"samtools mpileup -g -f {input.fa} {input.bam} | "        
		"bcftools call -mv - > {output}"

虽然sample是一个字典，但是展开的时候，只会使用他们的key值部分。


如果在配置文件后直接输出呢？
$ cat con1.sf 
configfile: "config.yaml"
a=config["samples"]
print(a)
SI=[a['A'], a['B']]
print(SI)
直接输出:
{'A': 'data/samples/A.fastq', 'B': 'data/samples/B.fastq'}
['data/samples/A.fastq', 'data/samples/B.fastq']


(1) 测试1 数组形式: 使用 数组 配置文件
$ cat config1.yaml 
samples:
  - 1.txt
  - 2.txt


$ cat con1.sf 
configfile: "config1.yaml"

rule all:
	input: expand("result/{sample}.html", sample=config["samples"])

rule cat6:
	input:
		fh="head.txt",
		txt="{sample}"
	output:"result/{sample}.html"
	shell: "cat {input.fh} {input.txt} > {output}"

$ snakemake -s con1.sf -j 1 



(2) 测试2 对象形式: 
$ cp con1.sf con2.sf 
仅仅修改第一行的配置文件名为 config2.yaml 
$ cat config2.yaml 
samples:
  A: 1.txt
  B: 2.txt

运行报错:
Missing input files for rule cat6:
A
==>出错原因：找不到配置文件的内容


修改成 config["samples"]["A"] 能拿到A对应的文件名，但是不能同时拿到B的。

$ cat con2.sf 
rule all:
        input: expand("result/{sample}.html", sample=config["samples"]["A"])

rule cat6:
        input:
                fh="head.txt",
                txt="{sample}"
        output:"result/{sample}.html"
        shell: "cat {input.fh} {input.txt} > {output}"
#




(3) 使用输入函数，解决配置文件的 对象形式 输入 问题
$ cat config1.yaml
samples:
 A: samples/A.fq
 B: samples/B.fq

$ cat r1.sf
configfile: "config1.yaml"

rule all:
	input: expand("mapped_reads/{sample}.bam", sample=config['samples'])

rule bwa_map:
	input:
		"genome.fa",
		config['samples']["{sample}"]
	output: "mapped_reads/{sample}.bam"
	threads: 8
	shell: #"bwa mem -t {threads} {input} | samtools view -Sb - > {output}"
		"cat {input} > {output}"

$ snakemake -s r1.sf -j 2
报错 KeyError in line 9 of /home/wangjl/test/snakemake/test2/r1.sf:
'{sample}'

config['samples']["{sample}"] 改为 config['samples']["{wildcards.sample}"]
还是报错
KeyError in line 9 of /home/wangjl/test/snakemake/test2/r1.sf:
'{wildcards.sample}'




为了理解错误的原因，并找到解决方法，我们需要理解Snakemake工作流程执行的一些原理，它执行分为三个阶段
	- step1 在初始化阶段，工作流程会被解析，所有规则都会被实例化
	- step2 在DAG阶段，也就是生成有向无环图，确定依赖关系的时候，所有的通配名部分都会被真正的文件名代替。
	- step3 在调度阶段，DAG的任务按照顺序执行

也就是说在初始化阶段，我们是无法获知通配符所指代的具体文件名，必须要等到第二阶段，才会有 wildcards变量出现。
也就是说之前的出错的原因都是因为第一个阶段没通过。
这个时候就需要输入函数推迟文件名的确定，可以用Python的匿名函数，也可以是普通的函数。



使用匿名函数后
$ cat r2.sf 
configfile: "config1.yaml"

rule all:
	input: expand("mapped_reads/{sample}.bam", sample=config['samples'])

rule bwa_map:
	input:
		"genome.fa",
		lambda wildcards: config["samples"][wildcards.sample]
		# config['samples']["{wildcards.sample}"]
	output: "mapped_reads/{sample}.bam"
	threads: 8
	shell: #"bwa mem -t {threads} {input} | samtools view -Sb - > {output}"
		"cat {input} > {output}"
运行 
$ snakemake -s r2.sf -j 2

检查结果
$ ls -lh mapped_reads/
total 8.0K
-rw-r--r--. 1 wangjl jinwf 76 Jun 16 16:59 A.bam
-rw-r--r--. 1 wangjl jinwf 69 Jun 16 16:59 B.bam


lambda wildcards: config["samples"][wildcards.sample]
写成
lambda x: config["samples"][x.sample]
也能工作。





(4) json格式的配置文件 (science的宏基因组流程为例)

json格式就是键值对格式，要求key和value都必须加引号。
对空格没有要求，空格仅仅是为了结构清晰。

$ cat config.sample.json
{
    "project" : "1511KMI-0007",
    "adapters" : "/data/tools/Trimmomatic/0.36/adapters/NexteraPE-PE.fa",
	"assembler" : "spades",
    "kmers" : "33,55,77,99,127",
    "assembly-klist" : {
       "longreads" : "21,33,55,77,99,127"
     }, 
	"treatment" : {
		"A" : ["MPR1-1n-CUR", "MPR1-4n-EST"],
		"B" : ["MPR3-3n-CUR"],
    },
    "data": {
		"MPR1-1n-CUR": { "forward":"MPR1-1n-CUR_1.fastq.gz", "reverse":"MPR1-1n-CUR_2.fastq.gz"},
		"MPR1-1n-EST": { "forward":"MPR1-1n-EST_1.fastq.gz", "reverse":"MPR1-1n-EST_2.fastq.gz"},
		#...
		"MPR3-4n-EST": { "forward":"MPR3-4n-EST_1.fastq.gz", "reverse":"MPR3-4n-EST_2.fastq.gz"}
	}
}


$ cat meta.sf 
if os.path.isfile("config.json"):#用于判断某一对象(需提供绝对路径)是否为文件
    configfile: "config.json"

# 最终需要很多文件，字符串使用split()分开成字符串数组。
rule final:
    input: expand("{project}/extract_16S/{sample}.bbduk.fa.gz \
                   {project}/diamond/{sample}.rma \
                   {project}/assembly/{assembler}/{treatment}/{kmers}/assembly.fa.gz \
                   {project}/stats/{assembler}/{treatment}/{kmers}/quast.report.txt \
                   {project}/stats/{assembler}/{treatment}/{kmers}/flagstat.linear.txt \
                   {project}/report/{project}.report.nb.html \
                   {project}/genecatalog/{assembler}/{kmers}/all.coverage.tsv \
                   {project}/genecatalog/{assembler}/{kmers}/all.diamond.nr.daa \
                   {project}/genecatalog/{assembler}/{kmers}/all.diamond.nr-taxonomy.txt \
                   {project}/genecatalog/{assembler}/{kmers}/all.coverage.taxonomy.txt \
                   {project}/genecatalog/{assembler}/{kmers}/all.coverage.taxonomy.ko.pfam.taxlevels.aggregated.tsv".split(),  project=config["project"], sample=config["data"], treatment=config["treatment"], assembler=config["assembler"], kmers=config["assembly-klist"])


# 多线程压缩函数pigz和pbzip2都是压缩函数，为了节省空间
rule merge_and_rename:
    input:
        forward = lambda wildcards: config["data"][wildcards.sample]['forward'],
        reverse = lambda wildcards: config["data"][wildcards.sample]['reverse']
		# 为什么使用 匿名函数？
    output:
        forward=protected("{project}/unpack/{sample}_1.fastq.gz"),
        reverse=protected("{project}/unpack/{sample}_2.fastq.gz"),
    threads: 16
    run:
        if os.path.splitext(input[0])[1] == ".bz2":
            shell("pbzip2 -p{threads} -dc {input.forward} | pigz -p {threads} > {output.forward}")
            shell("pbzip2 -p{threads} -dc {input.reverse} | pigz -p {threads} > {output.reverse}")
        if os.path.splitext(input[0])[1] == ".gz":
            shell("pigz -p {threads} -dc {input.forward} | pigz -p {threads} > {output.forward}")
            shell("pigz -p {threads} -dc {input.reverse} | pigz -p {threads} > {output.reverse}")











3. 规则参数 params

有些时候，shell命令不仅仅是由input和output中的文件组成，还需要一些静态的参数设置。如果把这些参数放在input里，则会因为找不到文件而出错，所以需要专门的 params用来设置这些参数。

或者从配置文件读取
params: 
	adapters = config["adapters"]


rule bwa_map:
    input:
		"data/genome.fa",
        lambda wildcards: config["samples"][wildcards.sample]
	output:
		"mapped_reads/{sample}.bam"
	threads: 8
	params: 
		rg=r"@RG\tID:{sample}\tSM:{sample}"    
	shell: 
		"bwa mem -R '{params.rg}' '-t {threads} {input} | samtools view -Sb - > {output}"

写在rule中的params的参数，可以在shell命令中或者是run里面的代码进行调用。

rule test_shell:
	params:
		head="-n 3",
		tail="-n 5"
	shell: 
		"ls / | head {params.head}  && echo '' &&"
		"ls / | tail {params.tail}"


$ cat r5.sf 
rule test_py:
	params:
		head="-n 3",
		tail="-n 5"
	run: 
		print('='*10)
		print( params )
		print( params['head'])

$ snakemake -s r5.sf -j 1
==========
-n 3 -n 5
-n 3

可见在py语句中，以上定义的params是字典形式。








4. 日志文件 log 

当工作流程特别的大，每一步的输出日志都建议保存下来，而不是输出到屏幕，这样子出错的时候才能找到出错的所在。 
snakemake非常贴心的定义了 log,用于记录日志。
好处就在于出错的时候，在 log里面定义的文件是不会被snakemake删掉，而output里面的文件则是会被删除。
继续修改之前的 bwa_map.


rule bwa_map:
    input:
		"data/genome.fa",
        lambda wildcards: config["samples"][wildcards.sample]
	output:
		"mapped_reads/{sample}.bam"
	params:
		rg="@RG\tID:{sample}\tSM:{sample}"
	log:
		"logs/bwa_mem/{sample}.log"
	threads: 8
	shell:
		"(bwa mem -R '{params.rg}' -t {threads} {input} | "
        "samtools view -Sb - > {output}) 2> {log}"
# 将标准错误重定向到了log中


==> 测试
$ cat log.sf:
rule log:
	log:
		"logs.log"
	shell: "ls xxd8/ 2>{log}"

$ snakemake -s log.sf -j1
报错了，但不明确，查看日志报错就很清楚:
$ cat logs.log 
ls: cannot access xxd8/: No such file or directory

注意：每一个rule的log文件名不能一样，否则会覆盖。










5. 临时文件和受保护的文件
Snakemake使用 temp()来将一些文件标记成临时文件，在执行结束后自动删除。以便节省硬盘空间。

rule bwa_map:
    input:
		"data/genome.fa",
		lambda wildcards: config["samples"][wildcards.sample]
    output:
		temp("mapped_reads/{sample}.bam")
    params:
		rg="@RG\tID:{sample}\tSM:{sample}"
    log:
		"logs/bwa_mem/{sample}.log"
    threads: 8
    shell:
		"(bwa mem -R '{params.rg}' -t {threads} {input} | "
        "samtools view -Sb - > {output}) 2> {log}"
# 当 samtools_sort运行结束后就会把"mapped_reads"下的BAM删掉。


同时由于比对和排序都比较耗时，得到的结果要是不小心被误删就会浪费大量计算时间，最后的方法就是用 protected()保护起来
rule samtools_sort:
    input:
		"mapped_reads/{sample}.bam"
    output:
		protected("sorted_reads/{sample}.bam")
    shell:
		"samtools sort -T sorted_reads/{wildcards.sample} "
        "-O bam {input} > {output}"

最后，snakemake就会在文件系统中对该输出文件写保护，也就是最后的权限为 -r--r--r--, 
在删除的时候会问你 rm:remove write-protectedregular file‘A.bam’?.








6. 消息 Messages 指令

rule NAME:
	input: "{sample}.txt"
	output: "out/{sample}.txt"
	threads: 8
	message: "Executing somecommand with {threads} threads on the following files {input}."
	shell: "head -n 2 {input} > {output}"

运行后看到
$ snakemake -s r2.sf -j1 out/{1,2}.txt
[Thu Jun 17 15:23:31 2021] 消息
Job 1: Executing somecommand with 1 threads on the following files 1.txt.

[Thu Jun 17 15:23:31 2021]
Finished job 1.
1 of 2 steps (50%) done

[Thu Jun 17 15:23:31 2021] 消息
Job 0: Executing somecommand with 1 threads on the following files 2.txt.

[Thu Jun 17 15:23:31 2021]
Finished job 0.
2 of 2 steps (100%) done






进阶部分小结
 - 使用 threads:定义不同规则所需线程数，有利于snakemake全局分配任务，最优化任务并行
 - 使用 configfile:读取配置文件，将配置和流程分离
 - snakemake在DAG阶段才会知道通配的具体文件名，因此在input和output出现的 wildcards就需要推迟到第二步。
 - 在 log里定义的日志文件，不会因任务失败而被删除
 - 在 params定义的参数，可以在shell和run中直接调用
 - temp()中的文件运行结束后会被删除，而 protected()中的文件会有写保护，避免意外删除。









========================================
|-- 怎么把自己写的脚本用到流程中呢？
----------------------------------------
假设自己写了一个脚本，可以这么执行： $ python3 script001.py -I 100 -O aa.txt
怎么把它整合到流程种呢？


1. 实例

$ pwd
/home/wangjl/data/chenxi/pipeline

$ tree -L 2
.
├── config.yaml-backup
├── main.sf
├── rules
│   ├── 01_get_fq_by_cb.sf
│   ├── 02_trim_adapter.sf
│   └── 03_select_cut_polyA_reads.sf
└── scripts
    └── get_cut_polyA_reads.py #写在这里的py脚本


(1) 入口 (流程目录)
$ cat main.sf 
configfile: "config.yaml"  ##这里定义snake脚本路径
SI=config["samples"]
ScriptPath="/home/wangjl/data/chenxi/pipeline/"

rule all:
	input:
		expand( "01_fq_cb/{sample}_extracted.fastq.gz", sample=SI),
		expand( "01_fq_cb/QC/{sample}_extracted_fastqc.html", sample=SI),
		expand( "02_trim/{sample}_extracted_trimmed.fq.gz", sample=SI),
		expand( "03_cutPolyA_reads/{sample}_polyA_reads.fq.gz", sample=SI),
		expand( "03_cutPolyA_reads/QC/{sample}_polyA_reads_fastqc.html", sample=SI),

include: "rules/01_get_fq_by_cb.sf"
include: "rules/02_trim_adapter.sf"
include: "rules/03_select_cut_polyA_reads.sf"


(2) 具体的流程细节 (流程目录)

$ cat rules/03_select_cut_polyA_reads.sf
rule select_cut_polyA_reads:
	input: "02_trim/{sample}_extracted_trimmed.fq.gz"
	output: "03_cutPolyA_reads/{sample}_polyA_reads.fq.gz"
	params: ScriptPath   #中转入口定义的变量: snake 脚本的路径
	log: "03_cutPolyA_reads/{sample}_polyA_reads.fq.gz.log"
	shell: "python3 {params}scripts/get_cut_polyA_reads.py -I {input}  -O {output} >{log} 2>&1" #构建该脚本的绝对地址


rule QC_after_trim_polyA:
	input: "03_cutPolyA_reads/{sample}_polyA_reads.fq.gz"
	output: "03_cutPolyA_reads/QC/{sample}_polyA_reads_fastqc.html"
	params: output_dir="03_cutPolyA_reads/QC"
	threads: 50
	log: "03_cutPolyA_reads/QC/{sample}_polyA_reads_fastqc.html.log"
	shell: "fastqc -t {threads} {input} -o {params.output_dir} >{log} 2>&1"


(3) 运行 (执行目录)

$ snakemake -s /home/wangjl/data/chenxi/pipeline/main.sf -p -j 5

打印出命令本身，其中一行是
python3 /home/wangjl/data/chenxi/pipeline/scripts/get_cut_polyA_reads.py -I 02_trim/S3_extracted_trimmed.fq.gz  -O 03_cutPolyA_reads/S3_polyA_reads.fq.gz >03_cutPolyA_reads/S3_polyA_reads.fq.gz.log 2>&1











========================================
|-- snakemake 高级：目录结构与模块化(wrapper/include/subworkflow )、依赖的自动部署(conda, Singularity)、集群、环境变量 envvars(提供密码) 
----------------------------------------
1. snakemake 模块化 提供了三个层次
https://snakemake.readthedocs.io/en/stable/snakefiles/modularization.html  最详细
https://snakemake.readthedocs.io/en/stable/tutorial/additional_features.html#modularization
https://www.jianshu.com/p/574021f11a68


(1) 模块化之level 1---简单封装 (wrapper)
Snakemake包装器存储库是可重复使用包装器的集合，可用于快速使用Snakemake规则和工作流中的流行工具。

$ cat xx.sf
rule xx:
    input:
        "mapped/{sample}.bam"
    output:
        "mapped/{sample}-xx.sorted.bam"
    params:
        "-m 4G"
    threads: 8
    wrapper:
        "0.2.0/bio/samtools/sort"

$ snakemake -s xx.sf -j1 mapped/{A,B}-xx.sorted.bam

$ ls -lt mapped/
total 17604
-rw-r--r--. 1 wangjl jinwf 2245385 Jun 17 11:19 B-xx.sorted.bam
-rw-r--r--. 1 wangjl jinwf 2242429 Jun 17 11:17 A-xx.sorted.bam


在这里，Snakemake将自动从https://bitbucket.org/snakemake/snakemake-wrappers/src/0.2.0/bio/samtools/sort/wrapper.py下载相应的包装器。
因此，0.2.0可以替换为您要使用的version标记或commit id。

由于包装器实现中的更改不会自动传播到您的工作流程中，因此可确保可重复性。

替代地，例如，对于开发而言，包装程序指令还可以指向完整的URL，包括本地URL file://。
每个包装器都定义必需的软件包和版本。结合--use-condaSnakemake 的标志，这些将自动部署。


包装器到底是干啥的呢？

从其他人sf脚本中看到的:
wrapper:
        "file://./bio/sickle_pe"
# Use the bwa and samtools wrapper from the snakemake-wrappers repository
wrapper:
        "0.0.11/bio/bwa_mem"



########
# 包装器实例: 就是提前写好的 shell 语句，直接引用。
########
$ cat bio/cat100/wrapper.py #文件名就叫 wrapper.py，引用时使用相对路径。
from snakemake.shell import shell
shell("echo 'line number 100' | cat - {snakemake.input} > {snakemake.output}")

$ cat r1.sf
rule catNumber:
	input:"1.txt"
	output:"2.txt"
	wrapper: "file://./bio/cat100"

$ snakemake -s r1.sf -j1

$ cat 2.txt 
line number 100
1
2
3

更多包装器实例
https://bitbucket.org/snakemake/snakemake-wrappers/src/master/bio/





(2) 模块化之level2 ---大型分析流程的整合 (include)
https://github.com/snakemake/snakemake/blob/main/docs/tutorial/additional_features.rst

为了可重复使用，或者构建大的工作流，有时候需要把工作流拆分成小的模块。
不同功能的rule分开放到不同的脚本中(小的Snakefile)，然后通过include语句将其包含在主Snakefile中。

include: "path/to/other/snakefile"

包含是相对于出现它们的Snakefile目录的。例如，如果以上Snakefile驻留在directory中my/dir，则Snakemake将在此处搜索include my/dir/path/to/other/snakefile，而与工作目录无关。


使用哪个配置文件？
如果你的snake脚本在目录/path/A/下，你的工作目录在/path/B/下，你的/path/C/下运行脚本 snakemake -s /path/A/xx.sf -j1 则找配置文件是相对于当前目录的，也即是读取/path/C/config.yaml。
但是配置文件是相对于运行脚本的目录的，在哪里调用snake脚本，就相对于当前pwd找配置文件。


在我们的日常分析中，为了流程的重复使用和方便后续修改维护，会把参数设置、依赖软件的路径、样本信息写在单独的配置文件里。
在这种设置中，所有规则都共享一个公共配置文件。

├── config.yaml
├── softwares.yaml
├── samples.json
├── scripts
│   ├── script1.py
│   └── script2.R
├── rules
│   ├── 01qc.smk
│   ├── 02mapping.smk
│   └── 03plot.smk
└── Snakefile

默认的目标规则（通常称为all-rule）不会受到include的影响。也就是说，无论您有多少个包含在第一个规则之上的内容，它始终将是Snakefile中的第一个规则。



示例: 
找 SNP 的流程: https://github.com/PaulArthurM/cbf_aml_pipeline/blob/master/Snakefile



########
# include 实例
########
|-config.yaml
|-main.sf
|-rule/
  |-addLine1.sf
  |-addLine2.sf

$ cat config.yaml 
samples:
  - 1
  - A
  - B

$ cat main.sf 
configfile: "config.yaml"
SI=config["samples"]

rule all:
        input: expand( "{sample}.out.2", sample=SI)

include: "rules/addLine1.sf"
include: "rules/addLine2.sf"

$ cat rules/addLine1.sf 
rule addL1:
        input:"{sample}.in.1"
        output:"{sample}.out.1"
        shell:"echo 'line1 by rule1' | cat - {input} > {output}"

$ cat rules/addLine2.sf 
rule addL2:
        input:"{sample}.out.1"
        output:"{sample}.out.2"
        shell:"echo 'line 2 by rule 2' | cat {input} -  > {output}"


路径都是相对于主文件的，被包含的文件在哪里无所谓。
提前按照配置文件准备好输入: 1.in.1, A.in.1, B.in.1
include的先后顺序也无关。

$ snakemake -s main.sf -j1
Job counts:
        count   jobs
        3       addL1
        3       addL2
        1       all
        7

$ cat A.out.2 
line1 by rule1
this is A
line 2 by rule 2






#####################
# 使用配置文件 yaml or json
#####################
$ cd test2/
$ cat config.yaml #格式有严格要求: 逗号后有空格, 冒号后有空格，不需要加引号了。
samples: [SRR7629152, SRR7629153, SRR7629154, SRR7629161, SRR7629162, SRR7629163]
workdir: /home/wangjl/data/ATAC/fq/test3/

$ cat config.json #格式很严格: 键值都要加引号，键值对后加逗号，最后一个不加。
{
	"samples":["SRR7629152","SRR7629153","SRR7629154", "SRR7629161", "SRR7629162", "SRR7629163"],
	"workdir":"/home/wangjl/data/ATAC/fq/test3/"
}


snake脚本中
$ cat test.sf
configfile: "config.yaml"
workdir: config["workdir"]

SI=config["samples"]
print(SI)
print(SI[0])
print(SI[1])

rule get:
	input:"1.txt"
	output:"2"
	shell:"cat {input} > {output}"

$ snakemake -s test.sf -j 1 -F
['SRR7629152', 'SRR7629153', 'SRR7629154', 'SRR7629161', 'SRR7629162', 'SRR7629163']
SRR7629152
SRR7629153

$ ls -lth ../test3/2
-rw-rw-r-- 1 wangjl wangjl 10 Jun 23 16:36 ../test3/2








(3) 模块化之level 3---构建子工作流程 (subworkflow)
https://snakemake.readthedocs.io/en/stable/snakefiles/modularization.html#snakefiles-sub-workflows

除了包括其他工作流程的规则外，Snakemake还允许依赖于其他工作流程的输出作为子工作流程。
在执行当前工作流程之前，将独立执行子工作流程。
因此，Snakemake可以确保在必要时创建或更新当前工作流程所依赖的所有文件。这允许在原本单独的数据分析之间创建链接。

subworkflow otherworkflow:
    workdir:
        "../path/to/otherworkflow"
    snakefile:
        "../path/to/otherworkflow/Snakefile"
    configfile:
        "path/to/custom_configfile.yaml"

rule a:
    input:
        otherworkflow("test.txt")
    output: ...
    shell:  ...


在这里，子工作流被命名为 “otherworkflow”，它位于工作目录中../path/to/otherworkflow。
snakefile位于同一目录中，并称为Snakefile。
如果snakefile未为子工作流定义，则假定它位于workdir位置并称为Snakefile，因此，在上面我们也可以省略snakefile关键字。
如果workdir未指定，则假定与当前相同。（可选的）定义configfile允许根据需要参数化子工作流。
从我们依赖的子工作流输出的文件将标有otherworkflow功能（请参见规则a的输入）。此功能自动确定文件的绝对路径（在此处../path/to/otherworkflow/test.txt）。


在执行时，snakemake首先尝试通过执行子工作流来创建（或更新，如果需要）test.txt（以及所有其他可能提到的依赖项）。然后执行当前的工作流程。这也可以递归发生，因为子工作流也可能有其自己的子工作流。














2. 流程依赖的自动部署
(1) 使用 conda 设置全局环境

1) 先用 $ conda create -n 项目名 python=版本号 创建一个全局环境，用于安装一些常用的软件，例如bwa、samtools、seqkit等
2) 将环境导出成yaml文件，也就是导出conda环境 
$ conda env export -n 项目名 -f environment2.yaml

3) 当你到了一个新的环境，你就可以用下面这个命令重建出你的运行环境
重新创建环境 $ conda env create -f environment2.yaml




(2) 局部环境：snakmake更高级的特性，也就是为每个rule定义专门的运行环境

使用conda安装定义在config中的环境
rule mytask:
    input:
        "path/to/{dataset}.txt"
    output:
        "result/{dataset}.txt"
    conda:
        "envs/some-tool.yaml"
    shell:
        "some-tool {input} > {output}"
其中 yaml中：
channels:
  - conda-forge
dependencies:
  - some-tool =2.3.1
  - some-lib =1.1.2
#



实例:
rule bwa_map:
    input:
		"data/genome.fa",
		"data/samples/A.fastq"
    output:
		"mapped_reads/A.bam"
    conda:
		"envs/map.yaml"
    shell:
		"""
		mkdir -p mapped_reads
		bwa mem {input} | samtools view -Sb - > {output}
		"""
随后在snakemake执行的目录下创建envs文件夹，增加map.yaml, 内容如下
name: map
channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
  - defaults
dependencies:
  - bwa=0.7.17
  - samtools=1.9
show_channel_urls: true

注意: YAML文件的 name行不是必要的，但是建议加上。

那么当你用 snakmake--use-conda执行时，他就会在 .snakemake/conda下创建专门的conda环境用于处理当前规则。
对于当前项目，该conda环境创建之后就会一直用于该规则，除非yaml文件发生改变。

如果你希望在实际运行项目之前先创建好环境，那么可以使用 --create-envs-only参数。


由于默认情况下，每个项目运行时只会在当前的 .snakemake/conda查找环境或者安装环境，所以在其他目录执行项目时，snakemake又会重新创建conda环境，如果你担心太占地方或者环境太大，安装的时候太浪费时间，你可以用 --conda-prefix指定专门的文件夹。








(3). singularity(类docker的容器技术)的整合 
rule mytask:
    input:
        "path/to/{dataset}.txt"
    output:
        "result/{dataset}.txt"
    singularity:
        "docker://biocontainers/some-tool#2.3.1"
    shell:
        "some-tool {input} > {output}"
#




(4). Singularity + Conda
singularity:
    "docker://continuumio/miniconda3:4.4.1" ## define OS
rule mytask:
    input:
        "path/to/{dataset}.txt"
    output:
        "result/{dataset}.txt"
    conda:
        "envs/some-tool.yaml"  ## define tools/libs
    shell:
        "some-tool {input} > {output}"
#








3. 集群投递/集群执行
(1)
$ snakemake --cluster "qsub -V -cwd -q 投递队列" -j 10
参数解释
# --cluster /-c CMD: 集群运行指令
# qusb -V -cwd -q， 表示输出当前环境变量(-V),在当前目录下运行(-cwd), 投递到指定的队列(-q), 如果不指定则使用任何可用队列

# -j N: 在每个集群中最多并行N核
# --local-cores N: 在每个集群中最多并行N核

# --cluster-config/-u FILE: 集群配置文件


(2)
$ snakemake --jobs 8 --cluster 'qsub -cwd -q normal -l vf=5g,p=1'
这样可以在normal节点，最多一次提交8个任务，最大内存5g，每个任务使用1个cpu。


//todo 如何设置？
如果你的任务依赖于其他任务的结果文件，Snakemake会等待直到这个任务的所有依赖文件都生成后再提交。








4. 环境变量 envvars 
使用场景？需要 密码凭证，而你又不想把密码写到任何文本中。

envvars:
    "SOME_VARIABLE",
    "SOME_OTHER_VARIABLE"

rule do_something:
    output:
         "test.txt"
    params:
        x=os.environ["SOME_VARIABLE"]
    shell:
        "echo {params.x} > {output}"


(1) 测试
$ cat env.sf
envvars:
    "SOME_VARIABLE"

rule do_something:
    output:
         "test.txt"
    params:
        x=os.environ["SOME_VARIABLE"]
    shell:
        "echo {params.x} > {output}"

怎么使用？
1)在当前 shell 创建一个临时变量
$ SOME_VARIABLE="this is a password."
$ echo $SOME_VARIABLE
this is a password.

2)然后暴露该变量为环境变量
$ export SOME_VARIABLE

3)执行脚本
$ snakemake -s env.sf -j 1
正常结束。

$ head test.txt 
this is a password.




















ref:
https://www.jianshu.com/p/8e57fd2b81b2

推荐阅读
[1] Reproductible workflow：https://lachlandeer.github.io/snakemake-econ-r-tutorial/
[2] Snakemake tutorial：https://slowkow.com/notes/snakemake-tutorial/
[3] Snakemake官方文档：https://snakemake.readthedocs.io/en/stable/index.html
[4] ATACseq pipeline：https://github.com/crazyhottommy/pyflow-ATACseq




========================================
|-- 怎么嵌套 子流程 Sub-Workflows? 表示依赖? //todo
----------------------------------------

https://snakemake.readthedocs.io/en/stable/snakefiles/modularization.html#snakefiles-modules
官方给的例子极其精简，很难流畅的运行。

subworkflow otherworkflow:
    workdir:
        "../path/to/otherworkflow"
    snakefile:
        "../path/to/otherworkflow/Snakefile"
    configfile:
        "path/to/custom_configfile.yaml"

rule a:
    input:
        otherworkflow("test.txt")
    output: ...
    shell:  ...

探索如下：
怎么运行子流程？done;
怎么解决几个子流程的依赖顺序？ -np 参数报错，去掉后可能按顺序正常。





1. flowA
$ cd /data/wangjl/test/snake/
$ mkdir flowA
$ cat flowA/main.sf 
rule all:
  input:"A.txt", "B.txt"
  output:"C.txt"
  shell: "cat {input} >{output}"

准备两个文本文件:
$ cat A.txt 
This is A.txt
$ cat B.txt 
from B.txt
运行:
$ snakemake -s flowA/main.sf -j 2

新生成的文件:
$ cat C.txt 
This is A.txt
from B.txt





2. flowB
和A类似，不过调用了一个py脚本，多停留几秒钟。
$ mkdir flowB
$ vim flowB/mergeWithPy.sf
#SI=["A", "B"]
configfile: "configfile.yaml"
SI=config["sample"]

rule all:
  input:expand("{index}.txt", index=SI)
  output:"C2.txt"
  run: 
    import time
    with open(output[0], "w") as out:
      for i in input:
        sample=i.split('.')[0]
        for line in open(i):
          time.sleep(1)
          out.write(sample+": "+ line)

配置文件
$ vim configfile.yaml
sample:
 - A
 - B
 - C


运行:
$ rm C2.txt 
$ snakemake -s flowB/mergeWithPy.sf -j 2

检查输入
$ cat C2.txt 
A: This is A.txt
B: from B.txt
C: This is A.txt
C: from B.txt




3. flowC 检查前两个生成的文件有几行
需要依赖前A/B个流程结束后的文件。
而B又依赖A。

第一版：
仅仅使用一个嵌套: 
特别注意：没有配置文件语句则报错：https://github.com/snakemake/snakemake/issues/24
$ vim mainC1.sf
subworkflow otherworkflow1:
	snakefile: "flowA/main.sf"
	configfile: "configfile.yaml" #虽然不用，也要指定一个配置文件。

rule all:
	input: otherworkflow1("C.txt")


运行:
$ snakemake -s mainC1.sf -j 2 -np
$ snakemake -s mainC1.sf -j 2





第2版：C依赖A+B
$ vim mainC2.sf
subworkflow otherworkflow1:
	workdir:
		"./"
	snakefile:
		"flowA/main.sf"
	configfile: "configfile.yaml"

subworkflow otherworkflow2:
	workdir:
		"./"
	snakefile:
		"flowB/mergeWithPy.sf"
	configfile:
		"configfile.yaml"

rule all:
	input:
		"WC.txt"

rule run1:
	input:
		otherworkflow1("C.txt"),
		otherworkflow2("C2.txt"),
	output:"WC.txt"
	shell: "wc {input} > {output}"

删除中间文件
$ rm C.txt C2.txt

运行总流程
$ snakemake -s mainC2.sf -j 2 -np #报错，C2.txt的依赖没解决
$ snakemake -s mainC2.sf -j 2

检查结果
$ cat WC.txt 
 2  5 25 /data/wangjl/test/snake/C.txt
 4 14 62 /data/wangjl/test/snake/C2.txt






第3版：B依赖A，C依赖A+B 失败
$ vim mainC3.sf
subworkflow otherworkflow1:
	workdir:
		"./"
	snakefile:
		"flowA/main.sf"
	configfile: "configfile.yaml"

subworkflow otherworkflow2:
	workdir:
		"./"
	snakefile:
		"flowB/mergeWithPy.sf"
	configfile:
		"configfile.yaml"

rule all:
	input:
		"WC.txt"

rule run1:
	input: otherworkflow1("C.txt")
	output: temp("done1.txt")
	shell: "touch {output[0]}"

rule run2:
	input: otherworkflow2("C2.txt"), "done1.txt"
	output: temp("done2.txt")
	shell: "touch {output}"


rule run3:
	input: "done1.txt", "done2.txt"
	output: "WC.txt"
	shell: "wc C.txt C2.txt > {output}"

删除中间文件
$ rm C.txt C2.txt WC.txt

运行总流程
$ snakemake -s mainC3.sf -j 2 -np #还是报错，flowB 无法找到以来的C.txt
$ snakemake -s mainC3.sf -j 2




第4版：B依赖A，C依赖A+B 
这个语法最终没有通过 https://github.com/snakemake/snakemake/issues/743
而是使用了 Module: https://github.com/snakemake/snakemake/pull/888

$ vim mainC4.sf
configfile: "configfile.yaml"

rule all:
	input:
		"WC.txt"

rule run1:
	input: "A.txt", "B.txt"
	output: "C.txt"
	subworkflow: "flowA/main.sf"

rule run2:
	input: "C.txt"
	output: "C2.txt"
	subworkflow: "flowB/mergeWithPy.sf"

rule run3:
	input: "C.txt", "C2.txt"
	output: "WC.txt"
	shell: "wc {input} > {output}"

删除中间文件
$ rm C.txt C2.txt WC.txt

运行总流程
$ snakemake -s mainC4.sf -j 2 -np #还是报错，flowB 无法找到以来的C.txt
$ snakemake -s mainC4.sf -j 2






========================================
|-- Input functions? 输入函数 or 匿名函数
----------------------------------------
https://snakemake.readthedocs.io/en/stable/tutorial/advanced.html#tutorial-input-functions

1. 执行的三个阶段

(1).In the initialization phase, the files defining the workflow are parsed and all rules are instantiated.
实例化阶段：定义workflow的文件被解析，实例化所有的rules。

(2).In the DAG phase, the directed acyclic dependency graph of all jobs is built by filling wildcards and matching input files to output files.
在DAG阶段：通过填充通配符，匹配输入和输出文件，建立有向无环依赖图。

(3).In the scheduling phase, the DAG of jobs is executed, with jobs started according to the available resources.
在调度阶段，DAG规定的job被执行，根据可用的资源开始启动job。



snakemake在第二步DAG阶段，才会有wildcards这个变量。



2. 输入函数有啥用？


(1) 设计简单的例子:

配置文件:
$ cat config.yaml
samples:
    A: data/A.fastq
    B: data/B.fastq

输入文件
$ cat data/A.fastq
this is A.fastq

$ cat data/B.fastq
this is B.fastq



写规则
$ cat main.snk
configfile: "config.yaml"
SI=config["samples"].keys()
print(SI)

rule all:
	input: expand("mapped/{sample}.bam", sample=SI)

rule bwa_map:
	input: 
		"data/{sample}.fastq"
	output:
		"mapped/{sample}.bam"
	shell:
		"cat {input} > {output}"

运行测试
$ snakemake -s main.snk -j 2 -pn



(2) 使用 函数作为input

写规则
$ cat main2.snk
configfile: "config.yaml"
SI=config["samples"]
print(SI)


rule all:
	input: expand("mapped/{sample}.bam", sample=SI)


def get_bwa_map_input_fastqs(wildcards):
	print(">>> ", config["samples"][wildcards.sample]);
	return config["samples"][wildcards.sample]

rule bwa_map:
	input: 
		# "data/{sample}.fastq" #原来
		# get_bwa_map_input_fastqs #这有啥优点呢？
		lambda wildcards: config["samples"][wildcards.sample]  #匿名函数写法
	output:
		"mapped/{sample}.bam"
	shell:
		"cat {input} > {output}"

$ snakemake -s main2.snk -j 2 -p
{'A': 'data/A.fastq', 'B': 'data/B.fastq'}
Building DAG of jobs...
>>>  data/A.fastq
>>>  data/B.fastq
Using shell: /bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
		1       all
		2       bwa_map
		3


输入函数接收一个wildcards对象作为single argument，这允许通过属性（此处是wildcards.sample）来获取wildcards的值。 
必须要返回一个字符串或者字符串列表，其可以被解释为输入文件的（此处我们返回config文件中包含的sample路径）。
一旦一个工作的wildcards值被确定，输入函数就可以被评估。


查看运行图，snakemake脚本中不能出现print()输出
$ snakemake -s main2.snk --dag -j 2 -p  | dot -Tsvg > main2.dag.svg



(3) input 使用 函数的优点

我的理解: 
使用函数的优点，是可以更灵活的设置输入文件。比如根据关键词的某些特点，选择从不同路径获取文件。
而 expand() 只能根据一个规则生成结果。


比如: 
def get_bwa_map_input_fastqs(wildcards):
	return config["samples"][wildcards.sample]


可以该写的更自主化: 根据 wildcards.sample 选择哪个文件
def get_bwa_map_input_fastqs(wildcards):
	if wildcards.sample.startswith("A"):
		return config["samplesA"]
	if wildcards.sample.startswith("B"):
		return config["samplesB"]



完整脚本 
$ cat main3.snk 
configfile: "config3.yaml"
SI=config["samples"]
print(SI)


rule all:
	input: expand("mapped/{sample}.bam", sample=SI)


def get_bwa_map_input_fastqs(wildcards):
	if wildcards.sample.startswith("A"):
		return config["samplesA"]
	if wildcards.sample.startswith("B"):
		return config["samplesB"]

rule bwa_map:
	input: 
		get_bwa_map_input_fastqs
	output:
		"mapped/{sample}.bam"
	shell:
		"cat {input} > {output}"


配置文件:
$ cat config3.yaml
samples:
    - A
    - B
samplesA:
    - data/A/R1.fastq
    - data/A/R2.fastq
samplesB:
    - data/B/R1.fastq
    - data/B/R2.fastq


数据文件:
$ tree data
data
├── A
│   ├── R1.fastq
│   └── R2.fastq
├── B
│   ├── R1.fastq
│   └── R2.fastq


运行
$ snakemake -s main3.snk -j 2 -p

结果:
$ ls -lth mapped/
total 8.0K
-rw-rw-r-- 1 wangjl wangjl 9 Sep 20 09:58 B.bam
-rw-rw-r-- 1 wangjl wangjl 9 Sep 20 09:58 A.bam

$ cat mapped/A.bam 
A R1
A 2

$ cat mapped/B.bam 
B 1
B R2



https://cloud.tencent.com/developer/ask/sof/560842
https://blog.csdn.net/qq_36333576/article/details/118465010











========================================
|-- *** 疑难杂症 ***
----------------------------------------

https://blog.csdn.net/weixin_44616693/article/details/125462010






========================================
|-- shell命令各种问题，如：怎么处理 awk 同时有单引号、双引号的报错？
----------------------------------------
1. 例子: 使用awk把逗号分割变为tab分割的文件
$ cat a.txt
a,1,2,3,4
b,2,3,80,2
z,10,34,2322,34432
d,3,4,8,1

$ cat a.txt | awk -F "," '{print $1"\t"$2"\t"$3"\t"$4"\t"$5}' > b.txt
$ cat b.txt 
a       1       2       3       4
b       2       3       80      2
z       10      34      2322    34432
d       3       4       8       1


(2) 使用 snakemake
$ cat awk.sf
rule doubleQuote:
	input: "a.txt"
	output: "b.txt"
	shell:"cat a.txt | awk -F "," '{print $1"\t"$2"\t"$3"\t"$4"\t"$5}' > b.txt"

$ snakemake -s awk.sf -j1
SyntaxError in line 4 of /home/wangjl/test/snakemake/test3/awk.sf:
unexpected character after line continuation character

unexpected character after line continuation character
连续行字符后的意外字符


## 报错了，怎么办？
	- 把双引号前面加上反斜线转义：\"，或者改为单引号。防止单双引号分不清界限。
	- awk的{}变为两层{{}}。因为snakemake的变量替换需要去掉一层{}。


$ cat awk2.sf
rule doubleQuote:
	input: "a.txt"
	output: "b.txt"
    shell:"cat a.txt | awk -F ',' '{{print $1\"\t\"$2\"\t\"$3\"\t\"$4\"\t\"$5}}' > b.txt"
    #shell:"cat a.txt | awk -F \",\" '{{print $1\"\t\"$2\"\t\"$3\"\t\"$4\"\t\"$5}}' > b.txt"

$ snakemake -s awk2.sf -j1
$ cat b.txt 
a       1       2       3       4
b       2       3       80      2
z       10      34      2322    34432
d       3       4       8       1


获取文件第8列，获取唯一值，去掉首行
$ shell: "cat {input} | awk -F '\t' '{{print $8}}' | awk -F'(' '{{print $1}}' | sed  -e 's/[ ]*\$//g' -e '1d' > {output}"



(3) 实战
$ samtools view map/SRR7629163.sambamba_rmdup.bam |  awk -F'\t' 'function abs(x){return ((x<0.0)?-x:x)} {print $1"\t"abs($9)}' | sort | uniq | cut -f 2 |head
66
45
53
61
55


$ cat a1.sf
rule fragment_length:
	input: "map/SRR7629163.sambamba_rmdup.bam"
	output: "test/frag_len.txt"
	log: "test/frag_len.log"
	shell: 
		"samtools view {input} | \
		awk -F'\t' 'function abs(x){{return ((x<0.0)?-x:x)}} {{print $1\"\t\"abs($9)}}'| \
		sort | uniq | cut -f2 >{output} 2>{log}"

$ snakemake -s a1.sf -j1
$ head test/frag_len.txt 
66
45
53
61
55


(4) 实战2：批量化
$ cat a2.sf
SI=["SRR7629163","SRR7629162"]

rule all:
	input: expand("test/{sample}.frag_len.txt", sample=SI)

rule fragment_length:
	input: "map/{sample}.sambamba_rmdup.bam"
	output: "test/{sample}.frag_len.txt"
	log: "test/{sample}.frag_len.txt.log"
	shell: 
		"samtools view {input} | \
		awk -F'\t' 'function abs(x){{return ((x<0.0)?-x:x)}} {{print $1\"\t\"abs($9)}}'| \
		sort | uniq | cut -f2 >{output} 2>{log}"

$ snakemake -s a2.sf -j1
$ head test/SRR7629162.frag_len.txt
60
43
$ ls test/ -lth
total 664K
-rw-rw-r-- 1 wangjl wangjl 198K Jun 21 21:35 SRR7629162.frag_len.txt
-rw-rw-r-- 1 wangjl wangjl    0 Jun 21 21:35 SRR7629162.frag_len.txt.log
-rw-rw-r-- 1 wangjl wangjl 231K Jun 21 21:35 SRR7629163.frag_len.txt
-rw-rw-r-- 1 wangjl wangjl    0 Jun 21 21:35 SRR7629163.frag_len.txt.log








========================================
|-- 能否把 snakemake 脚本和输出分开? 使用 workdir: "path/to/workdir"
----------------------------------------
1. In snakemake documentation:

"All paths in the snakefile are interpreted relative to the directory snakemake is executed in. This behaviour can be overridden by specifying a workdir in the snakefile:"

workdir: "path/to/workdir"

So just put that at the begining of your snakefile and all inputs and outputs will be interpreted relative to this path.

这个值可以在配置文件中定义。


(1) 实例 
$ cd test4
$ cat s1.sf
workdir: "../test3/"
rule cat:
	input:"1.txt"
	output:"from_test2.txt"
	shell:"cat {input} > {output}"

$ snakemake -s s1.sf -j 1
$ head ../test3/from_test2.txt 
1



ref:https://stackoverflow.com/questions/40941249/snakemake-is-there-a-way-to-specify-an-output-directory-for-each-rule







========================================
|-- rule all 是干什么的？其input就是目标文件，然后倒序查找其依赖谁的output
----------------------------------------
1.最重要的一条规则：all

前面我们已经讲过，snakemake 通过各个规则的输入、输出来确定执行顺序和依赖关系，snakemake 每次默认执行第一个规则之后，会通过该规则的依赖关系（输入）来决定接下来执行哪个规则。
所以，为了将所有规则都调用起来，我们需要在工作流文件最开始写下第一个规则：all 规则，来调取各支线最后一条规则的输出，这样层层递推，就可以将所有关联规则都调用起来。示例如下：

rule all:
    input:
        "stat/stat.txt"

#下面加一个合并文件的rule



$ cat stat/stat.txt 
i am stat.txt

$ cat sort2.sf 
rule all:
    input:
        "stat/stat.txt"
    shell: 
        "echo 'this is rule all' && cat {input}"
rule sort:
	input: "words.txt"
	output: "words.sorted.txt"
	shell: "sort {input} > {output}"

执行结果: 
$ snakemake -s sort2.sf 
Building DAG of jobs...
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1

rule all:
    input: stat/stat.txt
    jobid: 0

this is rule all
i am stat.txt
Finished job 0.
1 of 1 steps (100%) done

问题: 为什么只执行一个rule，第二个rule怎么不执行呢？


(2) 这就解释了为什么rule all 是最后的规则了。
找到rule all中的input，整个流程就结束了。
为了找到这个文件在哪里，就需要看其他rule的输出，就是倒推执行过程。


$ cat sort3.sf 
rule all:
    input:
        "words.sorted.txt"  # 如果这个是最终文件，则该规则最后执行。
    shell: 
        "echo 'this is rule all' && cat {input}"
rule sort:
	input: "words.txt"
	output: "words.sorted.txt"
	shell: "sort {input} > {output}"

执行结果 
$ snakemake -s sort3.sf 
Building DAG of jobs...
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1

rule all:
    input: words.sorted.txt
    jobid: 0

this is rule all
apple
beach
book
good
pear
zoo
Finished job 0.
1 of 1 steps (100%) done




2. 啥意思？不懂GNU Make
rule all:
    input:
        expand('{sample}.txt', sample=SAMPLES)

rule all的input是流程最终的目标文件，类似于GNU Make，从顶部指定目标。
expand是Snakemake的特有函数，类似列表推导式。


Snakemake的工作流程是这样的：
- 先读rule all，了解目标文件是Sample1.txt和Sample2.txt。



3. 测试: 有人在QQ群问为什么报错？ 
(1)
$ cat filter.sf 
SI=["test_1"]

rule all:
	input:
		expand("{sample}_1_trimmed.fq", sample=SI),
		expand("{sample}_2_trimmed.fq", sample=SI)
rule fileter:
	input:
		"{sample}_1.fq"
		"{sample}_2.fq"
	output:
		"{sample}_1_trimmed.fq"
		"{sample}_2_trimmed.fq"
	params:
		"./"
	shell: "echo {params} {input[0]} {input[1]} 1>{output[0]} 2>{output[1]}"

报错:
$ snakemake -s filter.sf 
Building DAG of jobs...
MissingInputException in line 3 of /home/wangjl/test/SnakeMake/filter.sf:
Missing input files for rule all:
test_1_2_trimmed.fq
test_1_1_trimmed.fq



(2) 我认为是 expand 不配套
$ cat filter3.sf 
SI=["test_1"]

rule all:
        input:
                expand("{sample}_1_trimmed.fq", sample=SI),
                expand("{sample}_2_trimmed.fq", sample=SI)
rule fileter:
        input:
                "{sample}_1.fq",
                "{sample}_2.fq"
        output:
                "{sample}_1_trimmed.fq",
                "{sample}_2_trimmed.fq"
        params:
                "./"
        shell: "cat {params}{input[0]} > {output[0]} && cat {input[1]} >{output[1]}"


执行结果 
$ snakemake -s filter3.sf 
Building DAG of jobs...
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	fileter
	2

rule fileter:
    input: test_1_1.fq, test_1_2.fq
    output: test_1_1_trimmed.fq, test_1_2_trimmed.fq
    jobid: 1
    wildcards: sample=test_1

Finished job 1.
1 of 2 steps (50%) done

localrule all:
    input: test_1_1_trimmed.fq, test_1_2_trimmed.fq
    jobid: 0

Finished job 0.
2 of 2 steps (100%) done





========================================
|-- 如何在rule内使用常量？使用 config 中定义的常量？ "xx{para}".format(para=para2) 或者 f"xx{para2}"
----------------------------------------
1. 用完后删除该变量：全局变量是隐患！不用了就立刻删掉
a="hi"
# 使用变量 
del a





2. 常量的表示: 

(1) 定义
$ head config.yaml
samples:
 - L


(2) 两种使用方法
$ cat main.sf
configfile: "config.yaml"
samples=config["samples"][0]

print(samples);

# 方法1
input: "B01_STAR_solo/{sample}_Aligned.sortedByCoord.out.bam".format( sample=samples)

# 方法2
input: f"B01_STAR_solo/{samples}_Aligned.sortedByCoord.out.bam"


















========================================
|-- snakemake 中限制内存的使用 --resources mem_mb=60000
----------------------------------------
1. 声明式的，不算真正的控制内存
运行:
$ snakemake -s Snakefile -j 40 --resources mem_mb=60000

规则中也要指定：
仅在命令行上指定--resources mem_mb=60000是不够的，还需要为要检查的规则指定mem_mb。例如。：

rule markdups:
    input: ...
    ouptut: ...
    resources:
        mem_mb= 20000
    shell: ...

rule sort:
    input: ...
    ouptut: ...
    resources:
        mem_mb= 1000
    shell: ...

这将以这样的方式提交作业，您在任何时候都不会超过60GB的总容量。例如，这将最多运行3个markdups作业，或2个markdups作业和20个sort作业，或60个sort作业。

不带mem_mb的规则将不计入内存使用量，对于复制文件等不需要太多内存的规则，这可能是正常的。





https://www.5axxw.com/questions/content/f8oy59






========================================
|-- 使用snakemake并行调用很慢的R脚本(必须是可拆解的、依赖id的R脚本)
----------------------------------------

## 这个脚本R脚本巨慢，依赖输入参数可以分拆成很多份
$ test.R
## 依赖2个传入的参数：起始、步长
# get parameters
myArgs<-commandArgs(TRUE)
n1=as.numeric( myArgs[1] )
len=as.numeric( myArgs[2] )

n2=n1+len
print( paste('[',n1, n2, ')'))

# load data
#genes2=readLines("input/genes2.gene.txt")
rnaM2=read.table("input/rnaM2.df.txt")

out_cor=rnaM2[n1:n2,]

print( dim(out_cor) )
write.table(out_cor, paste0("output_test/APA_regulator_Cor_", n1, "_", len, ".df.txt") )




$ cat config.yaml 
len: 20
n1:
 - 1
 - 21
 - 41

$ cat test.sf
configfile: "config.yaml"

rule all:
	input: expand("output_test/APA_regulator_Cor_{n1}_{len}.df.txt", n1=config["n1"], len=config["len"])

rule R_script:
	output: "output_test/APA_regulator_Cor_{n1}_{len}.df.txt"
	log: "output_test/APA_regulator_Cor_{n1}_{len}.df.txt.log"
	shell: "Rscript test.R {wildcards.n1} {wildcards.len} {output} >{log} 2>&1"
#

$ snakemake -s test.sf -j 4









========================================
|-- 实例: 使用snakemake构建 ATAC-seq 分析流程(fastq -> call peak )
----------------------------------------
先给个玩具，主要是学流程。

最佳实践: 
- 不要在脚本中添加目录等具体信息，添加到 params中，在shell中引用即可。
- 使用 threads 设置CPU内核数，在运行时该数字会根据你的输入临时调整。
- 推荐用配置文件，脚本中引用 configfile: "config.yaml"
	- 脚本头部设定工作目录 workdir: config["workdir"]



1. snakefile 脚本的5个步骤

开始按照步骤写规则：
$ cat ATAC-seq.smk
####################
# ATAC-seq workflow
####################
REP_INDEX=["rep1","rep2"]
INDEX_BT2="/home/wangjl/ref/hg38/bowtie2_hg38/hg38_only_chromosome"
PICARD="/home/wangjl/soft/bin/picard.jar"

rule all:
	input: 
		expand("macs2_result/ATAC_seq_{rep}_peaks.narrowPeak", rep=REP_INDEX)

# 第一步: 去接头
rule cutadapt:
	input:
		"raw_fastq/ATAC_seq_{rep}_R1.fq.gz",
		"raw_fastq/ATAC_seq_{rep}_R2.fq.gz"
	output:
		"clean_fastq/ATAC_seq_{rep}_R1.fq.gz", 
		"clean_fastq/ATAC_seq_{rep}_R2.fq.gz"
	log:
		"clean_fastq/ATAC_seq_{rep}_cutadapt.log"
	shell:
		"cutadapt -j 4 --time 1 -e 0.1 -O 3 --quality-cutoff 25 \
		-m 50 -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC \
		-A AGACGGGAAGAGCGTCGTGAGGGAAGAGTGTAGATCTCGGTGGTCGCGTATCATT \
		-o {output[0]} -p {output[1]} {input[0]} {input[1]} > {log} 2>&1"

# -j 4 使用4核。输出是 -o 和-p。最后的 > {log} 2>&1 我认为可以简写为 2>{log}，因为没有啥输出了。
# -O MINLENGTH, --overlap MINLENGTH 设置最小重叠长度(read, 接头)
#       Require MINLENGTH overlap between read and adapter for an adapter to be found. Default: 3
#-q [5'CUTOFF,]3'CUTOFF, --quality-cutoff [5'CUTOFF,]3'CUTOFF 去掉低质量的reads。
#		Trim low-quality bases from 5' and/or 3' ends of each read before adapter removal. Applied to both reads if
#		data is paired. If one value is given, only the 3' end is trimmed. If two comma-separated cutoffs are given,
#		the 5' end is trimmed with the first cutoff, the 3' end with the second.
#-m LEN[:LEN2], --minimum-length LEN[:LEN2]  最小reads长度，低于该值就去掉。
#         Discard reads shorter than LEN. Default: 0
#-a ADAPTER, --adapter ADAPTER  去接头3'端。如果是$则只看最后位置。
#		Sequence of an adapter ligated to the 3' end (paired data: of the first read). The adapter and subsequent
#		bases are trimmed. If a '$' character is appended ('anchoring'), the adapter is only found if it is a
#		suffix of the read.


# 第二步: 比对
rule bt2_mapping:
	input:
		"clean_fastq/ATAC_seq_{rep}_R1.fq.gz", 
		"clean_fastq/ATAC_seq_{rep}_R2.fq.gz
	output:
		"bam/ATAC_seq_{rep}_bt2_hg38.sam"
	log:
		"bam/ATAC_seq_{rep}_bt2_hg38.log"
	shell:
		"bowtie2 -x {INDEX_BT2} -p 4 -1 {input[0]} -2 {input[1]} -S {output} >{log} 2>&1"
# -x 是之前建立的索引。-p 是CPU核心数。-1和-2是输入文件R1和R2。-S 输出文件。

# 第三步: sam to bam, and sort
rule bam_sort:
	input:
		"bam/ATAC_seq_{rep}_bt2_hg38.sam"
	output:
		"bam/ATAC_seq_{rep}_bt2_hg38_sort.bam"
	log:
		"bam/ATAC_seq_{rep}_bt2_hg38_sort.log"
	shell:
		"samtools sort -O BAM -o {output} -T {output}.temp -@ 4 -m 2G {input} >{log} 2>&1"
# -O 输出格式。-o 输出文件名。-T 临时文件名。-@ CPU核心数。-m 内存上限。最后是输出。



# 第四步: 去重复。使用picard会输出2个，一个bam，一个matrix文件。
rule rm_duplicate:
	input:
		"bam/ATAC_seq_{rep}_bt2_hg38_sort.bam"
	outpu:
		"bam/ATAC_seq_{rep}_bt2_hg38_sort_rmdup.bam",
		"bam/ATAC_seq_{rep}_bt2_hg38_sort_rmdup.matrix"
	log:
		"bam/ATAC_seq_{rep}_bt2_hg38_sort_rmdup.log"
	shell:
		"java -Xms5g -Xmx10g -XX:ParallelGCThreads=4 \
		-jar {PICARD} MarkDuplicates \
		I={input} O={output[0]} M={output[1]} \
		ASO=coordinate REMOVE_DUPLICATES=true 2>{log}"
# -Xms5g 最小使用5G内存。-Xmx10g 最大10G内存。-XX:ParallelGCThreads=4 CPU核心数。
# O第一个输出，M第二个输出。ASO=coordinate 指定是按照坐标轴sort过。去掉重复。


# 第5步: call peak
rule call_peak:
	input:
		"bam/ATAC_seq_{rep}_bt2_hg38_sort_rmdup.bam"
	output:
		"macs2_result/ATAC_seq_{rep}_peaks.narrowPeak"
	params: #这2个设置是MACS2本身shell的问题，不是每个工具都这样
		"ATAC_seq_{rep}",
		"macs2_result"
	log:
		"macs2_result/ATAC_seq_{rep}_peaks.log"
	shell:
		"macs2 callpeak -c xx -t {input} -f BAM -g hs \
		--outdir {params[1]} -n {params[0]} -m 2 100 >{log} 2>&1"
# -c 对照。-t 处理组。-f 格式。-g 基因组类型 人。
# --outdir 输出文件夹。-n 输出文件名。-m 2 100 富集区间 2-100的输出。




(2) 测试 
dry run 检查语法错误
$ snakemake -s ATAC-seq.smk -np # -n 不是真的运行，-p 打印shell命令。


画流程图 
$ snakemake -s ATAC-seq.smk --dag | dot -Tsvg > ATAC-seq.svg


(3) 运行 
$ snakemake -s ATAC-seq.smk -p -j 2 # 使用2个进程
$ snakemake -s ATAC-seq.smk -p -j 2 & 


看进程pid
$ ps | cut -d ' ' -f 2
$ ps | cut -d ' ' -f 2 | grep -P "^\d{1,}"  ## 显示进程号
$ ps | cut -d ' ' -f 2 | grep -P "^\d{1,}" | xargs kill ## 杀进程




refer:
https://www.bilibili.com/video/av45832590?p=6








========================================
配置文件格式优缺点：ini, JSON or YAML-formatted configuration files //todo
----------------------------------------

适合人类编写：ini > toml > yaml > json > xml > plist
可以存储的数据复杂度：xml > yaml > toml ~ json ~ plist > ini


1.YAML 语言（发音 /ˈjæməl/ ）的设计目标，就是方便人类读写。它实质上是一种通用的数据串行化格式。

Key-Value Pairs
  冒号分割，比如first_key: value1, value2
  key不能有空格，value可以有
  需注意数字，如果是integer可以写123，如果要strings要写"123"。在CWL中所有的baseCommand都需要是strings，所以必须加引号：baseCommand: [echo, "42"]


它的基本语法规则如下。
	大小写敏感
	使用缩进表示层级关系
	缩进时不允许使用Tab键，只允许使用空格。
	缩进的空格数目不重要，只要相同层级的元素左侧对齐即可

# 表示注释，从这个字符一直到行尾，都会被解析器忽略。

YAML 支持的数据结构有三种。
	对象：键值对的集合，又称为映射（mapping）/ 哈希（hashes） / 字典（dictionary）
	数组：一组按次序排列的值，又称为序列（sequence） / 列表（list）
	纯量（scalars）：单个的、不可再分的值


(1)对象 Maps
对象的一组键值对，使用冒号结构表示。
animal: pets
转为 JavaScript { animal: 'pets' }

用YAML格式进行分级，下一级“children”属于上一级“parent”。
用两个空格进行缩进，不能用tab。

Yaml 也允许另一种写法，将所有键值对写成一个行内对象。
hash: { name: Steve, foo: bar } 
转为 JavaScript { hash: { name: 'Steve', foo: 'bar' } }


(2)数组 Arrays
用-来为一个key提供多个value或objects（key-value对）。

一组连词线开头的行，构成一个数组。
- Cat
- Dog
- Goldfish
转为 JavaScript [ 'Cat', 'Dog', 'Goldfish' ]


数据结构的子成员是一个数组，则可以在该项下面缩进一个空格。
-
 - Cat
 - Dog
 - Goldfish
转为 JavaScript [ [ 'Cat', 'Dog', 'Goldfish' ] ]


数组也可以采用行内表示法。
animal: [Cat, Dog]
转为 JavaScript { animal: [ 'Cat', 'Dog' ] }

$ cat do.yaml
touchfiles:
	- foo.txt
	- bar.dat
	- baz.txt

exclusive_parameters:
	type:
		- type: record
		  name: itemC
		  fileds:
			itemC:
				type: string
				inputBinding: #设置input出现在command命令行的形式(前缀名，是否和前缀名有空格......)
					prefix: -C
		- type: record
		  name: itemD
		  fields:
			itemD:
				type: string
				inputBinding:
					prefix: -D
# end








(3)四、复合结构
对象和数组可以结合使用，形成复合结构。
languages:
 - Ruby
 - Perl
 - Python 
websites:
 YAML: yaml.org 
 Ruby: ruby-lang.org 
 Python: python.org 
 Perl: use.perl.org 

转为 JavaScript 如下。
{ languages: [ 'Ruby', 'Perl', 'Python' ],
  websites: 
   { YAML: 'yaml.org',
     Ruby: 'ruby-lang.org',
     Python: 'python.org',
     Perl: 'use.perl.org' } }

(4)纯量
纯量是最基本的、不可再分的值。以下数据类型都属于 JavaScript 的纯量。

字符串
布尔值
整数
浮点数
Null
时间
日期

数值直接以字面量的形式表示。
umber: 12.30 
转为 JavaScript { number: 12.30 }


尔值用true和false表示。
isSet: true
转为 JavaScript { isSet: true }


null用~表示。
parent: ~ 
转为 JavaScript { parent: null }


时间采用 ISO8601 格式。
iso8601: 2001-12-14t21:59:43.10-05:00 
转为 JavaScript { iso8601: new Date('2001-12-14t21:59:43.10-05:00') }

日期采用复合 iso8601 格式的年、月、日表示。
date: 1976-07-31
转为 JavaScript { date: new Date('1976-07-31') }


YAML 允许使用两个感叹号，强制转换数据类型。
e: !!str 123
f: !!str true
转为 JavaScript { e: '123', f: 'true' }


(5)字符串
字符串是最常见，也是最复杂的一种数据类型。

字符串默认不使用引号表示。
str: 这是一行字符串
转为 JavaScript { str: '这是一行字符串' }


如果字符串之中包含空格或特殊字符，需要放在引号之中。
str: '内容： 字符串'
转为 JavaScript { str: '内容: 字符串' }



单引号和双引号都可以使用，双引号不会对特殊字符转义。
s1: '内容\n字符串'
s2: "内容\n字符串"
转为 JavaScript { s1: '内容\\n字符串', s2: '内容\n字符串' }



单引号之中如果还有单引号，必须连续使用两个单引号转义。
str: 'labor''s day' 
转为 JavaScript { str: 'labor\'s day' }


字符串可以写成多行，从第二行开始，必须有一个单空格缩进。换行符会被转为空格。
str: 这是一段
  多行
  字符串
转为 JavaScript { str: '这是一段 多行 字符串' }


多行字符串可以使用|保留换行符，也可以使用>折叠换行。
this: |
  Foo
  Bar
that: >
  Foo
  Bar
转为 JavaScript { this: 'Foo\nBar\n', that: 'Foo Bar\n' }


+表示保留文字块末尾的换行，-表示删除字符串末尾的换行。
s1: |
  Foo

s2: |+
  Foo


s3: |-
  Foo
转为 JavaScript { s1: 'Foo\n', s2: 'Foo\n\n\n', s3: 'Foo' }



字符串之中可以插入 HTML 标记。
message: |

  <p style="color: red">
    段落
  </p>
转为 JavaScript { message: '\n<p style="color: red">\n  段落\n</p>\n' }








ref:
https://www.cnblogs.com/linkenpark/p/8899165.html
YAML格式(阮一峰的博客)：http://www.ruanyifeng.com/blog/2016/07/yaml.html




========================================
|-- python 读写配置文件 conf.ini (configparser包)
----------------------------------------
1. 配置文件 
$cat conf.ini
[info] 
age = 21
name = chen
gender = male

[file]
in=raw/data1/r1.fq
out= map/data1/r1.bam

[token]
cookies = 46ca47ffbb06ebea09f8b9d812d1a7d3



2. 读取配置文件：
$ cat b1.py 
import configparser,os
 
base_dir = str(os.path.dirname(os.path.dirname(__file__)))
base_dir = base_dir.replace('\\', '/')
file_path = base_dir + "./config/conf.ini"
print("file_path=", file_path);

 
cf = configparser.ConfigParser()   # configparser类来读取config文件
cf.read(file_path)

cookies = cf.get("token", "cookies")
print(cookies)

age=cf.get('info', 'age');
print('age=', age)


# 运行结果：
$ python b1.py 
file_path= ./config/conf.ini
46ca47ffbb06ebea09f8b9d812d1a7d3
age= 21



3. 写配置文件
$ cat b2.py
import configparser,os
# 获取token写入配置文件
base_dir = str(os.path.dirname(os.path.dirname(__file__)))
base_dir = base_dir.replace('\\', '/')
file_path = base_dir + "./config/conf2.ini"
print(file_path)

# 写token
config = configparser.ConfigParser()
config.add_section("mysql")
config.set('mysql', 'host', "192.168.1.105")
with open(file_path, 'w')as conf:
	config.write(conf)


# 需要先提前建立空文件
$ touch config/conf2.ini 
# 运行结果：
$ python b2.py 
./config/conf2.ini

确实已经添加上了
$ head config/conf2.ini 
[mysql]
host = 192.168.1.105


注意：写的句子必须是字符，支持空字符。默认覆盖掉同名旧值，相当于改写更新。
config.set('mysql', 'usr','root')
config.set('mysql', 'passwd', '')
config.set('mysql', 'port', '21') #must be string



https://blog.csdn.net/ezreal_tao/article/details/84840679









========================================
工作流管理平台 Apache Airflow (Python写的) //todo
----------------------------------------


1. 简介
Airflow is a platform to programmatically author, schedule and monitor workflows.
Airflow是一个编程编写、调度和监控工作流的平台。

https://airflow.apache.org/
https://github.com/apache/airflow
https://airflow.incubator.apache.org/


Apache Airflow was started at Airbnb as open source from the very first commit. The community has about 500 active members who support each other in solving problems



(2)教程：https://blog.csdn.net/qazplm12_3/article/details/53065654






2. 测试容器版
(1) 下载容器 https://hub.docker.com/r/puckel/docker-airflow/
SOURCE: https://github.com/puckel/docker-airflow
docker: https://hub.docker.com/r/puckel/docker-airflow/~/dockerfile/

# sudo docker pull puckel/docker-airflow

(2).运行容器，并做端口映射、文件映射
# sudo docker run -d -p 8080:8080 -v /home/wangjl/docker_airFlow:/home/wangjl/ puckel/docker-airflow
fc1a921581

(3).进入容器
$ sudo docker exec -it fc1a bash

$ airflow version
v1.10.0

(4) 










========================================
R 脚本的流水线: drake (依赖 Java >= 6.0)
----------------------------------------

1. 简介
(1) 怎么看到的?
https://community.rstudio.com/t/question-on-managing-long-and-heavy-r-code-one-script-vs-multiple-pieces/11917/6
Have you thought about {drake} :package: to organize your big project workflow?

drake 类似 C的 make，好像又不是为R单独写的: https://github.com/Factual/drake



(2) 中文
大数据工作流开源系统之DRAKE 
	https://blog.csdn.net/qq1226317595/article/details/82833816

一个面向R的可再现性和高性能计算的流水线工具箱
	https://www.5axxw.com/wiki/content/q4g9cv


2. R 包 
(1) 安装
# Install the latest stable release from CRAN.
install.packages("drake")

# Alternatively, install the development version from GitHub.
install.packages("devtools")
library(devtools)
install_github("ropensci/drake")

(2) 












========================================
可视化报告的生成技术
----------------------------------------
现在的测序报告，就是一个有序的网页。

1.生成jpg图片，还是生成pdf图像？


首先保证网页不能乱码:
<meta http-equiv=Content-Type content="text/html;charset=utf-8">




========================================
|-- 网页嵌入pdf，使用embed、iframe或object
----------------------------------------

There are several ways to include a PDF file in your HTML document:

Using the <embed> and <object> tags, which are considered to be old-fashioned ways, because they are deprecated now.
Using the <a> or the <iframe> tag.



1. 推荐方式 iframe
<iframe src="URL"></iframe>
<iframe src="URL" height="200" width="300"></iframe>

(1) 主动指出单位
<iframe src="/uploads/media/xxfc.pdf" width="100%" height="500px">
</iframe>


(2) 中间插入文字
<iframe src="test_pdf.pdf" width="800" height="600"></iframe> 
通过的浏览器：360、Firefox、IE、Chrome

<iframe src="../pdf/sample-3pp.pdf#page=2" width="100%" height="100%">
	This browser does not support PDFs. Please download the PDF to view it: 
	<a href="../pdf/sample-3pp.pdf">Download PDF</a>
</iframe>


(3) 如何避免顶部工具条呢？在pdf文件名后加后缀 #toolbar=0
yy
<br>
<iframe src="00test.pdf#toolbar=0" width="240" height="320" ></iframe>
<br>
zz


(4) 更多后缀名
page=pagenum – Specifies a number (integer) of the page in the document. The document’s first page has a pagenum value of 1.
zoom=scale – Sets the zoom and scroll factors, using float or integer values. For example, a scale value of 100 indicates a zoom value of 100%.
view=Fit – Set the view of the displayed page.
scrollbar=1|0 – Turns scrollbars on or off.
toolbar=1|0 – Turns the toolbar on or off.
statusbar=1|0 – Turns the status bar on or off.
navpanes=1|0 – Turns the navigation panes and tabs on or off.


第一个后缀选项前面加上#，多个后缀选项之间使用&。
http://example.com/doc.pdf#Chapter5
http://example.com/doc.pdf#page=5
http://example.com/doc.pdf#page=3&zoom=200,250,100
http://example.com/doc.pdf#zoom=100
http://example.com/doc.pdf#page=72&view=fitH,100


remove or hide toolbar of embedded PDF.
<embed src="files/Brochure.pdf#toolbar=0&navpanes=0&scrollbar=0" type="application/pdf" width="100%" height="600px" />


对比发现，对于尺寸差不多的图，最大的是 svg，最小的是 pdf，以后可以考虑把pdf图嵌入网页。
-rw-r--r-- 1 wangjl wangjl  48K Nov  3 10:08 00test.svg
-rw-r--r-- 1 wangjl wangjl  19K Nov  2 23:01 00test.png
-rw-r--r-- 1 wangjl wangjl 6.6K Nov  2 23:00 00test.pdf


<iframe src="00test.pdf#toolbar=0&view=fit,100" width="340" height="380" ></iframe>








2. 过时的方式
(1) embed
<embed width="800" height="600" src="test_pdf.pdf"> </embed>
通过的浏览器：360、Firefox、IE、Chrome


<embed src="all.100ntUD-1S_gt5_with4A.pdf" type="application/pdf"  height="350px">
        <p><a href="all.100ntUD-1S_gt5_with4A.pdf">Download PDF</a></p>
</embed>


<embed src="../pdf/sample-3pp.pdf#page=2" type="application/pdf" width="100%" height="100%" internalinstanceid="8">


(2) Object 要在其data属性中设置pdf路径。
<object classid="clsid:CA8A9780-280D-11CF-A24D-444553540000" width="800" height="600" border="0"> <param name="SRC" value="test_pdf.pdf"></object> 
通过的浏览器：360、IE
未通过的浏览器：Firefox、Chrome


# chrome通过
<object classid="clsid:CA8A9780-280D-11CF-A24D-444553540000" width="800" height="600" border="0">  
    <param name="_Version" value="65539">  
    <param name="_ExtentX" value="20108">  
    <param name="_ExtentY" value="10866">  
    <param name="_StockProps" value="0">
    <param name="SRC" value="test_pdf.pdf">  
    <object data="test_pdf.pdf" type="application/pdf" width="800" height="600">   
    </object>  
</object>


# chrome通过
<object data="test_pdf.pdf" type="application/pdf" width="800" height="600"></object> 



<object data="../pdf/sample-3pp.pdf#page=2" type="application/pdf" width="100%" height="100%" internalinstanceid="10">
	<p>
		<b>Example fallback content</b>: This browser does not support PDFs. Please download the PDF to view it: 
		<a href="../pdf/sample-3pp.pdf">Download PDF</a>.
	</p>
</object>





3. 其他看到的例子
(1) <object><iframe></object>
Using an <object> with an <iframe> provides extra insurance if <object> is not supported.

<object data="../pdf/sample-3pp.pdf#page=2" type="application/pdf" width="100%" height="100%" internalinstanceid="9">
	<iframe src="../pdf/sample-3pp.pdf#page=2" width="100%" height="100%" style="border: none;">
		This browser does not support PDFs. Please download the PDF to view it: 
		<a href="../pdf/sample-3pp.pdf">Download PDF</a>
	</iframe>
</object>









10. 使用js插件 
PDFObject.js https://pdfobject.com/
pdf.js https://github.com/mozilla/pdf.js/





refer: 
https://pdfobject.com/static/
https://www.codexworld.com/embed-pdf-document-file-in-html-web-page/


========================================
|-- 网页中嵌入另一个网页，带滚动条
----------------------------------------

<iframe src="meme/meme.html" width="1000" frameborder="1" scrolling="auto"> </iframe>



========================================
|-- 填充word模板，生成word报告 //todo
----------------------------------------
1. java

2. python 实现







========================================
商业化的流程软件: SliverWorkspace — 新一代生物信息学低代码开发平台
----------------------------------------
1. 官网
https://www.sliverworkspace.com/
- docker: https://hub.docker.com/r/doujiangbaozi/sliverworkspace#!

- 作者的博客: 
	* 地址1 CSDN: https://blog.csdn.net/weixin_39900139/category_9306230.html
	* 地址2 腾讯云: https://cloud.tencent.com/developer/column/84144


现在用的 SliverWorkspace 是什么语言写的，怎么自定义？
- 可能的源码 https://github.com/search?q=sliverworkspace+&type=code
- 著作权人 张恒； 软件名称	SliverWorkspace生信分析系统（企业版）
	https://banquan.tianyancha.com/rj/17v14oem341ef33d488751119l344fa8 

// more at blog2;






========================================
通过web页面设置参数，调用后台命令
----------------------------------------
1.前台表单
提交后跳转到新路径
新路径在后台: 获取表单提交的数据，运行后台脚本。

如果脚本耗时太长怎么办？

2. flask 的后台执行
后台进程 执行耗时任务 ThreadPoolExecutor


3. RPC 框架可以吗？

https://www.jb51.net/article/127852.htm
基于JSON数据传输的RPC框架




生信云平台:
基迪奥: https://www.omicshare.com/
DNAnexus: https://platform.dnanexus.com/login
GeneDock: https://genedock.com/
百迈客 云平台 http://www.biomarker.com.cn/biocloud
荣联科技 http://biocloud.ronglian.com/
联川生物 https://www.omicstudio.cn/index
生信豆芽菜 http://www.tbfollow.com/



========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------



