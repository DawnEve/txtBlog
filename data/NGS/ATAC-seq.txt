ATAC-seq and Snakemake 




ATAC-seq命令，逐渐简化成一个个 shell，并最终变成了 Snakemake。
背景
课时1：序言，时长09:44
课时2：技术背景知识，时长03:50
课时3：学习资料推荐，时长04:00
课时4：提取分析流程，时长03:36
课时5：流程相关软件细化，时长03:00
课时6：实例文章解读，时长03:45
实战
课时7：软件安装及数据下载，时长06:30
课时8：fastq数据质控过滤，时长10:06
课时9：质控的可视化，时长09:28
课时10：使用bowtie2进行比对，时长05:19
课时11：比对后的bam文件的系列过滤，时长08:31
课时12：批处理比对全流程，时长06:26
课时13：比对过程总结，时长12:18
课时14：使用macs2找peaks，时长04:59
课时15：插入片段统计以及FRiP值计算，时长11:36
课时16：IDR检验技术重复性，时长06:01
课时17：deeptools，时长14:35
课时18：peaks的注释，时长11:49
课时19：atac-seq-19-找motif大全，时长08:29
课时20：我为什么我不用esATAC，时长07:23
外传
课时21：python模块版本调试



Snakemake https://snakemake.readthedocs.io/en/stable/index.html
Writing a RNA-Seq workflow with snakemake http://pedagogix-tagc.univ-mrs.fr/courses/ABD/practical/snakemake/snake_intro.html
Build bioinformatics pipelines with Snakemake https://slowkow.com/notes/snakemake-tutorial/





========================================
ChIP-seq, CUT&Tag and ATAC-seq //todo
----------------------------------------
Q: 想问一下组蛋白 ChIP-seq和转录因子 ChIP-seq分析流程一样吗

A: 分析差不多。都是质控，比对，peak calling，差异，注释，富集啥的
不过现在，ChIP 技术有点过时了，最新的替代技术是cut&tag


1. CUT&Tag 简介
CUT&Tag（Cleavage Under Targets and Tagmentation）是一种可应用于表观遗传学领域的DNA-蛋白质互作研究新技术，主要用于研究转录因子或组蛋白修饰在全基因组上的结合或分布位点。


蛋白质与DNA互作是基因转录调控的关键，也是启动基因转录的前提。2019年4月29日Henikoff实验室在Nature Communication上公布了新技术CUT&Tag，与传统的ChIP-seq相比，CUT&Tag技术方法简便易行、背景信号低、重复性好、让细胞量从10,000个降到了60个甚至单个细胞。

Kaya-Okur HS, et al. CUT&Tag for efficient epigenomic profiling of small samples and single cells. Nature Communication, 2019.





ref:
http://www.biomarker.com.cn/archives/18574
https://www.sohu.com/na/447519368_121007909
galaxy 分析 https://zhuanlan.zhihu.com/p/570968542
质控 https://zhuanlan.zhihu.com/p/507789582







========================================
ATAC-seq 背景介绍和应用
----------------------------------------

1. 背景知识

ATAC-seq (Assays for Transposase-Accessible Chromatin using sequencing) 是一种较新的全基因组范畴染色质开放区域的一种研究手段。比较之前的研究方法，ATAC-seq具有容易操作，不需要交连，有高信噪比，以及对样品总量要求低等优点。很多实验室纷纷使用ATAC-seq 与 RNA-seq, 及epi-genomics的数据结合在一起分析，以达到更加准确地分析基因差异表达与调控关系的目的。


(1). ATAC-seq具有以下显著优势
 - 操作简单省时，建库步骤简化，因此提高了可重复性并降低错误率；
 - 不依赖抗体，可实现对无ChIP级别抗体转录因子的结合位点分析；
 - 样本起始量大大降低，只需要10^5左右的细胞即可完成一次实验。
#

实验步骤
https://www.abcam.com/epigenetics/epigenetics-application-spotlight-atac-seq


(2). ATAC-seq研究应用方向
    1 染色体开放性图谱绘制
    2 表观修饰差异研究
        2017年，Debattama R. Sen等在Nature上发表T细胞耗竭的研究文章，发现在耗竭T细胞和效应T细胞之间，基因转录表达谱差异仅有9.75%，但ATAC-seq展示的染色质开放性差异高达44.48%，说明ATAC-seq的检测可以获得转录调控方面的重要信息，并且极为灵敏。
    3 胚胎发育表观遗传修饰
    4 肿瘤发生表观机制研究
    5 疾病潜在标志物的预测
    6 肿瘤异质性或分型研究
        2016年Snyder等发表在Cell上的文章证实，不同种类肿瘤的核小体定位存在显著差异，可以借此进行肿瘤分型研究。
        Cell-free DNA Comprises an In Vivo Nucleosome Footprint that Informs Its Tissues-Of-Origin[J]. Cell, 2016 
        https://www.ncbi.nlm.nih.gov/pubmed/26771485
    7 全基因组范围的转录因子（TFs）结合位点分析
        确定转录因子结合位点（TFBS）是理解转录调控机制，建立转录调控网络的关键，可以提供基因表达机制层面的重要信息。
        转录因子结合位点预测是ATAC-seq的“拿手好戏”，可以在一次实验中获得全基因组范围内全部的TFBS，再结合ChIP-seq的验证，可将所有转录因子的“行踪”尽收眼底~
#




(3) 相关文章
##==> ATAC-seq经典之作 2013 nature methods: https://www.nature.com/articles/nmeth.2688
论文标题：Transposition of native chromatin for fast and sensitive epigenomic profiling of open chromatin, DNA-binding proteins and nucleosome position
表观测序领域大牛William J Greenleaf和Howard Y Chang的经典之作，引用上千次的ATAC经典原文
翻译: https://mp.weixin.qq.com/s?__biz=MzA4NTIyMzk4Mg%3D%3D&mid=2247483678&idx=1&sn=de7b6bcbf76c2a9051d0db7a4d9cd074


其它文章：
2018-esATAC: an easy-to-use systematic pipeline for ATAC-seq data analysis  很有趣，使用R语言完成全流程  https://academic.oup.com/bioinformatics/article/34/15/2664/4924216 

2016-Cell- https://www.ncbi.nlm.nih.gov/pubmed/27374332，研究肿瘤转移和chromatin accessibility, 思路和fig值得学习

2016- cell- https://www.ncbi.nlm.nih.gov/pubmed/27863249  整合Chromatin accessibility, Hi-C,GWAS,RNA-Seq，ncRNA

2016- Nat Genet https://www.ncbi.nlm.nih.gov/pubmed/27526324 






========================================
ATAC-seq 分析教程：质控与分析方法
----------------------------------------
1. ATAC-seq数据质量评估注意

ENCODE的ATAC-seq数据标准，来源：https://www.encodeproject.org/atac-seq
ATAC-seq Data Standards and Processing Pipeline


质量标准
    1. 必须有2次或更多次生物学重复。十分珍贵或者稀有样本 除外，但必须做至少2次技术重复。
    2. 每次重复要有25 million非冗余、非线粒体、能够回帖的fragment（单端25 million reads，双端50 million reads = 25 milliion fragment）。
    3. 回帖率>95%，>80%可以接受。
    3. 用IDR计算重复的一致性，rescue和self consistency ratios都>2。
    4. 用以下指标控制PCR扩增对文库复杂性的影响：NRF>0.9, PBC1>0.9, and PBC2>3
    5. 各种peak文件必须满足如下要求：
        - 每个重复的peak数>150000，>100000可以接受（看到这里，理解了为啥ENCODE的ATAC-seq的peak file没法用了，原来是在追求peak数啊！还得自己重跑）
        - IDR peak>70000，>50000可以接受
        - 要存在无核小体区NFR
        - 要存在单核小体峰，好的ATAC-seq数据应该包含核小体，这样既能看开放染色质，又能看核小体。
    7. The fraction of reads in called peak regions (FRiP score)  >0.3，>0.2可以接受。对珍贵 tissues，不要求FRiP，但TSS富集还是要作为关键的衡量信噪比的指标。
    8. TSS富集分数阈值与参考基因组相关。
#

FRiP 是 The fraction of reads in called peak regions的缩写，指的是mapping到peak里的reads 除以总mapped reads。
https://blog.csdn.net/weixin_43569478/article/details/108079775

203491/50e6=0.4%; 是不是错了？ //todo

$ wc -l sample.tn5.shift.tagalign
$ bedtools intersect -a sample.tn5.shift.tagalign -b sample.narrowpeak  -wa -u | wc -l

$ samtools view merged/total.bam | wc
51916420 886379942 11027899754
$ bedtools intersect -a merged/total.bam -b macs2_result/total.bed -wa -u | wc -l
33828



(2)
ENCODE的pipeline里在用macs call完peak之后还有idr等的筛选。

encode原文:
Various peak files must meet certain requirements. Please visit the section on output files under the pipeline overview for more information on peak files.

a. The number of peaks within a replicated peak file should be >150,000, though values >100,000 may be acceptable.

b. The number of peaks within an IDR peak file should be >70,000, though values >50,000 may be acceptable.

c. A nucleosome free region (NFR) must be present.

d. A mononucleosome peak must be present in the fragment length distribution. These are reads that span a single nucleosome, so they are longer than 147 bp but shorter than 147*2 bp. Good ATAC-seq datasets have reads that span nucleosomes (which allows for calling nucleosome positions in addition to open regions of chromatin).







2. 知名分析流程
(1) harvard ATAC 分析流程
https://informatics.fas.harvard.edu/atac-seq-guidelines.html
https://informatics.fas.harvard.edu/atac-seq-guidelines-old-version.html
https://informatics.fas.harvard.edu/category/software.html


(2) ATAC 个性化分析思路
https://www.cnblogs.com/leezx/p/12953732.html

ChIP-seq流程(snakemake) https://zhuanlan.zhihu.com/p/48320500

质控指标 https://cloud.tencent.com/developer/article/1346044


(3) galaxy 培训资料
https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html


(4) 其它视频教程
ATAC-seq 分析流程入门 https://www.jianshu.com/p/09d3a0c7fb20
ATAC-Seq 数据分析一文就够(上） http://www.360doc.com/content/17/1218/22/19913717_714335567.shtml
ATAC-seq 分析（上）  https://www.jianshu.com/p/b7bd87315b6e
分接视频fq到homer注释峰 https://www.bilibili.com/video/BV16s411T7Fh


(5) 其它文字资料
ATAC分析大致过程：
https://adinasarapu.github.io/posts/2019/12/blog-post-atacseq/

这个教程比较系统，旁征博引，又有重要步骤的代码：
https://yiweiniu.github.io/blog/2019/03/ATAC-seq-data-analysis-from-FASTQ-to-peaks/






3. 基础
(1).复习核小体知识：https://en.wikipedia.org/wiki/ATAC-seq
核小体有间距，则间距部分会被Tn5酶插入测序接头，也就能看到reads 峰。
而核小体密集区域，则几乎没有reads，也就看不到任何峰。

如果一个基因处于开放区域，就是测到很多ATAC reads，则该基因一般就是高表达的。
如果一个promoter在开放区域，则其下游基因一般高表达。
如果一个enhancer 处在比较弱(少ATAC reads)的区域，则该 enhancer 不太容易发生近端效应。


(2) ATAC-seq 就是越松散的genome区域，测得的reads越多。该区域基因表达量比较多。
测序reads分布示意图 https://github.com/crazyhottommy/pyflow-ATACseq/blob/master/ATAC.jpg
https://blog.csdn.net/u012110870/article/details/102804164
https://www.jianshu.com/p/eb02b95cf049

ATAC文库构建通常借助Illumina的Nextera®（Illumina）试剂盒，然而Nextera文库涉及的序列不同于其他形式的文库。

### Tn5转座酶切割开放DNA
Tn5转座子切割相同的DNA片段并在片段两端添加如下序列，其中加粗斜体部分紧挨着文库的插入片段序列
序列如下:
Read 1 ——>  5’ TCGTCGGCAGCGTC[AGATGTGTATAAGAGACAG] 
Read 2 ——>  5’ GTCTCGTGGGCTCGG[AGATGTGTATAAGAGACAG] 
可以发现Read1、Read2部分序列一致： AGATGTGTATAAGAGACAG

### 随后PCR扩增连接P5、P7接头
连接测序接头以锚定在flowcell芯片上的接头以供测序反应：

序列如下：
[P5--index5-R1]  5’ AATGATACGGCGACCACCGAGATCTACAC[i5]TCGTCGGCAGCGTC 
[P7--index7-R2]  5’ CAAGCAGAAGACGGCATACGAGAT[i7]GTCTCGTGGGCTCGG

### 完整文库结构
Index 2 (i5)5’-[AATGATACGGCGACCACCGAGATCTACAC]IIIIIIII[TCGTCGGCAGCGTC->AGATGTGTATAAGAGACAG]-NNNNNN-[CTGTCTCTTATACACATCT<-CCGAGCCCACGAGAC]IIIIIIII[ATCTCGTATGCCGTCTTCTGCTTG]-3’ Index 1 (i7)

IIIIIIII: Index 2 (i5), 8 bases 
IIIIIIII: Index 1 (i7), 8 bases 
-NNNNNN-: 插入序列
ref:http://bioinformatics.cvr.ac.uk/blog/illumina-adapter-and-primer-sequences/

这个图很奇怪，
左边: P5,N5, index2(i5), 最后来个Read 1.
右边: P7,N7, index1(i7), 最后来个Read 2.
index内 14+19 + 19+15=67 形式上浪费这么多序列
	假设测150，还剩余 150-67=83
	假设测100，还剩余 150-67=33

如何过滤去接头？
如果截去接头序列需要在 CTGTCTCTTATACACATCT 位置截取，工具可以是cutadapt、trim_galore等类似可以自定义截去接头的软件。






(3) 分析过程： 五个大步骤
raw FASTQ cut adapter
mapping to the reference with aligner like bwa, bowtie2
sort alignment result (BAM files)
remove BAM file duplications
peak-calling with MACS2


测序数据文件是两个重复的双端测序：
ATAC_seq_rep1_R1.fq.gz ATAC_seq_rep1_R2.fq.gz 
ATAC_seq_rep2_R1.fq.gz ATAC_seq_rep2_R2.fq.gz 
ATAC_seq_rep3_R1.fq.gz ATAC_seq_rep3_R2.fq.gz 



ENCODE 项目公开了ATAC数据、标准和分析过程（基于wdl的），值得参考。
提供了从原始的fastq数据开到，到peak caling结束的基础分析功能，尽管缺少了下游的差异分析和motfi分析，这套流程依然值得推荐。
https://github.com/ENCODE-DCC/atac-seq-pipeline

trim, mapping, peak calling三部曲：通过cutadapt软件去除adapter和低质量序列，然后是bowitie2比对参考基因组，最后调用MACS2进行peak calling。














========================================
ATAC-seq分析走流程：基础分析 (测试数据)
----------------------------------------

1. 准备数据：用哪一套数据？
https://blog.csdn.net/weixin_43569478/article/details/108079790

(1) 检索 GEO
all: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE97669
ATAC-seq for HeLa cell line(n=2) PMID: 29731168
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE106145


ATAC-seq for MDA-MB-231, ctrl vs KO(n=6, 2ctrl, 2KO*2)
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE97583


HeLa ATAC-Seq, address the effect of TLK loss on chromatin accessibility(n=6, 2ctrl, 2KO*2)
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE131022


(2) 选中这个，并下载
ATAC-seq of MCF-7 cells post ligand treatment(n=18) 这个数据不错。


文章: https://www.ncbi.nlm.nih.gov/pubmed/31353221
Guan J, Zhou W, Hafner M, Blake RA et al. Therapeutic Ligands Antagonize Estrogen Receptor Function by Impairing Its Mobility. 
Cell 2019 Aug 8;178(4):949-963.e18. PMID: 31353221


Results: 
Ligand 4-OH tamoxifen, a selective ER modulator (SERM), significantly alters chromatin accessibility and partially mimics the effect of natural ligand E2 on chromatin accessibility in MCF-7 breast cells. 
Selective ER degraders (SERD) fulvestrant and GDC-0927 on the other hand have very little impact on chromatin accessibility.

https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE117940
GSM3315603	Vehicle Rep 1 SRR7629152
GSM3315604	Vehicle Rep 3
GSM3315605	Vehicle Rep 2

GSM3315612	4-OH tamoxifen Rep 3  SRR7629161
GSM3315613	4-OH tamoxifen Rep 2
GSM3315614	4-OH tamoxifen Rep 1


https://www.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA483774
$ cd /home/wangjl/data/ATAC/raw
$ cat SRR_Acc_List.txt
SRR7629152
SRR7629153
SRR7629154
SRR7629161
SRR7629162
SRR7629163

$ cat SRR_Acc_List.txt | head -n 3 | while read id; do echo $id; 
fasterq-dump --split-files -e 10 $id;
done;


$ cat SRR_Acc_List.txt | tail -n 3 | while read id; do echo $id; 
fasterq-dump --split-files -e 10 $id;
done;

# 22:36 --> 0:09, 1.5h 66G,
$ ls -lh
total 66G
-rw-rw-r-- 1 wangjl wangjl 4.3G Jun 17 22:48 SRR7629152_1.fastq
-rw-rw-r-- 1 wangjl wangjl 4.3G Jun 17 22:48 SRR7629152_2.fastq
-rw-rw-r-- 1 wangjl wangjl 5.9G Jun 17 23:27 SRR7629153_1.fastq
-rw-rw-r-- 1 wangjl wangjl 5.9G Jun 17 23:27 SRR7629153_2.fastq
-rw-rw-r-- 1 wangjl wangjl 5.3G Jun 18 00:01 SRR7629154_1.fastq
-rw-rw-r-- 1 wangjl wangjl 5.3G Jun 18 00:01 SRR7629154_2.fastq

-rw-rw-r-- 1 wangjl wangjl 4.8G Jun 17 22:51 SRR7629161_1.fastq
-rw-rw-r-- 1 wangjl wangjl 4.8G Jun 17 22:50 SRR7629161_2.fastq
-rw-rw-r-- 1 wangjl wangjl 6.3G Jun 17 23:32 SRR7629162_1.fastq
-rw-rw-r-- 1 wangjl wangjl 6.3G Jun 17 23:32 SRR7629162_2.fastq
-rw-rw-r-- 1 wangjl wangjl 6.5G Jun 18 00:09 SRR7629163_1.fastq
-rw-rw-r-- 1 wangjl wangjl 6.5G Jun 18 00:09 SRR7629163_2.fastq


Reads很短，就40bp?
$ head SRR7629152_1.fastq 
@SRR7629152.1 1 length=41
CACCANTGGCAGCCTAGCATTAGCAGGAATACCTTTCCTCA
+SRR7629152.1 1 length=41
A/AAA#AEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/


(3) 先压缩一下，减少磁盘占用
$ cat SRR_Acc_List.txt | tail -n 3 | while read id; do echo $id; 
gzip ${id}_1.fastq;
done;

变换 _1 为 _2, tail 为 head，共4种组合。
9:40 --> 10:28 可能之前就结束了。缩小为原来的 20%。

$ ls -lth
total 13G
-rw-rw-r-- 1 wangjl wangjl 1.3G Jun 18 00:09 SRR7629163_2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.3G Jun 18 00:09 SRR7629163_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.1G Jun 18 00:01 SRR7629154_2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.1G Jun 18 00:01 SRR7629154_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.2G Jun 17 23:32 SRR7629162_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.2G Jun 17 23:32 SRR7629162_2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.2G Jun 17 23:27 SRR7629153_2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 1.1G Jun 17 23:27 SRR7629153_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 909M Jun 17 22:51 SRR7629161_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 935M Jun 17 22:50 SRR7629161_2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 826M Jun 17 22:48 SRR7629152_1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 863M Jun 17 22:48 SRR7629152_2.fastq.gz

(4) 先抽取部分测试
$ zcat SRR7629152_1.fastq.gz | wc
111588868 223177736 4533824964
111/4 Million reads /sample. 抽取 1e5 reads/sample.

$ mkdir ../fq
$ cat SRR_Acc_List.txt | while read id; do echo $id; 
zcat ${id}_1.fastq.gz | head -n 400000 | gzip > ../fq/s_${id}_1.fastq.gz;
zcat ${id}_2.fastq.gz | head -n 400000 | gzip > ../fq/s_${id}_2.fastq.gz;
done;
# 几秒结束。

$ cd ../fq/
$ zcat s_SRR7629163_2.fastq.gz  | wc
 400000  800000 15147256















========================================
|-- QC: raw fastq 
----------------------------------------
先用抽样数据走流程
$ cd /data/wangjl/ATAC/fq/

(1) 质控
$ mkdir QC_raw
$ fastqc -t 10 *.fastq.gz -o QC_raw/


$ sudo pip3 install multiqc
$ multiqc --version
multiqc, version 1.10.1

$ mkdir QC_raw/multiqc/
$ multiqc QC_raw/*fastqc.zip -o QC_raw/multiqc/

检查: 
$ http-server -p 12345
多数40bp长度，没有其他异常，连adapter都没有。



========================================
|-- 去接头
----------------------------------------
(2) 去接头
https://github.com/ENCODE-DCC/atac-seq-pipeline/blob/master/src/detect_adapter.py
'Nextera ': b'CTGTCTCTTATA', #经过grep检测，这个接头最多，其他2个没有发现。


$ id="SRR7629163" && cutadapt -j 4 --time 1 -e 0.1 -O 3 --quality-cutoff 25 \
-m 15 -a CTGTCTCTTATA \
-A AGACGGGAAGAGCGTCGTGAGGGAAGAGTGTAGATCTCGGTGGTCGCGTATCATT \
-o clean/${id}_1.fastq.gz -p clean/${id}_2.fastq.gz s_${id}_1.fastq.gz s_${id}_2.fastq.gz > clean/cut.log 2>&1
## 98.1% rewrite.

$ ls *1.fastq.gz | sed -e 's/s_//' -e 's/_1.fastq.gz//'
SRR7629152
SRR7629153
SRR7629154
SRR7629161
SRR7629162
SRR7629163

$ ls *1.fastq.gz | sed -e 's/s_//' -e 's/_1.fastq.gz//' | while read id; do echo $id; 
cutadapt -j 4 --time 1 -e 0.1 -O 3 --quality-cutoff 25 \
-m 15 -a CTGTCTCTTATA \
-A AGACGGGAAGAGCGTCGTGAGGGAAGAGTGTAGATCTCGGTGGTCGCGTATCATT \
-o clean/${id}_1.fastq.gz -p clean/${id}_2.fastq.gz s_${id}_1.fastq.gz s_${id}_2.fastq.gz >> clean/cut.log 2>&1;
done;



========================================
|-- mapping with BWA (keep MAPQ>10) [优先使用: 比bowtie2峰多， 10x也用]
----------------------------------------
(3). mapping with BWA
因为reads太短了，决定使用 bwa。
$ bwa 
Program: bwa (alignment via Burrows-Wheeler transformation)
Version: 0.7.17-r1188

#########################
# 构建索引
#########################
UCSC GRCh38
$ cd /home/wangjl/data/ref/human/UCSC/
$ bwa index -a bwtsw -p bwa/hg38_fa hg38.fa.gz
其中 -p bwa/hg38_ 是指定前缀
939M Jan 16  2014 hg38.fa.gz
[main] Real time: 2974.649 sec(49min); CPU: 2929.810 sec

$ ls /home/wangjl/data/ref/human/UCSC/bwa -lth
total 5.3G
-rw-rw-r-- 1 wangjl wangjl 1.5G Jun 18 15:39 hg38_fa.sa
-rw-rw-r-- 1 wangjl wangjl  21K Jun 18 15:24 hg38_fa.amb
-rw-rw-r-- 1 wangjl wangjl  22K Jun 18 15:24 hg38_fa.ann
-rw-rw-r-- 1 wangjl wangjl 766M Jun 18 15:24 hg38_fa.pac
-rw-rw-r-- 1 wangjl wangjl 3.0G Jun 18 15:23 hg38_fa.bwt


#########################
# 比对 & sort
#########################
bwa mem genome.fa  A1_clean_1.fq.gz A1_clean_2.fq.gz | samtools sort -O bam -T A1 > A1.bam

$ cd /data/wangjl/ATAC/fq
$ mkdir map

$ id="SRR7629163"
$ bwa mem -t 10 -R "@RG\tID:${id}\tSM:${id}" /home/wangjl/data/ref/human/UCSC/bwa/hg38_fa  clean/${id}_1.fastq.gz clean/${id}_2.fastq.gz -o map/${id}.sam
##[main] Real time: 6.269 sec; CPU: 34.911 sec
## -t 10 线程数

$ grep -v '^[#@]' map/SRR7629163.sam |wc
 196244 3373449 38998448

$ samtools sort -O bam -T ${id}_tmp map/${id}.sam > map/${id}.sort.bam
## -T 是临时文件夹
$ ls -lth map/
total 45M
-rw-rw-r-- 1 wangjl wangjl 7.6M Jun 18 16:18 SRR7629163.sort.bam  #体积缩小为原来的19%。
-rw-rw-r-- 1 wangjl wangjl  38M Jun 18 16:15 SRR7629163.sam


二合一，批量（注意：使用双引号，变量才能替换）
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
bwa mem -t 10 -R "@RG\tID:${id}\tSM:${id}" /home/wangjl/data/ref/human/UCSC/bwa/hg38_fa  clean/${id}_1.fastq.gz clean/${id}_2.fastq.gz | samtools sort -O bam -T ${id}_tmp > map/${id}.sort.bam;
done;

$ ls -lth map/
total 44M
-rw-rw-r-- 1 wangjl wangjl 7.6M Jun 18 16:21 SRR7629163.sort.bam
-rw-rw-r-- 1 wangjl wangjl 7.0M Jun 18 16:21 SRR7629162.sort.bam
-rw-rw-r-- 1 wangjl wangjl 7.0M Jun 18 16:21 SRR7629161.sort.bam
-rw-rw-r-- 1 wangjl wangjl 7.9M Jun 18 16:21 SRR7629154.sort.bam
-rw-rw-r-- 1 wangjl wangjl 7.0M Jun 18 16:21 SRR7629153.sort.bam
-rw-rw-r-- 1 wangjl wangjl 7.6M Jun 18 16:21 SRR7629152.sort.bam

$ samtools view map/SRR7629154.sort.bam | wc
 197452 3398838 39337777




#########################
# index
#########################
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
samtools index map/${id}.sort.bam;
done;







========================================
|-- mapping with bowtie2 (keep MAPQ>30)
----------------------------------------
Bowtie 2 version 2.3.5.1 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)
$ bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]

http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#building-from-source

bowtie2 index:
https://genome-idx.s3.amazonaws.com/bt/GRCh38_noalt_as.zip
Length: 3749239215 (3.5G) [application/zip]

/home/wangjl/data/ref/human/GRCh38_noalt_as/


1. I will use Bowtie2 (since it was used in many tutorals and papers).

# better alignment results are frequently achieved with --very-sensitive
# use -X 2000 to allow larger fragment size (default is 500)
$ bowtie2 --very-sensitive -X 2000 -x $Bowtie2Index -1 ${sample}*_1.fq.gz -2 ${sample}*_2.fq.gz \
  -p $PPN 2> $${sample}.bowtie2.log | $path2samtools sort -@ $PPN -O bam -o ${sample}.sorted.bam
$path2samtools index -@ $PPN $WORKDIR/bowtie2/${sample}.sorted.bam


测试参数的意义 --very-sensitive
$ bowtie2 --very-sensitive -X 2000 -x /home/wangjl/data/ref/human/GRCh38_bowtie2/GRCh38_noalt_as -1 AB_R1.fq -2 AB_R2.fq -p 4 2>bowtie2.log >AB.sam

$ bowtie2 -x /home/wangjl/data/ref/human/GRCh38_bowtie2/GRCh38_noalt_as -1 AB_R1.fq -2 AB_R2.fq -p 4 2>bowtie2_2.log >AB_2.sam

加 --local 允许软剪切，有助于提高比对率。
$ bowtie2 --local -x /home/wangjl/data/ref/human/GRCh38_bowtie2/GRCh38_noalt_as -1 AB_R1.fq -2 AB_R2.fq -p 4 2>bowtie2_3.log >AB_3.sam


##-N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)
$ bowtie2 -N 1 -X 2000 -x /home/wangjl/data/ref/human/GRCh38_bowtie2/GRCh38_noalt_as -1 AB_R1.fq -2 AB_R2.fq -p 4 2>bowtie2_4.log >AB_4.sam


最后使用的形式：
$ bowtie2 -p 4 -N 1 -X 2000 -x {index} -1 {input[0]} -2 {input[1]} 2>{log} >{output}
$ samtools sort -@ {threads} -O bam -T {params.tmp} -o {output} {input} >{log} 2>&1




ref:https://yiweiniu.github.io/blog/2019/03/ATAC-seq-data-analysis-from-FASTQ-to-peaks/





========================================
|-- 去除线粒体上的reads，去重复 (推荐使用 sambamba，其次 picard，最不推荐 samtools)
----------------------------------------
由于细胞器DNA蛋白结合少，所以显然更容易被Tn5 转座酶切割，普通的ATAC-Seq的read就会有大量是细胞器的DNA，这就是为啥需要用INTACT技术。

此外如果不是PCR-free的建库方法，会有大量重复的read，也就需要标记或去除重复。



1. 去掉线粒体上的reads。
首先将不含质体的染色体名称写到一个chrlist文件中，染色体名称之间用空格隔开，然后执行如下命令即可得到去除质体的bam
$ samtools view -b A1.bam `cat chrlist` > A1.del_MT_PT.bam


执行
$ samtools view  map/SRR7629153.sort.bam| grep -v "^#"| awk '{print $3}' | sort | uniq -c| grep -E -v "GL|KI|JH|KB|chrM"
    678 *
  16554 chr1
   5261 chr10
   5175 chr11
   5527 chr12
   2483 chr13
   4475 chr14
   3155 chr15
   3655 chr16
   6137 chr17
   1604 chr18
   2189 chr19
   9700 chr2
   5257 chr20
   1694 chr21
   1163 chr22
   8552 chr3
   5334 chr4
   8615 chr5
   6031 chr6
   7196 chr7
   7400 chr8
   4485 chr9
   3427 chrX
    215 chrY

$ cat map/chrlist
chr1 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr2 chr20 chr21 chr22 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chrX chrY


$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id; 
samtools view -b map/${id}.sort.bam `cat map/chrlist` > map/${id}.del_MT.sort.bam;
done;






2. 去重复
用于后续分析的reads需要是唯一比对且去重复的，bwa比对结果可以通过MAPQ值来提取唯一比对reads，可以用picard、sambamba等软件去除dup，最终得到唯一比对且去重复的bam文件。

samtools rmdup 是如何行使去除PCR重复reads功能的？和 picard MarkDuplicates 现在改名为 GATK MarkDuplicates 有什么区别？

(1) The Genome Analysis Toolkit (GATK) v4.1.9.0
HTSJDK Version: 2.23.0
Picard Version: 2.23.3

$ gatk MarkDuplicates
Using GATK jar /data/wangjl/soft/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar
Running:
    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/wangjl/soft/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar MarkDuplicates
USAGE: MarkDuplicates [arguments]

命令行给出的例子
java -jar picard.jar MarkDuplicates \
I=input.bam \
O=marked_duplicates.bam \
M=marked_dup_metrics.txt



## 命令方式1: 新语法，官方推荐
$ id="SRR7629152"
$ gatk MarkDuplicates -I map/${id}.del_MT.sort.bam -O map/${id}.rmdup.bam -M map/${id}.rmdup.matrix -ASO coordinate -REMOVE_DUPLICATES true

$ samtools view map/SRR7629152.rmdup.bam | wc  #确实删掉了些行
 143879 2604778 31278683
$ samtools view map/SRR7629152.del_MT.sort.bam | wc
 145083 2481870 28650942


## 命令方式2: 控制内存和CPU占用，更安全的用法。
$ PICARD="/data/wangjl/soft/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar"
$ java -Xms5g -Xmx10g -XX:ParallelGCThreads=4 \
    -jar ${PICARD} MarkDuplicates \
    I=map/${id}.del_MT.sort.bam O=map/${id}.rmdup.bam M=map/${id}.rmdup.matrix \
    ASO=coordinate REMOVE_DUPLICATES=true 2>map/rmdup.log

$ samtools view map/SRR7629152.rmdup.bam | wc
 143879 2604778 31278683
和 方式1 结果一样。


批量运行，使用新语法
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id; 
gatk MarkDuplicates -I map/${id}.del_MT.sort.bam -O map/${id}.rmdup.bam -M map/${id}.rmdup.matrix -ASO coordinate -REMOVE_DUPLICATES true;
done;



$ ls -lth map
total 147M
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:38 SRR7629163.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 8.5M Jun 18 21:38 SRR7629163.rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:38 SRR7629162.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 7.3M Jun 18 21:38 SRR7629162.rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:38 SRR7629161.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 7.2M Jun 18 21:38 SRR7629161.rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:38 SRR7629154.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 8.5M Jun 18 21:38 SRR7629154.rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:38 SRR7629153.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 7.0M Jun 18 21:38 SRR7629153.rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 18 21:37 SRR7629152.rmdup.matrix
-rw-rw-r-- 1 wangjl wangjl 8.4M Jun 18 21:37 SRR7629152.rmdup.bam






(2) 使用 samtools rmdup 去重复 (去的太多了，不建议用)
Usage:  samtools rmdup [-sS] <input.srt.bam> <output.bam>
-s    rmdup for SE reads
-S    treat PE reads as SE in rmdup (force -s)

$ samtools rmdup -S map/${id}.del_MT.sort.bam map/${id}.samtools_rmdup.bam
[bam_rmdupse_core] 4139 / 144722 = 0.0286 in library '	'

$ samtools view map/SRR7629152.samtools_rmdup.bam | wc
 140944 2408371 27736668
这个删除的有点多啊，
samtools 140944 (保留97.14%)，对比着 picard 的 143879 (保留99.17%)





(3) 用samblaster和sambamba来代替picard做sam文件的去重复，这两款软件比picard快30倍 (保留reads数和picard一样)
https://www.it610.com/article/1228439887515062272.htm

要注意的是picard Markduplicates 和sambamba markdup的输入文件是bam格式，samblaster是sam格式。这里只测试 sambamba。
sambamba 主要有filter，merge,slice和duplicate等七个功能来处理sam/bam文件。


## 安装
https://github.com/lomereiter/sambamba/releases
$ wget https://github.com/biod/sambamba/releases/download/v0.8.0/sambamba-0.8.0-linux-amd64-static.gz
$ gunzip sambamba-0.8.0-linux-amd64-static.gz
$ chmod +x sambamba-0.8.0-linux-amd64-static
$ ln -s /home/wangjl/soft/sambamba-0.8.0-linux-amd64-static ~/bin/sambamba


## 查看版本号
$ sambamba --version
sambamba 0.8.0
 by Artem Tarasov and Pjotr Prins (C) 2012-2020
    LDC 1.10.0 / DMD v2.080.1 / LLVM6.0.1 / bootstrap LDC - the LLVM D compiler (0.17.4)

$ sambamba markdup --help
Usage: sambamba-markdup [options] <input.bam> [<input2.bam> [...]] <output.bam>
       By default, marks the duplicates without removing them
-r, --remove-duplicates  remove duplicates instead of just marking them
-t, --nthreads=NTHREADS  number of threads to use
--tmpdir=TMPDIR    specify directory for temporary files



##--> 去重
$ sambamba markdup -r -t 5 --tmpdir map/tmp/${id} map/${id}.del_MT.sort.bam map/${id}.sambamba_rmdup.bam

$ ls -lth map/
total 155M
-rw-rw-r-- 1 wangjl wangjl 2.7M Jun 18 22:00 SRR7629152.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 6.1M Jun 18 22:00 SRR7629152.sambamba_rmdup.bam
drwxrwxr-x 2 wangjl wangjl 4.0K Jun 18 22:00 SRR7629152_tmp
同时生成一个bai文件。注意临时文件夹没删

$ samtools view map/SRR7629152.sambamba_rmdup.bam | wc
 143879 2460899 28401109

reads数和 picard 一模一样，速度更快，且文件更小。


批量运行
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id; 
sambamba markdup -r -t 5 --tmpdir map/tmp/${id} map/${id}.del_MT.sort.bam map/${id}.sambamba_rmdup.bam;
done;


$ ls -lth map/
total 196M
-rw-rw-r-- 1 wangjl wangjl 2.7M Jun 18 22:04 SRR7629163.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 6.2M Jun 18 22:04 SRR7629163.sambamba_rmdup.bam
drwxrwxr-x 8 wangjl wangjl 4.0K Jun 18 22:04 tmp
-rw-rw-r-- 1 wangjl wangjl 2.5M Jun 18 22:04 SRR7629162.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 5.4M Jun 18 22:04 SRR7629162.sambamba_rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 5.2M Jun 18 22:04 SRR7629161.sambamba_rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 2.5M Jun 18 22:04 SRR7629161.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 2.6M Jun 18 22:04 SRR7629154.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 6.3M Jun 18 22:04 SRR7629154.sambamba_rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 5.1M Jun 18 22:04 SRR7629153.sambamba_rmdup.bam
-rw-rw-r-- 1 wangjl wangjl 2.5M Jun 18 22:04 SRR7629153.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 2.7M Jun 18 22:04 SRR7629152.sambamba_rmdup.bam.bai
-rw-rw-r-- 1 wangjl wangjl 6.1M Jun 18 22:04 SRR7629152.sambamba_rmdup.bam







3. 去除mapQ质量较低的reads
$ samtools view -h -f 2 -q 30 ${filename}.rmdup.bam


samtools view -h -f 2 -q 30 ${filename}.rmdup.bam \
| grep -v -e "mitochondria" -e "*" -e "chloroplast" \
| samtools sort -O bam -@ 10 -o - > ${filename}.last.bam
# 实时监测我们的数据发生了什么变化
samtools index ${filename}.last.bam
samtools flagstat ${filename}.last.bam > ./${filename}.last.stat




BWA MAPQ>10
Bowtie2 MAPQ>30

# Remove multi-mapped reads (i.e. those with MAPQ < 30, using -q in SAMtools)
$ samtools view -h -q 30 ${sample}.bam > ${sample}.rmMulti.bam





========================================
|-- map QC: deepTools 系列工具，比对结果质控 (TSS 热图 、插入片段长度分布图)
----------------------------------------
比对后，去掉线粒体、去重后的bam，再index后用于下游分析。


1.Reads在TSS前后的分布图

比对后得到的bam文件可以转化为bigWig（bw）格式，可通过可视化软件进行展示。deeptools软件可以实现bw格式转化和可视化展示。

(1) 首先需要在linux环境中安装deeptools软件，可以用以下命令实现bam向bw格式的转换：
$ id="SRR7629163"
## sambamba index -t 6 ${id}.sambamba_rmdup.bam ## 之前rmdup时已经自动生成过了
$ bamCoverage -b map/${id}.sambamba_rmdup.bam -o QC_map/${id}.bw;  ##必须提前index


## 批量化
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
bamCoverage -b map/${id}.sambamba_rmdup.bam -o QC_map/${id}.bw;
done;


二进制文件，也没法直接看文件内容。不报错就认为没问题吧。
得到的BW文件可以用 IGV 进行可视化





(2) 可以使用deeptools软件展示reads在特定区域的分布，如：
computeMatrix reference-point   \ # reference-pioint表示计算一个参照点附近的reads分布，与之相对的是scale-regions，计算一个区域附近的reads分布
       --referencePoint TSS   \#以输入的bed文件的起始位置作为参照点
       -S  A1.bw \ #可以是一个或多个bw文件
       -R  gene.bed \ #基因组位置文件
       -b 3000   \ #计算边界为参考点上游3000bp
       -a 3000   \ #计算边界为参考点下游3000bp，与-b合起来就是绘制参考点上下游3000bp以内的reads分布
       -o  A1.matrix.mat.gz \ #输出作图数据名称
#图形绘制
plotHeatmap \
 -m  A1.matrix.mat.gz\ #上一步生成的作图数据
 -out A1.pdf \ # 输出图片名称


computeMatrix 有两种模式可以选择，这里我们用的是reference-point，另外还有scale-regions，
- 前者适合看一个点附近的信号，后者则适合看指定的不同长度的区域。
- 当我们选用reference-point模式时，会默认用bed文件的第二列作为中心扩展。
- 工作原理 https://www.jianshu.com/p/ab2bb3f55d6f





## (i)下载bed格式的基因组位置文件(The BED file of the gene model can be downloaded from UCSC Table Browser.)
http://genome.ucsc.edu/cgi-bin/hgTables?command=start

genome: human
assembly: Dec. 2013 (GRCh38/hg38)
group: Genes and Gene Predictions
track: NCBI RefSeq
table: RefSeq All(ncbiRefSeq)
region: genome
output format: BED - browser extensible data
output file: hg38_refseq_whole_gene.bed

Click ‘get output’ button, and in the next page ‘Output refGene as BED’ click ‘get BED’ button.




##(ii) 统计
单个测试
$ computeMatrix reference-point   \
       --referencePoint TSS   \
       -S  QC_map/${id}.bw \
       -R  /home/wangjl/data/ref/human/UCSC/hg38_refseq_whole_gene.bed \
       -b 3000   \
       -a 3000   \
       -o  QC_map/matrix/${id}.matrix.mat.gz
很慢: 17:02-> 18:07


批量化，放后台运行。
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
computeMatrix reference-point   \
       --referencePoint TSS   \
       -S QC_map/${id}.bw \
       -R /home/wangjl/data/ref/human/UCSC/hg38_refseq_whole_gene.bed \
       -b 1000 \
       -a 1000 \
       -o QC_map/matrix/${id}.mat.gz &
done;
## 检查后台 $ ps -aux | grep computeMatrix
风扇声音很大。

https://deeptools.readthedocs.io/en/latest/content/tools/plotHeatmap.html
https://deeptools.readthedocs.io/en/latest/content/tools/plotProfile.html
$ plotHeatmap -m $matrix -out $heatmap #上下图都有
$ plotProfile -m $matrix -out $profile #只有上图





## (iii) 画图
单个测试
$ mkdir QC_map/pdf
$ plotHeatmap -m QC_map/matrix/${id}.mat.gz -out QC_map/pdf/${id}.pdf

批量化
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
plotHeatmap -m QC_map/matrix/${id}.mat.gz -out QC_map/pdf/${id}_.pdf;
done;


目测：峰图和热图都出来了，5/6看着正常。其中一个热图不正常，没颜色变化。

上面的峰图 profile 图：表示在每个gene的TSS位点的 reads 富集情况，可以理解为 peak，这说明 peak 大多数分布在TSS区域。
下面的热图 gene heatmap图：每行就是一个基因，因此这个热图一般很长。





(3) 还可以对 TES 可视化
$ id="SRR7629163"
$ computeMatrix scale-regions  -p 15  \
    -b 2000 -a 2000 \
    -R /home/wangjl/data/ref/human/UCSC/hg38_refseq_whole_gene.bed \
    -S QC_map/${id}.bw  \
    --skipZeros -o QC_map/matrix/${id}.body.gz  ## 耗时 10:02 --> 10:11(9min)
$ plotHeatmap -m QC_map/matrix/${id}.body.gz -out QC_map/matrix/${id}.body.Heatmap.pdf --plotFileFormat pdf  ## 上下图都有。
$ plotProfile -m QC_map/matrix/${id}.body.gz -out QC_map/matrix/${id}.body.Profile.pdf --plotFileFormat pdf --perGroup # 这个干啥的？貌似只有上图。

还可以输入多个参考基因组 -R all.bed chrY.bed ，比如一个全基因组bed，一个chrY.bed，然后 profile 就是2条不同颜色的线。
还可以输入多个输入 -S A.bw B.bw 

上图是 profile 图，可见基因 body 区， reads 富集程度远小于TSS/TES，因为大多数 peak 都不在基因内部（在基因间区）。






ref:
https://www.jianshu.com/p/9aa719faa4b5









2. 片段长度分布图(bam第9列)
https://www.jianshu.com/p/0a8d57dbfc3c

insert size与 fragment length 差别 http://tiramisutes.github.io/2016/09/19/Insert-Size.html

(1) 获取insert fragment 长度数据
$ echo $id  #SRR7629163

$ samtools view map/${id}.sambamba_rmdup.bam | \
awk -F'\t' 'function abs(x){return ((x < 0.0) ? -x : x)} {print $1"\t"abs($9)}' | \
sort | uniq | cut -f2 > map/${id}.fragment.length.txt
解释: 
awk中
	-F"\t" 按照tab分割
	然后定义一个函数 function abs(x){return ((x < 0.0) ? -x : x)} 表示返回绝对值。
	{print $1"\t"abs($9)} 打印第一列、第9列的绝对值
	
	
感觉不该使用uniq，因为不需要删重复？
$ samtools view map/${id}.sambamba_rmdup.bam | awk -F'\t' 'function abs(x){return ((x < 0.0) ? -x : x)} {print $1"\t"abs($9)}' |  sort | uniq -c | sort -k1n | head -n 80| tail
 1 SRR7629163.94576	0
      1 SRR7629163.97738	0
      1 SRR7629163.979	0
      1 SRR7629163.98342	0
      1 SRR7629163.98752	0
      1 SRR7629163.99702	0
      2 SRR7629163.100000	66
      2 SRR7629163.10001	45
      2 SRR7629163.10003	53
      2 SRR7629163.10004	61
目测除了0是出现1次，其他都是2次出现。
着重复是怎么来的呢？嗯，理解了，就是双端测序，每一端一行，一对reads是2行，分别记录本行信息和另一行的坐标，插入长度正负号不同。
$ samtools view map/${id}.sambamba_rmdup.bam | grep -P "SRR7629163.10004"
SRR7629163.10004	163	chr3	73049179	60	41M	=	73049200	61	AACTAAATGAATACATTCAAGATTAGAATACTTCTCGGGGC	AAAAAEEAEEE6EEEEEEEEEEEEEEEEAEEEEEAEEEEEE	NM:i:0	MD:Z:41	MC:Z:40M	AS:i:41	XS:i:19	RG:Z:${id}
SRR7629163.10004	83	chr3	73049200	60	40M	=	73049179	-61	ATTAGAATACTTCTCGGGGCCAGGTGTGGTGGCTCACGCC	AEEEEEEEEEAEEEEEEEEEE6EE/EEEE/EEE/EAAAAA	NM:i:0	MD:Z:40	MC:Z:41M	AS:i:40	XS:i:27	RG:Z:${id}








(2) 画图
hist 函数可以直接绘图，但是不太好看，可以用plot函数来画；同样density 函数也可以画图，但是也可以导出list,再进行plot 或者 ggplot 画图.

################
# R 包直接绘图
################
#BiocManager::install("ATACseqQC")
library(ATACseqQC)
fragSize <- fragSizeDist("map/SRR7629163.sambamba_rmdup.bam", "treat3")



################
# 原生R绘制
################

setwd("/data/wangjl/ATAC/fq")
getwd()

library(tidyverse)
data <- read.table("map/SRR7629163.fragment.length.txt")
dim(data) #73718     1
head(data)

######
# 过滤
######
# 去除0行
d2=data[data$V1>0,]
length(d2)
# 去掉太长的
d2=d2[d2<1200]
length(d2)

fragment=d2
length(fragment)


######
##Part1：基础语法画图
######
# 利用直方图统计频数分布，设置柱子个数
breaks_num <- 500
res <- hist(fragment, breaks = breaks_num, plot = FALSE)

# 添加坐标原点
plot(x = c(0, res$breaks),
     y = c(0, 0, res$counts) / 10^2,
     type = "l", col = "red",full="red",
     xlab = "Fragment length(bp)",
     ylab = expression(Normalized ~ read ~ density ~ 10^2),
     main = "Sample Fragment sizes")



######
##Part2：ggplot2 画图及其拼图
######
## 不同数据分布
df1 <- data.frame(x1 = c(0, res$breaks),y1=c(0, 0, res$counts) / 10^2)
p1 <- ggplot(df1,aes(x =x1,y = y1 ))+
  geom_line(col="red")+
  xlab("Fragment length(bp)")+
  ylab(expression(Normalized ~ read ~ count ~ 10^2))+
  ggtitle("Sample Fragment sizes")+
  theme_classic()
p1

## 画小图
df2 <- data.frame(x1 = c(0, res$breaks),y1=log10(c(0, 0, res$counts) / 10^2)+1)
p2 <- ggplot(df2,aes(x =x1,y = y1 ))+
  geom_line(col="red")+
  xlab("Fragment length(bp)")+
  ylab(expression(Normalized ~ read ~ count ~ (log)))+
  ggtitle("Sample Fragment sizes")+
  theme_classic()
p2

## 小图插入右上角
library(cowplot)
ggdraw() +
  draw_plot(p1, 0, 0, 1, 1) +
  draw_plot(p2, 0.5, 0.52, 0.5, 0.4) +
  draw_plot_label(c("A", "B"), c(0, 0.5), c(1, 0.92), size = 15)


######
##Part3：其他（密度图）
######
P3 <- ggplot(as.data.frame(fragment))+
  geom_density(aes(x=fragment),bw=3,color = "red")+
  xlim(0,NA)+
  scale_y_continuous(breaks = c(seq(0,0.004,0.001)),
                     labels=c(seq(0,4)),
                     name = expression(Normalized ~ read ~ density ~ 10^-3))+
  theme_classic()
P3





(3) 也可以使用工具: picard的CollectInsertSizeMetrics， bedtools的bamPEFragmentSize也都可以计算插入片段长度。

$ picard CollectInsertSizeMetrics \
I=./${filename}.last.bam \
O=./${filename}.last.insertsize \
H=./${filename}.last.hist.pdf












========================================
|-- ATAC的peak shift(Tn5酶的9bp缺口移动)：bed文件 正链右移4bp(+4)，负链左移5bp(-5)
----------------------------------------

ATAC使用Tn5转座酶来完成文库的构建工作，Tn5转座酶在连接adapter序列时，会存在9bp的gap。
这个切口的最中间，就是开放区的最中心。也就是移动后的片段端点所在位置。
https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/algorithms/overview


图见 [Curr Protoc Mol Biol. 2015] fig.1B
ATAC-seq: A Method for Assaying Chromatin Accessibility Genome-Wide
https://pubmed.ncbi.nlm.nih.gov/25559105/


1. 对于正链上的reads需要向右偏移4bp, 比对的起始位置加4，对于负链上的reads, 则向左偏移5bp, 比对的起始位置减5bp。

在Encode给出的ATAC pipeline中，对于原始的bam文件，首先利用bedtools转换成bed文件，
只保留reads比对上参考基因组的位置，然后再进行比对位置的偏移，具体的代码如下


def tn5_shift_ta(ta, out_dir):
	prefix = os.path.join( out_dir, os.path.basename(strip_ext_ta(ta)) )
	shifted_ta = '{}.tn5.tagAlign.gz'.format(prefix)
	cmd = 'zcat -f {} | '
	cmd += 'awk \'BEGIN {{OFS = "\\t"}}'
	cmd += '{{ if ($6 == "+") {{$2 = $2 + 4}} '
	cmd += 'else if ($6 == "-") {{$3 = $3 - 5}} print $0}}\' | '
	cmd += 'gzip -nc > {}'
	# 
	cmd = cmd.format( ta, shifted_ta)
	run_shell_cmd(cmd) 
	return shifted_ta

对于chip_seq的数据分析，拿到bam文件之后直接peak calling就可以了，
对于ATAC_seq而言，一定要偏移之后再进行peak calling。







ref:
http://www.360doc.com/content/19/1231/19/68068867_883377740.shtml
[搜 shift]https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html





========================================
|-- Peak calling
----------------------------------------
(4) MACS2 找 peak 
MACS2能够检测DNA片断的富集区域，是ATAC-seq数据call peak的主流软件。

$ macs2 --version
macs2 2.2.7.1

峰检出的原理如下：
首先将所有的reads都向3'方向延伸插入片段长度，然后将基因组进行滑窗，计算该窗口的dynamic λ，
λ的计算公式为：λlocal = λBG（λBG是指背景区域上的reads数目），
然后利用泊松分布模型的公式计算该窗口的显著性P值，最后对每一个窗口的显著性P值进行FDR校正。
默认校正后的P值（即qvalue）小于或者等于0.05的区域为peak区域。


(i)需要在linux环境中安装macs2软件，然后执行以下命令：
macs2 callpeak \
	-t A1.uni.dedup.bam \ #bam文件
	-n A1 \ # 输出文件前缀名
	--shift -100 \ #extsize的一半乘以-1
	--extsize 200 \ #一般是核小体大小
	--call-summits #检测峰顶信息
注：以上参数参考文献（Jie Wang，et.al.2018.“ATAC-Seq analysis reveals a widespread decrease of chromatin accessibility in age-related macular degeneration.” Nature Communications）


==> 尝试1: 报错
$ macs2 callpeak \
	-f BAM -g hs \
	-t map/${id}.sambamba_rmdup.bam \
	-n macs2_test/${id} \
	--shift -100 \
	--extsize 200 \
	--call-summits
报错
INFO  @ Fri, 18 Jun 2021 22:52:18: #2 looking for paired plus/minus strand peaks...
INFO  @ Fri, 18 Jun 2021 22:52:18: #2 number of paired peaks: 0
WARNING @ Fri, 18 Jun 2021 22:52:18: Too few paired peaks (0) so I can not build the model! Broader your MFOLD range parameter may erase this error. If it still can't build the model, we suggest to use --nomodel and --extsize 147 or other fixed number instead.
WARNING @ Fri, 18 Jun 2021 22:52:18: Process for pairing-model is terminated!



==> 尝试2: 不知道怎么改。试试软件给的建议: --nomodel and --extsize 147
$ mkdir macs2_test
$ macs2 callpeak \
	-f BAM -g hs \
	-t map/${id}.sambamba_rmdup.bam \
	-n macs2_test/${id}_73 \
	--nomodel --shift -73 --extsize 147 \
	--call-summits
## Shift 模型参数：
--nomodel	这个参数和extsize、shift是配套使用的，有这个参数才可以设置 extsize 和 shift 。
--extsize	当设置了 nomodel 时，MACS会用--extsize这个参数从5'->3'方向扩展reads修复fragments。比如说你的转录因子结合范围200bp，就设置这个参数是200。
--shift	当设置了--nomodel，MACS用这个参数从5'端移动剪切，然后用--extsize延伸，如果--shift是负值表示从3'端方向移动。
	建议ChIP-seq数据集这个值保持默认值为0，对于检测富集剪切位点如DNAsel数据集设置为EXTSIZE的一半。
	那ATAC怎么设置? ATAC-seq关心的是在哪切断，断点才是peak的中心，所以使用shift模型，--shift -75或-100

MACS2输出文件解
$ ls -lth macs2_test/
total 32K
-rw-rw-r-- 1 wangjl wangjl 3.7K Jun 19 09:54 SRR7629163_73_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.7K Jun 19 09:54 SRR7629163_73_summits.bed
-rw-rw-r-- 1 wangjl wangjl 5.1K Jun 19 09:54 SRR7629163_73_peaks.xls

$ head macs2_test/SRR7629163_73_peaks.narrowPeak
chr1    629325  629494  macs2_test/SRR7629163_73_peak_1         54      .       2.96016 11.3533 5.45488 71
chr1    629602  629967  macs2_test/SRR7629163_73_peak_2a        126     .       3.67855 19.1081 12.6363 100
chr1    629602  629967  macs2_test/SRR7629163_73_peak_2b        41      .       2.70196 9.90816 4.16214 318
chr1    631059  631344  macs2_test/SRR7629163_73_peak_3a        58      .       2.91161 11.8142 5.88874 78
chr1    631059  631344  macs2_test/SRR7629163_73_peak_3b        87      .       3.22924 14.9192 8.73225 208




==> 尝试3: 人细胞系ATAC-seq 数据call peak的参数设置如下：
$ macs2 callpeak -t map/${id}.sambamba_rmdup.bam --outdir macs2_test -n ${id}_shift100 \
	-f BAM -g hs --shift -100 --extsize 200 --nomodel \
	-B --SPMR --call-summits 2>macs2_test/sample.macs2.log
参数解释
-B, --bdg  Whether or not to save extended fragment pileup, and local lambda tracks (two files) at every bp into a bedGraph file. DEFAULT: False
	是否保存2个文件(默认: 否): extended fragment pileup, and local lambda tracks to bedGraph file.
-B --SPMR 会多2个输出文件。

$ ls macs2_test/ -lth
total 8.8M
-rw-rw-r-- 1 wangjl wangjl 3.1K Jun 19 10:10 sample.macs2.log
-rw-rw-r-- 1 wangjl wangjl 4.4M Jun 19 10:10 SRR7629163_shift100_control_lambda.bdg  ##额外多的文件1
-rw-rw-r-- 1 wangjl wangjl 2.8K Jun 19 10:10 SRR7629163_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 4.2K Jun 19 10:10 SRR7629163_shift100_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 2.0K Jun 19 10:10 SRR7629163_shift100_summits.bed
-rw-rw-r-- 1 wangjl wangjl 4.4M Jun 19 10:10 SRR7629163_shift100_treat_pileup.bdg ##额外多的文件2


## 批量化 
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
macs2 callpeak -t map/${id}.sambamba_rmdup.bam --outdir macs2 -n ${id}_shift100 \
	-f BAM -g hs --shift -100 --extsize 200 --nomodel \
	-B --SPMR --call-summits 2>macs2/${id}.macs2.log;
done;


$ ls macs2/*narrowPeak -lth
-rw-rw-r-- 1 wangjl wangjl 2.9K Jun 19 14:52 macs2/SRR7629163_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.2K Jun 19 14:52 macs2/SRR7629162_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.7K Jun 19 14:52 macs2/SRR7629161_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.7K Jun 19 14:52 macs2/SRR7629154_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.5K Jun 19 14:52 macs2/SRR7629153_shift100_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 1.7K Jun 19 14:52 macs2/SRR7629152_shift100_peaks.narrowPeak















(ii) 按照另一个教程的参数
$ macs2 callpeak -t map/${id}.sambamba_rmdup.bam -f BAM -g hs -n macs2_/${id} -m 2 100
参数 [-m MFOLD MFOLD]

## 批量化
$ makir macs2_/
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
macs2 callpeak -t map/${id}.sambamba_rmdup.bam -f BAM -g hs -n macs2_/${id} -m 2 100;
done;


$ ls -lth macs2_/*narrowPeak
-rw-rw-r-- 1 wangjl wangjl 3.6K Jun 19 14:54 macs2_/SRR7629163_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 3.2K Jun 19 14:54 macs2_/SRR7629162_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 3.1K Jun 19 14:54 macs2_/SRR7629161_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.2K Jun 19 14:54 macs2_/SRR7629154_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 3.2K Jun 19 14:54 macs2_/SRR7629153_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 1.5K Jun 19 14:54 macs2_/SRR7629152_peaks.narrowPeak



(iii) 另一个教程推荐使用 -f BAMPE
macs2 callpeak  -t <BED>  -f BEDPE  -n NAME  -g ce  --keep-dup all

Analyze only properly paired alignments with -f BAMPE. Here, the fragments are defined by the paired alignments' ends, and there is no modeling or artificial extension. Singleton alignments are ignored. This is the preferred option for using only properly paired alignments.

仅使用配对数据，单端被忽略。

$ macs2 callpeak -t map/${id}.sambamba_rmdup.bam  -f BAMPE --outdir macs2_/  -n ${id}_BAMPE  -g hs

$ ls -lth macs2_ | grep SRR7629163
-rw-rw-r-- 1 wangjl wangjl 3.4K Jun 22 20:06 SRR7629163_BAMPE_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 2.4K Jun 22 20:06 SRR7629163_BAMPE_summits.bed
-rw-rw-r-- 1 wangjl wangjl 4.7K Jun 22 20:06 SRR7629163_BAMPE_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 2.5K Jun 19 14:54 SRR7629163_summits.bed
-rw-rw-r-- 1 wangjl wangjl 3.6K Jun 19 14:54 SRR7629163_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 5.0K Jun 19 14:54 SRR7629163_peaks.xls
-rw-rw-r-- 1 wangjl wangjl  74K Jun 19 14:54 SRR7629163_model.r

比 -f BAM 少了2行。



















#############
# treat vs control 可以跳过下面这一段 macs分析
#############

比较两组间的呢？

$ macs2 callpeak -c map/SRR7629152.sambamba_rmdup.bam map/SRR7629153.sambamba_rmdup.bam \
	-t map/SRR7629161.sambamba_rmdup.bam map/SRR7629162.sambamba_rmdup.bam map/SRR7629163.sambamba_rmdup.bam \
	-f BAM -g hs -n macs2/treat_vs_control -m 2 100;

$ ls -lth macs2/ | grep vs
-rw-rw-r-- 1 wangjl wangjl    0 Jun 18 23:06 treat_vs_control_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 1.5K Jun 18 23:06 treat_vs_control_peaks.xls
-rw-rw-r-- 1 wangjl wangjl    0 Jun 18 23:06 treat_vs_control_summits.bed
-rw-rw-r-- 1 wangjl wangjl  95K Jun 18 23:06 treat_vs_control_model.r

这里没有 narrowPeak 文件，长度是0, why?


#==> 去掉一个处理组，保持处理和对照样本数一致(n=2:2):
$ macs2 callpeak \
	-c map/SRR7629152.sambamba_rmdup.bam map/SRR7629153.sambamba_rmdup.bam \
	-t map/SRR7629161.sambamba_rmdup.bam map/SRR7629162.sambamba_rmdup.bam \
	-f BAM -g hs -n macs2/treat_vs_control -m 2 100;

$ ls -lth macs2/ | grep vs
-rw-rw-r-- 1 wangjl wangjl  24K Jun 18 23:12 treat_vs_control_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  27K Jun 18 23:12 treat_vs_control_peaks.xls
-rw-rw-r-- 1 wangjl wangjl  18K Jun 18 23:12 treat_vs_control_summits.bed
-rw-rw-r-- 1 wangjl wangjl  82K Jun 18 23:12 treat_vs_control_model.r

$ Rscript macs2/treat_vs_control_model.r
能看到 Peak Model 图。
Cross−Correlation 图2条竖线。




#==> 使用三组，处理和对照样本数一致(n=3:3):
$ macs2 callpeak \
	-c map/SRR7629152.sambamba_rmdup.bam map/SRR7629153.sambamba_rmdup.bam map/SRR7629154.sambamba_rmdup.bam \
	-t map/SRR7629161.sambamba_rmdup.bam map/SRR7629162.sambamba_rmdup.bam map/SRR7629163.sambamba_rmdup.bam \
	-f BAM -g hs -n macs2/treat_vs_control-3 -m 2 100;

$ ls -lth macs2/ | grep -P "\-3"
-rw-rw-r-- 1 wangjl wangjl  47K Jun 19 09:39 treat_vs_control-3_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  52K Jun 19 09:39 treat_vs_control-3_peaks.xls
-rw-rw-r-- 1 wangjl wangjl  34K Jun 19 09:39 treat_vs_control-3_summits.bed
-rw-rw-r-- 1 wangjl wangjl  95K Jun 19 09:39 treat_vs_control-3_model.r


$ Rscript macs2/treat_vs_control-3_model.r
能看到 Peak Model 图。
Cross−Correlation 图就剩下一条竖线。



==> 使用2组，使用ATAC-seq推荐参数
## 在 macs2/下清理文件 ls -t | tail -n 20 | xargs -i mv {} dustbin/

$ macs2 callpeak \
	-c map/SRR7629152.sambamba_rmdup.bam map/SRR7629153.sambamba_rmdup.bam \
	-t map/SRR7629161.sambamba_rmdup.bam map/SRR7629162.sambamba_rmdup.bam \
	--shift -100 --extsize 200 --nomodel \
	-f BAM -g hs -n macs2/treat_vs_control-shift100 2>macs2/shift100.macs2.log
## 没有 model 图，去掉 --nomodel 再执行一遍有了。

$ macs2 callpeak \
	-c map/SRR7629152.sambamba_rmdup.bam map/SRR7629153.sambamba_rmdup.bam \
	-t map/SRR7629161.sambamba_rmdup.bam map/SRR7629162.sambamba_rmdup.bam \
	--shift -100 --extsize 200 \
	-f BAM -g hs -n macs2/treat_vs_control-shift100-m 2>macs2/shift100-m.macs2.log
$ Rscript macs2/treat_vs_control-shift100-m_model.r

















========================================
ATAC-seq 高级分析（peak注释、差异peak、motif、转录因子的footprint和核小体占位）
----------------------------------------
1. paper
ATAC-seq数据分析工具的比较和推荐（Genome Biology综述）
https://mp.weixin.qq.com/s?__biz=MzA4NTIyMzk4Mg%3D%3D&mid=2247483678&idx=1&sn=de7b6bcbf76c2a9051d0db7a4d9cd074
https://pubmed.ncbi.nlm.nih.gov/32014034/


2. 技巧
https://www.cnblogs.com/leezx/p/12953732.html







========================================
|-- Annotation 峰注释
----------------------------------------
在科研分析中我们往往需要将peak区域与基因联系起来，也就是通过对peak进行注释找到peak相关基因。

It is helpful to know what genomic features are near the peaks called by MACS2. One program that is commonly used to annotate peaks is ChIPseeker. Like MACS2, ChIPseeker was originally designed to be used in the analysis of ChIP-seq, but it works just as well with ATAC-seq.

ChIPseeker requires that the genome of interest be annotated with locations of genes and other features. The ChIPseeker user guide is extremely helpful in using this R/Bioconductor package.


常见的peak注释软件有 ChIPseeker、homer、PeakAnnotator等。



1. 以ChIPseeker为例，需要在R中安装ChIPseeker包和GenomicFeatures包，然后就可以进行分析了。

library(ChIPseeker)
library(GenomicFeatures)
txdb<- makeTxDbFromGFF('gene.gtf') #生成txdb对象，如果研究物种没有已知的TxDb,可以用GenomicFeatures中的函数生成
peakfile <-readPeakFile('A1_peaks.narrowPeak') #导入需要注释的peak文件
peakAnno <- annotatePeak(peakfile,tssRegion=c(-2000, 2000), TxDb=txdb)
## 用peak文件和txdb进行peak注释，这里可以通过tssRegion定义TSS区域的区间


对于peak注释的结果，也可以进行可视化展示，如：
p <- plotAnnoPie(peakAnno)

一个饼图。










2. homer 注释峰
(1) Usage: annotatePeaks.pl <peak file | tss> <genome version>  [additional options...]
$ annotatePeaks.pl H3K4Me3.bed hg19 1>ChIP-Seq_H3K4Me3_1_peakAnn.xls 2>H3K4Me3_annLog.txt
$ annotatePeaks.pl peak.bed hg19 > peak.annotation.xls

$ annotatePeaks.pl peaks.txt hg38 -gtf Ensembl_hg38.gtf -cTSS Ensembl_hg38.tss > annotated_peaks.txt
参数解释: https://www.biostars.org/p/360106/
User defined annotation files (default is UCSC refGene annotation): 默认使用UCSC refGene 注释。
		annotatePeaks.pl accepts GTF (gene transfer formatted) files to annotate positions relative to custom annotations, such as those from de novo transcript discovery or Gencode.
			不知道怎么翻译： 相对于自定义注释位置，该脚本也接受 GTF文件来注释位置，比如 de novo 转录本或 GenCode。
		-gtf <gtf format file> (Use -gff and -gff3 if appropriate, but GTF is better) 后面跟着gtf文件
		-gid (by default the GTF file is processed by transcript_id, use this option for gene_id) 这个指定使用gene id，默认是使用 转录本id。
		-ann <custom homer annotation file> (created by assignGenomeAnnotation, see website) 自定义 homer 注释文件。

Peak vs. tss/tts/rna mode (works with custom GTF file): 转录起始位点TSS
		If the first argument is "tss" (i.e. annotatePeaks.pl tss hg18 ...) then a TSS centric analysis will be carried out.  
			如果第一个参数是 tss，则会进行 TSS 为核心的的分析。
		Tag counts and motifs will be found relative to the TSS.  序列和 motif 都是相对于 TSS的。
		(no position file needed) ["tts" now works too - e.g. 3' end of gene]  现在 tss 参数也正常工作了，比如对于基因的3‘端。
		["rna" specifies gene bodies, will automaticall set "-size given"]  而参数 rna 则指定 gene bodies，会自动设置 “-size given”
		NOTE: The default TSS peak size is 4000 bp, i.e. +/- 2kb (change with -size option) 默认的TSS 峰是 +-2kb，使用参数 -size 设置。
		-list <gene id list> (subset of genes to perform analysis [unigene, gene id, accession, probe, etc.], default = all promoters)
			取子集分析，默认是 全部启动子。
		-cTSS <promoter position file i.e. peak file> (should be centered on TSS)
			围绕TSS。




(2) 实例
$ annotatePeaks.pl macs2/SRR7629152_shift100_peaks.narrowPeak hg38 1>macs2/ann/SRR7629152_peakAnn.xls #2>macs2/ann/SRR7629152_ann.log

##==> 报错:
!!!!Genome hg38 not found in /home/wangjl/soft/homer/.//config.txt
        To check if is available, run "perl /home/wangjl/soft/homer/.//configureHomer.pl -list"
        If so, add it by typing "perl /home/wangjl/soft/homer/.//configureHomer.pl -install hg38"
$ perl /home/wangjl/soft/homer/.//configureHomer.pl -list  #查看所有内置的物种
查了一下，发现有 hg38
下载安装。
安装很慢，主要是网速慢。
$ perl /home/wangjl/soft/homer/.//configureHomer.pl -install hg38
## Current base directory for HOMER is /home/wangjl/soft/homer/.//
顺便把常用的都安装了: hg19, mm10
$ perl /home/wangjl/soft/homer/.//configureHomer.pl -install hg19

下载的信息保存在homer安装目录的data/genome/目录下，
$ ls /home/wangjl/soft/homer/./data/genomes/
hg19  hg38

以hg38为例，在data/genome/hg38目录下，文件列表如下
$ ls /home/wangjl/soft/homer/data/genomes/hg38/ -lth
total 4.4G
drwxrwxr-x 2 wangjl wangjl 4.0K Jun 24 21:07 preparsed/
-rw-r--r-- 1 wangjl wangjl 673M Oct 19  2019 hg38.full.annotation
-rw-r--r-- 1 wangjl wangjl  42M Oct 19  2019 hg38.basic.annotation
-rw-r--r-- 1 wangjl wangjl 505M Oct 19  2019 hg38.repeats
-rw-r--r-- 1 wangjl wangjl 3.2M Oct 19  2019 hg38.aug
-rw-r--r-- 1 wangjl wangjl 161K Oct 19  2019 hg38.miRNA
-rw-r--r-- 1 wangjl wangjl  29M Oct 19  2019 hg38.splice3p
-rw-r--r-- 1 wangjl wangjl  29M Oct 19  2019 hg38.splice5p
-rw-r--r-- 1 wangjl wangjl 3.2M Oct 19  2019 hg38.stop
-rw-r--r-- 1 wangjl wangjl 3.2M Oct 19  2019 hg38.tss
-rw-r--r-- 1 wangjl wangjl 3.2M Oct 19  2019 hg38.tts
-rw-r--r-- 1 wangjl wangjl  24M Oct 19  2019 hg38.rna
-rw-r--r-- 1 wangjl wangjl  12K Oct 19  2019 chrom.sizes
drwxr-xr-x 5 wangjl wangjl 4.0K Oct 19  2019 annotations/
-rw-r--r-- 1 wangjl wangjl 3.1G Jan 16  2014 genome.fa

包含了参考基因组的fasta序列以及不同区域的区间文件。
查看一下 hg38.basic.annotation 文件，共7列。
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation | awk -F "\t" '{print $7}' 查看某一列
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation #各列不对齐
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation | column -ts $'\t'
Intergenic                           chr1  1      10873  +  N       1900000000
promoter-TSS (NR_046018)             chr1  10874  11974  +  P       1
non-coding (NR_046018, exon 1 of 3)  chr1  11975  12227  +  pseudo  165001
intron (NR_046018, intron 1 of 2)    chr1  12228  12612  +  I       1089459
non-coding (NR_046018, exon 2 of 3)  chr1  12613  12721  +  pseudo  165002
intron (NR_046018, intron 2 of 2)    chr1  12722  13220  +  I       1089459




同时在data/accession目录下，还有参考基因组对应的基因注释文件。
$ ls /home/wangjl/soft/homer/./data/accession
homologene.data  human2gene.tsv  human.description  taxids.tsv

其中 human2gene.tsv记录了基因的ubigene id, gene symbol等信息，内容如下所示
$ head /home/wangjl/soft/homer/./data/accession/human2gene.tsv
AAQ88605        388007  Hs.527795       NR_015340       ENSG00000187483         SERPINA13P
127219  -
ENSP00000379387 353274  Hs.250481       NM_181489       ENSG00000185219         ZNF445
28336   3499                                    IGHEP2
NM_173803       255027  Hs.720673       NM_173803       ENSG00000156968         MPV17L

human.description记录表了基因的功能描述，类别等信息 10列，只能一列一列看了
$ head /home/wangjl/soft/homer/./data/accession/human.description | awk -F'\t' '{print $10}'
$ head /home/wangjl/soft/homer/./data/accession/human.description #整体看时，不能按列对齐
GeneID  Unigene RefSeq  Ensembl name    alias   orf     chromosome      description     type
57573   Hs.230188       NM_020813       ENSG00000196263 ZNF471  ERP1|Z1971      -       19q13.43        zinc finger protein 471 protein-coding
113391337       Hs.148569                       ZEBTR   BX111   -       -       ZEB1 transcriptional regulator RNA      ncRNA
107984694                               LOC107984694    -       -       14q23.3 probable ribosome biogenesis protein RLP24 pseudogene   pseudo
8563    Hs.75361        NM_003678       ENSG00000100296 THOC5   C22orf19|Fmip|PK1.3|fSAP79      -       22q12.2 THO complex 5   protein-coding





(3)再次执行 峰注释，成功。
$ head  macs2/ann/SRR7629152_peakAnn.xls
PeakID (cmd=annotatePeaks.pl macs2/SRR7629152_shift100_peaks.narrowPeak hg38)   Chr     Start   End     Strand  Peak Score      Focus Ratio/Region Size Annotation      Detailed Annotation     Distance to TSS Nearest PromoterID      Entrez ID       Nearest Unigene Nearest Refseq Nearest Ensembl Gene Name       Gene Alias      Gene Description        Gene Type
SRR7629152_shift100_peak_8      chr13   109424078       109424389       +       669     NA      Intergenic      Intergenic      23537   NR_126361       104326054       Hs.569298       NR_126361       ENSG00000229792 LINC00399       TCONS_00021596  long intergenic non-protein coding RNA 399     ncRNA
SRR7629152_shift100_peak_6      chr11   10509241        10509764        +       286     NA      promoter-TSS (NM_001190702)     promoter-TSS (NM_001190702)     -316    NM_001190702    100463486               NM_001190702    ENSG00000255823 MTRNR2L8        HN8     MT-RNR2 like 8 protein-coding
SRR7629152_shift100_peak_18     chr5    80651408        80651656        +       281     NA      promoter-TSS (NM_001190470)     promoter-TSS (NM_001190470)     -524    NM_001190470    100462981               NM_001190470            MTRNR2L2        HN2     MT-RNR2 like 2protein-coding
# 我加的列编号
# 1                             2       3               4               5       6       7       8                                9                               10     11              12                      13                       14              16       17            


1 PeakID (cmd=annotatePeaks.pl macs2/SRR7629152_shift100_peaks.narrowPeak hg38)   
2 Chr     
3 Start   
4 End     
5 Strand  

6 Peak Score       ## 215
7 Focus Ratio/Region Size
8 Annotation              ## Intergenic
9 Detailed Annotation      ## (TAACCC)n|Simple_repeat|Simple_repeat
10 Distance to TSS    #-1780
11 Nearest PromoterID      ## NR_046018
12 Entrez ID              ## 100287102
13 Nearest Unigene  ## Hs.618434
14 Nearest Refseq   ## HR_046018
15 Nearest Ensembl   ## ENSG00000223972
16 Gene Name         ## DDX11L1
17 Gene Alias       ## CWH43-C|PGAP2IP
18 Gene Description        ## DEAD/H-box helicase 11 like 1
19 Gene Type  ## protein-coding


看注释的效果，就是看第8和9列的分布，然后用R回个饼图。
$ head  macs2/ann/SRR7629152_peakAnn.xls| awk -F "\t" '{print $8}'
$ cat macs2/ann/SRR7629152_peakAnn.xls| awk -F "\t" '{print $8}' | awk -F'(' '{print $1}'| sed 's/ $//' |   sort | uniq -c
      1 Annotation
      9 Intergenic
      5 intron
      5 promoter-TSS
      1 TTS
#






(4) 批量化
$ makir macs2/ann/
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
annotatePeaks.pl macs2/${id}_shift100_peaks.narrowPeak hg38 1>macs2/ann/${id}_peakAnn.txt 2>macs2/ann/${id}_ann.log;
done;









========================================
|-- 差异分析: R包 DiffBind /  bedtools subtract //todo
----------------------------------------

1. 差异peak代表着比较组合染色质开放性有差异的位点，ChIP-seq和ATAC-seq都可以用 DiffBind 进行差异分析。DiffBind通过bam文件和peak的bed文件计算出peak区域标准化的readcount，可以选择edgeR、DESeq2等模型进行差异分析。

DiffBind是鉴定两个样本间差异结合位点的一个R包。
主要用于peak数据集，包括对peaks的重叠和合并的处理，计算peaks重复间隔的测序reads数，并基于结合亲和力鉴定具有统计显著性的差异结合位点。适用的统计模型有DESeq、DESeq2、edgeR。

library("DiffBind")


准备输入文件
需要准备一个SampleSheet文件，与ChIPQC的方法一样。SampleSheet文件是根据实验设计和数据存储地址等信息创建的一个csv格式文件，包含的表头信息有"SampleID"、 "Tissue"、 "Factor"、 "Condition" 、"Treatment"、"Replicate" 、"bamReads" 、"ControlID"、 "bamControl" "Peaks"、 "PeakCaller"（bam,peak文件分别在比对和call peak的步骤产生）。

""、 ""、 ""、 "" 、""、"" 、"" 、""、 "bamControl" "Peaks"、 "PeakCaller"

SampleID	Treatment	bamReads	ControlID
ctrl1	ctrl	c1.bam 	
ctrl2	ctrl	c2.bam 	
treat1	treat	t1.bam 	
treat2	treat	t2.bam 	



dbObj <- dba(sampleSheet="SampleSheet.csv")








2. Comparing peak files
Determining genomic regions that are common or different to a set of peak files is best accomplished with BEDTools, a suite of software tools that enables "genome arithmetic."

(1) 重复内共有的峰 bedtools intersect
For example, bedtools intersect determines regions that are common to two peak files, such as replicates of the same experimental group.


(2) 实验-对照特有的峰 bedtools subtract
Finding differences between two peak files, such as control vs. experimental groups, is accomplished via bedtools subtract.






ref:
https://www.jianshu.com/p/f849bd55ac27
https://hbctraining.github.io/Intro-to-ChIPseq/lessons/08_diffbind_differential_peaks.html
ref:
峰值差异比较 bdgdiff
https://cloud.tencent.com/developer/article/1624562






========================================
|-- Motif 分析 ( homer )
----------------------------------------
1.Motif富集分析

活性开放的染色质区域通过结合特定的转录因子影响转录，转录因子结合识别的DNA序列成为motif，人体中大约有1600转录因子，其中一半多已经有明确的motif。对motif的分析包括motif富集分析和转录因子的footprint。

目前有很多Motif数据库，其中最普遍的JASPAR数据库，包含了很多物种的数据，可以很容易地通过APIs或Bioconductor的R包获得相关数据。除此之外，CIS-BP和TRANSFAC包含真核转录因子的motif；HOCOMOCO专注于人和小鼠；RegulonDB是E.coli数据。

Motif信息主要以txt文件格式保存，比如位置权重矩阵（as a position weight matrix , PWM）。HOMER、Bioconductor中的TFBSTools和motifmatchr包可以利用PWM搜索特定序列中的motif。PWMScan可以通过网页快速搜索motif。MEME suit里面的FIMO、MAST和MCAST在motif搜索上有不同的应用。MEME suit和PWMScan有网页版本，更容易使用。

得到每个peak region里motif的位置和频率，再和随机背景或其它条件比较，就可以做motif的富集分析。HOMER、MEME-AME、MEME-CentriMo和DAStk分别采用不同的统计检验比较peak和背景区域motif频率的差异。ChromVAR是为单细胞ATAC-seq开发的motif分析工具，把每个细胞当作一个重复。

需要注意的是并时不时所有的TFs都有鉴定好的motif，而同一个家族TFs有类似的motif。在软件选择上，MEME-CentriMo是用的比较多的一个网页应用，可以产生可视化的报告，单细胞ATAC-seq可以选择chromVAR软件。







2. homer 

HOMER is a suite of software designed for motif discovery. It takes a peak file as input and checks for the enrichment of both known sequence motifs and de novo motifs.

http://homer.ucsd.edu/homer/ngs/peakMotifs.html



ATAC分析得到的peak是染色质上的开放区域，这些染色质开放区域常常预示着转录因子的结合，因此对peak区域进行motif分析很有意义。常见的motif分析软件有homer和MEME。以homer软件为例，首先在linux环境中安装homer，然后用以下命令进行motif分析：

findMotifsGenome.pl \
  A1_peaks.bed \ #用于进行motif分析的bed文件
  genome.fa  \ #参考基因组fa文件
  A1  \ #输出文件前缀
  -size  given \ #使用给定的bed区域位置进行分析，如果填-size -100,50则是用给定bed中间位置的上游100bp到下游50bp的区域进行分析


$ findMotifsGenome.pl
	Program will find de novo and known motifs in regions in the genome
	Usage: findMotifsGenome.pl <pos file> <genome> <output directory> [additional options]
	Example: findMotifsGenome.pl peaks.txt mm8r peakAnalysis -size 200 -len 8

homer分析motif的原理及结果参见 http://homer.ucsd.edu/homer/motif/index.html
homer 找motif的原理: https://www.jianshu.com/p/9a31f5f01e7b


示例2: 
findMotifsGenome.pl H3K4Me3.bed hg19 H3K4Me3_motif -len 8,10,12
# peak文件：ChIP-Seq_H3K4Me3_1_homer.bed
# 基因组版本：hg19
# 输出路径：ChIP-Seq_H3K4Me3_1_motifDir
# motif长度：-len 8,10,12 
# motif的软件默认长度为8，10，12
​
-p <#> (Number of processors to use, default: 1) 设置线程数
-S <#> (Number of motifs to optimize, default: 25) 所寻找的motif数目，默认为25。25已经不算少了，如果自定义数目，推荐设置更少而不是更多。






(1). 实例1: 按照bed文件范围分析(start to end)

$ findMotifsGenome.pl \
  macs2/treat_vs_control-shift100-m_peaks.narrowPeak \
  /home/wangjl/data/ref/human/UCSC/hg38.fa  \
  homer/treat_vs_control-bed  \
  -size given


## 报错：
        Peak file looks good!                                 

        Background files for 150 bp fragments found.
!!!! Might have something wrong with preparsed files
!!!! Rerun and add "-preparse" to the command line to force recreation of the files
        Custom genome sequence file: /home/wangjl/data/ref/human/UCSC/hg38.fa.gz

        Scanning input files...
!!! Something is wrong... are you sure you chose the right length for motif finding?
!!! i.e. also check your sequence file!!!


是不是必须解压fasta文件?
## /home/wangjl/data/ref/human/UCSC
$ gunzip -c hg38.fa.gz >hg38.fa
删除刚建立的文件夹 preparsed/
重新运行。
这一次很慢，估计能出结果。
$ ls -lth preparsed/
total 896M
-rw-rw-r-- 1 wangjl wangjl  80M Jun 19 12:07 hg38.fa.93.gcbins
-rw-rw-r-- 1 wangjl wangjl  80M Jun 19 12:07 hg38.fa.93.cgbins
-rw-rw-r-- 1 wangjl wangjl 199M Jun 19 12:07 hg38.fa.93.cgfreq
-rw-rw-r-- 1 wangjl wangjl 375M Jun 19 12:07 hg38.fa.93.seq
-rw-rw-r-- 1 wangjl wangjl 165M Jun 19 12:06 hg38.fa.93.pos



批量化 
$ cat ../raw/SRR_Acc_List.txt | while read id; do echo $id;
findMotifsGenome.pl \
  macs2/${id}_shift100_peaks.narrowPeak \
  /home/wangjl/data/ref/human/UCSC/hg38.fa  \
  homer/${id}  \
  -size given;
done;




测试: 加上参数 -preparse, 后来发现narrowPeak是空文件时报这个错。
$ findMotifsGenome.pl macs2_result/SRR7629153_peaks.narrowPeak /home/wangjl/data/ref/human/UCSC/hg38.fa homer/SRR7629153_1/ -size given -p 10
## 21:03--> 
$ findMotifsGenome.pl macs2_result/SRR7629153_peaks.narrowPeak /home/wangjl/data/ref/human/UCSC/hg38.fa homer/SRR7629153_1/ -size given -p 10 -preparse
$ findMotifsGenome.pl macs2_result/SRR7629153_peaks.narrowPeak hg38 homer/SRR7629153_2/ -size given -p 10
## 21:07-->






(2). 按照 summit 上下游分析(下游100，上游50范围内)

NAME_summits.bed 是summit位置，是一个1碱基长度的bed文件。

$ findMotifsGenome.pl \
  macs2/treat_vs_control-shift100-m_summits.bed \
  /home/wangjl/data/ref/human/UCSC/hg38.fa  \
  homer_/treat_vs_control-summits  \
  -size  -100,50



(3) findMotifsGenome.pl -p 20 添加多线程
$ find . | grep "sf$" | xargs grep "findMotifsGenome" --color=auto
./rules/06homer_find_motif.sf:	shell:"findMotifsGenome.pl {input} {GENOME} {params.out_dir} -size given >{log} 2>&1"

Usage: findMotifsGenome.pl <pos file> <genome> <output directory> [additional options]
        Example: findMotifsGenome.pl peaks.txt mm8r peakAnalysis -size 200 -len 8

多线程参数：-p <#> (Number of processors to use, default: 1)

$ findMotifsGenome.pl peaks.txt mm8r peakAnalysis -size 200 -len 8

"findMotifsGenome.pl {input} {GENOME} {params.out_dir} -size given -p {threads} >{log} 2>&1"








3. 可视化
根据motif与已知转录因子的富集情况可以绘制气泡图，从而可以看到样本与已知转录因子的富集显著性。

已知转录因子的富集显著性
x轴为样本标签: A1, A2, B1, B2
Y轴为转录因子名字: JunB, Jun-AP1, Fra2, Fra1, Fosl2, CTCF, Atf3, AP-1
中间是dotplot，直径大小表示 motif enrichment(-logP)






ref:
https://www.jianshu.com/p/4e6b42152694
homer 参数：https://www.jianshu.com/p/467d970ec097














========================================
|-- 富集分析 //todo
----------------------------------------
通过注释得到的peak相关基因可以使用goseq、topGO等R包进行GO富集分析，用kobas进行kegg富集分析，也可以使用DAVID在线工具来完成富集分析。

可以通过挑选感兴趣的GO term或pathway进一步筛选候选基因。



















========================================
使用snakemake构建 ATAC-seq 分析流程: 从 fastq 到 call peak
----------------------------------------
1. 流程脚本:
https://github.com/crazyhottommy/pyflow-ATACseq

按照include模式写，写成一个就保存一个。

此后的修订记录在单独的库中:
https://github.com/DawnEve/snakemakeWorkflow
ref: https://github.com/snakemake-workflows



(2) 路径
脚本位置
/data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2

数据位置
/home/wangjl/data/ATAC/hair/raw/origin/  #192=2*96
$ ls -lth /home/wangjl/data/ATAC/hair/raw/origin/|head
total 3.0G
-r--r--r-- 1 wangjl wangjl 442M Jun 24 19:07 cgm-57_R1.fastq.gz
-r--r--r-- 1 wangjl wangjl 440M Jun 24 19:07 cgm-57_R2.fastq.gz
-r--r--r-- 1 wangjl wangjl  52K Jun 24 19:08 cgm-94_R1.fastq.gz
-r--r--r-- 1 wangjl wangjl  52K Jun 24 19:08 cgm-94_R2.fastq.gz




(2) 准备测试文件

工作目录
$ cd /data/wangjl/ATAC/fq/test5 

$ mkdir raw/
$ gunzip -c /home/wangjl/data/ATAC/hair/raw/origin/cgm-57_R1.fastq.gz | head -n 40000 | gzip > raw/cgm-57_R1.fastq.gz
$ gunzip -c /home/wangjl/data/ATAC/hair/raw/origin/cgm-57_R2.fastq.gz | head -n 40000 | gzip > raw/cgm-57_R2.fastq.gz

$ ln -s /home/wangjl/data/ATAC/hair/raw/origin/cgm-94_R1.fastq.gz raw/
$ ln -s /home/wangjl/data/ATAC/hair/raw/origin/cgm-94_R2.fastq.gz raw/



(3) 写 yaml 格式的配置文件 
$ ls raw/ > config.yaml
$ sed -i 's/_R[12]\.fastq\.gz//' config.yaml
$ sed -i 's/^/ - /' config.yaml 
$ sed -i '1i samples:' config.yaml
$ cat config.yaml 
samples:
 - cgm-57
 - cgm-94
 - cgm-94





(4) 另一种 yaml 格式配置文件的写法
$ cat config.yaml #格式有严格要求: 逗号后有空格, 冒号后有空格，不需要加引号了。
samples: [SRR7629152, SRR7629153, SRR7629154, SRR7629161, SRR7629162, SRR7629163]
workdir: /home/wangjl/data/ATAC/fq/test3
Snk_RootDir: /home/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo1

snake脚本中引用
$ cat test.sf
configfile: "config.yaml"
workdir: config["workdir"]
SI=config["samples"]










2. 边写边测试，模块化。

脚本结构
├── config.yaml-example
├── scripts
│   ├── script1.py
│   └── script2.R
├── rules
│   ├── 01qc.smk
│   ├── 02mapping.smk
│   └── 03plot.smk
└── main.sf

$ cd /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2


####===>>>>>>>>>>主程序脚本

$ cat main.sf 
configfile: "config.yaml"
SI=config["samples"]
BWA_INDEX="/home/wangjl/data/ref/human/UCSC/bwa/hg38_fa"

rule all:
	input:
		"QC_raw/multiqc/multiqc_report.html",
		#expand("clean2/{sample}_R1_val_1.fq.gz", sample=SI),
		#expand("clean/{sample}_R1.fastq.gz", sample=SI),
		"QC_clean/multiqc/multiqc_report.html",
		#expand("map/{sample}.sort.bam", sample=SI),
		#expand("map_clean/{sample}.final.bam", sample=SI),
		"map_clean/done.txt",
		expand("macs2_result/{sample}_peaks.narrowPeak", sample=SI),


include: "rules/01_QC_raw.sf"
include: "rules/02_trim.sf"
include: "rules/03_BWA_map.sf"
include: "rules/04_bam_filter.sf"
include: "rules/05_call_peak.sf"


## 原始质控 
$ cat rules/01_QC_raw.sf
rule QC_raw_fq:
	input: 
		"raw/{sample}_R1.fastq.gz",
		"raw/{sample}_R2.fastq.gz",
	output: 
		"QC_raw/{sample}_R1_fastqc.html",
		"QC_raw/{sample}_R2_fastqc.html",
	params:out_dir="QC_raw"
	threads: 2
	log: "QC_raw/{sample}_fastqc.html.log"
	shell: 
		"fastqc -t {threads} {input} -o {params.out_dir} >{log} 2>&1"

rule multi_QC_raw:
	input: expand("QC_raw/{sample}_R1_fastqc.html", sample=SI)
	output:"QC_raw/multiqc/multiqc_report.html"
	params: 
		in_dir="QC_raw",
		out_dir="QC_raw/multiqc/"
	log: "QC_raw/multiqc/multiqc_report.html.log"
	shell:"multiqc {params.in_dir}/*fastqc.zip -o {params.out_dir} >{log} 2>&1"


## 去接头
$ cat rules/02_trim.sf
rule trim_galore:
	input:
		"raw/{sample}_R1.fastq.gz",
		"raw/{sample}_R2.fastq.gz",
	output:
		"clean2/{sample}_R1_val_1.fq.gz",
		"clean2/{sample}_R2_val_2.fq.gz",
	params: out="clean2/"
	threads: 2
	log: "clean2/{sample}.trim.log"
	shell:"trim_galore -j {threads} -q 25 --phred33 --length 15 -e 0.1 --stringency 3 \
		--paired -o  {params.out}  {input} >{log} 2>&1"
## --stringency <INT>      Overlap with adapter sequence required to trim a sequence. Defaults to a
##	very stringent setting of 1, i.e. even a single bp of overlapping sequence
##	will be trimmed off from the 3' end of any read.

rule cut_adapter:
	input:
		"raw/{sample}_R1.fastq.gz",
		"raw/{sample}_R2.fastq.gz",
	output:
		"clean/{sample}_R1.fastq.gz",
		"clean/{sample}_R2.fastq.gz"
	log:"clean/{sample}_cutapapt.log"
	threads: 2
	shell:
		"cutadapt -j {threads} --time 1 -e 0.1 -O 3 --quality-cutoff 25 --pair-adapters \
		-m 15 -a CTGTCTCTTATA \
		-A AGACGGGAAGAGCGTCGTGAGGGAAGAGTGTAGATCTCGGTGGTCGCGTATCATT \
		-o {output[0]} -p {output[1]} {input[0]} {input[1]} >{log} 2>&1"

## -m LEN[:LEN2], --minimum-length LEN[:LEN2] 最小长度
## -O MINLENGTH, --overlap MINLENGTH 序列和接头重合长度
##   Require MINLENGTH overlap between read and adapter for an adapter to be found.
##   Default: 3
## -n COUNT, --times COUNT
##   Remove up to COUNT adapters from each read. Default: 1

# 结果行数是一样的，选更快的。


rule QC_clean_fq:
	input: 
		"clean/{sample}_R1.fastq.gz",
		"clean/{sample}_R2.fastq.gz"
	output: 
		"QC_clean/{sample}_R1_fastqc.html",
		"QC_clean/{sample}_R2_fastqc.html"
	params:out_dir="QC_clean"
	log: "QC_clean/{sample}_fastqc.html.log"
	threads: 2
	shell: 
		  "fastqc -t {threads} {input} -o {params.out_dir} >{log} 2>&1"

rule multi_QC_clean:
	input: expand("QC_clean/{sample}_R2_fastqc.html", sample=SI)
	output:"QC_clean/multiqc/multiqc_report.html"
	params: 
		in_dir="QC_clean",
		out_dir="QC_clean/multiqc/"
	log: "QC_clean/multiqc/multiqc_report.html.log"
	shell:"multiqc {params.in_dir}/*fastqc.zip -o {params.out_dir} >{log} 2>&1"





## 比对: 仅保存bam，删除sam中间文件
$ cat rules/03_BWA_map.sf
rule bwa_mapping:
	input:
		"clean/{sample}_R1.fastq.gz",
		"clean/{sample}_R2.fastq.gz"
	output:
		temp("map/{sample}_bwa_hg38.sam")
	params: bwa=r"@RG\tID:{sample}\tSM:{sample}"
	threads: 4
	log:"map/{sample}_bwa_hg38.sam.log"
	shell:
		"bwa mem -t {threads} -R '{params.bwa}' -o {output} {BWA_INDEX} {input} >{log} 2>&1"

rule sort_to_bam:
	input:"map/{sample}_bwa_hg38.sam"
	output:"map/{sample}.sort.bam"
	params: tmp="map/{sample}_tmp"
	threads: 2
	log: "map/{sample}.sort.bam.log"
	shell:"samtools sort -@ {threads} -O bam -T {params.tmp} -o {output} {input} >{log} 2>&1"

#$ samtools view map/cgm-57.sort.bam|  awk '{print $5}' | sort |uniq -c | sort -k1nr|head
#  14375 60
#   3434 0
#    345 40




## rm MT, rm dup, rm Low MapQ, then BAM QC (TSS分布图，插入片段长度)
$ cat rules/04_bam_filter.sf
rule index:
	input:"{sample}.bam"
	output:"{sample}.bam.bai"
	shell:"samtools index {input}"

## rm MT
rule get_chrlist:
	input:"map/{sample}.sort.bam"
	output:"map_clean/{sample}.chrlist"
	log:"map_clean/{sample}.chrlist.log"
	shell:"samtools view  {input}| awk '{{print $3}}' \
		| sort | uniq -c| grep -E -v 'GL|KI|JH|KB|chrM|\*' |  awk '{{print $2}}' | xargs > {output} 2>{log}"

rule del_MTs:
	input:"map_clean/{sample}.chrlist", "map/{sample}.sort.bam", "map/{sample}.sort.bam.bai"
	output:"map_clean/{sample}.sort.delMT.bam"
	threads: 2
	log:"map_clean/{sample}.sort.delMT.bam.log"
	shell:"samtools view -@ {threads} -bh {input[1]} `cat {input[0]}` > {output} 2>{log}"

# filter by MAPQ:
# 1 需要不需要按MAPQ过滤？需要，下例bowtie2按照>10过滤的。
# 2 三个过滤怎么排序好？不确定
# 3 怎么设置其他参数?
# https://informatics.fas.harvard.edu/atac-seq-guidelines-old-version.html#alignments
# https://github.com/ENCODE-DCC/atac-seq-pipeline/blob/master/src/encode_task_filter.py
rule rm_low_MAPQ:
	input:"map_clean/{sample}.sort.delMT.bam"
	output:"map_clean/{sample}.sort.delMT.HmapQ.bam"
	threads: 8
	shell:"samtools view -@ {threads} -bh -q 30 {input} > {output}"

# remove duplicate: 这个放最后的优势是，会自动生成bai文件
rule rm_dup:
	input:"map_clean/{sample}.sort.delMT.HmapQ.bam"
	output:"map_clean/{sample}.final.bam"
	threads: 8
	params: tmp="map_clean/tmp/{sample}"
	log:"map_clean/{sample}.sort.delMT.HmapQ.sambambaRmDup.bam.log"
	shell:"sambamba markdup -r -t {threads} --tmpdir {params.tmp} {input} {output} >{log} 2>&1"

rule clean_done:
	input: expand("map_clean/{sample}.final.bam", sample=SI)
	output: "map_clean/done.txt"
	params: tmp="map_clean/tmp/"
	shell: "rm -rf {params.tmp} && touch {output}"



(5) call peak 
$ cat rules/05_call_peak.sf
rule macs2_callpeak:
	input:
		"map_clean/{sample}.final.bam"
	output:
		"macs2_result/{sample}_peaks.narrowPeak"
	params:
		"{sample}",
		"macs2_result"
	log:
		"macs2_result/{sample}_peaks.narrowPeak.log"
	shell:
		"macs2 callpeak -t {input} -f BAM -g hs \
		--shift -100 --extsize 200 --nomodel \
		-B --SPMR --call-summits \
		--outdir {params[1]} -n {params[0]} >{log} 2>&1"





测试: 在工作目录
$ snakemake -s /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/main.sf -j 10 -p










3. 实例: Hair project (Run by ATAC-seq workflow version v0.3)

共96个文件
$ ls /home/wangjl/data/ATAC/hair/raw/origin/ | head
cgm-1_R2.fastq.gz
cgm-96_R2.fastq.gz

$ seq 1 4
1
2
3
4

(1) 放fq文件到raw中
$ cd /data/wangjl/ATAC/fq/test5/
$ path1="/home/wangjl/data/ATAC/hair/raw/origin/"
$ seq 1 96 | while read i; do 
id="cgm-"${i};
echo $id; 
ln -s ${path1}/${id}_R1.fastq.gz raw/;
ln -s ${path1}/${id}_R2.fastq.gz raw/;
echo $id >>raw/cid.txt
done;


$ zcat raw/cgm-2_R1.fastq.gz | head
@A00582:646:H7CJTDSX2:4:1101:27534:1141 1:N:0:CTAAACGG+AGAGGATA
CTTATACACATCTCCGAGCCCACGAGACCTAAACGGATCTCGGATGCCGTCTTGGGCGTGGAAAGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
+
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,:F,,:,,,F,,,,,,F,,,F,,,,,,:::FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF


(2) 写配置文件
$ cp raw/cid.txt config.yaml
$ sed -i "s/^/ - /" config.yaml

$ vim config.yaml ###add three lines on top
workdir: /data/wangjl/ATAC/fq/test5/
Snk_RootDir: /home/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2
samples: 


(3) 生成总的流程图
$ mkdir images
$ rootPath="/data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2"
$ snakemake -s ${rootPath}/main.sf --dag | dot -Tsvg > images/ATAC-seq_Sample.svg
$ snakemake -s ${rootPath}/main.sf --rulegraph 2> /dev/null | dot -Tsvg > images/ATAC-seq_Rule.svg


(4) 运行
## dry run
$ snakemake -s /home/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/main.sf -np

## 实际运行
$ snakemake -s /home/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/main.sf -j 8 -p
Job counts:
        count   jobs
        96      QC_clean_fq
        96      QC_raw_fq
        1       all
        96      bwa_mapping
        1       clean_done
        96      cut_adapter
        96      del_MTs
        96      get_chrlist
        96      index
        96      macs2_callpeak
        1       multi_QC_clean
        1       multi_QC_raw
        96      rm_dup
        96      rm_low_MAPQ
        96      sort_to_bam
        1060
[Thu Jul 15 23:05:42 2021] --> [Fri Jul 16 00:18:47 2021]
耗时 1h15min。







========================================
|-- 合并bam并质控: TSS附近reads分布图(巨慢)、插入片段密度图
----------------------------------------
1. 因为这个步骤太慢。

(1) 6个完全样本: 12个fq文件
[Wed Jun 23 20:35:59 2021] 开始
[Wed Jun 23 22:53:40 2021] 晚上走之前，37 of 105 steps (35%) done。map还没结束。

[Thu Jun 24 05:39:03 2021] 早上过来，  78 of 105 steps (74%) done，QC_map/matrix
[Thu Jun 24 09:50:30 2021] 中午吃饭前  94 of 105 steps (90%) done，MAP_QC_plotHeatmap
[Thu Jun 24 13:32:22 2021]  最后一个文件不知道为什么这么慢？     103 of 105 steps (98%) done

[Thu Jun 24 16:43:37 2021]  105 of 105 steps (100%) done
一共运行了 20h 多。
晚8:35-半夜0点 3.5h
0点-16:43      16.7h
Complete log: /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo1/.snakemake/log/2021-06-23T203559.284905.snakemake.log



(2) 同时单细胞效果很差，合并后做才有效果。
合并bam文件: 
$ ls map_clean/*final.bam|wc
     96      96    2583
$ ls map_clean/*final.bam| head
map_clean/cgm-10.final.bam
map_clean/cgm-11.final.bam
map_clean/cgm-12.final.bam
总行数 63.3万行。
$ ls map_clean/*final.bam |xargs -i samtools view {} | wc
 632936 10835066 242551552
#

$ samtools merge total.bam c1.bam c2.bam c3.bam c4.bam

$ ls map_clean/*final.bam |xargs -i ls -l {}
$ ls map_clean/*final.bam |xargs -i echo {}

$ mkdir merged
$ ls map_clean/*final.bam |xargs -i samtools merge merged/total.bam {} #失败

$ samtools merge merged/total.bam `ls map_clean/*final.bam |xargs`
$ samtools view  merged/total.bam |wc
 632936 10835066 242551552
# 和合并之前的测试一致。








## check 中间文件
$ cat map_clean/cgm-10.chrlist
chr1 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr2 chr20 chr21 chr22 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chrX chrY
$ samtools view  map_clean/cgm-10.final.bam | awk '{print $5}'  | sort |uniq -c | sort -k1nr
   4649 60
    135 40
     89 37
     46 46
     26 45
     22 48
$ samtools view -q 60 map_clean/cgm-10.final.bam  | wc
 4649   79233 1859898

# check 合并后的文件
$ samtools view merged/total.bam | awk '{print $5}'  | sort |uniq -c | sort -k1nr
 570980 60
  15499 40
   4243 46
   3593 48
   3403 45








2. TSS 分布图、插入片段长度密度图
需要提前 index .

$ cat bam_QC.sf
REF_BED="/home/wangjl/data/ref/human/UCSC/hg38_refseq_whole_gene.bed"
SI=["total"]

rule all:
	input:
		expand("QC_map/TSS/{sample}.TSS.heatmap.pdf", sample=SI), #BAM QC: TSS (巨慢)
		expand("QC_map/fragment_length/fragment_length_{sample}.pdf", sample=SI),

include: "rules/B01_mapQC.sf"



$ cat rules/B01_mapQC.sf
rule index:
	input: "{sample}.bam"
	output:"{sample}.bam.bai"
	shell:"samtools index {input}"

## target 1: TSS
rule MAP_QC_bamCoverage:
	input: "merged/{sample}.bam"
	output: "QC_map/{sample}.bw"
	log: "QC_map/{sample}.bw.log"
	shell: "bamCoverage -b {input} -o {output} >{log} 2>&1"

rule MAP_QC_computeMatrix:
	input: "QC_map/{sample}.bw"
	output:"QC_map/TSS/{sample}.mat.gz"
	log: "QC_map/TSS/{sample}.mat.gz.log"
	shell:"computeMatrix reference-point --referencePoint TSS \
       -S {input} -R {REF_BED} \
       -b 1000 -a 1000 -o {output} >{log} 2>&1"

rule MAP_QC_plotHeatmap:
	input: "QC_map/TSS/{sample}.mat.gz",
	output:"QC_map/TSS/{sample}.TSS.heatmap.pdf"
	log: "QC_map/TSS/{sample}.TSS.heatmap.pdf.log"
	shell:"plotHeatmap -m {input} -out {output}"


## target 2: fragment_length
rule get_fragment_length:
	input:"merged/{sample}.bam"
	output:"QC_map/fragment_length/{sample}.fragment.length.txt"
	log:"QC_map/fragment_length/{sample}.fragment.length.txt.log"
	shell:"samtools view {input} | \
		awk -F'\t' 'function abs(x){{return ((x < 0.0) ? -x : x)}} {{print $1\"\t\"abs($9)}}' | \
		sort | uniq | cut -f2 > {output} 2>{log}"

rule draw_plot:
	input: 
		"merged/{sample}.bam",
		"QC_map/fragment_length/{sample}.fragment.length.txt",
		"merged/{sample}.bam.bai"
	output: "QC_map/fragment_length/fragment_length_{sample}.pdf"
	params: key="{sample}"
	log: "QC_map/fragment_length/fragment_length_{sample}.pdf.log"
	script: "../scripts/B01_draw_fragment_length.R"
	

$ cat scripts/B01_draw_fragment_length.R
## 同时生成pdf和png
# myArgs<-commandArgs(TRUE)
infile=snakemake@input[[1]]
outfile=snakemake@output[[1]]
params.key=snakemake@params[["key"]]

# 记录日志，屏幕上还是显示很多
log <- file(snakemake@log[[1]], open="wt")
sink(log)

library("ATACseqQC");
pdf(outfile, width=5, height=5);
fragSizeDist(infile, params.key)
dev.off();

outfile2=paste0(substring(outfile, 1, nchar(outfile)-3), "png")
png(outfile2, width=1500, height=1500, res=300);
fragSizeDist(infile, params.key)
dev.off();


## 在工作目录运行
$ snakemake -s /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/bam_QC.sf -j 5 -p
[Fri Jul 16 11:03:40 2021] --> [Fri Jul 16 11:21:03 2021] 20min.





fIn="QC_map/fragment_length/total.fragment.length.txt"
data <- read.table(fIn, header = F)
# 设置插入片段长度的阈值，过滤掉太长的片段
length_cutoff <- 1200
fragment <- data$V1[data$V1 <= length_cutoff]
# 利用直方图统计频数分布，设置柱子个数
breaks_num <- 500
res <- hist(fragment, breaks = breaks_num, plot = FALSE)

#pdf('1.pdf', width=6, height=4);
# 添加坐标原点
plot(x = c(0, res$breaks),
     y = c(0, 0, res$counts) / 10^2,
     type = "l", col = "red", ylim=c(0,100),
     xlab = "Fragment length(bp)",
     ylab = expression(Normalized ~ read ~ density ~ 10^2),
     main = "Sample Fragment sizes")
# dev.off()






========================================
|-- 合并后 call peak 和 motif 分析
----------------------------------------
1. call peak and find motif 
$ cat mainBulk.sf 
SI=["total"]  

rule all:
	input:
		expand("homer/{sample}/knownResults.html", sample=SI)

rule macs2_callpeak:
	input:"merged/{sample}.bam"
	output:"macs2_result/{sample}_peaks.narrowPeak"
	params:
		"{sample}",
		"macs2_result"
	log: "macs2_result/{sample}_peaks.narrowPeak.log"
	shell: "macs2 callpeak -t {input} -f BAM -g hs \
		--shift -100 --extsize 200 --nomodel \
		-B --SPMR --call-summits \
		--outdir {params[1]} -n {params[0]} >{log} 2>&1"

include:"rules/06_find_motif.sf"


## 看教程使用 narrowPeak 文件找motif。
	1)https://www.jianshu.com/p/84b2933959e0
	findMotifsGenome.pl ${GenrichDir}/KO.narrowPeak hg38 ${MotifDir} -size 200
	
## 但是也有说使用 summit bed 文件找motif？
	
	
$ cat rules/06_find_motif.sf
GENOME   ="/home/wangjl/data/ref/human/UCSC/hg38.fa"
## 命令中指定输出目录
rule homer_find_motif: 
	input: "macs2_result/{sample}_peaks.narrowPeak"
	output: "homer/{sample}/knownResults.html"
	params: out_dir="homer/{sample}/"
	threads: 4
	log:"homer/{sample}/knownResults.html.log"
	#shell:"findMotifsGenome.pl {input} {GENOME} {params.out_dir} -size given >{log} 2>&1"
	shell:"findMotifsGenome.pl {input} {GENOME} {params.out_dir} -size given -p {threads} -preparse >{log} 2>&1"


# 所有的选项都在最后添加，否则会出错。
# -p <#> (Number of processors to use, default: 1)
# -preparse (force new background files to be created)

测试
$ snakemake -s /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/mainBulk.sf -j 5 -p
11:34 --> 12:03, 30min






(2) IGV 查看 
$ head  /data/wangjl/ATAC/hair/data_P1/macs2_result/06/total_peaks.narrowPeak 
chr1    17417   17621   total_peak_1    54      .       6.61902 10.1308 5.41469 102
chr1    88123   88401   total_peak_2    41      .       5.7377  8.38894 4.1436  151
chr1    128547  128747  total_peak_3    31      .       4.91803 6.88453 3.11127 115
chr1    181273  181513  total_peak_4    50      .       6.61765 9.69352 5.09812 116
chr1    191415  191635  total_peak_5    44      .       6.16438 8.77436 4.42415 106
chr1    633885  634339  total_peak_6    608     .       21.5278 69.3034 60.8075 188
chr1    778506  778841  total_peak_7    44      .       6.04396 8.87476 4.49802 129
chr1    832200  832445  total_peak_8    17      .       3.6     4.55696 1.77127 136
chr1    1074827 1075060 total_peak_9    13      .       2.52525 3.7002  1.35139 122
chr1    1511590 1511790 total_peak_10   14      .       2.89855 3.96367 1.48407 151

第 7 列是富集倍数。
按第7列排序：
$ sort -k7nr /data/wangjl/ATAC/hair/data_P1/macs2_result/06/total_peaks.narrowPeak | head | column -t
chr1   633885     634339     total_peak_6      608  .  21.5278  69.3034  60.8075  188
chr20  28513494   28513833   total_peak_1237   126  .  10.8696  18.5744  12.6431  161
chr1   206203018  206203335  total_peak_929    97   .  9.42029  15.4102  9.76858  155
chr1   121184480  121184756  total_peak_567    84   .  8.59375  13.7938  8.40303  140
chr1   143246801  143247109  total_peak_573    84   .  7.79817  13.8122  8.41075  158
chr1   184754577  184755234  total_peak_814    117  .  7.1066   17.601   11.7631  468
chr1   161459153  161459398  total_peak_685    56   .  7.04225  10.4928  5.65903  124
chr1   223992515  223992806  total_peak_1005   60   .  6.98925  10.9919  6.05824  161
chr20  47894696   47895299   total_peak_1346a  114  .  6.93069  17.236   11.4168  150
chr1   147242376  147242776  total_peak_594    87   .  6.79012  14.2738  8.77573  149











========================================
|-- *** 探索参数：MACS2 如何识别尽量多的峰？ ***
----------------------------------------
2. 探索如何获取更多 peak

(1) 输入
$ cd /data/wangjl/ATAC/hair/data_P1/
$ samtools view merged/total.bam |wc  #2.1Gb
51916420 886379942 11027899754
51,916,420 = 51 Million reads

## 而上次 (cgm-1_R1.fastq.gz) 的reads总数是：30Mb
$ samtools view /data/wangjl/ATAC/fq/test5/merged/total.bam|wc
 632936 10835066 242551552
632,936 = 0.6 Million reads.
仅发现了 32 个peak。


2) 提取chr1
$ samtools view -b -h merged/total.bam chr1 chr20 >merged/total_1_20.bam
2.1G Aug  5 20:13 merged/total.bam
252M Aug  6 20:27 merged/total_1_20.bam


(2) 预测插入片段长度
通过predictd子命令可以预测样本的fragment size，命令如下
macs2 predictd -i input.bam

$ macs2 predictd -i merged/total.bam
## predicted fragment length is 209 bps
$ Rscript predictd
产生一个图 predictd_model.pdf

这个预测出来的数值可用于下一步 --extsize 的值，取多个样本的均值即可。
官方也给出了推荐值，
	对于大多数的转录因子chip_seq数据，推荐值为200， 
	对于大部分组蛋白修饰的chip_seq数据，推荐值为147。



(3) 默认使用模型
MACS首先的工作是要确定一个模型，这个模型最关键的参数就是峰宽d，这个d就是bw(band width)，而它的一半就是shiftsize

$ macs2 callpeak -c controlFile.bam -t treatmentFile.bam -m 10 30 -p pvalue -f BAM -g gsize -B -n filename.preffix --outdir ChIP_seq/CallPeak 2>ChIP_seq/CallPeak/filename.macs2.log

$ macs2 callpeak \
-t ip.bam \
-c input.bam \
--outdir out_dir \
-n chip \
-g hs

完整版

$ macs2 callpeak -t $input \
	-m 10 30 -f BAM -g hs \
	-B -n total --outdir macs2_result/01/



使用子集测试：
$ input=merged/total_1_20.bam

1)实测版 01: 基于模型的
$ macs2 callpeak -t $input \
	-f BAM -g hs \
	-n total --outdir macs2_result/01/

$ ls -lth macs2_result/01/
total 340K
-rw-rw-r-- 1 wangjl wangjl 86K Aug  6 20:30 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 96K Aug  6 20:30 total_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 56K Aug  6 20:30 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl 97K Aug  6 20:30 total_model.r

1218 12180 87216 macs2_result/01/total_peaks.narrowPeak
$ head macs2_result/01/total_peaks.narrowPeak
chr1    17230   17520   total_peak_1    46      .       6.14192 8.87601 4.6407  148
chr1    88067   88375   total_peak_2    35      .       5.30705 7.29685 3.5407  115
chr1    181338  181663  total_peak_3    33      .       5.25624 7.06507 3.38641 132
chr1    191250  191562  total_peak_4    28      .       4.79904 6.26902 2.86454 189
chr1    633760  634354  total_peak_5    636     .       19.0553 72.1745 63.6785 263


2) 实测版 02: 基于模型的，-B 参数
$ macs2 callpeak -t $input \
	-f BAM -g hs \
	-B -n total --outdir macs2_result/02/

$ ls -lth macs2_result/02/
total 337M
-rw-rw-r-- 1 wangjl wangjl  86K Aug  6 20:35 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  96K Aug  6 20:35 total_peaks.xls
-rw-rw-r-- 1 wangjl wangjl  56K Aug  6 20:35 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl 169M Aug  6 20:35 total_control_lambda.bdg
-rw-rw-r-- 1 wangjl wangjl 169M Aug  6 20:35 total_treat_pileup.bdg
-rw-rw-r-- 1 wangjl wangjl  97K Aug  6 20:35 total_model.r

1218 12180 87216 macs2_result/02/total_peaks.narrowPeak
$ head macs2_result/02/total_peaks.narrowPeak

多了2个文件：
$ head macs2_result/02/total_control_lambda.bdg
chr1    0       12542   0.30253
chr1    12542   13894   0.31900
chr1    13894   15092   0.34800
chr1    15092   15098   0.31900
chr1    15098   83241   0.30253
$ awk '$4>14{print $0}' macs2_result/02/total_control_lambda.bdg|head
chr20   31063803        31063805        14.00700
chr20   31063809        31063838        14.00700
chr20   31063838        31063843        14.03600
chr20   31063843        31063845        14.06500
chr20   31063845        31063849        14.03600
chr20   31063849        31063853        14.0650

$ head macs2_result/02/total_treat_pileup.bdg 
chr1    0       10010   0.00000
chr1    10010   10092   1.00000
chr1    10092   10098   2.00000
chr1    10098   10170   3.00000
chr1    10170   10300   4.00000
chr1    10300   10382   3.00000



3) 实测版 02: 基于模型的，-B 参数 -m 10 30
默认是 DEFAULT:5 50

$ macs2 callpeak -t $input \
	-m 10 30 -f BAM -g hs \
	-B -n total --outdir macs2_result/03/

$ ls -lth macs2_result/03
total 337M
-rw-rw-r-- 1 wangjl wangjl  87K Aug  6 20:43 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  57K Aug  6 20:43 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl  97K Aug  6 20:43 total_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 169M Aug  6 20:43 total_control_lambda.bdg
-rw-rw-r-- 1 wangjl wangjl 169M Aug  6 20:43 total_treat_pileup.bdg
-rw-rw-r-- 1 wangjl wangjl  97K Aug  6 20:43 total_model.r

1236 12360 88574 macs2_result/03/total_peaks.narrowPeak
多了十几行。




4) 实测版 02: 不基于模型
$ macs2 callpeak -t $input -f BAM -g hs \
--shift -100 --extsize 200 --nomodel \
--outdir macs2_result/04/ -n total

$ ls -lth macs2_result/04
total 280K
-rw-rw-r-- 1 wangjl wangjl  65K Aug  6 20:48 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl  97K Aug  6 20:48 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 109K Aug  6 20:48 total_peaks.xls

输出3个文件。
1392  6960 65633 macs2_result/04/total_summits.bed
$ head macs2_result/04/total_summits.bed
chr1    17523   17524   total_peak_1    5.41469
chr1    88233   88234   total_peak_2    4.1436
chr1    128679  128680  total_peak_3    3.11127
chr1    181367  181368  total_peak_4    5.09812
chr1    191494  191495  total_peak_5    5.41469
chr1    634081  634082  total_peak_6    60.8075

峰比 基于模型的 数量多 150个。



5) 实测版 02: 不基于模型, 加参数 --call-summits
--CALL-SUMMITS
MACS will now reanalyze the shape of signal profile (p or q-score depending on the cutoff setting) to deconvolve subpeaks within each peak called from the general procedure. It's highly recommended to detect adjacent binding events. While used, the output subpeaks of a big peak region will have the same peak boundaries, and different scores and peak summit positions.
如果使用，大峰的子峰会有同样的peak边界，不同的peak summit 位置和打分。

$ macs2 callpeak -t $input -f BAM -g hs \
--shift -100 --extsize 200 --nomodel \
--call-summits \
--outdir macs2_result/05/ -n total

$ ls -lth macs2_result/05/
total 280K
-rw-rw-r-- 1 wangjl wangjl  66K Aug  6 20:51 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl  99K Aug  6 20:51 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 111K Aug  6 20:51 total_peaks.xls

1417  7085 66864 macs2_result/05/total_summits.bed
$ head macs2_result/05/total_summits.bed
chr1    17519   17520   total_peak_1    5.41469
chr1    88274   88275   total_peak_2    4.1436
chr1    128662  128663  total_peak_3    3.11127

目前最多的峰，可能有子峰。








6) 实测版 02: 不基于模型, 加参数 -B --SPMR --call-summits
$ macs2 callpeak -t $input -f BAM -g hs \
--shift -100 --extsize 200 --nomodel \
-B --SPMR --call-summits \
--outdir macs2_result/06/ -n total

$ ls -lth macs2_result/06/
total 337M
-rw-rw-r-- 1 wangjl wangjl  66K Aug  6 20:58 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl  99K Aug  6 20:58 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 111K Aug  6 20:58 total_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 169M Aug  6 20:58 total_control_lambda.bdg
-rw-rw-r-- 1 wangjl wangjl 168M Aug  6 20:58 total_treat_pileup.bdg

1417  14170 101274 macs2_result/06/total_peaks.narrowPeak
峰没变化。多了2个文件。




7) 实测版 02: 不基于模型, 输入 BAMPE，其他默认
$ macs2 callpeak -t $input -f BAMPE -g hs \
--shift -100 --extsize 200 --nomodel \
--outdir macs2_result/07/ -n total

$ ls -lth macs2_result/07/
total 132K
-rw-rw-r-- 1 wangjl wangjl 45K Aug  6 21:16 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 50K Aug  6 21:16 total_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 30K Aug  6 21:16 total_summits.bed

639  6390 45279 macs2_result/07/total_peaks.narrowPeak
$ head macs2_result/07/total_peaks.narrowPeak
chr1    88142   88357   total_peak_1    44      .       6.07583 9.48514 4.4531  38
chr1    191420  191562  total_peak_2    41      .       6.24604 9.12643 4.1663  114
chr1    633852  634239  total_peak_3    2030    .       47.0835 211.544 203.048 181
chr1    778606  778836  total_peak_4    28      .       5.36401 7.26307 2.82205 148
chr1    820987  821182  total_peak_5    27      .       4.59181 7.11394 2.72181 70
chr1    1724272 1724438 total_peak_6    35      .       5.6817  8.2884  3.53621 113

这是目前最少的峰了。



8) 实测版 02: 不基于模型, 改变shift=0
$ macs2 callpeak -t $input -f BAM -g hs \
--shift 0 --extsize 200 --nomodel \
-B --SPMR --call-summits \
--outdir macs2_result/08/ -n total

$ ls -lth macs2_result/08/
total 337M
-rw-rw-r-- 1 wangjl wangjl  83K Aug  6 21:30 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  55K Aug  6 21:30 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl  92K Aug  6 21:30 total_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 169M Aug  6 21:30 total_control_lambda.bdg
-rw-rw-r-- 1 wangjl wangjl 168M Aug  6 21:30 total_treat_pileup.bdg

1180 11800 84103 macs2_result/08/total_peaks.narrowPeak
$ head macs2_result/08/total_peaks.narrowPeak
chr1    17317   17521   total_peak_1    53      .       6.61902 10.1308 5.31352 102
chr1    88085   88357   total_peak_2    40      .       5.7377  8.38894 4.0457  103
chr1    128638  128838  total_peak_3    30      .       4.91803 6.88453 3.04562 93
chr1    181343  181573  total_peak_4    39      .       5.88235 8.29379 3.98104 130



9) 实测版 02: 不基于模型, 改变shift=-73
$ macs2 callpeak -t $input -f BAM -g hs \
--shift -73 --extsize 146 --nomodel \
-B --SPMR --call-summits \
--outdir macs2_result/09/ -n total

$ ls -lth macs2_result/09/
total 337M
-rw-rw-r-- 1 wangjl wangjl  54K Aug  6 21:33 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl  81K Aug  6 21:33 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  91K Aug  6 21:33 total_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 169M Aug  6 21:33 total_control_lambda.bdg
-rw-rw-r-- 1 wangjl wangjl 168M Aug  6 21:33 total_treat_pileup.bdg

1157  5785 54380 macs2_result/09/total_summits.bed






10) 实测版 02: 不基于模型, 改变shift=-209/2  使用 macs2 预测的插入片段长度
$ macs2 predictd -i merged/total.bam
## predicted fragment length is 209 bps

$ macs2 callpeak -t $input -f BAM -g hs \
--shift -105 --extsize 210 --nomodel \
-B --SPMR --call-summits \
--outdir macs2_result/10/ -n total

1452  14520 104221 macs2_result/10/total_peaks.narrowPeak
$ head macs2_result/10/total_peaks.narrowPeak
chr1    17412   17626   total_peak_1    53      .       6.56238 9.96532 5.33633 107
chr1    88118   88406   total_peak_2    40      .       5.68643 8.24479 4.0876  152
chr1    128542  128752  total_peak_3    30      .       4.87409 6.76147 3.06804 121
chr1    181238  181518  total_peak_4    50      .       6.5312  9.50983 5.00517 150

比 extsize=200 的1417个峰多了一点。




ii) --extsize 290 时：这个290是使用模型预测时，模型图给出的290.
$ ls -lth macs2_result/10/
total 337M
-rw-rw-r-- 1 wangjl wangjl 121K Aug  6 22:00 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  79K Aug  6 22:00 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl 134K Aug  6 22:00 total_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 169M Aug  6 22:00 total_control_lambda.bdg
-rw-rw-r-- 1 wangjl wangjl 169M Aug  6 22:00 total_treat_pileup.bdg

1708  17080 123049 macs2_result/10/total_peaks.narrowPeak

$ head macs2_result/10/total_peaks.narrowPeak
chr1    17372   17666   total_peak_1    48      .       6.14192 8.87601 4.80747 147
chr1    88078   88446   total_peak_2    46      .       6.0652  8.69812 4.68379 178
chr1    128502  128792  total_peak_3    27      .       4.5489  5.95299 2.76385 164
chr1    181193  181558  total_peak_4    53      .       6.5703  9.58873 5.3512  184
chr1    191370  191678  total_peak_5    45      .       5.9988  8.58116 4.59402 145
chr1    633843  634367  total_peak_6    612     .       18.2501 67.9451 61.2553 232
chr1    778465  778873  total_peak_7    51      .       5.93878 9.29499 5.13861 203
chr1    820745  821242  total_peak_8    21      .       3.02343 4.93823 2.15209 230
chr1    832210  832500  total_peak_9    20      .       3.46457 4.74838 2.0379  115
chr1    1074805 1075108 total_peak_10   23      .       2.72412 5.27212 2.3442  144

这是目前最多的峰。
可以使用模型预测，看pdf图中的长度，作为 extsize 再使用 非模型，做一次。






11) 实测版 02: 不基于模型, 改变shift=-1000/2 ，是不是越大峰越多
$ macs2 callpeak -t $input -f BAM -g hs \
--shift -500 --extsize 1000 --nomodel \
-B --SPMR --call-summits \
--outdir macs2_result/11/ -n total

1155 11550 82765 macs2_result/11/total_peaks.narrowPeak






12) 对合并后的bam sort一下呢？
$ samtools sort -@ 4 -O bam -o merged/total.sort.bam merged/total.bam
$ macs2 callpeak -t merged/total.sort.bam -f BAM -g hs --shift -100 --extsize 200 --nomodel --call-summits --outdir macs2_result/12/ -n total

3560  35600 256468 macs2_result/12/total_peaks.narrowPeak
结果和原来一样。





13) --keep-dup all 参数 
--keep-dup KEEPDUPLICATES
It controls the behavior towards duplicate tags at the exact same location -- the same coordination and the same strand. 
The 'auto' option makes MACS calculate the maximum tags at the exact same location based on binomal distribution using 1e-5 as pvalue cutoff; and the 'all' option keeps every tags. 
If an integer is given, at most this number of tags will be kept at the same location. 
The default is to keep one tag at the same location. Default: 1
该参数控制着重复片段：相同链上的相同坐标。
	'auto' 让MACS计算同一个点点的最多tag，基于二项分布，1e-5作为p值 cutoff；
	'all' 保留所有 tag。
	如果给整数，表示同一个位点最多保留reads数。默认是1.

Note, if you've used samtools or picard to flag reads as 'PCR/Optical duplicate' in bit 1024, MACS2 will still read them although the reads may be decided by MACS2 as duplicate later. 
If you plan to rely on samtools/picard/any other tool to filter duplicates, please remove those duplicate reads and save a new alignment file then ask MACS2 to keep all by '--keep-dup all'. 
注意：如果使用其他工具识别重复序列，就先去掉重复，然后设置 macs2 的参数 '--keep-dup all'
按这个描述，应该加上！

$ macs2 callpeak --keep-dup all  -t $input -f BAM -g hs --shift -100 --extsize 200 --nomodel --call-summits --outdir macs2_result/13/ -n total

4135  41350 297537 macs2_result/13/total_peaks.narrowPeak
这是目前最多的峰。



## for all
$ macs2 callpeak --keep-dup all  -t merged/total.bam -f BAM -g hs --shift -100 --extsize 200 --nomodel --call-summits --outdir macs2_result/13_/ -n total
17178  171780 1245371 macs2_result/13_/total_peaks.narrowPeak
现在终于上一万了：这是目前最多的峰。







14) 尝试使用bed文件 call peak

$ echo $input
merged/total_1_20.bam

#BAM to BED（可以使用bedtools的bamToBed工具）
$ bedtools bamtobed -i merged/total_1_20.bam >merged/total_1_20.bed
$ samtools view merged/total_1_20.bam|wc
6167048 105261434 1307655743
$ wc merged/total_1_20.bed
6167048  37002288 444016703 merged/total_1_20.bed
共6million PE reads == 3million fragment.

$ head merged/total_1_20.bed
chr1    10092   10128   NB502151:36:HJWCLBGXJ:2:23308:5252:9191/1       32      +
chr1    10098   10135   NB502151:36:HJWCLBGXJ:4:13402:10236:19044/1     30      +
chr1    10165   10203   NB502151:36:HJWCLBGXJ:4:13402:10236:19044/2     30      -


$ macs2 callpeak --keep-dup all  -t merged/total_1_20.bed -f BED -g hs --shift -100 --extsize 200 --nomodel --call-summits --outdir macs2_result/14/ -n total
#1  total tags in treatment: 6167048
try1:报错：struct.error: unpack requires a buffer of 4 bytes 可能是因为文件格式 -f 没指定好。

29553  295530 2157855 macs2_result/14/total_peaks.narrowPeak
这就2.9万个峰了！！

$ sort -k7nr macs2_result/14/total_peaks.narrowPeak|head | column -t
chr1   633888     634248     total_peak_13      6100  .  41.0948  618.525  610.029  161
chr20  28513494   28513833   total_peak_24786   619   .  24.5455  67.5991  61.9005  156
chr20  44843410   44843714   total_peak_26371   1821  .  20.8511  188.176  182.14   134
chr1   228620821  228621169  total_peak_20184   273   .  15.8824  32.5124  27.3884  166
chr1   121184474  121184758  total_peak_11711   258   .  15.2941  30.9257  25.849   146
chr1   206202920  206203371  total_peak_17916   263   .  14.8936  31.4074  26.3158  242
chr20  31157634   31158218   total_peak_25057a  591   .  14.2857  64.8298  59.1845  131

$ sort -k7n macs2_result/14/total_peaks.narrowPeak|head | column -t
chr1   16924260   16924585   total_peak_1530    13  .  1.70388  2.65224  1.31456  86
chr1   155008403  155008658  total_peak_12742   13  .  1.75202  2.64877  1.31247  218
chr1   26282545   26282780   total_peak_2404    14  .  1.75659  2.78987  1.4056   96
chr1   155062397  155062638  total_peak_12745   13  .  1.77305  2.65746  1.3176   119
chr1   204149708  204150071  total_peak_17715   13  .  1.78317  2.69161  1.34133  145
chr20  33664031   33664411   total_peak_25280a  13  .  1.78571  2.70021  1.34544  71
chr1   16975286   16975486   total_peak_1538    13  .  1.80636  2.76992  1.39232  84
chr1   2327938    2328298    total_peak_200     14  .  1.80807  2.84397  1.4426   245
最后这个区域全部都是峰？



##测试：去掉bed文件第4列，发现的峰一样多，也就说标准bed输入时，macs2没使用“双端测序”这个信息！
$ awk '{print $1"\t"$2"\t"$3"\t*\t"$5"\t"$6}' merged/total_1_20.bed > merged/total_1_20-v2.bed 
$ head merged/total_1_20-v2.bed 
chr1    10092   10128   *       32      +
chr1    10098   10135   *       30      +
chr1    10165   10203   *       30      -

$ macs2 callpeak --keep-dup all  -t merged/total_1_20-v2.bed -f BED -g hs --shift -100 --extsize 200 --nomodel --call-summits --outdir macs2_result/14_2/ -n total
#1  total tags in treatment: 6167048
29553  295530 2157855 macs2_result/14_2/total_peaks.narrowPeak




### For all reads 
$ bedtools bamtobed -i merged/total.bam >merged/total.bed
$ wc merged/total.bed
$ head merged/total.bed

$ macs2 callpeak --keep-dup all  -t merged/total.bed -f BED -g hs --shift -100 --extsize 200 --nomodel --call-summits --outdir macs2_result/14_/ -n total
使用bed文件确实慢。
#1  total tags in treatment: 51916420 => 51 million reads;

134677 1346770 9913942 macs2_result/14_/total_peaks.narrowPeak
## 这就 13万个peak了？？

$ sort -k7nr macs2_result/14_/total_peaks.narrowPeak | head|column -t
chr21  8234289    8234727    total_peak_74567   14798  .  41.1765  1486.88  1479.87  213
chr1   633890     634246     total_peak_2       6121   .  41.0948  618.525  612.18   159
chr21  8219741    8220283    total_peak_74566   8224   .  40.2941  828.89   822.421  273
chr10  133689258  133689623  total_peak_18567   3590   .  39.3333  365.119  359.013  173
chr17  22521267   22521601   total_peak_49268   11343  .  39.0016  1141.11  1134.32  135

$ sort -k7n macs2_result/14_/total_peaks.narrowPeak | head|column -t
chrY   56832523  56832865  total_peak_128173b  19  .  1.4963   3.91013  1.95375  325
chr16  46389933  46390898  total_peak_45405a   73  .  1.55011  10.9063  7.35045  113
chrY   56833302  56833513  total_peak_128174   26  .  1.56869  4.84994  2.62797  103
chr16  46388673  46388873  total_peak_45404    69  .  1.61464  10.4024  6.94128  92
chr21  10420080  10420306  total_peak_74620    13  .  1.79211  3.05371  1.39901  114









15) 参数 -bedpe 生成PE的bed，再 call peak(感觉还是有问题，插入片段太大了)
https://www.biostars.org/p/187204/#187206

try1:
$ bedtools bamtobed -bedpe -i merged/total_1_20.bam >merged/total_1_20.pe.bed
很多warning：*****WARNING: Query NB502151:36:HJWCLBGXJ:3:13406:17749:9463 is marked as paired, but its mate does not occur next to it in your BAM file.  Skipping. 


try2: https://yiweiniu.github.io/blog/2019/03/ATAC-seq-data-analysis-from-FASTQ-to-peaks/
# the BAM file should be sorted by read name beforehand
$ samtools sort -n -T tmp/1_20 -o merged/total_1_20.sorted.bam merged/total_1_20.bam

# The bedtools command should extract the paired-end alignments as bedpe format, then the awk command should shift the fragments as needed
$ bedtools bamtobed -i merged/total_1_20.sorted.bam -bedpe 2>merged/total_1_20.sorted.bed.log | awk -v OFS="\t" '{if($9=="+"){print $1,$2+4,$6+4}else if($9=="-"){print $1,$2-5,$6-5}}' >merged/total_1_20.sorted.bed


## check 中间文件
$ bedtools bamtobed -i merged/total_1_20.sorted.bam -bedpe 2>/dev/null >merged/total_1_20.sorted.PE.bed
3051104  30511040 292292978 merged/total_1_20.sorted.PE.bed
$ cat merged/total_1_20.sorted.PE.bed | head | column -t
chr1   95346712   95346741   chr1   95346808   95346845   NB502151:36:HJWCLBGXJ:1:11101:1061:13846  40  +  -
chr1   216089113  216089151  chr1   216089481  216089518  NB502151:36:HJWCLBGXJ:1:11101:1072:11727  60  +  -
chr1   34843720   34843757   chr1   34843774   34843812   NB502151:36:HJWCLBGXJ:1:11101:1084:6959   60  +  -
chr1   31331262   31331299   chr1   31331306   31331344   NB502151:36:HJWCLBGXJ:1:11101:1138:1789   40  +  -
chr1   170822853  170822890  chr1   170822883  170822921  NB502151:36:HJWCLBGXJ:1:11101:1165:7121   60  +  -
chr20  62435035   62435069   chr20  62435059   62435097   NB502151:36:HJWCLBGXJ:1:11101:1184:5665   60  +  -
chr1   43434059   43434095   chr1   43434097   43434135   NB502151:36:HJWCLBGXJ:1:11101:1207:18273  60  +  -

$ cat merged/total_1_20.sorted.PE.bed | awk '{print $9}' | sort | uniq -c
    943 -  ##为什么负链上这么少呢？ //todo
3050161 +

$ awk '$9=="-"' merged/total_1_20.sorted.PE.bed | head | column -t
chr20  28561526   28561563   chr20  28561537   28561574   NB502151:36:HJWCLBGXJ:1:11105:7193:1990    60  -  +
chr1   43904759   43904796   chr1   43904760   43904789   NB502151:36:HJWCLBGXJ:1:11105:19838:9761   40  -  +
chr20  51867817   51867854   chr20  51867817   51867846   NB502151:36:HJWCLBGXJ:1:11105:22690:19578  60  -  +
chr20  29881294   29881332   chr20  29881307   29881344   NB502151:36:HJWCLBGXJ:1:11109:16781:16970  31  -  +

$ wc merged/total_1_20.sorted.bed.log # 这0.064million reads(占比2.1%)是不能配对的，可能在按 MAPQ 过滤时去掉的。
64840 1361640 9803451 merged/total_1_20.sorted.bed.log
$ head merged/total_1_20.sorted.bed.log
*****WARNING: Query NB502151:36:HJWCLBGXJ:1:11101:1326:6825 is marked as paired, but its mate does not occur next to it in your BAM file.  Skipping. 
*****WARNING: Query NB502151:36:HJWCLBGXJ:1:11101:1512:10500 is marked as paired, but its mate does not occur next to it in your BAM file.  Skipping. 
*****WARNING: Query NB502151:36:HJWCLBGXJ:1:11101:1986:1121 is marked as paired, but its mate does not occur next to it in your BAM file.  Skipping.

$ wc merged/total_1_20.sorted.bed
3051104  9153312 72613608 merged/total_1_20.sorted.bed
$ head merged/total_1_20.sorted.bed
chr1    95346716        95346849
chr1    216089117       216089522
chr1    34843724        34843816
chr1    31331266        31331348
chr1    170822857       170822925
## 3051104  9153312 72613608 merged/total_1_20.sorted.bed



try3:使用参数 -f BEDPE 报错。
$ macs2 callpeak --keep-dup all  -t merged/total_1_20.sorted.bed -f BEDPE -g hs --shift -100 --extsize 200 --nomodel --call-summits --outdir macs2_result/15/ -n total
## AssertionError: Right position must be larger than left position, check your BED file at line: b'chr1\t107299657\t4602297\n'
## 找原因
$ cat merged/total_1_20.sorted.bed | grep "107299657"
chr1    107299657       4602297
chr1    107299657       4602297
$ cat merged/total_1_20.sorted.PE.bed | grep "107299653"
chr1    107299653       107299685       chr20   4602255 4602293 NB502151:36:HJWCLBGXJ:1:21302:26815:16322       60      +       -
chr1    107299653       107299685       chr20   4602255 4602293 NB502151:36:HJWCLBGXJ:2:23205:22166:16656       60      +       -
==> 原因是染色体不同，理应跳过的！这可能是染色体重组？更提前的措施是，比对时设置最大跨度！



try4: 第1和第4列同一个染色体上才能用 （感觉缺口矫正有问题，看 try6）
$ cat merged/total_1_20.sorted.PE.bed | awk -v OFS="\t" '$1==$4{if($9=="+"){print $1,$2+4,$6+4}else if($9=="-"){print $1,$2-5,$6-5}}' >merged/total_1_20.sorted2.bed
## 3051072  9153216 72612866 merged/total_1_20.sorted2.bed
$ head merged/total_1_20.sorted2.bed
chr1    95346716        95346849
chr1    216089117       216089522
chr1    34843724        34843816
chr1    31331266        31331348

$ macs2 callpeak --keep-dup all  -t merged/total_1_20.sorted2.bed -f BEDPE -g hs --shift -100 --extsize 200 --nomodel --outdir macs2_result/15/ -n total
#1 fragment size = 2232.4  # 这个值很异常！
#1  total fragments in treatment: 3051072

1856  18560 135767 macs2_result/15/total_peaks.narrowPeak


try5: 尝试测试插入大小
$ cat merged/total_1_20.sorted2.bed | awk '{print $0"\t"$3-$2}' | sort -k4nr >tmp.txt
$ head tmp.txt
chr1    23169911        232508892       209338981
chr1    24638794        226367125       201728331
chr1    24638794        226367125       201728331
chr1    45217695        233662221       188444526
chr1    10639928        197354846       186714918
$ tail tmp.txt
chr20   57818615        57818634        19
chr20   58221174        58221193        19
chr20   59838079        59838098        19
chr20   61562166        61562185        19
chr20   61566949        61566968        19
chr20   61611066        61611085        19
chr20   62126377        62126396        19
chr20   62710587        62710606        19
chr20   7416730 7416749 19
chr20   8706493 8706512 19
## 超过500的占了多少？
$ cat tmp.txt | awk '{if($4>500){print "T"}else{print "F"} }' | sort | uniq -c 
$ cat tmp.txt | awk '{if($4>500){print "T"}else{print "F"} }' | sort | uniq -c 
3035699 F
  15373 T
$ cat tmp.txt | awk '{if($4>1000){print "T"}else{print "F"} }' | sort | uniq -c 
3050936 F
    136 T
按照500过滤一下
$ cat merged/total_1_20.sorted2.bed | awk '$3-$2<500{print $0}' >merged/total_1_20.sorted2_.bed
$ macs2 callpeak --keep-dup all -t merged/total_1_20.sorted2_.bed -f BEDPE -g hs --shift -100 --extsize 200 --nomodel --outdir macs2_result/15/ -n total
#1 fragment size = 117.0 
#1  total fragments in treatment: 3035496
2529  25290 180815 macs2_result/15/total_peaks.narrowPeak


比较一下这两个bed文件的差异。
merged/total_1_20.sorted2.bed 本测试15 3051072
merged/total_1_20.PE.bed 下一个测试16  3025179
## for 16
$ cat merged/total_1_20.PE.bed | awk '{print $0"\t"$3-$2}' | sort -k4nr >tmp2.txt
$ head tmp2.txt 
chr1    233004825       233005413       588
chr20   57027943        57028531        588
chr1    157606904       157607491       587
chr20   58208877        58209464        587
chr1    172472203       172472789       586

$ tail tmp2.txt 
chr1    1427450 1427452 2
chr1    200487872       200487874       2
chr1    202624801       202624803       2


check:
$ sort -k7nr macs2_result/15/total_peaks.narrowPeak | head | column -t
chr1  161402366  161475898  total_peak_1292  273  .  7.54983  28.3794  27.3213  56913
chr1  206149950  206412109  total_peak_1571  186  .  6.25922  19.6376  18.6038  53167
chr1  219592584  219616892  total_peak_1819  117  .  5.98008  12.7457  11.7678  11958
chr1  223996771  224008332  total_peak_1848  108  .  5.70826  11.8287  10.8694  8239
chr1  16732851   16760860   total_peak_8     63   .  4.25813  7.11051  6.34689  26519
chr1  223561966  223605231  total_peak_1845  63   .  4.25813  7.11051  6.34689  1863


最后也是连片的区域 
chr1:897,330-924,072
chr1:16,610,910-16,637,652






try6: 感觉try4不应该整体移动, 正链+4，负链-5 向着中心移动。
$ cat merged/total_1_20.sorted.PE.bed | awk -v OFS="\t" '$1==$4{if($9=="+"){print $1,$2+4,$6-5}else if($9=="-"){print $1,$2-5,$6+4}}' >merged/total_1_20.sorted3.bed
$ head merged/total_1_20.sorted3.bed
chr1    95346716        95346840
chr1    216089117       216089513
chr1    34843724        34843807
chr1    31331266        31331339

$ cat merged/total_1_20.sorted3.bed | awk '$3-$2<500{print $0}' >merged/total_1_20.sorted3_.bed
3051072  9153216 72612866 merged/total_1_20.sorted3.bed
3037273  9111819 72282913 merged/total_1_20.sorted3_.bed
$ macs2 callpeak --keep-dup all -t merged/total_1_20.sorted3_.bed -f BEDPE -g hs --nomodel --outdir macs2_result/15/ -n total
#1 fragment size = 108.2 ##参数正常了
#1  total fragments in treatment: 3037273
1938  19380 138369 macs2_result/15/total_peaks.narrowPeak








16) 再试 bed PE 模式(官方转bed PE)
https://pypi.org/project/MACS2/

$ macs2 randsample -i merged/total_1_20.bam -f BAMPE -p 100 -o merged/total_1_20.PE.bed
## # total fragments/pairs in alignment file: 3025179 
## 参数解释: -p 抽样百分数

$ wc merged/total_1_20.PE.bed
 3025179  9075537 71989774 merged/total_1_20.PE.bed
$ head merged/total_1_20.PE.bed
chr20   60209   60386
chr20   60284   60378
chr20   60285   60416
chr20   60294   60356
chr20   60304   60381

$ macs2 callpeak --keep-dup all  -t merged/total_1_20.PE.bed -f BEDPE -g hs --shift -100 --extsize 200 --nomodel --outdir macs2_result/16/ -n total
#1 fragment size = 116.7
#1  total fragments in treatment: 3025179
2817  28170 201633 macs2_result/16/total_peaks.narrowPeak







17) 实测版 02: 不基于模型, 输入 BAMPE，其他默认
注释：7) 没有使用 --keep-dup all

$ macs2 callpeak --keep-dup all  -t $input -f BAMPE -g hs --shift -100 --extsize 200 --nomodel --outdir macs2_result/17/ -n total
#1 fragment size = 116.7
#1  total fragments in treatment: 3025179
2817  28170 201633 macs2_result/17/total_peaks.narrowPeak


## For all:
$ macs2 callpeak --keep-dup all  -t merged/total.bam -f BAMPE -g hs --shift -100 --extsize 200 --nomodel --outdir macs2_result/17_/ -n total
#1 fragment size = 118.2 
#1  total fragments in treatment: 25450338
7809  78090 560519 macs2_result/17_/total_peaks.narrowPeak






18) 实测版 02: 不基于模型, 输入 BAMPE，不使用shift模型：--shift -100 --extsize 200 
--nomodel 参数也没作用，结果一模一样

$ macs2 callpeak --keep-dup all  -t $input -f BAMPE -g hs --nomodel --outdir macs2_result/18/ -n total
$ macs2 callpeak --keep-dup all -t $input -f BAMPE -g hs --outdir macs2_result/18_2/ -n total
#1 fragment size = 116.7 
#1  total fragments in treatment: 3025179
2817  28170 201633 macs2_result/18/total_peaks.narrowPeak


## For all:
$ macs2 callpeak --keep-dup all  -t merged/total.bam -f BAMPE -g hs --outdir macs2_result/18_/ -n total
#1 fragment size = 118.2
#1  total fragments in treatment: 25450338
7809  78090 560519 macs2_result/18_/total_peaks.narrowPeak






19) 实测版 02: 不基于模型, 输入 BAM，使用shift模型：
$ macs2 callpeak --keep-dup all  -t $input -f BAM -g hs \
--shift -100 --extsize 200 --nomodel \
--outdir macs2_result/19/ -n total
#1 tag size = 37.0 
#1  total tags in treatment: 3025179
4081  40810 293153 macs2_result/19/total_peaks.narrowPeak

$ sort -k7nr macs2_result/19/total_peaks.narrowPeak | head|column -t
chr1   633895     634277     total_peak_7     2935  .  37.5     302.045  293.549  186
chr20  44843410   44843714   total_peak_3828  901   .  19.112   96.2704  90.1547  144
chr20  28513494   28513833   total_peak_3585  204   .  14.0845  26.0948  20.4998  184
chr1   206202988  206203335  total_peak_2636  155   .  11.8056  20.7927  15.5817  184
chr1   187317487  187317700  total_peak_2350  206   .  11.1111  26.2241  20.6174  126
chr20  29080367   29080675   total_peak_3602  200   .  11.0687  25.5501  20.0118  11

$ sort -k7nr macs2_result/19/total_peaks.narrowPeak | tail|column -t
chr20  33852223   33852423   total_peak_3703  16  .  2.76596  3.88464  1.64723  4
chr20  45963881   45964108   total_peak_3848  20  .  2.75974  4.53365  2.01844  65
chr1   17694870   17695085   total_peak_239   19  .  2.73312  4.47242  1.98304  145
chr1   27557500   27557735   total_peak_357   21  .  2.69886  4.71593  2.12677  169
chr1   54748267   54748539   total_peak_734   19  .  2.69841  4.39229  1.93545  97
chr1   209805109  209805336  total_peak_2675  19  .  2.68987  4.37252  1.92258  51
chr1   3999801    4000034    total_peak_71    18  .  2.67559  4.18095  1.81391  101
chr20  62500715   62500915   total_peak_4046  18  .  2.61538  4.19916  1.81997  178
chr1   153566477  153566677  total_peak_1828  21  .  2.59259  4.75008  2.14647  105
chr1   26282580   26282780   total_peak_335   18  .  2.44186  4.33258  1.8987   185


## for all 
$ macs2 callpeak --keep-dup all  -t  merged/total.bam -f BAM -g hs \
--shift -100 --extsize 200 --nomodel \
--outdir macs2_result/19_/ -n total
#1 tag size = 37.0 
#1  total tags in treatment: 25450338
16849  168490 1219965 macs2_result/19_/total_peaks.narrowPeak




20) PE 加参数: --cutoff-analysis
--cutoff-analysis     While set, MACS2 will analyze number or total length of peaks that can be called by different p-value cutoff then output a summary table to help user decide a better cutoff. 
The table will be saved in NAME_cutoff_analysis.txt file. Note, minlen and maxgap may affect the results. 
WARNING: May take ~30 folds longer time to finish. 
The result can be useful for users to decide a reasonable cutoff value. DEFAULT: False

设置后，输出不同的p值，方便用户决定哪个cutoff值更好。
输出到文件 NAME_cutoff_analysis.txt。注意： minlen 和 maxgap 会影响到这个结果。
警告：可能需要30倍的时间。默认 False。

$ macs2 callpeak --keep-dup all  -t $input -f BAMPE -g hs \
--cutoff-analysis \
--outdir macs2_result/20/ -n total
#1 fragment size = 116.7
#1  total fragments in treatment: 3025179
2817  28170 201633 macs2_result/20/total_peaks.narrowPeak

$ less macs2_result/20/total_cutoff_analysis.txt #34行
pscore  qscore  npeaks  lpeaks  avelpeak
9.90    5.74    71      12598   177.44
9.60    5.48    82      14334   174.80
9.30    5.21    90      15451   171.68
9.00    4.97    100     17105   171.05
8.70    4.72    112     19405   173.26
8.40    4.46    124     21372   172.35
8.10    4.21    137     23666   172.74
7.80    3.97    146     25982   177.96
7.50    3.72    163     28998   177.90
7.20    3.48    183     32527   177.74
6.90    3.24    204     36491   178.88
6.60    3.01    229     41106   179.50
...
5.10    2.01    594     109636  184.57
4.80    1.84    783     143463  183.22
4.50    1.68    1060    194905  183.87
4.20    1.52    1527    278563  182.43
3.90    1.38    2284    416494  182.35
3.60    1.24    3474    640088  184.25
3.30    1.11    5521    1026117 185.86
3.00    0.98    8782    1656993 188.68
2.70    0.86    14254   2765520 194.02
2.40    0.74    23127   4663168 201.63
2.10    0.63    38554   8178000 212.12
1.80    0.53    62405   14088883        225.77
1.50    0.43    98484   24233112        246.06
1.20    0.33    149622  41603662        278.06
0.90    0.23    213639  105953896       495.95
0.60    0.23    267658  157547834       588.62
0.30    0.15    210496  235975642       1121.05

尝试解读上表：
-log10(0.05)=1.30103，大概对应着中间q值1.24-1.38之间，峰的个数也是 3474-2284 之间，是2817.

查帮助文件 
Peak calling arguments:
-q QVALUE, --qvalue QVALUE    Minimum FDR (q-value) cutoff for peak detection. DEFAULT: 0.05. -q, and -p are mutually exclusive.
	检测峰的最小q值。默认是 0.05
-p PVALUE, --pvalue PVALUE    Pvalue cutoff for peak detection. DEFAULT: not set.
	-q, and -p are mutually exclusive. If pvalue cutoff is set, qvalue will not be calculated and reported as -1 in the final .xls file.
	如果设置了p值，则不计算q值，并且最终xls文件中的q记为-1.

> -log10(0.01) #[1] 2
也就说q设置为2则可能只有600个峰。
$ macs2 callpeak --keep-dup all -t $input -f BAMPE -g hs \
-q 0.01 \
--outdir macs2_result/20_/ -n total
601  6010 42521 macs2_result/20_/total_peaks.narrowPeak
确实，说明我的理解是对的。

但是，第4列是什么？






21) 尝试IGV track图：-B参数，多输出2个 bedGraph 文件
$ macs2 callpeak --keep-dup all -t $input -f BAMPE -g hs \
-B \
--outdir macs2_result/21/ -n total
2817  28170 201633 macs2_result/21/total_peaks.narrowPeak

$ ls -lth macs2_result/21/
total 495M
-rw-rw-r-- 1 wangjl wangjl 131K Aug 11 15:24 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl 197K Aug 11 15:24 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 219K Aug 11 15:24 total_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 328M Aug 11 15:24 total_control_lambda.bdg
-rw-rw-r-- 1 wangjl wangjl 167M Aug 11 15:24 total_treat_pileup.bdg
多了2个文件，还挺大！
10787912  43151648 343090268 macs2_result/21/total_control_lambda.bdg
$ head macs2_result/21/total_control_lambda.bdg 
chr1    0       12517   0.13071
chr1    12517   12520   0.13415
chr1    12520   12521   0.14582
chr1    12521   12525   0.15165
chr1    12525   12542   0.15748
chr1    12542   13828   0.16332
chr1    13828   13894   0.16915
chr1    13894   15092   0.17498

5498630  21994520 174883243 macs2_result/21/total_treat_pileup.bdg
$ head macs2_result/21/total_treat_pileup.bdg
chr1    0       10092   0.00000
chr1    10092   10098   1.00000
chr1    10098   10170   2.00000
chr1    10170   10171   3.00000
chr1    10171   10203   4.00000
chr1    10203   10272   3.00000

尝试载入IGV中。
pileup图很像是峰图，可能是平移过后的峰图。
lambda 不知道是什么，可能是背景噪音。


If -B parameter was used when running macs2 callpeak, you would get bedGraph files together with narrowPeak files. Someone would use these bedGraph files to create browser tracks (e.g. ParkerLab - ATAC-seq lab for BIOINF545), while others say they look kind of weird (Biostars - How to compare bigwig tracks of two ATAC libraries). I do not know yet.

## I would like to create bigWig files for visualizing using bamCoverage in deepTools. It provides several different ways to normalize the signal.
# bam to bigwig, normalize using 1x effective genome size
# effective genome size: https://deeptools.readthedocs.io/en/latest/content/feature/effectiveGenomeSize.html
$ bamCoverage --numberOfProcessors 8 --binSize 10 --normalizeUsing RPGC \
  --effectiveGenomeSize $effect_genome_size --bam sample1.shifted.bam -o sample1.shifted.bw



##try2: 尝试 -B --SPMR (signal per million reads for fragment pileup profiles. 相当于是标准化了，多个样本之间可比较了)
$ macs2 callpeak --keep-dup all -t $input -f BAMPE -g hs \
-B --SPMR \
--outdir macs2_result/21_/ -n total

#3 In the peak calling step, the following will be performed simultaneously:                                                                                     
#3   Write bedGraph files for treatment pileup (after scaling if necessary)... total_treat_pileup.bdg                                                            
#3   Write bedGraph files for control lambda (after scaling if necessary)... total_control_lambda.bdg
#3   --SPMR is requested, so pileup will be normalized by sequencing depth in million reads.

2817  28170 201633 macs2_result/21_/total_peaks.narrowPeak


$ ls -lth macs2_result/21_/ 
total 495M
-rw-rw-r-- 1 wangjl wangjl 197K Aug 11 16:00 total_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl 131K Aug 11 16:00 total_summits.bed
-rw-rw-r-- 1 wangjl wangjl 219K Aug 11 16:00 total_peaks.xls
-rw-rw-r-- 1 wangjl wangjl 328M Aug 11 16:00 total_control_lambda.bdg
-rw-rw-r-- 1 wangjl wangjl 167M Aug 11 16:00 total_treat_pileup.bdg

从IGV中看 pileup 最高峰有点低，相比这只有 -B 参数。





22) --nolambda 参数
$ macs2 callpeak --keep-dup all -t $input -f BAMPE -g hs \
--nolambda \
-B --SPMR --outdir macs2_result/22/ -n total

224225  2242250 16665870 macs2_result/22/total_peaks.narrowPeak
22万个峰！这是目前最多的！


IGV查看经验：
- 第7列 富集倍数 如果太小，则说明 reads 太少，没法判断是否是峰。保留7th>5
- 有些行跨度太大，好几千，看着也不是峰啊！ $3-$2<800

$ cat macs2_result/22/total_peaks.narrowPeak | awk '$3-$2>4000{print $0}' |wc
   4438   44380  334317
>2000 18508行
>1000 54617行
>800  71194行
<800 152922行  ==> 
<300  68081行
<200  37151行
<100     0

$ cat macs2_result/22/total_peaks.narrowPeak | awk '($3-$2<800 && $7>5){print $0}' > macs2_result/22/tmp
	17743  177430 1318244 macs2_result/22/tmp
$ cat macs2_result/22/tmp | head
chr1    17366   17542   total_peak_2    139     .       9.72844 17.3739 13.9072 118
chr1    88085   88405   total_peak_5    76      .       6.19083 9.93801 7.69937 95
chr1    128619  128740  total_peak_8    63      .       5.30642 8.20818 6.33241 57
chr1    133099  133435  total_peak_9    76      .       6.19083 9.93801 7.69937 88
chr1    180697  181492  total_peak_13   91      .       7.07523 11.7256 9.13118 767
chr1    629434  630190  total_peak_24   91      .       7.07523 11.7256 9.13118 502
chr1    633734  634458  total_peak_26   6787    .       214.026 687.289 678.793 299
chr1    778415  778949  total_peak_32   91      .       7.07523 11.7256 9.13118 245
chr1    779080  779365  total_peak_33   63      .       5.30642 8.20818 6.33241 148
chr1    779720  780067  total_peak_34   91      .       7.07523 11.7256 9.13118 241


$ cat macs2_result/22/tmp | sort -k7nr | head | column -t
chr1   633734     634458     total_peak_26      6787  .  214.026  687.289  678.793  299
chr1   230198039  230198738  total_peak_161699  605   .  29.1853  66.1561  60.5562  520
chr1   211811620  211812135  total_peak_146357  468   .  23.8789  51.9518  46.8257  250
chr1   187317377  187317931  total_peak_126278  446   .  22.9945  49.6367  44.6139  153
chr20  21802351   21802529   total_peak_195683  380   .  20.3413  42.7922  38.0069  56
chr1   113530489  113530991  total_peak_86986   358   .  19.4569  40.5466  35.8228  170
chr1   200365851  200366484  total_peak_138557  358   .  19.4569  40.5466  35.8228  60
chr1   150486341  150486579  total_peak_95786   294   .  16.8037  33.9295  29.4779  103
chr1   52542616   52543028   total_peak_32089   294   .  16.8037  33.9295  29.4779  284
chr20  28513495   28514049   total_peak_199195  294   .  16.8037  33.9295  29.4779  159


$ cat macs2_result/22/tmp | sort -k7nr | tail | column -t
chr20  9896976  9897287   total_peak_184941  63  .  5.30642  8.20818  6.33241  168
chr20  9922988  9923197   total_peak_184961  63  .  5.30642  8.20818  6.33241  134
chr20  994133   994614    total_peak_177508  63  .  5.30642  8.20818  6.33241  285
chr20  9947845  9948590   total_peak_184984  63  .  5.30642  8.20818  6.33241  211
chr20  995649   995871    total_peak_177511  63  .  5.30642  8.20818  6.33241  146
chr20  9956907  9957367   total_peak_184996  63  .  5.30642  8.20818  6.33241  209
chr20  9978356  9978877   total_peak_185013  63  .  5.30642  8.20818  6.33241  224
chr20  9993613  9994070   total_peak_185032  63  .  5.30642  8.20818  6.33241  195
chr20  9996873  9997352   total_peak_185037  63  .  5.30642  8.20818  6.33241  80
chr20  9999659  10000396  total_peak_185039  63  .  5.30642  8.20818  6.33241  364







23) --broad 参数
--broad       If set, MACS will try to call broad peaks using the --broad-cutoff setting. 
	Please tweak '--broad-cutoff' setting to control the peak calling behavior. 
	如果设置了， macs 会找宽峰，使用 --broad-cutoff 的设定。请微调 该值来控制峰的查找。
	
	At the meantime, either -q or -p cutoff will be used to define regions with 'stronger enrichment' inside of broad peaks. 
	同时，在宽峰内部将使用-q或-p来定义 强富集 区域。
	
	The maximum gap is expanded to 4 * MAXGAP (--max-gap parameter). 
	最大宽度是 4 * MAXGAP。参数 --max-gap
	
	As a result, MACS will output a 'gappedPeak' and a 'broadPeak' file instead of 'narrowPeak' file. 
	Note, a broad peak will be reported even if there is no 'stronger enrichment' inside. DEFAULT: False
	作为结果， macs会产生 'gappedPeak' and a 'broadPeak' 而不是 'narrowPeak' 文件。
	注意: 

--broad-cutoff BROADCUTOFF
	Cutoff for broad region. This option is not available unless --broad is set. 
	If -p is set, this is a pvalue cutoff, otherwise, it's a qvalue cutoff. 
	Please note that in broad peakcalling mode, MACS2 uses this setting to control the overall peak calling behavior, then uses -q or -p setting to define regions inside broad region as 'stronger' enrichment. DEFAULT: 0.1

--max-gap MAXGAP      Maximum gap between significant sites to cluster them together. The DEFAULT value is the detected read length/tag size.




$ macs2 callpeak --keep-dup all -t $input -f BAMPE -g hs \
--broad \
-B --SPMR --outdir macs2_result/23/ -n total

#1 fragment size = 116.7
#1  total fragments in treatment: 3025179
12513 112617 860775 macs2_result/23/total_peaks.broadPeak
(1.2万峰)

$ head macs2_result/23/total_peaks.broadPeak
chr1    10098   10300   total_peak_1    17      .       3.2399  4.47443 1.71986
chr1    17409   17525   total_peak_2    64      .       6.06884 10.0913 6.40948
chr1    78903   79704   total_peak_3    13      .       2.87775 3.84965 1.36823
chr1    88142   88393   total_peak_4    21      .       3.6085  5.03199 2.19771
chr1    181115  181486  total_peak_5    27      .       4.37492 6.00221 2.79336
chr1    190809  191614  total_peak_6    22      .       4.09041 5.21487 2.24304
chr1    633852  634239  total_peak_7    782     .       16.4946 82.8415 78.2465








小结：截止目前，峰较多的是 -100， 200 组合。
	最多的是 10) ii): 
位置	峰个数	参数与结论
/data/wangjl/ATAC/hair/data_P1/macs2_result/05/  1417 --call-summits 不应该加该参数
/data/wangjl/ATAC/hair/data_P1/macs2_result/06/  1417 -B 多2个输出文件
/data/wangjl/ATAC/hair/data_P1/macs2_result/13/  4153 --keep-dup all 如果前面用其他软件去重过，则必须加该参数。
## 13 之前的可以不看了。
/data/wangjl/ATAC/hair/data_P1/macs2_result/14/ 29553 bed (2.9万峰)感觉这个有问题；仅仅是峰多，为什么多呢？经测试(去掉第4列峰一样)，确实没使用PE这个信息。
## 15 及以后没有 --call-summits 参数
/data/wangjl/ATAC/hair/data_P1/macs2_result/15/  1938 bedtools 生成的 bedpe，做了片段移动，和16有差异
/data/wangjl/ATAC/hair/data_P1/macs2_result/16/  2817 BEDPE --keep-dup all 最可能是对的(1)

/17/   2817 BAMPE --keep-dup all 结果同16，比bed省事
/18/   2817 BAMPE --keep-dup all 结果同16，不加 shift/extsize 参数: 最可能是对的(1)
/19/   4081 BAM --keep-dup all 加 shift/extsize 参数。最多的峰。最可能是对的(2)
/20/   2817 BAMPE --cutoff-analysis --keep-dup all。结果同18。多了个q值与峰数量对照表。
/21/   2817 BAMPE --keep-dup all -B 及 --SPMR 。结果同18。多了个2个bdg文件。
## 21 及以后加上 -B --SPMR 参数，不影响结果，但是多了2个输出文件。
/22/ 224225 BAMPE (22万峰) --keep-dup all --nolambda。目前最多的峰(不靠谱?!)！ 经过初步过滤，剩下 17743 个峰(1.7万)





综述上 macs2 有2个模式:
- PE 模式
- shift/extend 模式















========================================
|-- 黑名单区域的峰不要：The DAC Exclusion List Regions (previously named "DAC Blacklisted Regions")
----------------------------------------
1. One may want to filter peaks using DAC Blacklisted Regions.

The DAC Exclusion List Regions (previously named "DAC Blacklisted Regions") aim to identify a comprehensive set of regions in the human genome that have anomalous, unstructured, high signal/read counts in next gen sequencing experiments independent of cell line and type of experiment. 

https://www.encodeproject.org/annotations/ENCSR636HFF/

Lab custom GRCh38 (ENCAN833UHG) processed data (1 File)
$ wget https://www.encodeproject.org/files/ENCFF356LFX/@@download/ENCFF356LFX.bed.gz
$ gunzip ENCFF356LFX.bed.gz

$ wc ENCFF356LFX.bed 
  910  2730 21720 ENCFF356LFX.bed

$ head ENCFF356LFX.bed 
chr1    628903  635104
chr1    5850087 5850571
chr1    8909610 8910014
chr1    9574580 9574997
chr1    32043823        32044203
chr1    33818964        33819344






2. The following code was from ENCODE - ATAC-seq Data Standards and Prototype Processing Pipeline

PEAK="${PREFIX}.narrowPeak"
FILTERED_PEAK="${PREFIX}.narrowPeak.filt.gz"
bedtools intersect -v -a ${PEAK} -b ${BLACKLIST} \
  | awk 'BEGIN{OFS="\t"} {if ($5>1000) $5=1000; print $0}' \
  | grep -P 'chr[0-9XY]+(?!_)' | gzip -nc > ${FILTERED_PEAK}

The 5th column of narroPeak file is integer score for display calculated as int(-10*log10qvalue). Since currently this value might be out of the [0-1000] range defined in UCSC Encode narrowPeak format, the awk code is to assign values greater than 1000 to 1000.

我的版本
BLACKLIST="ENCFF356LFX.bed"
PEAK="${PREFIX}.narrowPeak"
FILTERED_PEAK="${PREFIX}.narrowPeak.filt.gz"
bedtools intersect -v -a ${PEAK} -b ${BLACKLIST} \
  | awk 'BEGIN{OFS="\t"} {if ($5>1000) $5=1000; print $0}' \
  | grep -P 'chr[0-9XY]+(?!_)' | gzip -nc > ${FILTERED_PEAK}








========================================
|-- 峰注释及可视化: 饼图
----------------------------------------
1. 峰注释

$ annotatePeaks.pl macs2_result/total_peaks.narrowPeak hg38 >anno/total_peakAnn.xls 
http://homer.ucsd.edu/homer/ngs/annotation.html



2. 结果可视化：饼图

$ head anno/total_peakAnn.xls -n3
PeakID (cmd=annotatePeaks.pl macs2_result/total_peaks.narrowPeak hg38)  Chr     Start   End     Strand  Peak Score      Focus Ratio/Region Size Annotation      Detailed Annotation     Distance to TSS Nearest PromoterID      Entrez ID       Nearest Unigene Nearest Refseq   Nearest Ensembl Gene Name       Gene Alias      Gene Description        Gene Type
total_peak_1    chr1    9940    10249   +       215     NA      Intergenic      (TAACCC)n|Simple_repeat|Simple_repeat   -1780   NR_046018       100287102       Hs.618434       NR_046018       ENSG00000223972 DDX11L1 -       DEAD/H-box helicase 11 like 1   pseudo
total_peak_8    chr15   41031867        41032170        +       200     NA      intron (NM_017553, intron 24 of 35)     (CACAG)n|Simple_repeat|Simple_repeat    78547   NM_024111       79094   Hs.155569       NM_024111       ENSG00000128965 CHAC1   -       ChaC glutathione specific gamma-glutamylcyclotransferase 1       protein-coding


使用第八列画图
$ cat anno/total_peakAnn.xls | awk -F "\t" '{print $8}' | head
Annotation
Intergenic
intron (NM_017553, intron 24 of 35)
Intergenic
TTS (NR_146144)
Intergenic

$ cat anno/total_peakAnn.xls | awk -F "\t" '{print $8}' | awk -F"(" '{print $1}' | sed  -e 's/[ ]*$//g' -e "1d" > anno/anno.col8-2.txt 
$ head anno/anno.col8-2.txt 
Intergenic
intron 
Intergenic


> R
d1=readLines("anno/anno.col8-2.txt")
d2=sub(" ", '', d1)
d3=data.frame(table(d2))
colnames(d3)=c("location", "Freq")
d3=d3[order(-d3$Freq),]

pdf("anno/pie.pdf", width=4, height=4)
par(mar=c(3,2,2,6))
pie(d3$Freq, labels=paste0(d3$location, '(', d3$Freq, ')' ) )
dev.off()





3. 写到流程中
在  mainBulk.sf 中添加目标，及include语句。

$ cat rules/07_peak_annotation.sf
rule peak_annotate:
	input: "macs2_result/{sample}_peaks.narrowPeak"
	output: "anno/{sample}_peakAnn.xls"
	log: "anno/{sample}_peakAnn.xls.log"
	shell: "annotatePeaks.pl {input} hg38 >{output} 2>{log}"

rule get_8th_col:
	input: "anno/{sample}_peakAnn.xls"
	output: "anno/{sample}_anno.8th_col.txt"
	shell: "cat {input} | awk -F '\t' '{{print $8}}' | awk -F'(' '{{print $1}}' | sed  -e 's/[ ]*\$//g' -e '1d' > {output}"
	
rule draw_pie_plot:
	input: "anno/{sample}_anno.8th_col.txt"
	output: "anno/{sample}_pieplot.pdf"
	script: "../scripts/F07_pie_plot.R"

$ cat scripts/F07_pie_plot.R 
input=snakemake@input[[1]]
output=snakemake@output[[1]]

d1=readLines(input)
d2=sub(" ", '', d1)
d3=data.frame(table(d2))
colnames(d3)=c("location", "Freq")
d3=d3[order(-d3$Freq),]

pdf(output, width=4, height=4)
par(mar=c(3,2,2,6))
pie(d3$Freq, labels=paste0(d3$location, '(', d3$Freq, ')' ) )
dev.off()


工作目录中运行
$ snakemake -s /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/mainBulk.sf -j 5 -p





========================================
|-- 获取ATAC reads matrix: 行为峰，列为sample
----------------------------------------
1. 制作bed文件

$ head macs2_result/total_peaks.narrowPeak
chr1    633888  634322  total_peak_1    628     .       21.5278 69.3034 62.8234 185
chr1    1724255 1724472 total_peak_2    61      .       6.16824 11.1123 6.11602 96
chr1    2133178 2133401 total_peak_3    32      .       3.29815 7.46768 3.27843 112

$ awk '{print $1"\t"$2"\t"$3"\t"$4"\t"$5"\t"$6}' macs2_result/total_peaks.narrowPeak > macs2_result/total.bed
$ head macs2_result/total.bed
chr1    633888  634322  total_peak_1    628     .
chr1    1724255 1724472 total_peak_2    61      .
chr1    2133178 2133401 total_peak_3    32      .
chr1    3755485 3755688 total_peak_4    15      .
chr1    3857218 3857468 total_peak_5    48      .
chr1    6455050 6455251 total_peak_6    20      .




2. 按照bed区间，统计bam文件中的reads数
$ bedtools multicov

Tool:    bedtools multicov (aka multiBamCov)
Version: v2.29.2
Summary: Counts sequence coverage for multiple bams at specific loci.

Usage:   bedtools multicov [OPTIONS] -bams aln.1.bam aln.2.bam ... aln.n.bam -bed <bed/gff/vcf>

(1) 测试1：用2个bam
$ bedtools multicov -bams map_clean/P101.final.bam map_clean/P102.final.bam -bed macs2_result/total.bed | head
chr1    633888  634322  total_peak_1    628     .       2       10
chr1    1724255 1724472 total_peak_2    61      .       0       0
chr1    2133178 2133401 total_peak_3    32      .       0       0
chr1    3755485 3755688 total_peak_4    15      .       0       0
chr1    3857218 3857468 total_peak_5    48      .       0       0
chr1    6455050 6455251 total_peak_6    20      .       0       0
chr1    6613440 6613730 total_peak_7    51      .       2       0
chr1    6785235 6785458 total_peak_8    51      .       0       0
chr1    7783911 7784113 total_peak_9    25      .       0       2
chr1    7961419 7961619 total_peak_10   39      .       0       0


(2) bam全部用上
$ bedtools multicov -bams `ls map_clean/P1*.final.bam` -bed macs2_result/total.bed >matrix0.txt


(3) 使用R读取文件
/data/wangjl/ATAC/hair/data_P1/matrix0.txt
dt=read.table("/data/wangjl/ATAC/hair/data_P1/matrix0.txt", header = F)
dim(dt)
dt[1:4,1:8]
rownames(dt)=dt$V4

dt2=dt[, -c(1:6)]
colnames(dt2)=paste0("P1", seq(1,96))
dt2[1:4,1:8]
dim(dt2) #3560   96

write.table(dt2, "P1_raw_counts.txt")

然后就可以用 Seurat 分析了。






3. 写成流程

$ cat rules/08_get_matrix.sf 
rule get_bed:
	input:"macs2_result/{sample}_peaks.narrowPeak"
	output: "macs2_result/{sample}.bed"
	shell:"awk '{{print $1\"\t\"$2\"\t\"$3\"\t\"$4\"\t\"$5\"\t\"$6}}' {input} >{output}"

# 依赖每个样本的去重、去MT后的bam文件
rule get_matrix:
	input: "macs2_result/total.bed"
	output: "matrix/matrix0.txt"
	log: "matrix/matrix0.txt.log"
	shell: "bedtools multicov -bams `ls map_clean/*.final.bam` -bed {input} >{output}"

# 获取样本名字
rule get_sample_name:
	output: "matrix/colname.txt"
	shell: "ls map_clean/*.final.bam |  sed -e 's/map_clean\///' -e 's/\.final\.bam//' >{output}"

rule refine_matrix:
	input: "matrix/matrix0.txt", "matrix/colname.txt"
	output: "matrix/ATAC_raw_counts_matrix.txt"
	script: "../scripts/F08_refine_matrix.R"




$ cat scripts/F08_refine_matrix.R
input=snakemake@input[[1]]
input2=snakemake@input[[2]]
output=snakemake@output[[1]]

dt=read.table(input, header = F)
#dim(dt);dt[1:4,1:8]
rownames(dt)=dt$V4

dt2=dt[, -c(1:6)]
colnames(dt2)= readLines(input2);
# dt2[1:4,1:8]; dim(dt2)
write.table(dt2, output)




工作目录中运行
$ snakemake -s /data/wangjl/soft/snakemakeWorkflow/ATAC-seq/demo2/mainBulk.sf -j 5 -p
1min;






========================================
|-- 生成报告 html文件，可在浏览器中查看与下载结果
----------------------------------------

$ vim getReport.sf 
configfile: "config.yaml"
workdir: config["workdir"]

SI=config["samples"]

#SI=["SRR7629152","SRR7629153"]

rule all:
	input: 
		expand("QC_map/TSS/{sample}.TSS.heatmap.pdf", sample=SI), #BAM QC: TSS (very slow:40min 1e4 reads)
		"report/index.html"

include:"rules/report.sf"




###### 其他文件
$ cat rules/report.sf
# SI=["SRR7629152","SRR7629153"]
rule report:
	input:
		"images/ATAC-seq_Rule.svg"
	output:"report/index.html"
	run: 
		import time
		timsString=time.strftime("%Y/%m/%d %H:%M:%S", time.localtime()) 

		head='''
<meta charset="utf-8" />
<style>
.content{width:800px; margin:10px auto;}
h2{
	font-style: normal;
	padding-left: 20px;
	padding-right: 20px;
	margin: 10px 0;
	border-bottom: 1px solid #d3d3d3;
	font-size: 25px;
	background-image: linear-gradient(to bottom,#f3f3f3 0,#e3e3e3 100%);
	border-radius: 5px;
}
h3{
	padding: 5px 10px;
    font-size: 20px;
    text-shadow: 0 0 1px #999;
    color: #333;
    border-bottom: 1px solid #D9D9D9;
    font-weight: bold;
    margin-top: 10px;
}
.content a {
    color: #0593d3;
    text-decoration: none;
}
.h200{
	height:200px;
}
.footer{color: #aaa; background:black; border-top:20px; padding: 20px; border-top:1px solid black;}
</style>
		<title>ATAC-seq Report</title>
		'''
		
		workflow="<div class='content'><h1>ATAC-seq Report</h1>"+ timsString + "<br>" + \
		"<h2>Workflow</h2> <h3>Rules</h3>  <image class=h200 src='../images/ATAC-seq_Rule.svg'>" +\
		"<h3>Samples</h3>  <image class=h200 src='../images/ATAC-seq_Sample.svg'><br>"
		
		files='<h2>Output</h2><h3>raw/</h3> Raw fastq file' +\
		'<h3>QC_raw/</h3> <a target="_blank" href="../QC_raw/multiqc/multiqc_report.html">report</a>, <a href="../QC_raw/multiqc/multiqc_report.html.log">log</a>' +\
		'<h3>clean/</h3> fastq files after cut adaptor ' +\
		'<h3>QC_clean/</h3> <a target="_blank" href="../QC_clean/multiqc/multiqc_report.html">report</a>, <a target="_blank" href="../QC_clean/multiqc/multiqc_report.html.log">log</a>'+\
		'<h3>map/</h3> bam files after BWA mapping to hg38 '+\
		'<h3>map_clean/</h3> bam files after rm MT, low MAPQ, duplicates.'+\
		'<h3>QC_map/</h3>'+\
		'	<p>QC_map/fragment_length/: 插入片段长度分布图 barplot (pdf files) of fragment length distribution</p>%s'+\
		'	<p>QC_map/TSS/: TSS附近reads 热图。 heatmap 可选 运行巨慢。</p>%s'+\
		'<h3>macs2_result/</h3> .narrowPeak for each file.%s' +\
		'<h3>homer/</h3> motif results. %s</div>'
		
		footer="<div class=footer><div class='content'> Sample number: "+ str(len(SI)) +"<br> The end | "+timsString+" </div></div>"
		
		#################
		# get BAM QC links
		#################
		fragLen="<hr>Click and View <ul>"
		pngs=""
		for i in SI:
			fnames="<li><a href='../QC_map/fragment_length/fragment_length_%s.pdf'>fragment_length_%s.pdf</a>, <a href='../QC_map/fragment_length/%s.fragment.length.txt'>%s.fragment.length.txt</a></li>" % (i,i, i,i)
			fragLen += fnames
			#
			pngs+="<img class=h200 src='../QC_map/fragment_length/fragment_length_%s.png'>" % i;
		fragLen+="</ul>"+pngs
		
		
		TSS="<hr>Click and View <ul>"
		for i in SI:
			fnames="<li><a href='../QC_map/TSS/%s.TSS.heatmap.pdf'>%s.TSS.heatmap.pdf</a></li>" % (i,i)
			TSS += fnames
		TSS+="</ul>"
		
		#################
		# get macs results links
		#################
		macs2="<hr>Download <ul>"
		for i in SI:
			fnames="<li><a href='../macs2_result/%s_peaks.narrowPeak'>%s_peaks.narrowPeak</a>, <a href='../macs2_result/%s_peaks.xls'>%s_peaks.xls</a>, <a href='../macs2_result/%s_summits.bed'>%s_summits.bed</a></li>" % (i,i, i,i, i,i)
			macs2 += fnames
		macs2+="</ul>"
		
		#################
		# get homer result
		#################
		homer="<hr>Click and View<ul>"
		for i in SI:
			fnames="<li><a href='../homer/%s/knownResults.html'>%s knownResults.html</a>, <a href='../homer/%s/homerResults.html'>%s homerResults.html</a>,</li>" % (i,i, i,i)
			homer += fnames
		homer+="</ul>"
		
		
		files=files % (fragLen, TSS, macs2, homer)
		
		html=head+workflow+ files+footer;
		
		# save
		fw=open( output[0],'w')
		fw.write(html)
		fw.close()
#


$ snakemake -s getReport.sf -j 4
Job counts:
	count	jobs
	2	MAP_QC_bamCoverage
	2	MAP_QC_computeMatrix
	2	MAP_QC_plotHeatmap
	1	all
	7
耗时 22min，2个样本。
[Wed Jun 23 13:04:42 2021]
[Wed Jun 23 13:22:41 2021]







========================================
Cell Ranger ATAC
----------------------------------------
1. 简介
https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/what-is-cell-ranger-atac

cellranger-atac count takes FASTQ files from cellranger-atac mkfastq and performs ATAC analysis, including:
	Read filtering and alignment
	Barcode counting
	Identification of transposase cut sites
	Detection of accessible chromatin peaks
	Cell calling
	Count matrix generation for peaks and transcription factors
	Dimensionality reduction
	Cell clustering
	Cluster differential accessibility


2. 具体算法
https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/algorithms/overview

(1)去接头：cutadapt https://cutadapt.readthedocs.io/en/stable/guide.html
(2)比对： 改进版的 BWA-MEM https://github.com/lh3/bwa
We align read pairs using a fixed prior on the insert size distribution, which is assumed to be gaussian with a mean of 250 and standard deviation of 150. 
BWA-MEM divides read pairs into batches and determines this number on the fly. 
The results of either approach are very similar especially for high MAPQ read pairs that are used in downstream analyses.

on the fly 在飞行中；忙忙碌碌

固定先验插入长度，是一个高斯分布，均值250，sd-150.
BWA 把碱基对分批次，然后运行中估算该值。
对于高 MAPQ 的reads，两种方法估算的该值很相近。


(3) Duplicate Marking
While processing the group of identically aligned read-pairs as described above, once the original fragment is marked, Cell Ranger ATAC determines 
- if the fragment is mapped with MAPQ > 30 on both reads, 
- is not mitochondrial, 
- not chimerically mapped, 
- and maps to a primary contig (a gene-containing contig). 

If the fragment passes these filters, Cell Ranger ATAC creates one entry in the fragments.tsv.gz file marking the start and end of the fragment 
- after adjusting the 5' ends of the read-pair to account for transposition, during which the transposase occupies a region of DNA 9 base pairs long. 
考虑Tn5酶的9bp切口，一对测序结果: 正链+4bp，负链-5bp


Each entry is tab-separated and the file is position-sorted and 
- then run through the SAMtools tabix command with default parameters.


SAMtools tabix: http://www.htslib.org/doc/tabix.html
	tabix – Generic indexer for TAB-delimited genome position files
	Tabix indexes a TAB-delimited genome position file in.tab.bgz and creates an index file (in.tab.bgz.tbi or in.tab.bgz.csi) when region is absent from the command-line. The input data file must be position sorted and compressed by bgzip which has a gzip(1) like interface.


(4) ATAC Peak Calling
峰要和背景噪音(随机切割)分开。
As the ends of each fragment are indicative of regions of open chromatin.







========================================
ChIP-seq 分析
----------------------------------------


ref:
ChIP-seq双端测序数据分析 https://www.jianshu.com/p/5fb041f09953














========================================
ATAC-seq 与 RNA-seq 整合分析
----------------------------------------
1. 
ATAC-seq凭借样本需求量低、建库流程简单成为表观基因组学研究的新宠。

ATAC-seq的研究对象限于染色质开放区域的分布和变化，后期分析要与其他组学紧密结合，才能解决完整的生物学问题；而RNA-seq可以检测表观遗传机制对基因表达的调控效果，通常与表型变化具有较高相关性，是研究中普遍使用的测序工具。眼馋高分文章里大牛们的各式“炫图”固然不错，但“万变不离其宗”，找准符合研究目标的分析策略和展现形式才是根本。


(2) 套路一 两种组学数据，是否存在相关性？

展示方式之一是双组学的散点图。基于各基因组窗口在不同组学数据中的归一化强度绘制散点图，能从整体上展示染色质可接近性与表达水平之间的关系，也可以进一步根据回归模型来计算其是否存在相关性。

从以往研究的结果来看，转录水平的调控往往是多种表观调控机制共同作用的结果。基因组不同区域的表达水平在受到染色质可接近性影响的同时，也可能会被其他因素干预。分析RNA-seq与ATAC-seq结果是否相关，一定程度可以评估用染色质可接近性来解释转录水平乃至下游表型变化的可信度。

此类分析常见的展示方式之一是双组学的散点图。基于各基因组窗口在不同组学数据中的归一化强度绘制散点图，能从整体上展示染色质可接近性与表达水平之间的关系，也可以进一步根据回归模型来计算其是否存在相关性。


x: RPM, y: FPKM; 


(2) 套路二 合理设定标准，筛选关键基因

通常认为，比较组间发生差异表达的基因与表型差异可能具有因果关系，而染色质可接近性变化可能是差异表达的原因之一。除了上面的韦恩图，我们还可以用象限图更好地对在两种组学层面都发生变化的基因进行细分，并选择关注的象限进行渐进性富集分析（GO、KEGG、GSEA等），进一步从功能层面锁定关键基因集。


差异ATAC peaks相关基因（左侧圆）和差异表达基因（右侧圆）的差异分析结果

差异ATAC peaks相关基因和差异表达基因的象限图展示 x:peak FC, y: DEG FC
根据peak信号强度和表达水平的变化，将存在差异的基因分为四个类群.



(3) 套路三 关注特定区域，展示变化趋势

从以往研究来看，基因转录起始位点（TSS）周边通常被认为是启动子区域，其可接近性变化会影响转录起始元件及调控元件的结合，对基因表达产生较为直接的影响。很多研究会将这一区域作为重点，关注其可接近性变化与基因转录水平的关系。

针对这一需求，常见的展现形式包括折线图和热图。前者适合做多个基因的数据整合，后者可以显示各个基因的情况。在这两种基本作图思路的指导下，我们可以进一步细分需要分析并展示的内容。比如将关注范围固定在TSS或整个转录区域上下游一定范围、将基因根据表达量/差异表达/ATAC信号强度等进行分群、在热图绘制中是否进一步细划窗口等。有了这些更为细化的分析思路，一张CNS级美图分分钟就能出来！

根据表达量将基因分为3个类群（low、med、hig），并对其TSS上下游2kb区域内进行划窗，绘制折线图（上）和热图（下，因图形较长仅截取首尾）

一张高分文献配图[1]，对数据质量要求很高，但作图方法并不复杂~

根据上面的介绍不难看出其作图思路：
- 基于启动子区ATAC信号在比较组间变化情况，将基因分为三个类群（High、Dynamic、Low）；
- 在不同类群内，根据各基因在不同比较组内表达情况进行排序；
- 统计启动子区ATAC信号强度和对应基因的表达量，绘制完整的双组学热图。









ref:
https://www.sohu.com/a/401338432_120084361



========================================
----------------------------------------







========================================
----------------------------------------








========================================
----------------------------------------







========================================
----------------------------------------










