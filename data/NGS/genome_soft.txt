基因组分析常用软件和流程






========================================
数据质控 与 比对
----------------------------------------


========================================
|-- fastq数据的过滤、修剪、错误去除和质控[fastqc + cutadapt]
----------------------------------------
1.质控用fastqc
http://www.bioinformatics.babraham.ac.uk/projects/fastqc/

$ fastqc --version
FastQC v0.11.5

-o参数，指定输出到文件夹


This has some good examples: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/
Then you can look into the following: Trimmomatic, Fastx-toolkit to trim data.


更多质控软件： http://blog.sina.com.cn/s/blog_9b78c9110101couq.html





2.质控的参数：
http://www.biotrainee.com/thread-324-1-1.html

(1)如果测序质量不合格，则需要：
fastq_quality_filter -v -Q 64 -q 20 -p 75 -i sample.fastq -o sample_filtered.fastq

(2)如果都是reads的前6个bp碱基有问题，则需要
fastx_trimmer -v -f 7 -l 36 -i sample_filtered.fastq -o sample_filtered_and_trimmed.fastq

(3)如果混入了大量的接头，则需要！
cutadapt -m 20 -e 0.1 -a GATCGGAAGAGCACACGTCTGAACTCCAGTCACACA sample2.fastq \ -o sample2--cutadapt.fastq
需要自己去查自己的接头是什么序列：https://github.com/csf-ngs/fastq ... ontaminant_list.txt

质控可视化：
(A) Average quality score for each base position, 
(B) GC content distribution, 
(C) Average Phred quality score distribution, 
(D) Base composition and 
(E) read length distribution for both input (red) and HQ filtered (green) data. 
(F) Percentage of reads with different quality score ranges at each base position. 

如果是特殊测序，质控需要加一些步骤
WES：??


问题：如果双端测序，有一个被过滤掉了，下一步bwa比对不通过怎么办？http://www.biotrainee.com/thread-324-1-1.html
解答：用cutadapt软件来对双端测序数据去除接头 http://www.bio-info-trainee.com/1920.html
Posted on 2016年10月6日
一般来讲，我们对测序数据进行QC，就三个大的方向：Quality trimming， Adapter removal， Contaminant filtering。
当我们是双端测序数据的时候，去除接头时，也会丢掉太短的reads，就容易导致左右两端测序文件reads数量不平衡，有一个比较好的软件能解决这个问题，我比较喜欢的是cutadapt软件的PE模式来去除接头！尤其是做基因组或者转录组de novo 组装的时候，尤其要去掉接头，去的干干净净！

既然fastqc能探测到你的接头，说明它里面内置了所有的接头序列，在github可以查到：https://github.com/csf-ngs/fastqc/blob/master/Contaminants/contaminant_list.txt
或者：Download common Illumina adapters from   https://github.com/vsbuffalo/scythe/blob/master/illumina_adapters.fa
TruSeq Universal Adapter AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Small RNA 3p Adapter 1 ATCTCGTATGCCGTCTTCTGCTTG
最严重的一般是TruSeq Universal Adapter ， 而且它检测到你其它的接头可能就是这个 TruSeq Universal Adapter 的一部分而已~！ 




3.剪切接头
$ cutadapt --version
1.18

See https://cutadapt.readthedocs.io/ for full documentation.
https://www.biomart.cn/news/16/2842104.htm

测试方法：
$ cutadapt -o output.fastq.gz input.fastq.gz
$ tail -n 4 input.fastq | cutadapt -a AACCGGTT

## PE reads
$ cutadapt -a ADAPTER_FWD -A ADAPTER_REV -o out.1.fastq -p out.2.fastq reads.1.fastq reads.2.fastq
-p is the short form of --paired-output. 



要用PE模式来去除接头，保证过滤后的reads还是数量继续平衡的。
--pair-filter=both
 Processing reads on 5 cores in paired-end legacy mode ...
 WARNING: Legacy mode is enabled. Read modification and filtering options *ignore* the second read. To switch to regular paired-end mode, provide the --pair-filter=any option or use any of the -A/-B/-G/-U/--interleaved options.

--max-n=COUNT  Discard reads with more than COUNT 'N' bases. If COUNT在0和1之间，则为比例。
	--max-n=10 最多10个N碱基，否则舍弃；
-e 0.1 错误率超过10%舍弃。-e 引物匹配允许错误率，我调置0.15，一般引物20bp长允许3个错配，为了尽量把引物切干净 
-j N 或者--cores=N 多核
-O 5 Require MINLENGTH overlap between read and adapter for an adapter to be found. Default: 3
-q 30 3'端质量低于30的舍弃
-m 100 去除接头后如果read长度小于50舍弃
-o 输出文件
-p 另一个输出文件

看图，需要除掉低质量的碱基。
$ cutadapt --max-n=10 -e 0.1 -j 5 -q 30 -m 100 --pair-filter=both -o outN.1.fastq -p outN.2.fastq ../0_fastqc/B1709060700_NDHE00383-A84-A68_AH7277CCXY_L7_1.fq ../0_fastqc/B1709060700_NDHE00383-A84-A68_AH7277CCXY_L7_2.fq

Finished in 454.01 s (11 us/read; 5.30 M reads/minute).

=== Summary ===

Total read pairs processed:         40,082,567
  Read 1 with adapter:                       0 (0.0%)
  Read 2 with adapter:                       0 (0.0%)
Pairs that were too short:             626,386 (1.6%)
Pairs with too many N:                     811 (0.0%)
Pairs written (passing filters):    39,455,370 (98.4%)

Total basepairs processed: 12,024,770,100 bp
  Read 1: 6,012,385,050 bp
  Read 2: 6,012,385,050 bp
Quality-trimmed:             509,713,908 bp (4.2%)
  Read 1:   163,408,047 bp
  Read 2:   346,305,861 bp
Total written (filtered):  11,454,864,271 bp (95.3%)
  Read 1: 5,813,730,111 bp
  Read 2: 5,641,134,160 bp



可以删除那些原始fq数据了，保留一份fq.gz即可。





========================================
|-- 比对：BWA的安装 与使用
----------------------------------------
1.http://bio-bwa.sourceforge.net/

BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the rest two for longer sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate. BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads.

BWA适用于变异比对。


########
尝试系统的：太老旧
$ sudo yum install bwa
版本号：
Program: bwa (alignment via Burrows-Wheeler transformation)
Version: 0.5.9-r16
发现版本太低，重新安装。
########



2.下载 与安装
https://sourceforge.net/projects/bio-bwa/files/

https://github.com/lh3/bwa/releases
说明文件是这么说的：
git clone https://github.com/lh3/bwa.git
cd bwa; make
./bwa index ref.fa
./bwa mem ref.fa read-se.fq.gz | gzip -3 > aln-se.sam.gz
./bwa mem ref.fa read1.fq read2.fq | gzip -3 > aln-pe.sam.gz


$ wget https://github.com/lh3/bwa/releases/download/v0.7.17/bwa-0.7.17.tar.bz2
$ tar -xjvf bwa-0.7.17.tar.bz2
$ cd bwa-0.7.17
$ make #报错 utils.c:33:10: fatal error: zlib.h: No such file or directory

$ sudo apt-get install zlib1g-dev

$ make #正常了
$ ./bwa        
Program: bwa (alignment via Burrows-Wheeler transformation)
Version: 0.7.17-r1188








3.比对
更快的是使用 BWA mem | samtools sort 从fastq获得bam。
https://biology.stackexchange.com/questions/59493/how-to-convert-bwa-mem-output-to-bam-format-without-saving-sam-file
bwa mem ref.fa in.fq | samtools view -bS - > out.bam
## The - in samtools view tells it to read from stdin.

(1)建立索引
$ bwa index -a bwtsw -p index/GRCm38_ Mus_musculus.GRCm38.dna.primary_assembly.fa
[bwa_index] Pack forward-only FASTA... 8.25 sec
[bwa_index] Construct SA from BWT and Occ... 766.48 sec
[main] Version: 0.7.17-r1188
[main] CMD: bwa index -a bwtsw -p index/GRCm38_ Mus_musculus.GRCm38.dna.primary_assembly.fa
[main] Real time: 2508.653 sec; CPU: 2425.633 sec
检查
$ ls /home/wangjl/data/ref/mouse/ensembl/index/GRCm38_* -lth
-rw-rw-r-- 1 wangjl wangjl 1.3G Aug 25 15:01 /home/wangjl/data/ref/mouse/ensembl/index/GRCm38_.sa
-rw-rw-r-- 1 wangjl wangjl  11K Aug 25 14:49 /home/wangjl/data/ref/mouse/ensembl/index/GRCm38_.amb
-rw-rw-r-- 1 wangjl wangjl 5.5K Aug 25 14:49 /home/wangjl/data/ref/mouse/ensembl/index/GRCm38_.ann
-rw-rw-r-- 1 wangjl wangjl 652M Aug 25 14:48 /home/wangjl/data/ref/mouse/ensembl/index/GRCm38_.pac
-rw-rw-r-- 1 wangjl wangjl 2.6G Aug 25 14:48 /home/wangjl/data/ref/mouse/ensembl/index/GRCm38_.bwt




(2) 开始比对
$ cd /data/wangjl/ATAC/CRC_ATAC/
$ id="1" 
$ bwa mem -t 5 -R "@RG\tID:${id}\tSM:${id}" /home/wangjl/data/ref/mouse/ensembl/index/GRCm38_ raw/${id}_R1.fq.gz raw/${id}_R2.fq.gz | samtools sort -@ 5 -O bam -T /tmp/${id} -o map/${id}.sorted.bam -
## [main] Real time: 11020.968 sec; CPU: 54746.452 sec 
## [bam_sort_core] merging from 30 files and 5 in-memory blocks...
11020/3600 = 3.061111 小时

## 这个形式不知道是否可用？
$ bwa mem -t 5 -R "@RG\tID:${id}\tSM:${id}" /home/wangjl/data/ref/mouse/ensembl/index/GRCm38_ raw/${id}_R1.fq.gz raw/${id}_R2.fq.gz | samtools sort -@ 5 -O bam -T /tmp/${id}_ > map/${id}_.sort1.bam

对比其中一个的输出，一模一样。能比的就是时间差异了: 
3e07f2eaf5b2faf2921d3907dd6b9372  map/2.sorted.bam
3e07f2eaf5b2faf2921d3907dd6b9372  map/2_.sort1.bam


## 示例1:
$ id="ERR7016189"
$ bwa mem -t 5 -R "@RG\tID:${id}\tSM:${id}" /home/wangjl2/data/ref/hg38/gencode/index/GRCh38_bwa raw/${id}.fastq | samtools sort -@ 4 -O bam -T /tmp/${id} -o map/${id}.sorted.bam -

## 示例2:
$ bwa mem -M -t 4 \
-R '@RG\tID:C6C0TANXX_2\tSM:ZW177\tLB:ZW177lib\tPL:ILLUMINA' \
./genome_index/genome.fa \
sample1_R1.fastq.gz sample1_R2.fastq.gz > sample1.sam
参数解释:
-t 4 设置线程数，默认可能用到12个
-R: add Read Group description (more about it in a minute)
-M: if a read is split (different parts map to different places) mark all parts other than main as “secondary
alignment” (technicality, but important for GATK which ignores secondary alignments)
默认输出是 sam 格式。

Read Group 描述信息: -R '@RG\tID:C6C0TANXX_2\tSM:ZW177\tLB:ZW177lib\tPL:ILLUMINA'
  会在 bam 中加入头信息: @RG ID:C6C0TANXX_2 SM:ZW177 LB:ZW177lib PL:ILLUMINA
  ID: Unique ID of a collection of reads sequenced together, typically: Illumina lane (+barcode or sample)
  SM: Sample name
  LB: DNA prep Libray ID
  PL: Sequencing platform
Each alignment record will be marked with Read Group ID (here: C6C0TANXX_2), so that programs in downstream analysis know where the read is from.
Read groups, sample and library IDs are important for GATK operation!
这个可用于区分样本信息:
  One flowcell: HL5WNCCXX, two lanes (2 and 3), each with samples A and B (2-plex) from library my_lib
  @RG ID:HL5WNCCXX_2_A SM:A LB:mylib PL:ILLUMINA
  @RG ID:HL5WNCCXX_3_A SM:A LB:mylib PL:ILLUMINA

  @RG ID:HL5WNCCXX_2_B SM:B LB:mylib PL:ILLUMINA
  @RG ID:HL5WNCCXX_3_B SM:B LB:mylib PL:ILLUMINA









##########
https://informatics.fas.harvard.edu/short-introduction-to-bwa.html

/00_genome - reference sequence 
/01_fastqs - raw Illumina reads

(1) 建立索引
$ bwa index -p 00_genome/Falb 00_genome/Falbicolis.chr5.fa.gz
-p 是前缀。这样会在 00_genome/ 下产生几个新文件，开头是Falb


(2) shell 批量化获得sam
mkdir -p 02_bams
for INDEX in 1 2 3 34 35;
do
   bwa mem -M -t 1 -R "@RG\tID:COL_${INDEX}\tSM:COL_${INDEX}" 00_genome/Falb \
   01_fastqs/Falb_COL${INDEX}.1.fastq.gz \
   01_fastqs/Falb_COL${INDEX}.2.fastq.gz \
   > 02_bams/Falb_COL${INDEX}.sam #2> Falb_COL${INDEX}.log
done



(3) sam转为bam

for INDEX in 1 2 3 34 35;
do
  picard SortSam \
  I=02_bams/Falb_COL${INDEX}.sam \
  O=02_bams/Falb_COL${INDEX}.sorted.bam \
  SORT_ORDER=coordinate \
  CREATE_INDEX=true
done

(4) 创建 index 

for INDEX in 1 2 3 34 35
do
  picard BuildBamIndex \
  I=02_bams/Falb_COL${INDEX}.sorted.bam
done














4. 其他参数的探讨

Does BWA call SNPs like MAQ?
No, BWA only does alignment. Nonetheless, it outputs alignments in the SAM format which is supported by several generic SNP callers such as samtools and GATK.
比对之后去重，
然后用samtools或者GATK call SNP。

http://bio-bwa.sourceforge.net/bwa.shtml


(1)建立索引

# 后面是探索 -a 参数，对于human和mouse，使用默认值即可。

参考基因组
$ ls -lth /home/data/Homo_sapiens/UCSC/hg38/Sequence/WholeGenomeFasta/genome.fa
-rwxrwxr-x. 1 wangjl wangjl 3.0G Aug 19  2015 /home/data/Homo_sapiens/UCSC/hg38/Sequence/WholeGenomeFasta/genome.fa


Algorithm for constructing BWT index. Available options are:
有两种建立参考基因组的策略：
is	IS linear-time algorithm for constructing suffix array. It requires 5.37N memory where N is the size of the database. IS is moderately fast, but does not work with database larger than 2GB. IS is the default algorithm due to its simplicity. The current codes for IS algorithm are reimplemented by Yuta Mori.
is 相对快，但是参考基因组不能大于2GB。
bwtsw	Algorithm implemented in BWT-SW. This method works with the whole human genome.
bwtsw能用于人全基因组分析。

https://zhuanlan.zhihu.com/p/36267250
-a指定建立索引的算法，bwtsw，is或者rb2，以前没有rb2，而是div。is还是适合小于2G的基因组，bwtwt是大于10M的基因组才行，这个地方不要选错了，细菌基因组一般都小于10M，只能用is算法，人基因组是3G，只能用bwtsw，处于中间的选择哪个都可以。


$ pwd
/home/data/Homo_sapiens/UCSC/hg38/Sequence/BWAIndex/version0.7.x
$ bwa index genome.fa
##14:51 to 16:05
#...
##[BWTIncConstructFromPacked] 680 iterations done. 6184133946 characters processed.
生成 hg19.fasta.amb、hg19.fasta.ann、hg19.fasta.bwt、hg19.fasta.pac、hg19.fasta.sa 五个文件。
#[main] Version: 0.7.17-r1188 
#[main] CMD: bwa index genome.fa
#[main] Real time: 4420.476 sec(73min); CPU: 4367.864 sec


好像不对，人基因组大于3GB，应该使用另一种索引方式：
$ bwa index -a bwtsw genome.fa
#[main] Version: 0.7.17-r1188
#[main] CMD: bwa index -a bwtsw genome.fa
#[main] Real time: 4529.281 sec; CPU: 4443.430 sec


https://www.biostars.org/p/53546/
按照这个来看，不指定，则bwa自动选择，选择的依据还是基因组大小。
// simplified sample from bwtindex.c:
if (algo_type == "auto") {
    if (l_pac > 50000000)
        algo_type = "bwtsw";
    else
        algo_type = "is";
}
所以，我以上两个建立的索引应该是一样。
比一下md5，真的一模一样。并且和version0.6.0一模一样。






(2)比对：bwa mem 或者 bwa aln 哪个好呢？ gatk 推荐 bwa mem
$ bwa mem ref.fa read1.fq read2.fq > aln-pe.sam
-t 5 线程
-M            mark shorter split hits as secondary
-R STR        read group header line such as '@RG\tID:foo\tSM:bar' [null]
  定义头文件 -R '@RG\tID:foo\tSM:bar'，如果在此步骤不进行头文件定义，在后续GATK分析中还是需要重新增加头文件。

示例代码
$ bwa mem -t 8 -M -R '@RG\tID:${name}\tLB:${name}\tPL:ILLUMINA\tPM:X10\tSM:${name}' ${INDEX} ${RAW_DATA}/${name}_1.fastq ${RAW_DATA}/${name}_2.fastq > ${WORKING_DIR}/2018rerun/processed_bam/${name}.sam


实际代码
$ INDEX="/home/data/Homo_sapiens/UCSC/hg38/Sequence/BWAIndex/genome.fa"
$ bwa mem -t 6 -M ${INDEX} ../1_cleanData/outN.1.fastq ../1_cleanData/outN.2.fastq > processed.sam
20:52 to 21:50

[main] Version: 0.7.17-r1188
[main] CMD: bwa mem -t 6 -M /home/data/Homo_sapiens/UCSC/hg38/Sequence/BWAIndex/genome.fa ../1_cleanData/outN.1.fastq ../1_cleanData/outN.2.fastq
[main] Real time: 3491.090 sec(0.96hour); CPU: 20559.125 sec










========================================
**** bam文件的处理（质控、标记重复、校准）和转vcf
----------------------------------------



========================================
|-- Picard: 对sam文件进行进行重新排序（reorder）、标记PCR重复
----------------------------------------
1. Picard官网：http://broadinstitute.github.io/picard/
A set of command line tools (in Java) for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.



2.下载
$ wget https://github.com/broadinstitute/picard/releases/download/2.18.15/picard.jar
$ vim ~/.bashrc #添加一行
	PICARD="/home/wangjl/Soft/picard.jar"
$ source ~/.bashrc




3.使用Picard排序：比对是按照字典顺序1,10,2,改为1,2,10

$ java -Xmx20g -jar $PICARD SortSam SORT_ORDER=coordinate INPUT=${WORKING_DIR}/2018rerun/processed_bam/${name}.sam OUTPUT=${WORKING_DIR}/2018rerun/processed_bam/${name}.bam




4. 标记PCR重复
已经整合到 gatk4 中了。

例子见 NGS/ATAC-seq: 去重复


gatk3?
$ java -jar $PICARDDIR/picard.jar \
MarkDuplicates \
INPUT=sample1.sorted.bam \
OUTPUT=sample1.sorted.dedup.bam \
METRICS_FILE=sample1.sorted.dedup.metrics.txt


gatk4:
$ id="SRR7629152"
$ gatk MarkDuplicates -I map/${id}.del_MT.sort.bam -O map/${id}.rmdup.bam -M map/${id}.rmdup.matrix -ASO coordinate -REMOVE_DUPLICATES true

重复的只保留一份，其他加上 0x400 标记，不会被用于下游。
In the resulting BAM file, only one fragment from each duplicate group survives unchanged, other
duplicate fragments are given a flag 0x400 and will not be used downstream.

可选：每个文库做一个重复标记，也就是对一个文库的所有分组。
Optimally, detection and marking of duplicate fragments should be done per library, i.e., over all read groups corresponding to a given library. 

实践中，对每个lane做重复标记就足够了。
In practice, often sufficient to do it per lane (read group).





========================================
|-- 标记重复：Samblaster
----------------------------------------

https://github.com/GregoryFaust/samblaster
https://doi.org/10.1093/bioinformatics/btu314

1.安装 (支持linux/mac OS Version 10.7以上)
$ git clone git://github.com/GregoryFaust/samblaster.git
$ cd samblaster
$ make
$ sudo cp samblaster /usr/local/bin/
$ samblaster --version
samblaster: Version 0.1.24


2.简介
Author: Greg Faust (gf4ea@virginia.edu)
Tool to mark duplicates and optionally output split reads and/or discordant pairs.
Input sam file must contain paired end data, contain sequence header and be sorted by read ids.
Output will be all alignments in the same order as input, with duplicates marked with FLAG 0x400.


samblaster is a fast and flexible program for marking duplicates in read-id grouped1 paired-end SAM files. It can also optionally output discordant read pairs and/or split read mappings to separate SAM files, and/or unmapped/clipped reads to a separate FASTQ file. When marking duplicates, samblaster will require approximately 20MB of memory per 1M read pairs.


refer: https://github.com/GregoryFaust/samblaster


3.中文
https://www.sohu.com/a/246097230_99971433






========================================
|-- Sambamba: process your BAM data faster!（samtools的竞争者）
----------------------------------------

1.Sambamba 官网
https://github.com/biod/sambamba
http://lomereiter.github.io/sambamba/

http://lomereiter.github.io/sambamba/docs/sambamba-view.html
sambamba主要有filter，merge,slice和duplicate等七个功能来处理sam/bam文件。

推荐：https://www.sohu.com/a/246097230_99971433
推荐两款sam文件处理小工具samblaster和sambamba，它们具有排序、比对信息查看等常用功能之外，最棒的是可以用来代替picard去除重复序列，在筛选标准不变的前提下速度能提升30倍以上

下载地址
https://github.com/lomereiter/sambamba/releases
安装：拷贝至全局环境变量路径即可

$ wget https://github.com/biod/sambamba/releases/download/v0.6.8/sambamba-0.6.8-linux-static.gz
$ gunzip sambamba-0.6.8-linux-static.gz
$ mv sambamba-0.6.8-linux-static sambamba
$ chmod +x sambamba
$ sudo mv sambamba /usr/local/bin/
$ sambamba -v
#sambamba 0.6.8



## 预览bam文件
sambamba view OPTIONS <input.bam | input.sam> [region1 [...]]



refer:
1.官网 http://lomereiter.github.io/sambamba/docs/sambamba-view.html
2.中文参考：
http://blog.sciencenet.cn/home.php?mod=space&uid=1094241&do=blog&id=1041577
https://www.sohu.com/a/246097230_99971433
https://blog.csdn.net/msw521sg/article/details/65938377






========================================
|-- 用bcftools来call variation
----------------------------------------

我们可以考虑比较对3种不同的bam文件来分别call variation的差异，探索对bam文件的不同过滤模式对snp calling的影响，分别是，
  原始比对的bam文件，
  去除低质量和多比对还有PCR重复的bam，
  以及用GATK进行realign的bam文件。

Variant Call Format (VCF) and its binary counterpart BCF.
https://mp.weixin.qq.com/s?__biz=MzAxMDkxODM1Ng==&mid=2247483812&idx=1&sn=c3f7ce44a4f164f0d5549e11a22e702f



1.安装
(1) Ubuntu 安装
http://samtools.github.io/bcftools/

准备
$ sudo apt install git
$ sudo apt-get install autocon #for autoheader 失败，找不到
$ sudo apt-get install autoconf #for autoheader 成功

可能要尝试的包：https://www.biostars.org/p/328831/
https://raw.githubusercontent.com/samtools/bcftools/develop/INSTALL
sudo apt-get update
sudo apt-get install gcc
sudo apt-get install make
sudo apt-get install libbz2-dev
sudo apt-get install zlib1g-dev
sudo apt-get install libncurses5-dev 
sudo apt-get install libncursesw5-dev
sudo apt-get install liblzma-dev

1) 编译 htslib
$ cd ~/data/Downloads/
$ git clone https://github.com/samtools/htslib.git
$ cd htslib
$ git submodule update --init --recursive
$ make

报错:fatal error: lzma.h: No such file or directory
解决: $ sudo apt install libbz2-dev
还是报错，尝试 $ sudo apt install zlib1g-dev
尝试 $ sudo apt-get install liblzma-dev #可以了


报错: fatal error: curl/curl.h: No such file or directory
解决: $ sudo apt-get install  libcurl4-openssl-dev
编译通过。


2) 编译 bcftools
$ cd ..
$ git clone https://github.com/samtools/bcftools.git
$ cd bcftools
## The following 2 is optional: 
$ autoheader && autoconf 
$ ./configure --enable-libgsl --enable-perl-filters prefix=/home/wangjl2/

报错:configure: error: libgsl development files not found
解决：$ sudo apt install libgsl-dev

报错：configure: error: Unable to find the Perl_call_pv() function for perl filters
解决：无法解决，那就去了吧

$ ./configure --enable-libgsl prefix=/home/wangjl2/
$ make
$ make install

$ bcftools --version
bcftools 1.17-18-g60c65eb
Using htslib 1.17-15-g3e0fd29
Copyright (C) 2023 Genome Research Ltd.


为了使用BCFtools plugins，需要设置环境变量：
export BCFTOOLS_PLUGINS=/path/to/bcftools/plugins







(2) CentOS 非root安装
查看帮助文件 bcftools/INSTALL

$ cd /home/wangjl/software/
$ git clone --recurse-submodules git://github.com/samtools/htslib.git ##把老的htslib重命名 mv htslib htslib_old
$ git clone git://github.com/samtools/bcftools.git
$ cd bcftools/
$ autoheader && autoconf 
$ ./configure --prefix=/home/wangjl/ --enable-libgsl --enable-perl-filters
$ make 
$ make install

2) make时报错
/home/wangjl/software/bcftools/peakfit.c:556: undefined reference to `gsl_multifit_fdfsolver_test'
搜了一下:
The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers.
http://www.gnu.org/software/gsl/

$ wget https://mirror.ibcp.fr/pub/gnu/gsl/gsl-latest.tar.gz
$ tar zxvf gsl-latest.tar.gz
$ cd gsl-2.7/
$ ./configure --prefix=/home/wangjl
$ make 
$ make install

然后，回到 bcftools 的目录内，重新安装最后两步骤
$ make clean 
$ make 
还是同样的报错。

3) 要不去掉 --enable-libgsl 
$ ./configure --prefix=/home/wangjl/ --enable-perl-filters
$ make 
又报错
main.o:(.rodata+0x248): undefined reference to `main_polysomy'

4) 再降级
$ ./configure --prefix=/home/wangjl/
$ make 
$ make install

调整优先级
$ vim ~/.bashrc #最后新增一行
export PATH=/home/wangjl/bin:$PATH
$ source ~/.bashrc
$ bcftools


$ which bcftools
~/bin/bcftools

$ bcftools 
Version: 1.12-66-g42baa31 (using htslib 1.12-50-g0313654)

凑合用吧，报错了再说。












2.使用
bcftools — utilities for variant calling and manipulating VCFs and BCFs.
https://samtools.github.io/bcftools/bcftools.html

对排序好的bam数据用samtools生成bcf文件：
$ samtools mpileup -ugf ../hs38DH.fa hs2.sort.bam  >hs2.bcf

由于生成的是二进制格式的数据，需要进行解析或者转换成vcf：
$ bcftools view hs2.bcf >hs2.vcf


(2)bcftools合并vcf文件
$ bcftools merge A.vcf.gz B.vcf.gz C.vcf.gz -Oz -o ABC.vcf.gz

https://www.cnblogs.com/chenwenyan/p/9273726.html










3. call SNP from bam 

Program: bcftools (Tools for variant calling and manipulating VCFs and BCFs)
Version: 1.11-35-g8a744dd (using htslib 1.11-27-g246c146)


$ ref="/data/wangjl/soft/GATK/resources/bundle/hg38/Homo_sapiens_assembly38.fasta"
$ samtools mpileup -ugf $ref ../align/*.bam | bcftools call -vmO z -o out2.vcf.gz




ref:
http://cbsu.tc.cornell.edu/lab/doc/Variant_workshop_Part1.pdf

Aligner:
  BWA: BWA mem – aligner of choice in GATK
  Bowtie
  SOAP






========================================
gatk--Variant Discovery in High-Throughput Sequencing Data
----------------------------------------
参考全文 https://www.cnblogs.com/nkwy2012/p/6322775.html



怎么组装 单倍体?
Need local multi-read re-alignment or local haplotype assembly (expensive!)




GATK Best Practices Workflow for DNA-Seq
https://bioinformaticsworkbook.org/dataAnalysis/VariantCalling/gatk-dnaseq-best-practices-workflow.html




GATK4全基因组数据分析最佳实践 ，我以这篇文章为标志，终结当前WGS系列数据分析的流程主体问题 | 完全代码
原创： 矿工  碱基矿工  2018-05-11




1.官网： https://software.broadinstitute.org/gatk/
Genome Analysis Toolkit
Variant Discovery in High-Throughput Sequencing Data


(1)如何安装：https://www.jianshu.com/p/825e7d618838
$ wget https://github.com/broadinstitute/gatk/releases/download/4.0.11.0/gatk-4.0.11.0.zip
$ unzip gatk-4.0.11.0.zip
$ tree -L 1 gatk-4.0.11.0
gatk-4.0.11.0
├── gatk
├── gatk-completion.sh
├── gatkcondaenv.intel.yml
├── gatkcondaenv.yml
├── GATKConfig.EXAMPLE.properties
├── gatkdoc
├── gatk-package-4.0.11.0-local.jar
├── gatk-package-4.0.11.0-spark.jar
├── gatkPythonPackageArchive.zip
├── README.md
└── scripts

解压缩之后，可以看到两个后缀为.jar的文件，local用于本地运行，spark用于在spark集群上运行。实际使用时，直接用gatk这个可执行文件就行了。

建立软链接
$ cd /home/wangjl/bin
$ ln -s ~/Soft/gatk-4.0.11.0/gatk   

通过一个简单的命令，查看程序是否正确安装
$ gatk --list
这个命令能够打印出所有的子命令(估计有几百个)，如果打印出来结果，说明程序安装正确。部分子命令截图如下
USAGE:  <program name> [-h]

Available Programs:
----- ----- ----- ----- ----- ----- ----- ----- ----- 
Base Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters
    CheckIlluminaDirectory (Picard)              Asserts the validity for specified Illumina basecalling data.
    CollectIlluminaBasecallingMetrics (Picard)   Collects Illumina Basecalling metrics for a sequencing run.
    CollectIlluminaLaneMetrics (Picard)          Collects Illumina lane metrics for the given BaseCalling analysis directory.
    ExtractIlluminaBarcodes (Picard)             Tool determines the barcode for each read in an Illumina lane.
    CollectIlluminaLaneMetrics (Picard)          Collects Illumina lane metrics for the given BaseCalling analysis directory.                                                                                                                             [252/1916]
    ExtractIlluminaBarcodes (Picard)             Tool determines the barcode for each read in an Illumina lane.
    IlluminaBasecallsToFastq (Picard)            Generate FASTQ file(s) from Illumina basecall read data.
    IlluminaBasecallsToSam (Picard)              Transforms raw Illumina sequencing data into an unmapped SAM or BAM file.
    MarkIlluminaAdapters (Picard)                Reads a SAM or BAM file and rewrites it with new adapter-trimming tags.

----- ----- ----- ----- ----- ----- ----- ----- ----- 
Copy Number Variant Discovery:                   Tools that analyze read coverage to detect copy number variants.
    AnnotateIntervals                            (BETA Tool) Annotates intervals with GC content, mappability, and segmental-duplication content
    CallCopyRatioSegments                        (BETA Tool) Calls copy-ratio segments as amplified, deleted, or copy-number neutral
    CollectAllelicCountsSpark                    Collects ref/alt counts at sites.
    CombineSegmentBreakpoints                    (EXPERIMENTAL Tool) Combine the breakpoints of two segment files and annotate the resulting intervals with chosen columns from each file.
    CreateReadCountPanelOfNormals                (BETA Tool) Creates a panel of normals for read-count denoising
    DenoiseReadCounts                            (BETA Tool) Denoises read counts to produce denoised copy ratios
    DetermineGermlineContigPloidy                (BETA Tool) Determines the baseline contig ploidy for germline samples given counts data
    FilterIntervals                              (BETA Tool) Filters intervals based on annotations and/or count statistics
...

子命令后面如果有(picard), 说明这个功能是继承于picard软件，从这里也可以看出，GATK4集成了picard软件的功能。再不需要像之前版本一样，混合使用picard 和 gatk 了。

GATK4 的最佳实践给出了5套pipeline
1)Germline SNPs + Indels
2)Somatic SNVs + Indels
3)RNAseq SNPs + Indels
4)Germline CNVs
5)Somatic CNVs

以上五套pipeline 可以根据研究对象是DNA还是RNA进行划分：DNA 测序（包含1,2,4,5）和RNA 测序（3）。可以看到，GATK 更多的是倾向于DNA 测序数据的分析。对于DNA测序而言，主要识别SNP和CNV 两大类型的变异，每种变异类型又有Germline和Somatic的区别。

Germline指的是在胚胎发育早起出现的变异，这种变异会在所有细胞中广泛存在，是可以遗传给后代的变异；Somatic指的是体细胞变异，身体特定区域或者组织中出现的变异。通常不会遗传给后代。

在所有的pipeline之前，都存在一个数据预处理步骤data pre-processing。
GATK4 版本的最佳实践并不是直接给出了每个步骤对应的代码，而是给出了几套它们自己编写的流程，以供参考。这些流程以WDL这种workflow 语言进行编写。官方对于WDL, 也给出了详细的文档，帮助我们了解。

总结
GATK4整合了picard软件，在算法上进行了优化，新增了许多新的功能。
官网给出了基于GATK4的pipeline, 以WDL这种workflow 流程管理语言编写。



(2)下载参考数据
https://gatkforums.broadinstitute.org/gatk/discussion/1215/how-can-i-access-the-gsa-public-ftp-server


GATK在进行BQSR和VQSR的过程中会使用到R软件绘制一些图，因此，在运行GATK之前最好先检查一下是否正确安装了R和所需要的包，所需要的包大概包括ggplot2、gplots、bitops、caTools、colorspace、gdata、gsalib、reshape、RColorBrewer等。如果画图时出现错误，会提示需要安装的包的名称。



用浏览器登陆 ftp://ftp.broadinstitute.org/bundle/
annovar/humandb folder will have its own dbnsfp file eg hg38_dbnsfp*.txt.



2.GATK Best Practices
https://software.broadinstitute.org/gatk/best-practices/



3.Quick Start Guide:Take a brief orientation tour and get started today
https://software.broadinstitute.org/gatk/documentation/quickstart.php


========================================
|-- gatk4.0 实践 (hg19) 当前主流
----------------------------------------
主要参考： https://www.plob.org/article/11698.html
	https://cloud.tencent.com/developer/news/180158
其他参考： http://starsyi.github.io/2016/05/25/%E5%8F%98%E5%BC%82%E6%A3%80%E6%B5%8B%EF%BC%88BWA-SAMtools-picard-GATK%EF%BC%89/


############
[hg19版本]对于外显子测序分析，下载hg19数据吧，GATK官方建议
############


0.准备数据

##0.1 下载参考数据
For Best Practices short variant discovery in exome and other targeted sequencing: b37/hg19
(https://software.broadinstitute.org/gatk/download/bundle 2018.12.5)

链接ftp后下载：
location: ftp.broadinstitute.org/bundle
username: gsapubftp-anonymous
password:


输入如下命令： $ lftp ftp.broadinstitute.org -u gsapubftp-anonymous
回车后键入空格，便可进入resource bundle。进入其中名为bundle的目录，找到最新版的hg19目录。


$ cd /home/data/GATK/resources/bundle/hg19
## ftp://ftp.broadinstitute.org/bundle/hg19/ #11:12
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.gz &
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.dict.gz & 
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.fai.gz & 

wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/dbsnp_138.hg19.vcf.gz & 
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/dbsnp_138.hg19.vcf.idx.gz & 

wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.gz & 
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.idx.gz & 

wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/1000G_phase1.snps.high_confidence.hg19.sites.vcf.gz & 
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/1000G_phase1.snps.high_confidence.hg19.sites.vcf.idx.gz & 

wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/hapmap_3.3.hg19.sites.vcf.gz & 
wget -b ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/hapmap_3.3.hg19.sites.vcf.idx.gz & 

注意下载文件均为压缩文件，需要解压才能使用。
解压：
$ gunzip ucsc.hg19.fasta.gz
$ gunzip *gz

##0.2 建立bwa索引文件
$ mkdir bwa_index 
$ cd bwa_index
$ 
$ bwa index -a bwtsw \
	-p gatk_hg19 ../ucsc.hg19.fasta \
	1>hg19.bwa_index.log 2>&1 &
## 耗时 main] Real time: 4507.269 sec; CPU: 4451.448 sec








1.开始比对：map to reference[工具:BWA, MergeBamAlignments]
$ pwd
/home/data/ex23/wangjl_bwa

##1.1设定参考基因组，样本名
$ INDEX="/home/data/GATK/resources/bundle/hg19/bwa_index/gatk_hg19"
$ sample="EX23"


#使用raw reads进行mapping，由于该样本测了2个lane（每个R1、R2两个文件，共4个文件），每个lane分别map，最后合并sam文件。
$ bwa mem -t 6 -M -R "@RG\tID:$sample\tSM:$sample\tLB:WES\tPL:Illumina" $INDEX ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L1_1.fq.gz ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L1_2.fq.gz >${sample}_L1.sam
$ bwa mem -t 6 -M -R "@RG\tID:$sample\tSM:$sample\tLB:WES\tPL:Illumina" $INDEX ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L2_1.fq.gz ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L2_2.fq.gz >${sample}_L2.sam

也可以先合并文件，https://sourceforge.net/p/bio-bwa/mailman/message/31053122/
示例：bwa mem '<zcat R1_001.fastq.gz R1_002.fastq.gz R1_003.fastq.gz' '<zcat R2_001.fastq.gz R2_002.fastq.gz R2_003.fastq.gz' > out.sam
$ bwa mem -t 6 -M -R "@RG\tID:$sample\tSM:$sample\tLB:WES\tPL:Illumina" $INDEX '<zcat ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L1_1.fq.gz ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L2_1.fq.gz' '<zcat ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L1_2.fq.gz ../../RawData/B18091902001/B18091902001_NDHE06390-A25-A29_AHF3VNDMXX-new_L2_2.fq.gz' >${sample}.sam
[main] Real time: 4452.334 sec; CPU: 26123.824 sec





2.标记重复：Mark Duplicates[工具: MarkDuplicates, SortSam]

#2.1sort by coordinate, 对sam文件进行进行重新排序（reorder）
$ gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" SortSam -SO coordinate -I $sample.sam -O $sample.bam 1>gatk_SortSam.log 2>&1 &
#生成 EX23.bam #这个新生成的bam文件真小，只有16%(6.9G Bam/41G Sam)
## real    81m29.543s


#2.2 统计比对情况(可选)
做到这一步需要对序列比对情况进行统计，如果比对情况很差需要查找原因。
## sudo yum install gnuplot #需要依赖画图

samtools index -@ 6 $sample.bam  #-@ 6 线程数

samtools flagstat -@ 6 $sample.bam > ${sample}.alignment.flagstat 
samtools stats -@ 6 $sample.bam > ${sample}.alignment.stat
plot-bamstats -p ${sample}_QC ${sample}.alignment.stat #好几个图


#2.3 标记重复，主要是去掉PCR带来的重复。
nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" MarkDuplicates -I $sample.bam -O ${sample}_marked.bam -M $sample.metrics 1>log.mark 2>&1 &
##11:19->11:38

查看被标记重复的reads数。网站上sam文件flag的解释：1024表示是PCR重复Reads。
$ samtools view -f 1024 EX23_marked.bam|wc
17753967 320890672 8235579827

为wes.sorted.MarkDuplicates.bam创建索引文件，它的作用能够让我们可以随机访问这个文件中的任意位置，而且后面的步骤也要求这个BAM文件一定要有索引.
samtools index -@ 6 ${sample}_marked.bam  #-@ 6 线程数


//todo 
#步骤FixMateInformation有必要吗？貌似不必要了。
nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" FixMateInformation -I ${sample}_marked.bam -O ${sample}_marked_fixed.bam -SO coordinate 1>log.fix 2>&1 &




#2.4 Base (Quality Score) Recalibration[工具: BaseRecalibrator, Apply Recalibration, AnalyzeCovariates (optional)]
GATK4在核心算法层面并没太多的修改，但参数设置还是有些改变的，并且取消了RealignerTargetCreator、IndelRealigner，应该是HaplotypeCaller继承了这部分功能。


局部比对 Local relignment(可选)
(https://gatkforums.broadinstitute.org/gatk/discussion/1247/what-should-i-use-as-known-variantssites-for-running-tool-x)
It is recommended on GATK website that indelrealigner step could safely be ignored if haplotype caller is to be used for variant calling. 
也就是说接下来一步中用haplotype caller的话，就不需要再用indelrealigner步骤了。

1000G_phase1.indels.b37.vcf和Mills_and_1000G_gold_standard.indels.b37.vcf 这两个文件来自千人基因组和Mills项目，里面记录了那些项目中检测到的人群Indel区域。





3.变异检测：使用GATK的HaplotypeCaller命令
HaplotypeCaller: Call germline SNPs and indels via local re-assembly of haplotypes

The HaplotypeCaller is capable of calling SNPs and indels simultaneously via local de-novo assembly of haplotypes in an active region.


HaplotypeCaller和那些直接应用贝叶斯推断的算法有所不同，它会先推断群体的单倍体组合情况，计算各个组合的几率，然后根据这些信息再反推每个样本的基因型组合。因此它不但特别适合应用到群体的变异检测中，而且还能够依据群体的信息更好地计算每个个体的变异数据和它们的基因型组合。

一般来说，在实际的WGS流程中对HaplotypeCaller的应用有两种做法，差别只在于要不要在中间生成一个gVCF：

（1）直接进行HaplotypeCaller，这适合于单样本，或者那种固定样本数量的情况，也就是执行一次HaplotypeCaller之后就老死不相往来了。否则你会碰到仅仅只是增加一个样本就得重新运行这个HaplotypeCaller的坑爹情况（即，N+1难题），而这个时候算法需要重新去读取所有人的BAM文件，这将会是一个很费时间的痛苦过程；

（2）每个样本先各自生成gVCF，然后再进行群体joint-genotype。这其实就是GATK团队为了解决（1）中的N+1难题而设计出来的模式。gVCF全称是genome VCF，是每个样本用于变异检测的中间文件，格式类似于VCF，它把joint-genotype过程中所需的所有信息都记录在这里面，文件无论是大小还是数据量都远远小于原来的BAM文件。这样一旦新增加样本也不需要再重新去读取所有人的BAM文件了，只需为新样本生成一份gVCF，然后重新执行这个joint-genotype就行了。


本例就一个病人样本，就使用单样本模式。

## 首先设置好软件地址
GENOME=/home/data/GATK/resources/bundle/hg19/ucsc.hg19.fasta
DBSNP=/home/data/GATK/resources/bundle/hg19/dbsnp_138.hg19.vcf

nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" HaplotypeCaller \
	-R $GENOME -I ${sample}_marked.bam --dbsnp $DBSNP -O ${sample}_raw.vcf 1>log.HC 2>&1 &
##18:50-2:39 Elapsed time: 468.53 minutes(7.8h).
Running:
    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx12G -Djava.io.tmpdir=./ -jar /home/wangjl/Soft/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar HaplotypeCaller -R /home/data/GATK/resources/bundle/hg19/ucsc.hg19.fasta -I EX23_marked.bam --dbsnp /home/data/GATK/resources/bundle/hg19/dbsnp_138.hg19.vcf -O EX23_raw.vcf


最后得到的vcf文件如下：
$ ls -lth
total 55G
-rw-rw-r--. 1 wangjl wangjl 278K Dec  8 02:39 log.HC
-rw-rw-r--. 1 wangjl wangjl 5.7M Dec  8 02:39 EX23_raw.vcf.idx
-rw-rw-r--. 1 wangjl wangjl 116M Dec  8 02:39 EX23_raw.vcf





4.变异质控和过滤
Filter Variants by Variant (Quality Score) Recalibration[工具: VariantRecalibrator, ApplyRecalibration]

(1)
https://cloud.tencent.com/developer/news/180158
https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145

高灵敏度来避免漏掉稀有突变，但是需要过滤来减低大量的假阳性。
已经建立的功率raw variant的方法是VQSR（variant quality score recalibration），用机器学习识别稀有位点，为其分配一个比QUAL靠谱的VQSLOD分。
然后就可以用这个分来过滤结果，产生一个符合我们期待质量水品的子集，平衡特异性和敏感性。

接下来的recalibration 算法需要高质量的变异集合来训练和作为真集合，很多物种没有该数据。
也需要很多数据来学习profiles of good vs. bad variants，所以对于一个或少量样本不适用，也不适用于on targeted sequencing data, on RNAseq, and on non-model organisms。
这样就要使用hard-filtering了。为具体的注释设置阈值，并平等的应用到所有的变异上。查看方法学文章和相关FAQ。

我们现在正在尝试基于神经网络的更强大、更灵活的方法，来替代VQSR。


(2)执行硬过滤
https://gatkforums.broadinstitute.org/gatk/discussion/39/variant-quality-score-recalibration-vqsr
要注意的是，VQSR只适用于samples数目大于30的exome sequencing或者whole genome sequenicing。如果sample数目不够或者capture region过小，请参考http://www.broadinstitute.org/gatk/guide/topic?name=best-practices中对于此类问题的解决方法建议。一般是使用VariantFiltration进行hard filter。

$ pwd
/home/data/ex23/wangjl_bwa/3_callSNP

(1)# 使用SelectVariants，选出SNP(time=1.2min)
time gatk SelectVariants \
    -select-type SNP \
    -V ../2_mapping/${sample}_raw.vcf \
    -O ${sample}.snp.vcf.gz

# 为SNP作硬过滤(time=1.26min)
time gatk VariantFiltration \
    -V ${sample}.snp.vcf.gz \
    --filter-expression "QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0" \
    --filter-name "PASS" \
    -O ${sample}.snp.filter.vcf.gz



(2)# 使用SelectVariants，选出Indel(time=1.06min)
time gatk SelectVariants \
    -select-type INDEL \
    -V ../2_mapping/${sample}_raw.vcf \
    -O ${sample}.indel.vcf.gz

# 为Indel作过滤(time=1.03min)
time gatk VariantFiltration \
    -V ${sample}.indel.vcf.gz \
    --filter-expression "QD < 2.0 || FS > 200.0 || SOR > 10.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0" \
    --filter-name "PASS" \
    -O ${sample}.indel.filter.vcf.gz



(3) # 重新合并过滤后的SNP和Indel
time gatk MergeVcfs \
    -I ${sample}.snp.filter.vcf.gz \
    -I ${sample}.indel.filter.vcf.gz \
    -O ${sample}.filter.vcf.gz

# 删除无用中间文件
## $ rm -f ${sample}.snp.vcf.gz* ${sample}.snp.filter.vcf.gz* ${sample}.indel.vcf.gz* ${sample}.indel.filter.vcf.gz*


(4) 查看结果文件
$ zcat EX23.filter.vcf.gz|grep -v "#"|head

$ zcat EX23.filter.vcf.gz|grep -v "#"|wc
627989 6279890 123423242





========================================
|-- gatk4.0 实践 (hg38) 未来趋势
----------------------------------------
主要参考： http://www.bio-info-trainee.com/3144.html
其他参考： https://www.plob.org/article/7009.html
GATK4最佳实践-数据预处理篇 https://www.jianshu.com/p/e306dc2307e5

GATK4 最佳实践-生殖细胞突变的检测与识别 https://www.jianshu.com/p/6f3198b7a070

官方推的GATK4教程：https://drive.google.com/drive/folders/1U6Zm_tYn_3yeEgrD1bdxye4SXf5OseIt






0.准备数据
##0.1 下载参考数据
$ mkdir -p /home/data/GATK/resources/bundle/hg38
$ cd /home/data/GATK/resources/bundle/hg38
## ftp://ftp.broadinstitute.org/bundle/hg38/
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/dbsnp_146.hg38.vcf.gz 2>dbsnp_146.hg38.vcf.gz.log & 
#nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/dbsnp_146.hg38.vcf.gz.tbi & 
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz 2>Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.log & 
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi 2>Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi.log & 
#nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.fasta.gz & 
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.fasta.fai 2>Homo_sapiens_assembly38.fasta.fai.log & 
#nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict & 
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/1000G_phase1.snps.high_confidence.hg38.vcf.gz 2>1000G_phase1.snps.high_confidence.hg38.vcf.gz.log & 
nohup wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi 2>1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi.log & 

##0.2 建立bwa索引文件
$ mkdir bwa_index 
$ cd bwa_index
$ bwa index -a bwtsw \
	-p gatk_hg38 ../Homo_sapiens_assembly38.fasta \
	1>hg38.bwa_index.log 2>&1 &
##-p STR    prefix of the index [same as fasta name]
##[main] Real time: 4686.724 sec; CPU: 4628.838 sec









1.开始比对：map to reference

##1.1设定参考基因组，样本名
$ INDEX="/home/data/GATK/resources/bundle/hg38/bwa_index/gatk_hg38"
$ sample="WES01_2"

#1.2开始比对
#使用clean data进行mapping
## bwa mem -t 6 -M -R "@RG\tID:$sample\tSM:$sample\tLB:WES\tPL:Illumina" $INDEX ../1_cleanData/outN.1.fastq ../1_cleanData/outN.2.fastq >$sample.sam #WES01.sam


是否需要在第一步做fastqc和cutadapt过滤？
如果过滤，则不容易mark duplicate。
因为mark是比对5'和3'端，一样就认为重复。


#使用raw reads进行mapping
$ bwa mem -t 6 -M -R "@RG\tID:$sample\tSM:$sample\tLB:WES\tPL:Illumina" $INDEX ../0_fastqc/B1709060700_NDHE00383-A84-A68_AH7277CCXY_L7_1.fq ../0_fastqc/B1709060700_NDHE00383-A84-A68_AH7277CCXY_L7_2.fq >$sample.sam ##WES01_2.sam
[main] Real time: 3044.140 sec; CPU: 17879.608 sec









2.对bam进行clean：Mark Duplicates
#2.1sort by coordinate
$ gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" SortSam -SO coordinate -I $sample.sam -O $sample.bam 1>log.sort 2>&1 &
#生成 WES01_2.bam #这个新生成的bam文件真小，只有10%多点(4.1G Bam/34G Sam)


#2.2 索引和统计
$ sudo yum install gnuplot #需要依赖画图

samtools index $sample.bam
# -@ 6 线程数
samtools flagstat -@ 6 $sample.bam > ${sample}.alignment.flagstat 
samtools stats -@ 6 $sample.bam > ${sample}.alignment.stat
echo plot-bamstats -p ${sample}_QC ${sample}.alignment.stat ##？没有看到图//todo


#2.3 标记重复
nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" MarkDuplicates -I $sample.bam -O ${sample}_marked.bam -M $sample.metrics 1>log.mark 2>&1 &
##11:19->11:38

nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" FixMateInformation -I ${sample}_marked.bam -O ${sample}_marked_fixed.bam -SO coordinate 1>log.fix 2>&1 &

samtools index ${sample}_marked_fixed.bam


为了节省磁盘空间，最后只需要留学一个bam文件即可.


3.Base(Quality Score) Recalibration(optional)
??




4.直接找变异：使用GATK的HaplotypeCaller命令

## 首先设置好软件地址
GENOME=/home/data/GATK/resources/bundle/hg38/Homo_sapiens_assembly38.fasta
DBSNP=/home/data/GATK/resources/bundle/hg38/dbsnp_146.hg38.vcf.gz

nohup gatk --java-options "-Xmx12G -Djava.io.tmpdir=./" HaplotypeCaller \
	-R $GENOME -I ${sample}_marked_fixed.bam --dbsnp $DBSNP -O ${sample}_raw.vcf 1>log.HC 2>&1 &
##14:03->
## Elapsed time: 492.58 minutes.(8Hours)

Running:
    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx12G -Djava.io.tmpdir=./ -jar /home/wangjl/Soft/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar HaplotypeCaller -R /home/data/GATK/resources/bundle/hg38/Homo_sapiens_assembly38.fasta -I WES01_2_marked_fixed.bam --dbsnp /home/data/GATK/resources/bundle/hg38/dbsnp_146.hg38.vcf.gz -O WES01_2_raw.vcf


最后得到的vcf文件如下；
$ ls -lth
total 110G
-rw-rw-r--. 1 wangjl wangjl 323K Nov  9 22:15 log.HC
-rw-rw-r--. 1 wangjl wangjl 2.3M Nov  9 22:15 WES01_2_raw.vcf.idx
-rw-rw-r--. 1 wangjl wangjl 185M Nov  9 22:15 WES01_2_raw.vcf

这些VCF格式的变异记录该如何理解，以及GATK的每一个步骤是否仍然必须?





############################
对vcf文件的常用统计和处理
############################
1)Count variants:
$ grep -v "#" WES01_2_raw.vcf | wc

2)Extract sites located between positons 10000 and 20000 on chromosome chr2 and save them in a new VCF file:
$ head -1000 WES01_2_raw.vcf | grep "#" > new_file.vcf 
$ grep -v "#" WES01_2_raw.vcf | \
	awk '{if($1=="chr2" && $2 >=10000 && $2 <=20000) print}' >> new_file.vcf

3)Extract variants with quality (QUAL) greater than 100 (the resulting file will have no header!):
$ grep -v "#" WES01_2_raw.vcf | awk '{if($6>100) print}' > good_variants








5.获取的snp和indel文件，并使用硬过滤的策略
GATK4.0和全基因组数据分析实践（下）
原创： 矿工  碱基矿工  2018.3月23日

(1)
# 使用SelectVariants，选出SNP
time gatk SelectVariants \
    -select-type SNP \
    -V ../2_mapping/WES01_2_raw.vcf \
    -O WES01_2_.snp.vcf.gz

# 为SNP作硬过滤
time gatk VariantFiltration \
    -V WES01_2_.snp.vcf.gz \
    --filter-expression "QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0" \
    --filter-name "PASS" \
    -O WES01_2_.snp.filter.vcf.gz
	
(2)
# 使用SelectVariants，选出Indel
time gatk SelectVariants \
    -select-type INDEL \
    -V ../2_mapping/WES01_2_raw.vcf \
    -O WES01_2_.indel.vcf.gz

# 为Indel作过滤
time gatk VariantFiltration \
    -V WES01_2_.indel.vcf.gz \
    --filter-expression "QD < 2.0 || FS > 200.0 || SOR > 10.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0" \
    --filter-name "PASS" \
    -O WES01_2_.indel.filter.vcf.gz

(3)
# 重新合并过滤后的SNP和Indel
time gatk MergeVcfs \
    -I WES01_2_.snp.filter.vcf.gz \
    -I WES01_2_.indel.filter.vcf.gz \
    -O WES01_2_.filter.vcf.gz

# 删除无用中间文件
$ rm -f WES01_2_.snp.vcf.gz* WES01_2_.snp.filter.vcf.gz* WES01_2_.indel.vcf.gz* WES01_2_.indel.filter.vcf.gz*



https://biohpc.cornell.edu/lab/doc/Variant_workshop_Part1.pdf
https://biohpc.cornell.edu/lab/doc/Variant_workshop_Part2.pdf










========================================
|-- Workflow Description Language (WDL)语言
----------------------------------------
1.简介
http://www.openwdl.org/

语法方便人读写的指定数据处理流程的方法。The Workflow Description Language (WDL) is a way to specify data processing workflows with a human-readable and -writeable syntax. 

直接定义任务，链接起来，并行处理。WDL makes it straightforward to define analysis tasks, chain them together in workflows, and parallelize their execution. 

The language makes common patterns simple to express, while also admitting uncommon or complicated behavior; and strives to achieve portability not only across execution platforms, but also different types of users. Whether one is an analyst, a programmer, an operator of a production system, or any other sort of user, WDL should be accessible and understandable.


2.Getting Started 快速入门教程
https://software.broadinstitute.org/wdl/documentation/quickstart

(1)基本结构 Base Structure
https://software.broadinstitute.org/wdl/documentation/structure.php
https://blog.csdn.net/theomarker/article/details/79627804

WDL有5个基本成分： workflow, task, call, command and output. 
没有明确的input定义组件；输入参数（指定参数，包括输入和输出文件名）。
还有些可选组件可以指定运行时参数（环境条件，比如Docker image），作者和邮件等meta信息，输入和输出的parameter_meta 描述——但现在我们已经不再担心这些了。

在一个小型的WDL脚本中，一个叫做 myWorkflowName 的workflow，和两个tasks(task_A和task_B)







(2)添加变量 Add Variables: 外部设定输入文件、参数
(3)添加管道 Add Plumbing: 把组件组合成管线pipeline
(4)验证语法 Validate Syntax: 防止少了一个分号之类的错误


3.Running a WDL
产生一个JSON模板，指定输入（很简单）。
执行WDL脚本的选项，我们用的引擎是Cromwell.

(1)Specify Inputs

(2)Execute!



4.When you're ready
(1) WDLTool: https://github.com/broadinstitute/wdltool/releases
wget https://github.com/broadinstitute/wdltool/releases/download/0.14/wdltool-0.14.jar
(2) Text editor:用的 vim
(3) Cromwell: 可以执行WDL，描述数据处理和分析流程，包含命令行工具(比如执行 Variant Discovery的GATK管线)
如果你熟悉GATK，可能听过用Qscripts写的叫做Queue的执行引擎。Cromwell and WDL 是 Queue and Qscripts的一个用户友好的替代.

https://github.com/broadinstitute/cromwell/releases
$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar
$ java -jar cromwell.jar #Cromwell是一个开源的由Java编写支持WDL的执行工具，需要java8环境。

下载jar包，并放到路径中。

Running WDL on Cromwell locally
$ java -jar Cromwell.jar run myWorkflow.wdl --inputs myWorkflow_inputs.json



(4) java8
https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html

$ java --version
openjdk 10.0.2 2018-07-17
OpenJDK Runtime Environment (build 10.0.2+13-Ubuntu-1ubuntu0.18.04.3)
OpenJDK 64-Bit Server VM (build 10.0.2+13-Ubuntu-1ubuntu0.18.04.3, mixed mode)

(5) Docker (optional)
https://docs.docker.com/install/


(6) Programs to be pipelined 需要流程化的程序 
我们的教程使用GATK和Picard工具展示如何写WDL脚本。首先安装这俩工具，按照GATK即可。
此外还推荐安装R包gsalib。











========================================
**** vcf的注释和过滤
----------------------------------------


========================================
|-- 用vcftools来处理vcf文件 //todo
----------------------------------------
1.概述
https://vcftools.github.io/index.html
VCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project. The aim of VCFtools is to provide easily accessible methods for working with complex genetic variation data in the form of VCF files.

http://www.1000genomes.org/ 改名为 http://www.internationalgenome.org/


This toolset can be used to perform the following operations on VCF files:

过滤 Filter out specific variants
比较 Compare files
汇总 Summarize variants
转换 Convert to different file types
验证合并 Validate and merge files
求交集 Create intersections and subsets of variants

VCFtools consists of two parts, a perl module and a binary executable. 
The perl module is a general Perl API for manipulating VCF files, whereas the binary executable provides general analysis routines.




2.下载与安装
https://vcftools.github.io/downloads.html


## Ubuntu 18.04 LTS
$ wget -c https://github.com/vcftools/vcftools/releases/download/v0.1.16/vcftools-0.1.16.tar.gz

$ tar zxvf vcftools-0.1.16.tar.gz
$ cd vcftools-0.1.16$
$ ./configure
$ make
$ sudo make install

$ vcftools --version
VCFtools (0.1.16)








3.使用案例
bcftools或vcftools提取指定区段的vcf文件（extract specified position）
http://www.cnblogs.com/chenwenyan/p/9213394.html
https://www.biostars.org/p/162872/


如果只想提取指定位置（specific position）的基因型（genotypes），则可以用到vcftools工具

$ vcftools --gzvcf file.vcf.gz --positions specific_position.txt --recode --out specific_position.vcf

specific_position.txt的输入格式如下：
1 842013
1 891021
1 903426
1 949654
1 1018704


帮助：https://vcftools.github.io/man_latest.html#EXAMPLES












========================================
|-- 变异功能注释 ANNOVAR (hg19)
----------------------------------------

将原始fq文件通过FastQC-bwa-samtools|GATK等流程最终得到vcf文件，也就是记录某些位点变异的文本文件。但只是通过看vcf文件我们是不知道些变异位点到底是位于基因的exon、intron、UTR等的哪些区域的。所以我们需要对vcf文件也就是这些变异位点进行注释。

https://www.jianshu.com/p/6284f57664b9
目前对于variant进行注释的软件主要有4个: Annovar,  SnpEff, VEP(variant Effect Predictor), Oncotator[没听说过]。
最常用的vcf注释软件有annovar和snpEff。



1. ANNOVAR简介(邮件注册才能下载)
ANNOVAR是由王凯编写的一个注释软件，可以对SNP和indel进行注释，也可以进行变异的过滤筛选。
官网 http://annovar.openbioinformatics.org/en/latest/

ANNOVAR能够利用最新的数据来分析各种基因组中的遗传变异。主要包含三种不同的注释方法，Gene-based Annotation（基于基因的注释）、Region-based Annotation（基于区域的注释）、Filter-based Annotation（基于筛选的注释）。

ANNOVAR由Perl编写。
优点：提供多个数据可直接下载、支持多种格式、注释直观；
缺点：没有数据库的物种无法注释。


软件新闻请关注谷歌小组： 
https://groups.google.com/forum/#!forum/annovar






2.安装 https://www.jianshu.com/p/b2b70202d7f2
$ tar zxvf annovar.latest.tar.gz ##解压 
解压后生成annovar文件夹，里面有6个perl脚本程序和两个文件夹，其中一个是example文件夹，另一个是已经建立好的hg19或者GRCh37的humandb的数据库文件夹，可用于人的注释。

加入到path中
$ vim ~/.bashrc 
##末尾加入
export PATH=/home/wangjl/Soft/annovar/:$PATH 

$ source ~/.bashrc
then typing annotate_variation.pl would be okay instead of typing perl annotate_variation.pl). 

First, we need to download appropriate database files using annotate_variation.pl, 
and next we will run the table_annovar.pl program to annotate the variants in the example/ex1.avinput file.








3.使用Annovar建库[hg19]
Function: annotate a list of genetic variants against genome annotation databases stored at local disk. 
人的注释方法，官网介绍的很详细，但仅仅有人的数据库肯定是满足不了大家的需求。

先在annovar文件夹里面创建 humandb19 文件夹（名字可自取），命令
$ cd /home/wangjl/Soft/annovar/
$ mkdir humandb19

然后一个一个执行命令：使用annovar文件夹下的perl程序annotate_variation.pl
## --downdb                    download annotation database 
## --webfrom <string>          specify the source of database (ucsc or annovar or URL) (downdb operation)  

## --thread <int>              use multiple threads for filter-based annotation
# --maxgenethread <int>       max number of threads for gene-based annotation (default: 6)


(1)$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar refGene humandb19/
这个命令是下载 hg19 的refGene的文件，保存在 humandb19 文件下，自动解压后文件名为 hg19_refGene.txt。
total 218M
-rw-rw-r--. 1 wangjl wangjl  993 Dec 10 10:47 annovar_downdb.log
-rw-rw-r--. 1 wangjl wangjl 200M Jun  2  2017 hg19_refGeneMrna.fa
-rw-rw-r--. 1 wangjl wangjl  18M Jun  2  2017 hg19_refGene.txt
-rw-rw-r--. 1 wangjl wangjl 808K Jun  2  2017 hg19_refGeneVersion.txt

(2)$ annotate_variation.pl -buildver hg19 -downdb cytoBand humandb19/
-rw-rw-r--. 1 wangjl wangjl  31K Jun 15  2009 hg19_cytoBand.txt

(3)$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar exac03 humandb19/ 
-rw-rw-r--. 1 wangjl wangjl  23M Nov 30  2015 hg19_exac03.txt.idx
-rw-rw-r--. 1 wangjl wangjl 601M Nov 30  2015 hg19_exac03.txt

(4) 有两个分别是20160606和20170929发布的。
$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar avsnp147 humandb19/
-rw-rw-r--. 1 wangjl wangjl 885M Jun  7  2016 hg19_avsnp147.txt.idx
-rw-rw-r--. 1 wangjl wangjl 5.9G Jun  7  2016 hg19_avsnp147.txt

$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar avsnp150 humandb19/
-rw-rw-r--. 1 wangjl wangjl 916M Sep 30  2017 hg19_avsnp150.txt.idx
-rw-rw-r--. 1 wangjl wangjl  13G Sep 30  2017 hg19_avsnp150.txt

(5)
whole-exome SIFT, PolyPhen2 HDIV, PolyPhen2 HVAR, LRT, MutationTaster, MutationAssessor, FATHMM, MetaSVM, MetaLR, VEST, CADD, GERP++, DANN, fitCons, PhyloP and SiPhy scores from dbNSFP version 3.0a
$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar dbnsfp30a humandb19/

$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar dbnsfp35c humandb19/
## same as above, suitable for commercial use
hg19_dbnsfp35c.txt.gz   20181119        4245316428
hg19_dbnsfp35c.txt.idx.gz       20181119        5084100

前五个命令下载一堆注释文件到 humandb19/






####1. 尝试下载数据库。官方下载数据库到底有多少？
http://annovar.openbioinformatics.org/en/latest/user-guide/download/ 直接在这个目录搜索hg19下的数据库名字。
或
$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar avdblist humandb19/
则在 humandb19 目录下下载一个文本文件hg19_avdblist.txt，包含现有的 hg19 的数据库320行。
用grep查找相关库的名字 $ cat humandb19/hg19_avdblist.txt|grep exac


###2. 该选择哪些数据库？
https://doc-openbio.readthedocs.io/projects/annovar/en/latest/user-guide/filter/#summary-of-databases

For frequency of variants in whole-genome data:

1000g2015aug: latest 1000 Genomes Project dataset with allele frequencies in six populations including ALL, AFR (African), AMR (Admixed American), EAS (East Asian), EUR (European), SAS (South Asian). These are whole-genome variants.
kaviar_20150923: latest Kaviar database with 170 million variants from 13K genomes and 64K exomes.
hrcr1: latest Haplotype Reference Consortium database with 40 million variants from 32K samples in haplotype reference consortium
cg69: allele frequency in 69 human subjects sequenced by Complete Genomics. useful to exclude platform specific variants.
gnomad_genome: allele frequency in gnomAD database whole-genome sequence data on multiple populations.
For frequency of variants in whole-exome data:

exac03: latest Exome Aggregation Consortium dataste with allele frequencies in ALL, AFR (African), AMR (Admixed American), EAS (East Asian), FIN (Finnish), NFE (Non-finnish European), OTH (other), SAS (South Asian).
esp6500siv2: latest NHLBI-ESP project with 6500 exomes. Three separate key words are used for 3 population groupings: esp6500siv2_all, esp6500siv2_ea, esp6500siv2_aa.
gnomad_exome: allele frequency in gnomAD database whole-exome sequence data on multiple populations.


(6)$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar snp142 humandb19/
-rw-rw-r--. 1 wangjl wangjl 104M Jan 24  2016 hg19_snp142.txt.idx
-rw-rw-r--. 1 wangjl wangjl  20G Jan 24  2016 hg19_snp142.txt


(7)$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar 1000g2015aug humandb19/

分为sas 南亚 ，eas 东亚 等好几个文件。 
-rw-r--r--. 1 wangjl wangjl  89M Aug 26  2015 hg19_ALL.sites.2015_08.txt.idx
-rw-r--r--. 1 wangjl wangjl 3.2G Aug 26  2015 hg19_ALL.sites.2015_08.txt
-rw-r--r--. 1 wangjl wangjl  87M Aug 26  2015 hg19_AFR.sites.2015_08.txt.idx
-rw-r--r--. 1 wangjl wangjl  85M Aug 26  2015 hg19_AMR.sites.2015_08.txt.idx
-rw-r--r--. 1 wangjl wangjl  85M Aug 26  2015 hg19_SAS.sites.2015_08.txt.idx
-rw-r--r--. 1 wangjl wangjl  85M Aug 26  2015 hg19_EUR.sites.2015_08.txt.idx
-rw-r--r--. 1 wangjl wangjl 1.5G Aug 26  2015 hg19_AFR.sites.2015_08.txt
-rw-r--r--. 1 wangjl wangjl  85M Aug 26  2015 hg19_EAS.sites.2015_08.txt.idx
-rw-r--r--. 1 wangjl wangjl 1.1G Aug 26  2015 hg19_AMR.sites.2015_08.txt
-rw-r--r--. 1 wangjl wangjl 968M Aug 26  2015 hg19_SAS.sites.2015_08.txt
-rw-r--r--. 1 wangjl wangjl 879M Aug 26  2015 hg19_EUR.sites.2015_08.txt
-rw-r--r--. 1 wangjl wangjl 857M Aug 26  2015 hg19_EAS.sites.2015_08.txt



(8)见(4) 
(9)$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar clinvar_20180603 humandb19/
CLINVAR database with Variant Clinical Significance (unknown, untested, non-pathogenic, probable-non-pathogenic, probable-pathogenic, pathogenic, drug-response, histocompatibility, other) and Variant disease name
hg19|clinvar_20180603|Clinvar version 20180603 with separate columns (CLNALLELEID CLNDN CLNDISDB CLNREVSTAT CLNSIG)|20180708
-rw-rw-r--. 1 wangjl wangjl 1.3M Nov 20 12:08 hg19_clinvar_20180603.txt.idx
-rw-rw-r--. 1 wangjl wangjl  72M Nov 20 12:08 hg19_clinvar_20180603.txt

(10)$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar cosmic70 humandb19/
-rw-rw-r--. 1 wangjl wangjl 6.6M Sep 12  2014 hg19_cosmic70.txt.idx
-rw-rw-r--. 1 wangjl wangjl  87M Sep 12  2014 hg19_cosmic70.txt


(11)
$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar esp6500siv2_all humandb19/
$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar esp6500siv2_ea humandb19/

alternative allele frequency in All subjects in the NHLBI-ESP project with 6500 exomes, including the indel calls and the chrY calls. This is lifted over from hg19 by myself.

-rw-rw-r--. 1 wangjl wangjl  88M Dec 23  2014 hg19_esp6500siv2_all.txt
-rw-rw-r--. 1 wangjl wangjl  17M Dec 23  2014 hg19_esp6500siv2_all.txt.idx

-rw-rw-r--. 1 wangjl wangjl  57M Dec 23  2014 hg19_esp6500siv2_ea.txt
-rw-rw-r--. 1 wangjl wangjl  15M Dec 23  2014 hg19_esp6500siv2_ea.txt.idx

(12)
$ annotate_variation.pl -buildver hg19 -downdb -webfrom annovar ljb26_all humandb19/
ljb26_all: whole-exome SIFT, PolyPhen2 HDIV, PolyPhen2 HVAR, LRT, MutationTaster, MutationAssessor, FATHMM, MetaSVM, MetaLR, VEST, CADD, GERP++, PhyloP and SiPhy scores from dbNSFP version 2.6

2020-12-21 23:44:36 (573 KB/s) - ‘hg38_ljb26_all.txt.gz’ saved [2311601075/2311601075]










4.使用 table_annovar 注释[hg19] 
annovar有三种注释方式Gene-based Annotation(基于基因的注释)，Region-based Annotation（基于区域的注释），Filter-based Annotation（基于过滤的注释）。看起来很复杂实际做起来很简单，用table_annovar.pl进行注释，可一次性完成三种类型的注释。


(1)注释语句实例
1)annovar 官方句子
$ table_annovar.pl example/ex1.avinput humandb38/ -buildver hg38 -out myanno -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -csvout -polish --xreffile example/gene_xref.txt 

#最后用 --xreffile 和 -xref 有啥区别？应该没啥区别，尽量用全长。 
--xreffile <file>           specify a cross-reference file for gene-based annotation 

该一个命令使用 table_annovar ，使用 ExAC version 0.3 (referred to as exac03) dbNFSP version 3.0a (referred to as dbnsfp30a), dbSNP version 147 with left-normalization (referred to as avsnp147) databases 并删除临时文件, 生成的输出文件为 myanno.hg19_multianno.txt.
 
没有注释信息的用点号.填充。前面的注释行用#开头。交叉引用的文件包含了15个类的基因注释。
也可以用gene_fullxref.txt代替gene_xref.txt，可以从这里下载：

支持直接输入vcf文件 table_annovar.pl can directly support input and output of VCF files (the annotation will be written to the INFO field of the output VCF file). Let's try this:
$ table_annovar.pl example/ex2.vcf humandb38/ -buildver hg38 -out myanno3 -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput

Similarly, if you generated variant calls from human GRCh38 coordinate, add -buildver hg38 in every command, 




2)诺禾致源给的csv报告中的频率
#[25] "X1000g2015aug_Chinese"(x) "X1000g2015aug_eas"     "X1000g2015aug_all"    
#[28] "esp6500siv2_all"       "ExAC_ALL"(x)              "ExAC_EAS"(x) 

http://annovar.openbioinformatics.org/en/latest/user-guide/download/
hg19|1000g2015aug (6 data sets)|The 1000G team fixed a bug in chrX frequency calculation. Based on 201508 collection v5b (based on 201305 alignment)	|20150824
hg19|esp6500siv2_ea|alternative allele frequency in European American subjects in the NHLBI-ESP project with 6500 exomes, including the indel calls and the chrY calls. This is lifted over from hg19 by myself|20141222
hg19|exac03|ExAC 65000 exome allele frequency data for ALL, AFR (African), AMR (Admixed American), EAS (East Asian), FIN (Finnish), NFE (Non-finnish European), OTH (other), SAS (South Asian)). version 0.3. Left normalization done.|20151129

-rw-r--r--. 1 wangjl wangjl 3.2G Aug 26  2015 hg19_ALL.sites.2015_08.txt
-rw-r--r--. 1 wangjl wangjl 857M Aug 26  2015 hg19_EAS.sites.2015_08.txt
-rw-rw-r--. 1 wangjl wangjl  88M Dec 23  2014 hg19_esp6500siv2_all.txt
-rw-rw-r--. 1 wangjl wangjl  57M Dec 23  2014 hg19_esp6500siv2_ea.txt
-rw-rw-r--. 1 wangjl wangjl 601M Nov 30  2015 hg19_exac03.txt


ExAC数据库简介：https://www.jianshu.com/p/da8ddd7e4903
 1000 Genomes（共2,504个,全基因组和外显子）
 ESP（6,503个，仅外显子）
 ExAC（60,706个，仅外显子）



3)公众号见到的，使用了更多的库
table_annovar.pl example/ex1.avinput humandb/ -buildver hg19 -out myanno -remove -protocol refGene,cytoBand,genomicSuperDups,esp6500siv2_all,1000g2014oct_all,1000g2014oct_afr,1000g2014oct_eas,1000g2014oct_eur,snp138,ljb26_all -operation g,r,r,f,f,f,f,f,f,f -nastring . -csvout

# -buildver hg38 表示使用hg38版本
# -out myanno 表示输出文件的前缀为myanno
# -remove 表示删除注释过程中的临时文件
# -protocol 表示注释使用的数据库，用逗号隔开，且要注意顺序
# -operation 表示对应顺序的数据库的类型（g代表gene-based、r代表region-based、f代表filter-based），用逗号隔开，注意顺序
# -nastring . 表示用点号替代缺省的值
# -csvout 表示最后输出.csv文件

# --polish   polish the protein notation for indels (such as p.G12Vfs*2)
# --xreffile <file>     specify a cross-reference file for gene-based annotation

# -otherinfo   To show the categorical predictions in the output file



4) csdn博客中的例子
https://blog.csdn.net/theomarker/article/details/82872201
$ table_annovar.pl CJ-258-candidate.vcf annovar2/humandb/hg19/ -buildver hg19 -out anno/CJ-258-candidate -remove -protocol refGene,genomicSuperDups,phastConsElements46way,esp6500siv2_all,exac03,1000g2014oct_eas,1000g2014oct_all,avsnp142,clinvar_20180603,scsnv,revel,mcap,cosmic68wgs,ljb26_all -operation g,r,r,f,f,f,f,f,f,f,f,f,f,f -nastring . -vcfinput


原文：https://blog.csdn.net/theomarker/article/details/82873582 
$ table_annovar.pl diabetes-candidate.vcf /hg19/ -buildver hg19 -out anno/diabetes-candidate -remove -protocol refGene,genomicSuperDups,phastConsElements46way,esp6500siv2_all,exac03,1000g2014oct_eas,1000g2014oct_all,gnomad_exome,avsnp142,clinvar_20170130,scsnv,revel,mcap,cosmic68wgs,ljb26_all -operation g,r,r,f,f,f,f,f,f,f,f,f,f,f,f -nastring . -vcfinput





(2) vcf文件位置：/home/data/ex23/wangjl_bwa/3_callSNP/EX23.filter.vcf.gz

1)预处理：
转化gatk输出为annovar输入:
$ perl convert2annovar.pl infile.vcf -format vcf4 > outfile

$ zcat EX23.filter.vcf.gz|wc
 628110 6280582 123434549
$ zcat EX23.filter.vcf.gz|grep -v "^#" |wc
 627989 6279890 123423242
$ convert2annovar.pl EX23.filter.vcf.gz -format vcf4 > outfile
NOTICE: Finished reading 628110 lines from VCF file
NOTICE: A total of 627989 locus in VCF file passed QC threshold, representing 544359 SNPs (377176 transitions and 167183 transversions) and 84378 indels/substitutions
NOTICE: Finished writing 544359 SNP genotypes (377176 transitions and 167183 transversions) and 84378 indels/substitutions for 1 sample
WARNING: 256 invalid alternative alleles found in input file

行数基本没有变化：行数反而多了？
$ wc outfile
628993  5031944 25837245 outfile

每行丢失的信息太多了。
之前：chrM    152     rs117135796     T       C       139.03  PASS    AC=2;AF=1.00;AN=2;DB;DP=4;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=34.76;SOR=0.693      GT:AD:DP:GQ:PL  1/1:0,4:4:12:167,12,0
之后：chrM    152     152     T       C       hom     139.03  4 

$ mv outfile outfile EX23.filter.vcf.avinput
$ head /home/data/ex23/wangjl_bwa/3_callSNP/EX23.filter.vcf.avinput
chrM    152     152     T       C       hom     139.03  4
chrM    195     195     C       T       hom     127.03  4
chrM    303     304     CC      -       hom     146.87  5
chrM    410     410     A       T       hom     222.80  7



2)运行注释

##run_A
$ table_annovar.pl /home/data/ex23/wangjl_bwa/3_callSNP/EX23.filter.vcf.avinput humandb19/ -buildver hg19 -out EX23_ -remove -protocol refGene,cytoBand,1000g2015aug_all,1000g2015aug_eas,esp6500siv2_all,exac03,snp142,avsnp150,ljb26_all,clinvar_20180603,dbnsfp30a -operation g,r,f,f,f,f,f,f,f,f,f -nastring . -csvout
## time 10:35-10:57
# 漏掉2个下载好的库，再来一遍。


i)For functional prediction of variants in whole-exome data:
dbnsfp30a: this dataset already includes SIFT, PolyPhen2 HDIV, PolyPhen2 HVAR, LRT, MutationTaster, MutationAssessor, FATHMM, MetaSVM, MetaLR, VEST, CADD, GERP++, DANN, fitCons, PhyloP and SiPhy scores, but ONLY on coding variants

Clinvar version 20180603 with separate columns (CLNALLELEID CLNDN CLNDISDB CLNREVSTAT CLNSIG)

具体小库怎么指定？难道要用：ExAC_ALL所有人群，ExAC_EAS东亚人？看来不行，除了1000g之外，其他库名都要和文件名一致。
Error: the required database file humandb19/hg19_exac03_eas.txt does not exist.
hg19_1000g2015aug_Chinese.txt does not exist.


##run_B
$ nohup table_annovar.pl /home/data/ex23/wangjl_bwa/3_callSNP/EX23.filter.vcf.avinput humandb19/ -buildver hg19 -out EX23_B -remove -protocol refGene,cytoBand,1000g2015aug_eas,1000g2015aug_all,esp6500siv2_ea,esp6500siv2_all,exac03,snp142,avsnp150,ljb26_all,clinvar_20180603,dbnsfp30a,dbnsfp35c -operation g,r,f,f,f,f,f,f,f,f,f,f,f -nastring . -csvout &


##run_C
$ nohup table_annovar.pl /home/data/ex23/wangjl_bwa/3_callSNP/EX23.filter.vcf.gz humandb19/ -buildver hg19 -out EX23_C -remove -protocol refGene,cytoBand,1000g2015aug_eas,1000g2015aug_all,esp6500siv2_ea,esp6500siv2_all,exac03,snp142,avsnp150,ljb26_all,clinvar_20180603,dbnsfp30a,dbnsfp35c -operation g,r,f,f,f,f,f,f,f,f,f,f,f -nastring . -vcfinput &



3)优化后从vcf->avinput->csv结果
#dir
cd /home/wangjl/software/annovar/

# vcf 2 avinput
convert2annovar.pl  -includeinfo -allsample -withfreq -format vcf4 /home/wangjl/data/190521/callSNP/19C001854_WES001_CapNGS.filter.vcf.gz > /home/wangjl/data/190521/callSNP/19C001854_WES001_CapNGS.filter_B_.avinput

#avinput to csv
table_annovar.pl /home/wangjl/data/190521/callSNP/19C001854_WES001_CapNGS.filter_B_.avinput  \
	humandb19/ -buildver hg19  \
	-outfile /home/wangjl/data/190521/callSNP/19C001854_WES001_CapNGS.filter_B_ -remove  \
	-protocol refGene,cytoBand,1000g2015aug_eas,1000g2015aug_all,esp6500siv2_ea,esp6500siv2_all,exac03,snp142,avsnp150,ljb26_all,clinvar_20180603,dbnsfp30a,dbnsfp35c  \
	-operation g,r,f,f,f,f,f,f,f,f,f,f,f -nastring . -otherinfo -csvout

#



4)使用py构建的.avinput输入文件，需要注释出位置(exon、intron、UTR3等)和基因名字。
$ annotate_variation.pl -geneanno -dbtype refGene -buildver hg19 all.gt5_no4A.avinput /home/wangjl/data/software/annovar/humandb19/ -out all.gt5_no4A.anno_3
$ head all.gt5_no4A.anno_3.variant_function
UTR3    PFKP(NM_001323069:c.*221T>G,NM_001323068:c.*221T>G,NM_001323071:c.*221T>G,NM_002627:c.*221T>G,NM_001323067:c.*221T>G,NM_001323072:c.*221T>G,NM_001242339:c.*221T>G,NM_001345944:c.*221T>G,NM_001323070:c.*221T>G,NM_001323073:c.*221T>G,NM_001323074:c.*221T>G) chr10 3178992  3178992 T       G       chr10:3178992:+
UTR3    AKR1C1(NM_001353:c.*208T>G)     chr10   5020142 5020142 T       G       chr10:5020142:+
UTR3    AKR1C2(NM_001321027:c.*222A>C,NM_001354:c.*222A>C,NM_205845:c.*222A>C)  chr10   5031966 5031966 T       G       chr10:5031966:-
UTR3    AKR1C2(NM_001321027:c.*208A>C,NM_001354:c.*208A>C,NM_205845:c.*208A>C)  chr10   5031980 5031980 T       G       chr10:5031980:-
UTR3    AKR1C2(NM_001321027:c.*207A>C,NM_001354:c.*207A>C,NM_205845:c.*207A>C)  chr10   5031981 5031981 T       G       chr10:5031981:-

#第一列位置utr3等，
#2 基因及相关突变
#3 chr1
#4 5 start end
#6 7 ref alt 
#8 PAS_id






(3)致病基因的筛选
https://blog.csdn.net/theomarker/article/details/82872201
1)筛选标准：

对于得到的结果，筛选方式：基于ExAC、1000G MAF<0.01/0.005过滤 --> CLINSIG/CLINVAR挑选致病和可能致病的位点
–> 若CLINVAR注释上，根据ACMG评估
–> 若CLINVAR没有注释上，选择nonsynonymous SNV（这里表示错义突变，非同义还包括其他类型）+ scSNV≠0的位点（可预测可变剪切）–> genomimcsuperdups>0表示位于同源区，过滤掉

自己的想法：以上是一种筛选方式，自己在做的方法（参考）：对SNV，nonsynonymous -> exonic/splicing -> exclude MAF > 0.01 in union(1000G, ExAC, dbsnp138nonflag) -> exlude dups -> prediction tools (SIFT, PolyPhen2, metaSVM等等）留下damaging的位点。

对indel，exonic/splicing -> frameshift + stop change -> exlcude variant databases -> exclude dups -> select protein damaging loci。

注：LoF (frameshift, stop gain/loss, nonsense) + missense
经筛选之后，对于得到的位点对应的gene list，可以有2个工具对疾病和表型找找关联已经基因间相互关系。这里两个工具更多是对单基因病。
工具一：panelAPP(https://panelapp.genomicsengland.co.uk/panels/) 目前包含了222个panel的基因资料
工具二：phenolyzer(http://phenolyzer.wglab.org/) 输入表型/疾病 + gene list + email可得到基因间关系，基因重要性等信息。有网页和本地版本。


2) task 3 CNV分析建议
筛选过程过，波动特别大的外显子，去掉。对>100kb的CNV，基本比较确信。
DiagnoseTargets工具可以给定区域平均深度。




3)筛选工具：pyhton-pandas包。




refer:
https://www.jianshu.com/p/7607c894eaae









========================================
|-- 注释的解读
----------------------------------------
http://annovar.openbioinformatics.org/en/latest/user-guide/gene/

1.统计数值文件的位置频率
$ awk -F"," '{print $6}' 19C001854_WES001_CapNGS.filter_B_.hg19_multianno.csv|sort|uniq -c |sort -k 1nr
 222124 "intronic"
 190721 "intergenic"
  37504 "ncRNA_intronic"
  24512 "exonic"
  18021 "ncRNA_exonic"
  14195 "UTR3"
   8980 "upstream"
   6660 "downstream"
   4849 "UTR5"
    476 "upstream;downstream"
    216 "splicing"
    103 "ncRNA_splicing"
     50 "exonic;splicing"
     12 "UTR5;UTR3"
      9 "ncRNA_exonic;splicing"
      2 "ncRNA_UTR5"
      1 Func.refGene
#
2.解释这些位置的意义：
(1)第一列的可能值及其意义：
Value	/Default precedence	/Explanation	/Sequence Ontology
exonic	/1/	variant overlaps a coding	/exon_variant (SO:0001791)
splicing	/1/	variant is within 2-bp of a splicing junction (use -splicing_threshold to change this)	/splicing_variant (SO:0001568)
ncRNA	2	variant overlaps a transcript without coding annotation in the gene definition (see Notes below for more explanation)	non_coding_transcript_variant (SO:0001619)
UTR5	3	variant overlaps a 5' untranslated region	5_prime_UTR_variant (SO:0001623)
UTR3	3	variant overlaps a 3' untranslated region	3_prime_UTR_variant (SO:0001624)
intronic	4	variant overlaps an intron	intron_variant (SO:0001627)
upstream	5	variant overlaps 1-kb region upstream of transcription start site	upstream_gene_variant (SO:0001631)
downstream	5	variant overlaps 1-kb region downtream of transcription end site (use -neargene to change this)	downstream_gene_variant (SO:0001632)
intergenic	6	variant is in intergenic region	intergenic_variant (SO:0001628)


(2)对上表的解释：
The value of the first column takes the following precedence (as of December 2010 and later version of ANNOVAR): exonic = splicing > ncRNA> > UTR5/UTR3 > intron > upstream/downstream > intergenic. The precedence defined above is used to decide what function to print out when a variant fit multiple functional categories. Note that:

1)the "exonic" here refers only to coding exonic portion , but not UTR portion, as there are two keywords (UTR5, UTR3) that are specifically reserved for UTR annotations.
这里 外显子 仅仅指的是编码部分的外显子，不包含UTR部分（被统计到UTR5和UTR3中了）。

2)"splicing" in ANNOVAR is defined as variant that is within 2-bp away from an exon/intron boundary by default, but the threshold can be changed by the --splicing_threshold argument. Before Feb 2013, if "exonic,splicing" is shown, it means that this is a variant within exon but close to exon/intron boundary; this behavior is due to historical reason, when a user requested that exonic variants near splicing sites be annotated with splicing as well. However, I continue to get user emails complaining about this behavior despite my best efforts to put explanation in the ANNOVAR website with details. Therefore, starting from Feb 2013 , "splicing" only refers to the 2bp in the intron that is close to an exon, and if you want to have the same behavior as before, add -exonicsplicing argument.
可剪切位点在ANNOVAR默认定义为在exon/intron边界2bp范围内，可以通过--splicing_threshold参数修改。
在2013年2月前，"exonic,splicing"值的是在exon内但是靠近exon/intron边界；这是历史原因，当时用户希望splicing位点附近的外显子也注释为同时splicing。然而，虽然网站上写的很细致，但是我还是经常受到邮件抱怨这个。
所以2013年2月之后，splicing仅仅指的是intron区内靠近exon 2bp的位点，如果想使用之前的行为，加上-exonicsplicing 参数。


3)If a variant is located in both 5' UTR and 3' UTR region (possibly for two different genes), then the "UTR5,UTR3" will be printed as the output.
如果一个位点同时位于5' UTR and 3' UTR区域（可能是不同基因），则会被注释为"UTR5,UTR3"。

4)The term "upstream" and "downstream" is defined as 1-kb away from transcription start site or transcription end site, respectively, taking in account of the strand of the mRNA; the --neargene threshold can be used to adjust this threshold.
"upstream" and "downstream"指的是转录本TSS上游、TES下游1kb的部分，考虑mRNA链的方向；
--neargene threshold 可以调整这个值。


5)If a variant is located in both downstream and upstream region (possibly for 2 different genes), then the "upstream,downstream" will be printed as the output. In 2011 June version of ANNOVAR, the splicing annotation is improved. If the splicing site is in intron, then all isoforms and the corresponding base change will be printed. For example,
如果一个位点同时在downstream and upstream区域（可能是2个基因），将注释成"upstream,downstream"。
在2011年6月的ANNOVAR，splicing注释改进了。如果splicing位点在intron中，所有的变体和相应的碱基改变都会打印出来。

splicing SMS(NM_004595:c.447+2T>G) X 21895357 21895357 T G hetero 8 15
splicing DMD(NM_004011:c.48+1A>C) X 31803228 31803228 T G homo 117 30
splicing BAGE(NM_001187:c.14+1A>G),BAGE4(NM_181704:c.14+1A>G),BAGE5(NM_182484:c.14+1A>G) 21 10120594 10120594 T C hetero 66 53


Several technical notes are discussed below. 几个技术问题：

Technical Notes: ncRNA above refers to RNA without coding annotation. It does not mean that this is a RNA that will never be translated; it merely means that the user-selected gene annotation system was not able to give a coding sequence annotation. It could still code protein products and may have such annotations in future versions of gene annotation or in another gene annotation system. For example, BC039000 is regarded as ncRNA by ANNOVAR when using UCSC Known Gene annotation, but it is regarded as a protein-coding gene by ANNOVAR when using ENSEMBL annotation. If the goal of the user is to find known (well-annotated) microRNA or other known (well-annotated) non-coding RNA, then the region-based annotation should be used and the wgRNA track should be selected. Read instructions here.

Technical Notes: if the first codon of a transcript is deleted, it will be reported as wholegene deletion by ANNOVAR because the gene cannot be translated.


如图所示：
In the figure above, SNP1 is an intergenic variant, as it is >1kb away from any gene, SNP2 is a downstream variant, as it is 1kb from the 3'end of the NADK gene; SNP3 is a UTR3 variant; SNP4 is an intronic variant; SNP5 is an exonic variant.






========================================
|-- 变异功能注释 ANNOVAR (hg38)
----------------------------------------

B3.使用Annovar建库[hg38]

先在annovar文件夹里面创建 humandb38 文件夹（名字可自取），命令
$ cd /home/wangjl/Soft/annovar/
$ mkdir humandb38

然后一个一个执行命令：使用annovar文件夹下的perl程序annotate_variation.pl
(1)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar refGene humandb38/
这个命令是下载 hg38 的refGene的文件，保存在 humandb38 文件下，自动解压后文件名为 hg38_refGene.txt。
total 235M
-rw-rw-r--. 1 wangjl wangjl  993 Nov 13 16:17 annovar_downdb.log
-rw-rw-r--. 1 wangjl wangjl 216M Jun  2  2017 hg38_refGeneMrna.fa
-rw-rw-r--. 1 wangjl wangjl  19M Jun  2  2017 hg38_refGene.txt
-rw-rw-r--. 1 wangjl wangjl 810K Jun  2  2017 hg38_refGeneVersion.txt

(2)$ annotate_variation.pl -buildver hg38 -downdb cytoBand humandb38/
-rw-rw-r--. 1 wangjl wangjl  45K Aug 11  2014 hg38_cytoBand.txt

(3)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar exac03 humandb38/ 
-rw-rw-r--. 1 wangjl wangjl  23M Nov 30  2015 hg38_exac03.txt.idx
-rw-rw-r--. 1 wangjl wangjl 600M Nov 30  2015 hg38_exac03.txt

(4)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar avsnp147 humandb38/
-rw-rw-r--. 1 wangjl wangjl 5.9G Jun  2  2016 hg38_avsnp147.txt
-rw-rw-r--. 1 wangjl wangjl 884M Jun  2  2016 hg38_avsnp147.txt.idx

(5)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar dbnsfp30a humandb38/
-rw-rw-r--. 1 wangjl wangjl  14G Oct 16  2015 hg38_dbnsfp30a.txt
-rw-rw-r--. 1 wangjl wangjl  18M Nov 17  2015 hg38_dbnsfp30a.txt.idx
前五个命令下载一堆注释文件到 humandb38/。



## --downdb                    download annotation database 
## --webfrom <string>          specify the source of database (ucsc or annovar or URL) (downdb operation)  

## --thread <int>              use multiple threads for filter-based annotation
# --maxgenethread <int>       max number of threads for gene-based annotation (default: 6)



#### 尝试下载其他数据库？官方下载数据库
http://annovar.openbioinformatics.org/en/latest/user-guide/download/

$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar avdblist humandb38/
则在 humandb38 目录下下载一个文本文件，包含现有的 hg38 的数据库92行。


该选择哪些数据库？
https://doc-openbio.readthedocs.io/projects/annovar/en/latest/user-guide/filter/#summary-of-databases


直接下载方式，可以根据这句报错推测
WARNING: Some files cannot be downloaded, including http://www.openbioinformatics.org/annovar/download/hg19_snp142.txt.gz


vcf 文件的说明和什么是 Left-normalization ：
http://annovar.openbioinformatics.org/en/latest/articles/VCF/



(7)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar 1000g2015aug humandb38/
-rw-r--r--. 1 wangjl wangjl 3.2G Aug 26  2015 hg38_ALL.sites.2015_08.txt
-rw-r--r--. 1 wangjl wangjl 1.5G Aug 26  2015 hg38_AFR.sites.2015_08.txt
分为sas 南亚 ，eas 东亚 等好几个文件。 

https://www.jianshu.com/p/4278896661f2
千人基因组计划（http://www.internationalgenome.org/）。通过注释，可以知道该变异在全部参与千人基因组计划人群中的突变频率，参与人群来自于非洲AFR（African），美洲AMR（Admixed American），东亚EAS（EastAsian），欧洲EUR（European），南亚SAS（South Asian）等区域。



(8)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar avsnp150 humandb38/
## hg38	avsnp150	dbSNP150 with allelic splitting and left-normalization	20170929
-rw-rw-r--. 1 wangjl wangjl  13G Sep 30  2017 hg38_avsnp150.txt
-rw-rw-r--. 1 wangjl wangjl 918M Sep 30  2017 hg38_avsnp150.txt.idx


(9)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar clinvar_20180603 humandb38/
CLINVAR database with Variant Clinical Significance (unknown, untested, non-pathogenic, probable-non-pathogenic, probable-pathogenic, pathogenic, drug-response, histocompatibility, other) and Variant disease name
Clinvar version 20180603 with separate columns (CLNALLELEID CLNDN CLNDISDB CLNREVSTAT CLNSIG)
hg38	clinvar_20180603	same as above	20180708

-rw-rw-r--. 1 wangjl wangjl  72M Jul  9 11:50 hg38_clinvar_20180603.txt
-rw-rw-r--. 1 wangjl wangjl 1.3M Jul  9 11:50 hg38_clinvar_20180603.txt.idx


(10)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar cosmic70 humandb38/

COSMIC database version 70? on WGS data
hg38	cosmic70	same as above	20150428

-rw-rw-r--. 1 wangjl wangjl  87M Apr 29  2015 hg38_cosmic70.txt
-rw-rw-r--. 1 wangjl wangjl 6.6M Apr 29  2015 hg38_cosmic70.txt.idx



(11)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar esp6500siv2_all humandb38/
alternative allele frequency in All subjects in the NHLBI-ESP project with 6500 exomes, including the indel calls and the chrY calls. This is lifted over from hg19 by myself.
-rw-rw-r--. 1 wangjl wangjl  88M Dec 23  2014 hg38_esp6500siv2_all.txt


(12)$ annotate_variation.pl -buildver hg38 -downdb -webfrom annovar ljb26_all humandb38/
ljb26_all: whole-exome SIFT, PolyPhen2 HDIV, PolyPhen2 HVAR, LRT, MutationTaster, MutationAssessor, FATHMM, MetaSVM, MetaLR, VEST, CADD, GERP++, PhyloP and SiPhy scores from dbNSFP version 2.6

-rw-rw-r--. 1 wangjl wangjl  11G May 21  2015 hg38_ljb26_all.txt








B4.使用 table_annovar 注释 

(1) 官方句子
$ table_annovar.pl example/ex1.avinput humandb38/ -buildver hg38 -out myanno -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation gx,r,f,f,f -nastring . -csvout -polish --xreffile example/gene_xref.txt #最后用 --xreffile 和 -xref 有啥区别？ 

该一个命令使用 table_annovar ，使用 ExAC version 0.3 (referred to as exac03) dbNFSP version 3.0a (referred to as dbnsfp30a), dbSNP version 147 with left-normalization (referred to as avsnp147) databases 并删除临时文件, 生成的输出文件为 myanno.hg19_multianno.txt.
 
没有注释信息的用点号.填充。
The header line starts with #. The cross-reference file then contains 15 types of annotations for genes. 

You can run the same command above but change -xref file from gene_xref.txt to gene_fullxref.txt, and the result file can be downloaded from here. 

支持直接输入vcf文件 table_annovar.pl can directly support input and output of VCF files (the annotation will be written to the INFO field of the output VCF file). Let's try this:
$ table_annovar.pl example/ex2.vcf humandb38/ -buildver hg38 -out myanno3 -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput

Similarly, if you generated variant calls from human GRCh38 coordinate, add -buildver hg38 in every command, 


(2)公众号见到的，使用了更多的库
table_annovar.pl example/ex1.avinput humandb/ -buildver hg19 -out myanno -remove -protocol refGene,cytoBand,genomicSuperDups,esp6500siv2_all,1000g2014oct_all,1000g2014oct_afr,1000g2014oct_eas,1000g2014oct_eur,snp138,ljb26_all -operation g,r,r,f,f,f,f,f,f,f -nastring . -csvout

# -buildver hg38 表示使用hg38版本
# -out myanno 表示输出文件的前缀为myanno
# -remove 表示删除注释过程中的临时文件
# -protocol 表示注释使用的数据库，用逗号隔开，且要注意顺序
# -operation 表示对应顺序的数据库的类型（g代表gene-based、r代表region-based、f代表filter-based），用逗号隔开，注意顺序
# -nastring . 表示用点号替代缺省的值
# -csvout 表示最后输出.csv文件

# --polish   polish the protein notation for indels (such as p.G12Vfs*2)
# --xreffile <file>     specify a cross-reference file for gene-based annotation


(3)诺禾致源给的csv报告中的频率
#[25] "X1000g2015aug_Chinese"(x) "X1000g2015aug_eas"     "X1000g2015aug_all"    
#[28] "esp6500siv2_all"       "ExAC_ALL"(x)              "ExAC_EAS"(x) 

-rw-r--r--. 1 wangjl wangjl 3.2G Aug 26  2015 hg38_ALL.sites.2015_08.txt
-rw-r--r--. 1 wangjl wangjl 856M Aug 26  2015 hg38_EAS.sites.2015_08.txt
-rw-rw-r--. 1 wangjl wangjl  88M Dec 23  2014 hg38_esp6500siv2_all.txt
-rw-rw-r--. 1 wangjl wangjl 600M Nov 30  2015 hg38_exac03.txt

$ grep 1000g2015 humandb38/hg38_avdblist.txt
## hg38_1000g2015aug.zip   20150826        2732158830

$ grep ljb26 humandb38/hg38_avdblist.txt
## hg38_ljb26_all.txt.gz   20150520        2311601075


自己整合一个句子
报错1: Error in argument: -csvout is not compatible with -vcfinput， 其实还是希望能输出csv的。
$ time table_annovar.pl /home/data/example201710/wangjl_bwa/3_callSNP/1WES01_2_.filter.vcf humandb38/ -buildver hg38 -out 2WES01_ -remove -protocol refGene,cytoBand,esp6500siv2_all,ALL.sites.2015_08,EAS.sites.2015_08,exac03,avsnp147,avsnp150,dbnsfp30a,ljb26_all -operation g,r,f,f,f,f,f,f,f,f -nastring . -vcfinput -xreffile example/gene_fullxref.txt


报错2: annovar Argument "T" isn't numeric in numeric eq (==) at annotate_variation.pl line 2583, <DB> line 2169376.

Try using '-protocol 1000g2015aug_all' in your command, and let us know if that solves your problem or not.

<annotate_variation.pl -filter -dbtype ljb26_all -buildver hg38 -outfile 3WES01csv_ 2WES01_.avinput humandb38/ -otherinfo>








//todo
(1) ## 使用第一次执行的中间文件 37min.
$ time table_annovar.pl 2WES01_.avinput humandb38/ -buildver hg38 -out 3WES01csv_ -remove -protocol refGene,cytoBand,esp6500siv2_all,ALL.sites.2015_08,EAS.sites.2015_08,exac03,avsnp147,avsnp150,dbnsfp30a,ljb26_all -operation g,r,f,f,f,f,f,f,f,f -nastring . -csvout -xreffile example/gene_fullxref.txt



(2)先转化为.avinput
$ convert2annovar.pl -format vcf4 example/ex2.vcf > ex2wjl.avinput
# -format vcf4 指定格式为vcf

$ convert2annovar.pl -format vcf4 1WES01_2_.filter.vcf > 1WES01_2_.filter.avinput
转换失败，没报错，但是结果好像不像。

$ time table_annovar.pl /home/data/example201710/wangjl_bwa/3_callSNP/1WES01_2_.filter.avinput humandb38/ -buildver hg38 -out 4WES01aviinput_ -remove -protocol refGene,cytoBand,esp6500siv2_all,ALL.sites.2015_08,EAS.sites.2015_08,exac03,avsnp147,avsnp150,dbnsfp30a,ljb26_all -operation g,r,f,f,f,f,f,f,f,f -nastring . -csvout -xreffile example/gene_fullxref.txt

















5.Annovar 软件注释流程介绍
https://www.cnblogs.com/zkkaka/p/6146137.html




能用的
$ table_annovar.pl example/ex2.vcf humandb38/ -buildver hg38 -out my3 -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput


正常样本
$ table_annovar.pl /home/data/example201710/wangjl_bwa/2_mapping/test.vcf humandb38/ -buildver hg38 -out test1 -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput

过滤后的数据样本
$ table_annovar.pl /home/data/example201710/wangjl_bwa/3_callSNP/test.vcf humandb38/ -buildver hg38 -out testF -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput


过滤后的数据 全部
$ time table_annovar.pl /home/data/example201710/wangjl_bwa/3_callSNP/1WES01_2_.filter.vcf humandb38/ -buildver hg38 -out 1WES01_2 -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation g,r,f,f,f -nastring . -vcfinput










#####
$ sed "s/^chr//" test.csv > test2.csv

$ table_annovar.pl /home/data/example201710/wangjl_bwa/2_mapping/test2.vcf humandb38/ -buildver hg38 -out WEStest2_ -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation gx,r,f,f,f -nastring . -csvout -polish -xref example/gene_xref.txt




annotate_variation.pl -geneanno -dbtype refGene -buildver hg38  -out test01_ example/ex1.avinput humandb38/

annotate_variation.pl -buildver hg38 ex1.avinput humandb38/











========================================
**** 流程可视化： BRB-SeqTools 及 参考基因组 及参考文献
----------------------------------------

BRB-SeqTools is a new tool for NGS data analysis. It will support the importing, pre-processing, variant calling, and somatic mutation annotation of both RNA-Seq and DNA-Seq data, in either .FASTQ or .bam file format. 



1.参考基因组的md5sum值：https://taichimd.us/mediawiki/index.php/Seqtools
BRB-SeqTools： https://linus.nci.nih.gov/seqtools/




2.参考基因组
(1)ClinVar使用的是hg38基因组：
https://www.ncbi.nlm.nih.gov/clinvar/RCV000139793/#clinical-assertions



(2)refGene.txt 的说明：
$ head refGene.txt -n 2
75      NM_001172656    chr4    -       2271323 2420370 2272451 2420050 12      2271323,2273037,2273401,2274899,2275788,2306015,2321896,2339133,2341179,2343204,2355659,2420011,        2272583,2273141,2273506,2275016,2275943,2307263,2321998,2339223,2341382,2343342,2355800,2420370,    0       ZFYVE28 cmpl    cmpl    0,1,1,1,2,2,2,2,0,0,0,0,

每列的解释：http://genome.ucsc.edu/cgi-bin/hgTables

wget -c -O mm9.refGene.txt.gz http://hgdownload.soe.ucsc.edu/goldenPath/mm9/database/refGene.txt.gz
wget -c -O mm10.refGene.txt.gz http://hgdownload.soe.ucsc.edu/goldenPath/mm10/database/refGene.txt.gz
wget -c -O hg19.refGene.txt.gz http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/refGene.txt.gz
wget -c -O hg38.refGene.txt.gz http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/refGene.txt.gz

$ gunzip hg38.refGene.txt.gz
$ less -S hg38.refGene.txt

提取其中的带有geneID的一列，并且没有重复
$ awk '{print $7}' refGene.list.txt | sort -u > uniqu.refGene.list.txt
查看列数
$ wc -l uniqu.refGene.list.txt 




(3)GTF格式的refGene如何在Ensembl及UCSC下载
http://blog.sina.com.cn/s/blog_751bd9440102v76a.html


(4) igenome
http://jp.support.illumina.com/sequencing/sequencing_software/igenome.html





3.单基因遗传病文章

单基因遗传病——家系样本研究思路
基因测序产业网  2016-05-06

[1] https://en.wikipedia.org/wiki/Genetic_disorder

[2] Feng R, Sang Q, Kuang Y,et al. Mutationgs in TUBB8 and Human Oocyte Meiotic Arrest. N Engl J Med. 2016, 374(3): 223-32. 
外显子组测序解析卵细胞发育障碍


[3] Lukacs V, Mathur J, Mao R, et al. Imparied PIEZO1 function in patients with a novel autosomal recessive congentital lymphatic dysplasia. Nat Commun. 2015, 6: 8329.
外显子组测序解析常染色体隐性先天性淋巴管发育不良


最全！华大基因单基因遗传病检测科研成果汇编
原创： 华大医学  华大医学  2018.9月3日








========================================
**** 拷贝数变异 CNV 
----------------------------------------


========================================
|-- Control-Freec:检测拷贝数变异(CNV)的神器
----------------------------------------
1. 简介
http://bioinfo-out.curie.fr/projects/freec/


Control-Freec 既可以检测拷贝数变异CNV，还可以分析杂合性缺失LOH。官网如下
http://boevalab.com/FREEC/

Control-FREEC is a tool for detection of copy-number changes and allelic imbalances (including LOH) using deep-sequencing data originally developed by the Bioinformatics Laboratory of Institut Curie (Paris). Nowdays, Control-FREEC is supported by the team of Valentina Boeva at Institut Cochin, Inserm(Paris). 

在检测拷贝数变异时，支持全基因组测序，全外显子测序，目标区域捕获测序等多种测序方案，
对于全基因组数据，分析是不需要提供对照样本；
对于全外显子测序和目标区域捕获测序，必须提供对照样本。

# Control-FREEC
Copy number and genotype annotation from whole genome and whole exome sequencing data.







2.安装准备
(1)安装必须准备
http://boevalab.com/FREEC/tutorial.html#install

10Gb of RAM
Linux or MACS OS; only older versions of Control-FREEC (up to v6.2) are available for Windows
R installed
	R version 3.5.1
samtools installed if the input files are in .BAM format
	Version: 1.9 (using htslib 1.9)
bedtools installed if you wish to create minipileup files from .BAM
	bedtools v2.25.0
sambamba installed if you wish to speed up reading of .BAM files
	sambamba 0.6.8 by Artem Tarasov and Pjotr Prins (C) 2012-2018



(2)##https://github.com/BoevaLab/FREEC/releases
$ wget -O FREEC-11.5.tar.gz https://github.com/BoevaLab/FREEC/archive/v11.5.tar.gz
$ tar xzvf FREEC-11.5.tar.gz
$ cd FREEC-11.5
在FREEC-11.4下有3个目录：
data目录保存的是配置文件的模板，包含WGS和WES两套模板；
scripts目录下是一些常用的脚本；
src目录下就是软件的源代码，freec可执行文件就位于这个目录。

$ cd src
$ make
$ make install #报错，可能必须手动安装快捷方式了

$ cd ~/bin
$ ln -s ~/Soft/FREEC-11.5/src/freec
$ freec -h
Control-FREEC v11.5 : a method for automatic detection of copy number alterations, subclones and for accurate estimation of contamination and main ploidy using deep-sequencing data





文件配置方式：http://boevalab.com/FREEC/tutorial.html#CONFIG




http://boevalab.com/FREEC/
Starting from Control-FREEC v10.6, Control-FREEC can work on exome-seq data without a control sample. 






3.Control-FREEC v11.5
$ pwd
/home/wangjl/bin
$ ln -s /home/wangjl/Soft/FREEC-11.5/src/freec



3.配置文件很复杂
http://boevalab.com/FREEC/
http://boevalab.com/FREEC/tutorial.html#CONFIG

Starting from Control-FREEC v10.6, Control-FREEC can work on exome-seq data without a control sample. 



















========================================
[大纲] 各种培训班
----------------------------------------

####################
# 1. [大纲]第十八期临床遗传病生信实操培训班（上海班）
####################
基因游侠  基因检测与解读  2018.10月15日


“在课上系统学习了质控的三大要点，拿到检测机构的原始数据后我就可以自己评估谁的实验做的好不好了，对于阴性结果的案例，我也学会在IGV中查看感兴趣的基因有没有完全覆盖，我还知道判断一个基因是否为患者的致病基因，需要从临床、遗传模式及生物信息三个方面考虑，缺一不可！”

1.原始数据fastq到bam到vcf
2.vcf与表型相关变异关系的分析过程。



内容：

基因侦探王剑
1.NGS实验基本流程
2.新致病基因的鉴定
3.NGS数据质量控制
4.致病基因（变异）的筛选与评估
5.变异精准解读影响因素
6.NGS分析CNV
7.案例分析


基因游侠
1、CentOS系统的介绍
2、实际操作fastQC查看数据质量
3、speedseq比对原始数据
4、如何分析覆盖度与测序深度
5、GATK call突变及位点过滤
6、annovar注释及筛选
7、IGV查看reads比对情况
8、怎么分析trio新发突变
9、怎么使用Exomiser分析复合杂合变异
10、怎么筛选大家系多个样本候选位点
11、以DMD为例，利用NGS分析外显子缺失与重复
12、XYAutoFilter软件的使用
13、利用WES分析SMN1拷贝数

14、cnv分析原理与案例









####################
# 2. [大纲]遗传病测序数据分析解读科普系列 第2期 生信分析基础：了解数据处理过程
####################
原创： 熊吉的熊  瀚垚生物医学  2018.8月2日

1.数据前处理
	fastq ->BQSR.bam

第一部分主要讲述从fastq到可以call变异的bam文件之间主要发生了什么。
重点理解，为什么比对目前最常用的是bwa，为什么要对bam进行sort，markdup和BQSR操作。

bwa优势：快和准

bam文件对很多人的印象可能是
第一步：打开igv
第二步：载入bam文件
第三步：输入看变异



2.如何得到变异vcf文件
	bam -> vcf

重点理解GATK的HaplotypeCaller都做了什么，为什么在众多的call variant软件中，HC使用范围最广。
适当了解变异后处理过程中推荐使用人群数据和VQSR的目的。



3.qc和统计
	fastq/bam/vcf的质量评估
通过质控或统计数据判断数据的质量

当拿到处理的数据后，第一时间其实并不是进行数据解读，其实首先需要检验数据的质量，即判断是否合格，否则对一个质量不达标的数据进行下游处理，很大可能会造成时间和精力的浪费。

主要理解fastq/bam/vcf的数据质量基本判断原则。










####################
# 3.[大纲]逸仙生信临床解读培训班第二天
####################
廖健伟  逸仙细胞分子论坛  2018.9月9日

2018年9月9日，逸仙细胞分子论坛的首届生物信息临床解读培训班迎来了第二天的课程

1.上午首先由颜彬博士介绍人群数据库和疾病数据库的使用。人群频率数据与疾病数据是胚系变异评级分类的重要证据，人群数据库可用于获得某变异在人群中的发生频率，而发生频率的高低可作为一个预测指标，区分良性变异和致病性变异；疾病数据库可用于查找疾病相关变异并对变异的致病性进行评估。颜博士重点介绍了1000 genomes及ExAC人群数据库和ClinVar、HGMD、OMIM等疾病数据库。

2.接着，温晓君老师介绍了参考基因组及计算机预测软件。参考基因组是人类基因组计划的产物，记录基因完整的信息包括编码序列、调控序列等，随着研究的进行，参考基因组也不断更新。温老师主要介绍了参考基因组NCBI、UCSC，ensemble等数据库在基因变异解读中重要性及参考基因组的下载方式，并结合ACMG指南，以变异解读实例详细地介绍了预测软件如错义变异的预测软件SIFT、Polyphen-2，剪切预测软件MaxExtScan、NetGen2等软件的应用。

3.然后，温晓君与廖健伟博士依次作了遗传病基因变异与肿瘤基因变异的解读与病例分享。遗传病非常复杂多样，根据其变异范围大小可分为单基因多基因疾病、基因组疾病、染色体疾病。温老师首先介绍了根据不同的疾病，检测方法及策略的不同，包括一代测序、靶向测序、医学外显子、全外显子、全基因组测序，其中，重点介绍了利用医学外显子测序检测单基因遗传病的适用范围及检测流程。最后她以两个病例详细介绍了医学外显子测序辅助诊断单基因遗传病的整个流程，包括检测前遗传咨询、实验流程及数据分析、变异评级。

然而，肿瘤基因检测与遗传疑难病有所区别，除了检测胚系变异，体细胞变异的检测同样重要。而对肿瘤组织的体细胞变异，目前有推荐以用药指导作为分类标准，目的主要是指导靶向治疗。廖博士介绍了药物相关数据库如oncoKB、My cancer Genome、Clinical Trials，可作为肿瘤基因变异分类的参照。


4.下午，先由欧阳能太教授分享了NGS实验室的设计经验，讲述了实验室的功能分区、气压、温湿度等对NGS检测的重要性，并就控制实验室压差和温湿度的暖通系统作了详细介绍。

5.接着，萧晓琴老师讲解了qPCR及一代测序在临床的应用。这两种方法各有优劣，获得的信息量也大不相同，根据临床的需求进行选择，qPCR法主要用于特定基因已知热点突变的检测；一代测序主要用于单个基因的测序及基因多态性分析；也常应用于二代测序得到的变异位点的验证及家属验证。

6.然后，李晓娟博士介绍了基因芯片及FISH技术的临床应用。临床检测染色体数量及结构，FISH和基因芯片是目前仍在使用的重要方法，并且各有优势。如果是针对已知的染色体数量和结构（3-20Mb）的检测，则FISH方法可以在显示受检细胞内部染色体变化，快速、方便。如果检测更微小的结构变异，则可通过基因芯片的方法对全基因组水平中发生的5kb以上的拷贝数扩增或缺失进行检测。但针对平衡易位还是需要传统的核型分析及FISH的方法来进行原位的观察。

7.随后，廖健伟博士负责解答临床基因检测的相关咨询问题。问题涉及基因检测项目的选择、发生退费的原因、检测部位选择、报告时效差别、基因检测报告的解析等。

8.最后，蒋圆玲老师为我们介绍了NIPT的检测原理及临床应用。染色体异常疾病是新生儿出生缺陷中常见的疾病，现阶段并没有有效的治疗方法，只能通过产前诊断进行出生干预才能有效地降低出生缺陷。讲者简单介绍了传统产前筛查和产前诊断方式；然后介绍了基于高通量测序的无创产前筛查（Noninvasive Prenatal Testing, NIPT）技术，并详细从技术的的发展起源、检测原理以及在临床中应用等方面进行了介绍。











####################
#4. [大纲]儿童遗传分子诊断技术质控及数据一体化信息管理——新华医院的经验探索尝试
####################
聚道科技GeneDock  聚道科技GeneDock  2018.6月22日

2018分子及遗传诊断高峰论坛于6月14-15日在上海成功举办。上海交通大学医学院附属新华医院余永国主任为大家带来题为《复杂多样的儿童遗传分子诊断技术质控及数据一体化信息管理——新华医院的经验探索尝试》的精彩报告。

随着现代生物医学技术的发展，分子诊断是精准医学实践中不可或缺的一个重要手段。遗传病的三级预防措施能够有效降低出生缺陷：
一级预防：孕前禁止近亲结婚，杂合子检出
二级预防：怀孕期羊水细胞、绒毛膜细胞的酶学、基因产前诊断
三级预防：新生儿筛查


此外，儿童疑难罕见遗传病的常规基因诊断报告，经历从发起申请、进行实验、生信分析、到出遗传报告等过程，需要45-60天的周期。为遗传病患儿的救治赢得更多宝贵时间，缩短检测各个环节的时间尤为重要。过去，检测申请、分子实验过程记录、检测报告签发等信息记录主要通过从医院系统下载上传表格，通过EXCEL记录实现。这给项目沟通、数据流通、人员协作都带来了很大的不便。

分子检测需要不同团队、各个成员沟通协作，为了确保检测结果的准确性，新华医院采取三级审核的制度。比如基因诊断报告流程：
第一review：要求产生精准的数据并运行工作流
第二review：要求分析数据，双人核对，列出依据，讨论结果
第三review：医生审核位点，一代验证，撰写报告
实验室信息化平台的搭建对促进实验室信息同步、协作带来了很大的便利，能够提高协作效率，缩短检测周期。

1.分子平台临床信息和检测自动化搭建（LIMS系统）




========================================
call小鼠的SNP用什么工具？
----------------------------------------
varscan2
或者直接 samtools+bcftools;



========================================
GWAS分析：plink (Whole genome data analysis toolset)
----------------------------------------

1. 简介
# PLINK (1.07) 
http://zzz.bwh.harvard.edu/plink/

The focus of PLINK is purely on analysis of genotype/phenotype data, so there is no support for steps prior to this (e.g. study design and planning, generating genotype or CNV calls from raw data). Through integration with gPLINK and Haploview, there is some support for the subsequent visualization, annotation and storage of results.


# PLINK (1.9) 
http://www.cog-genomics.org/plink/1.9/
This is a comprehensive update to Shaun Purcell's PLINK command-line program, developed by Christopher Chang with support from ...


PLINK (2.0) 
http://www.cog-genomics.org/plink/2.0/





2. 安装 

(1) 老版本（尽量不用）
$ wget http://zzz.bwh.harvard.edu/plink/dist/plink-1.07-x86_64.zip
$ unzip plink-1.07-x86_64.zip
$ echo 'export PATH=/home/wangjl/data/soft/plink-1.07-x86_64:$PATH' >> ~/.bashrc
$ source ~/.bashrc

$ plink
@--------------------- ---------------- -------------------@
|        PLINK!       |     v1.07      |   10/Aug/2009     |
|--------------------- ---------------- -------------------|
(C) 2009 Shaun Purcell, GNU General Public License, v2
For documentation, citation & bug-report instructions: http://pngu.mgh.harvard.edu/purcell/plink/




(2) 新版本 
$ wget http://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20201019.zip
$ unzip plink_linux_x86_64_20201019.zip
Archive:  plink_linux_x86_64_20201019.zip
  inflating: plink
  inflating: LICENSE
  inflating: toy.ped
  inflating: toy.map
  inflating: prettify
# 新建 plink1.9/ 并把以上放入该文件夹。
$ echo 'export PATH=/home/wangjl/data/soft/plink1.9:$PATH' >> ~/.bashrc
$ source ~/.bashrc

## 检查版本号
$ plink
PLINK v1.90b6.21 64-bit (19 Oct 2020)          www.cog-genomics.org/plink/1.9/
(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3

  plink <input flag(s)...> [command flag(s)...] [other flag(s)...]
  plink --help [flag name(s)...]
#







ref:
https://www.jianshu.com/p/6768c811bd96







========================================
ADMIXTURE: fast ancestry estimation 群体分类
----------------------------------------
1. 
http://dalexander.github.io/admixture/download.html

ADMIXTURE is a software tool for maximum likelihood estimation of individual ancestries from multilocus SNP genotype datasets. It uses the same statistical model as STRUCTURE but calculates estimates much more rapidly using a fast numerical optimization algorithm.

Specifically, ADMIXTURE uses a block relaxation approach to alternately update allele frequency and ancestry fraction parameters. Each block update is handled by solving a large number of independent convex optimization problems, which are tackled using a fast sequential quadratic programming algorithm. Convergence of the algorithm is accelerated using a novel quasi-Newton acceleration method. The algorithm outperforms EM algorithms and MCMC sampling methods by a wide margin. For details, see our publications.


可视化: 
https://dnagenics.com/admixture-studio/







2. 下载和安装
$ wget http://dalexander.github.io/admixture/binaries/admixture_linux-1.3.0.tar.gz
$ tar zxvf admixture_linux-1.3.0.tar.gz
$ mv dist/* .
$ rmdir dist 

$ echo 'export PATH=/home/wangjl/data/soft/admixture_linux-1.3.0:$PATH' >> ~/.bashrc
$ source ~/.bashrc

$ admixture --version
****                   ADMIXTURE Version 1.3.0                  ****
****                    Copyright 2008-2015                     ****
****           David Alexander, Suyash Shringarpure,            ****
****                John  Novembre, Ken Lange                   ****
****                                                            ****
****                 Please cite our paper!                     ****
****   Information at www.genetics.ucla.edu/software/admixture  ****

1.3.0







ref:
http://wap.sciencenet.cn/blog-2577109-1175503.html






========================================
tassel: 关联分析算p值
----------------------------------------

1. 下载和安装 

https://tassel.bitbucket.io/

$ wget https://bitbucket.org/tasseladmin/tassel-5-standalone/get/V5.2.67.tar.gz
$ tar zxvf V5.2.67.tar.gz
$ echo 'export PATH=/home/wangjl/data/soft/tasseladmin-tassel-5-standalone-d4b9c9be6a30:$PATH' >> ~/.bashrc 
$ source ~/.bashrc

$ run_pipeline.pl
[main] INFO net.maizegenetics.tassel.TasselLogging - Tassel Version: 5.2.67  Date: November 10, 2020




2. 说明书

https://bytebucket.org/tasseladmin/tassel-5-source/wiki/docs/ExecutingTassel.pdf








========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



