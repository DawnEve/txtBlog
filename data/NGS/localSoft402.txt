本地生信软件
	- 本地运行的生信软件
	- 包括可以下载安装的tool/VM/docker等

生物信息学常见1000个软件的安装代码
http://www.360doc.com/content/17/1014/11/42030643_694826019.shtml


Hemberg-lab单细胞转录组数据分析（五）
http://www.360doc.com/content/19/0403/11/51784026_826127283.shtml




========================================
Galaxy生信分析平台-搭建（本地化）
----------------------------------------
1.简介
https://usegalaxy.org/

Galaxy is an open source, web-based platform for data intensive biomedical research. If you are new to Galaxy start here or consult our help resources. You can install your own Galaxy by following the tutorial and choose from thousands of tools from the Tool Shed.




2.在本地搭建
http://www.bioinfo-scrounger.com/archives/683








========================================
测序数据质控 fastqc 及报告解读
----------------------------------------
1.download and install(for centOS7 no root)
http://www.bioinformatics.babraham.ac.uk/projects/fastqc/
http://www.bioinformatics.babraham.ac.uk/projects/download.html#fastqc

(1)
$ wget http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.7.zip
$ unzip fastqc_v0.11.7.zip
$ cd FastQC
#添加运行权限
$ sudo chmod 755 fastqc
#添加软连接
$ ln -s /home/wangjl/software/FastQC/fastqc /home/wangjl/bin/fastqc

$ fastqc -v
FastQC v0.11.7
OK now.

安装：http://www.bioinformatics.babraham.ac.uk/projects/fastqc/INSTALL.txt
README：http://www.bioinformatics.babraham.ac.uk/projects/fastqc/README.txt



(2) 更多选项
$ fastqc --help  帮助
FastQC - A high throughput sequence QC analysis tool

fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 .. seqfileN

-o用来指定输出文件的所在目录，注意是不能自动新建目录的。
	输出的结果是.zip文件，默认自动解压缩，命令里加上--noextract则不解压缩。
	--outdir=/some/other/dir/ 指定输出文件位置
-f用来强制指定输入文件格式，默认会自动检测。
-c用来指定一个contaminant文件，fastqc会把overrepresented sequences往这个
contaminant文件里搜索。contaminant文件的格式是"Name\tSequences"，#开头的行是注释。
加上 -q 会进入沉默模式，即不出现下面的提示：
Started analysis of target.fq
Approx 5% complete for target.fq
Approx 10% complete for target.fq


如果输入的fastq文件名是target.fq，fastqc的输出的压缩文件将是target.fq_fastqc.zip。解压后，查看html格式的结果报告


(3) 常用命令
$ nohup fastqc -o . -t 5 -f fastq SRR3101251.fastq &
-o . 表示输出文件位置，输出到fastq文件所在目录时可以忽略该参数
-t 5：表示开5个线程运行。每个thread分配250MB内存。32位系统最多6threads。
-f fastq 表示使用的输入文件格式，支持Valid formats are bam,sam,bam_mapped,sam_mapped and fastq

简单用法：
$ fastqc untreated.fq -o fastqc_out_dir/

当fastq文件比较多时，也可以批量执行：
$ fastqc /path_to_fq/*.fq -o fastqc_out_dir/

#多线程
$ fastqc -t 20 /home/wangjl/data/apa/fq_files/bc/c19_ROW13_R2.fastq -o /home/wangjl/data/apa/190610APA/QC_before_trim/

或者后台执行
$ nohup fastqc -t 10 ../fastq/19C001854_WES001_CapNGS_R1.fq -o fastqc_out/ >R1.log 2>&1 &




2)输入bam文件呢？
$ fastqc  -t 5 -o hg19_mm10_rmdup_QC/ -f bam hg19_mm10_rmdup/c01_ROW01.rmdup.bam








2.fastqc结果报告怎么看？
fastqc结果该怎么看及切割接头：
http://www.huangshujia.me/2017/08/25/2017-08-25-Begining-WGS-Data-Analysis-Fastq-Data-Quality-Control.html
https://www.plob.org/article/5987.html



(4) Per Base Sequence Content
对所有reads的每一个位置，统计ATCG四种碱基（正常情况）的分布：

横轴为位置，纵轴为百分比。 正常情况下四种碱基的出现频率应该是接近的，而且没有位置差异。因此好的样本中四条线应该平行且接近。当部分位置碱基的比例出现bias时，即四条线在某些位置纷乱交织，往往提示我们有overrepresented sequence的污染。当所有位置的碱基比例一致的表现出bias时，即四条线平行但分开，往往代表文库有bias (建库过程或本身特点)，或者是测序中的系统误差。
当任一位置的A/T比例与G/C比例相差超过10%，报"WARN"；当任一位置的A/T比例与G/C比例相差超过20%，报"FAIL"。


(9) Duplicate Sequences
统计序列完全一样的reads的频率。测序深度越高，越容易产生一定程度的duplication，这是正常的现象，但如果duplication的程度很高，就提示我们可能有bias的存在（如建库过程中的PCR duplication）。


(10) Overrepresented Sequences
如果有某个序列大量出现，就叫做over-represented。fastqc的标准是占全部reads的0.1%以上。和上面的duplicate analysis一样，为了计算方便，只取了fq数据的前200,000条reads进行统计，所以有可能over-represented reads不在里面。而且大于75bp的reads也是只取50bp。如果命令行中加入了-c contaminant file，出现的over-represented sequence会从contaminant_file里面找匹配的hit（至少20bp且最多一个mismatch），可以给我们一些线索。
当发现超过总reads数0.1%的reads时报”WARN“，当发现超过总reads数1%的reads时报"FAIL"。

建库接头序列 https://github.com/csf-ngs/fastqc/blob/master/Contaminants/contaminant_list.txt








3.Ubuntu1804 报错处理
(1)$ fastqc *.fastq &
Analysis complete for 19C001854_WES001_CapNGS_R1.fq
Exception in thread "Thread-1" java.awt.AWTError: Assistive Technology not found: org.GNOME.Accessibility.AtkWrapper
        at java.awt.Toolkit.loadAssistiveTechnologies(Toolkit.java:807)
        at java.awt.Toolkit.getDefaultToolkit(Toolkit.java:886)
        at sun.swing.SwingUtilities2.getSystemMnemonicKeyMask(SwingUtilities2.java:2020)
        at javax.swing.plaf.basic.BasicLookAndFeel.initComponentDefaults(BasicLookAndFeel.java:1158)

解决方法:
$ sed -i 's/^assistive_technologies=/#&/' /etc/java-8-openjdk/accessibility.properties
Or just comment out below line in  /etc/java-8-openjdk/accessibility.properties
assistive_technologies=org.GNOME.Accessibility.AtkWrapper











========================================
|-- MultiQC: 批量显示QC结果的利器
----------------------------------------

1.安装
pip3 install multiqc
multiqc --help

$ fastqc --version
FastQC v0.11.7

$ multiqc --version
multiqc, version 1.7






2.使用 
(1) fastqc获得每个文件的QC报告
#单个文件：
$ fastqc untreated.fq -o fastqc_out_dir/

#批量化 先获取QC结果 *gz
ls ../R2_left/*.fastq | while read id; do fastqc -t 4 $id -o .; done


(2)multiqc 批量汇总QC报告
# multiqc
multiqc *fastqc.zip --pdf #需要提前安装某些包
# or
multiqc *fastqc.zip -o /path/to/store/




例子:
$ ls ../R2_left/c2_ROW01_R2.fastq | while read id; do fastqc -t 40 $id -o ./; done
$ multiqc *fastqc.zip -o /data/jinwf/wangjl/c1_2019APA/sc/combine/sc_fq/



refer:
https://blog.csdn.net/ada0915/article/details/77201871




========================================
|-- fastp: 质量控制 (Quality Control) //todo
----------------------------------------
# 新建 clean 文件夹存放 fastp 质控之后的数据
mkdir -p clean

# 单样本处理实例，默认选项
fastp -w 20 -i raw/sample1_R1.fq.gz -I raw/sample1_R2.fq.gz \
-o clean/sample1_R1.fastq -O clean/sample1_R2.fastq \
--report_title "${i} fastp report" \
--json clean/${i}_fastp.json --html clean/${i}_fastp.html

# 多样本处理
# fastp质控
for i in `tail -n +2 metadata.txt| cut -f1`
do

fastp -w 20 -i raw/${i}_R1.fq.gz -I raw/${i}_R2.fq.gz \
-o clean/${i}_R1.fastq -O clean/${i}_R2.fastq \
--report_title "${i} fastp report" \
--json clean/${i}_fastp.json --html clean/${i}_fastp.html

done







========================================
测序文件预处理:切除接头adapter/primer等
----------------------------------------
1.概述
In addition, poly(A) (or poly(T) on the reverse strands) could also be removed by cutadapt or FASTXToolkit. 

Tools to remove adapter sequences from next-generation sequencing data
http://bioscholar.com/genomics/tools-remove-adapter-sequences-next-generation-sequencing-data/
https://www.biostars.org/p/98707/

1) Remove adapter sequences using fastX toolkit
2) Run fastQC to identify read quality and trim accordingly
3) Run Deconseq to remove contaminants
4) Remove short reads (<10nt)
5) Assemble using bowtie2 against reference (nb our strain is not exactly the reference but should similar enough)


去接头的软件包括 Trimmomatic、cutadapt、fastx_toolkit、fastp 等。


Q&A:
1.没必要去除polyA，因为不比对到任何地方?
https://www.biostars.org/p/148743/
Have you tried mapping the RNA-seq data yet? It may not be necessary to remove polyA stretches because they won't map to a unique location and you may already have enough reads to not worry about it. 




========================================
|-- 高通量测序数据质控神器—Trimmomatic （也是一个java程序）
----------------------------------------
1. 简介
(1) 文章
Trimmomatic: A flexible read trimming tool for Illumina NGS data
Citations
Bolger, A. M., Lohse, M., & Usadel, B. (2014). Trimmomatic: A flexible trimmer for Illumina Sequence Data. Bioinformatics, btu170.

高通量测序数据质控神器——Trimmomatic。这个于 2014 年发表在 Bioinformatics 上的软件，至今为止在 Web of Science 上可以检索到 2,098 次引用，而在谷歌学术上更是达到了惊人的 3,391 次：


(2) 这个软件为什么深受大家的喜爱呢？今天分析一下它在质控方面的强大之处。

1). 无脑安装、使用"简单"、运行速度可观
这个软件是用 Java 写的，运行效率比较高.

2). 强大的去接头能力
一般的质控软件在处理含有接头序列的 reads 时，通常采用 "在允许错配的情况下，如果分析的 read 匹配一定数量的接头序列即去除这条 read 或从匹配开始的位置截断 read，仅保留匹配位置之前的部分序列" 的方式。

如果采取 "去除含有接头序列的 reads" 的方式，会造成测序数据的浪费 (如果片段选择没有控制好，整个 lane 会有很大一部分数据含有接头序列，怎么办？);

如果采取 "从匹配开始的位置截断 read，仅保留匹配位置之前的部分序列" 的方式，对于只含有少数几个碱基的 reads，普通的质控软件是处理不了的（又该怎么办？）。

But，Trimmomatic 有两种模式：Single End Mode 和 Paired End Mode，对于单端测序数据，它和其它软件相比没有明显的优势；但如果是双端测序的数据，Trimmomatic 采用两种去接头方式，更强大，更彻底！




(3) 优势 

Trimmomatic 有两种模式：Single End Mode 和 Paired End Mode，对于单端测序数据，它和其它软件相比没有明显的优势；但如果是双端测序的数据，Trimmomatic 采用两种去接头方式，更强大，更彻底！

图略：https://www.plob.org/article/12130.html

i)普通模式：匹配一定数量的接头序列即截断序列，保留匹配起始位置之前的序列，
A、如果从 reads 的开始就匹配到接头序列的话，整条 reads 会被去除；
B、如果是从 reads 的其它部分匹配到接头序列，则从匹配的位置截断序列，保留包含接头的部分。

ii)超级强大的回文模式，如上图 Ｃ和 D 所示
想要了解回文模式去接头的原理，我们需要先熟悉一下：测序结果中的接头序列来自哪里？ 由于只有当插入片段的长度小于测序的读长时才会在测序结果中出现接头序列。那么对于含有接头的片段，正反向的 reads 在除接头之外的部分应该是反向互补的。因此，对于双端测序数据的处理上，Trimmomatic 在考虑接头匹配情况的同时也检查正反向 reads 的序列，从而更加有效的去掉接头序列。理论上，即使 read 仅含有 1 个碱基的接头序列，这 1 个碱基也能被切除！

细节见文档 http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf









2. 使用: 优点是内置有接头数据
(0) 安装
https://github.com/usadellab/Trimmomatic
http://www.usadellab.org/cms/?page=trimmomatic

需要安装过 java(略)
$ wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.39.zip
$ unzip Trimmomatic-0.39.zip
$ ll /home/wangjl/soft/Trimmomatic-0.39/trimmomatic-0.39.jar



(1) Paired End: 双端测序 
With most new data sets you can use gentle quality trimming and adapter clipping.

通常我们不需要做首尾截掉。 对PE数据通常使用 keepBothReads 参数，你可以留下甚至冗余的序列，以便让流程更好管理。
You often don't need leading and traling clipping. Also in general keepBothReads can be useful when working with paired end data, you will keep even redunfant information but this likely makes your pipelines more manageable. 
注意 keepBothReads 前面的:2，这是回文模式最少的 adapter 长度，你甚至可以设置为1。（默认是保守的8）
Note the additional :2 in front of keepBothReads this is the minimum adapter length in palindrome mode, you can even set this to 1. (Default is a very conservative 8)

$ java -jar trimmomatic-0.39.jar PE \
	input_forward.fq.gz input_reverse.fq.gz \
	output_forward_paired.fq.gz output_forward_unpaired.fq.gz \
	output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz \
	ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36


for reference only (less sensitive for adapters) 只有首尾两行有差异。
$ java -jar trimmomatic-0.35.jar PE -phred33 \
	input_forward.fq.gz input_reverse.fq.gz \
	output_forward_paired.fq.gz output_forward_unpaired.fq.gz \
	output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz \
	ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36


This will perform the following: 最后一行的质控都干了啥？(括号里对应着命令的最后一行)
- 去接头 Remove adapters (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10)
- 去掉前面低质量序列 Remove leading low quality or N bases (below quality 3) (LEADING:3)
- 去掉后面低质量序列 Remove trailing low quality or N bases (below quality 3) (TRAILING:3)
- 按照4碱基滑窗，切除平均低于15的碱基。Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15 (SLIDINGWINDOW:4:15)
- 低于最低长度的去掉 Drop reads below the 36 bases long (MINLEN:36)



测试:
$ cd /data/wangjl/ATAC/fq/test4
$ java -jar /home/wangjl/soft/Trimmomatic-0.39/trimmomatic-0.39.jar PE  \
	raw/SRR7629152_1.fastq.gz raw/SRR7629152_2.fastq.gz \
	clean2/SRR7629152_1.paired.fq.gz clean2/SRR7629152_1.unpaired.fq.gz \
	clean2/SRR7629152_2.paired.fq.gz clean2/SRR7629152_2.unpaired.fq.gz \
	ILLUMINACLIP:/home/wangjl/soft/Trimmomatic-0.39/adapters/TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36

查看接头文件的内容
$ cat /home/wangjl/soft/Trimmomatic-0.39/adapters/TruSeq3-PE.fa
>PrefixPE/1
TACACTCTTTCCCTACACGACGCTCTTCCGATCT
>PrefixPE/2
GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT

关于adapter： 目前绝大部分的illumina的Hiseq和Miseq系列使用的都是Truseq3，过去的GA2测序仪使用的是Truseq2，PE/SE对应单端还是双端测序 ， 如果使用的不是illumina测的，可以按照里面的格式自己新建一个接头文件，但其中的命名要注意。详情见官网主页



try1: 报错 java.io.FileNotFoundException: /data/wangjl/ATAC/fq/test4/TruSeq3-PE.fa (No such file or directory) 
try2: 在软件安装目录 adapters/ 下，添加绝对地址后OK。
check: 原文3M，生成文件
-rw-rw-r-- 1 wangjl wangjl 3.0M Jul 19 14:39 SRR7629152_2.paired.fq.gz
-rw-rw-r-- 1 wangjl wangjl 2.3K Jul 19 14:39 SRR7629152_2.unpaired.fq.gz
没有质控报告。
速度很快！几秒。



(2) Single End: 单端测序 
$ java -jar trimmomatic-0.35.jar SE -phred33 input.fq.gz output.fq.gz ILLUMINACLIP:TruSeq3-SE:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36
This will perform the same steps, using the single-ended adapter file

测试: 
$ java -jar /home/wangjl/soft/Trimmomatic-0.39/trimmomatic-0.39.jar SE -phred33 \
	raw/SRR7629152_1.fastq.gz clean2/SRR7629152_1.clean.fq.gz \
	ILLUMINACLIP:/home/wangjl/soft/Trimmomatic-0.39/adapters/TruSeq3-SE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36

check: 笔者上面的双端，去掉的有点多。
-rw-rw-r-- 1 wangjl wangjl 2.9M Jul 19 14:52 SRR7629152_1.clean.fq.gz













========================================
|-- FASTXToolkit(http://hannonlab.cshl.edu/fastx_toolkit/)
----------------------------------------
https://github.com/DawnEve/NGS_training/blob/master/day3.markdown
说明书： http://hannonlab.cshl.edu/fastx_toolkit/commandline.html


下载和安装(http://hannonlab.cshl.edu/fastx_toolkit/install_centos.txt) 
http://hannonlab.cshl.edu/fastx_toolkit/download.html
$ axel -n 20 http://hannonlab.cshl.edu/fastx_toolkit/fastx_toolkit_0.0.13_binaries_Linux_2.6_amd64.tar.bz2
$ tar -xjvf fastx_toolkit_0.0.13_binaries_Linux_2.6_amd64.tar.bz2
添加路径：将fastx_toolkit的路径加入.bashrc最后一行。并执行source ~/.bashrc 使配置立即生效
export PATH=/home/wangjl/software/fastx_toolkit/bin/:$PATH



#数据过滤
$ fastq_quality_filter -q 30 -p 100 -i test_1.fq -o test_1_filter.fq -Q 33
$ fastq_quality_filter -q 30 -p 100 -i test_2.fq -o test_2_filter.fq -Q 33
-q 30 最低质量分数是30才保留该碱基
-p 80 是最低合格的百分比，Minimum percent of bases that must have [-q] quality.这个地方有问题//todo

-Q 33防止报错。因为默认使用的phred64，而很多使用的是phred33。

不能输入.gz格式的文件！！变通方式：
$ zcat /home/wangjl/data/scFQ/c12_A1.fa.gz | fastq_quality_filter -q 25 -p 60 -z -o /home/wangjl/data/scFQ_filtered/c12_A1_filtered.fq.gz -Q 33

再次fastqc，
$ fastqc -o ./ -t 15 c12_A1_filtered.fq.gz
发现超过100区域碱基不平衡。
问题：Total Sequences（也就是reads数）从3132226减少到1591872。

怎么去除polyA尾巴？





========================================
|-- cutadapt: 一个python包
----------------------------------------
https://cutadapt.readthedocs.io/en/stable/

安装python3: https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3.1-Linux-x86_64.sh

(1)使用pip安装：
$ pip install --user --upgrade cutadapt
检查版本号：
$ cutadapt --version
1.14



$ cutadapt --help
cutadapt version 2.3 #2019.6.15
#示例
$ cutadapt -a ADAPTER [options] [-o output.fastq] input.fastq
-j CORES, --cores CORES   指定cpu数量
--trim-n              Trim N's on ends of reads. 感觉应该先去两端的N

-a ADAPTER, --adapter ADAPTER
	Sequence of an adapter ligated to the 3' end (paired data: of the first read). 
	The adapter and subsequent bases are trimmed. 
	If a '$' character is appended ('anchoring'), the adapter is only found if it is a suffix of the read.
	去除3'端接头及之后的序列。如果加上$后缀，则只在接头是后缀时去除。
-g ADAPTER, --front ADAPTER
	Sequence of an adapter ligated to the 5' end (paired data: of the first read). 
	The adapter and any preceding bases are trimmed. Partial matches at the 5' end are allowed. 
	If a '^' character is prepended ('anchoring'), the adapter is only found if it is a prefix of the read.
	去除5'端接头及之前的序列。允许5'端部分匹配。如果有^前缀，则只在接头是前缀时去除。
-b ADAPTER, --anywhere ADAPTER
	如果第一个碱基和接头匹配，则和-g参数类似，否则和-a参数类似。常用语挽救失败的文库，如果你知道接头位置，不要使用该参数！

-e RATE, --error-rate RATE
	Maximum allowed error rate as value between 0 and 1	(no. of errors divided by length of matching region).Default: 0.1 (=10%)
	最大允许错误，0到1之间，默认0.1
-n COUNT, --times COUNT
    Remove up to COUNT adapters from each read. Default: 1
	每个序列最多去除几个接头？默认是1

-q [5'CUTOFF,]3'CUTOFF, --quality-cutoff [5'CUTOFF,]3'CUTOFF
	Trim low-quality bases from 5' and/or 3' ends of each read before adapter removal. Applied to both reads if data is paired. 
	If one value is given, only the 3' end is trimmed. 
	If two comma-separated cutoffs are given, the 5' end is trimmed with the first cutoff, the 3' end with the second.
	去除质量分数低的序列。如果只给一个值，则只去除3'端。如果逗号隔开的2个值，则第一个过滤5'端，第二个过滤3'端。

-m LEN[:LEN2], --minimum-length LEN[:LEN2]
    Discard reads shorter than LEN. Default: 0
	丢弃长度太短的序列，默认0不丢弃



(2)按照碱基质量剪切：
cutadapt要求输入文件结尾必须是fq，fastq或者fq.gz，fastq.gz，不能是fa.gz，否则报错。
http://cutadapt.readthedocs.io/en/stable/guide.html
$ cutadapt -q 10 -o output.fastq input.fastq

$ cutadapt -q 25 -m 90 -o /home/wangjl/data/scFQ_filtered/c12_A1_filtered.fastq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
#3'端去掉质量分数小于25的碱基，舍弃小于90bp的序列。


(3)测试 cutadapt 中去除polyA尾巴的参数
去除polyA尾巴 For poly-A trimming, for example, you would write:
$ cutadapt -a "A{20}" -o output.fastq input.fastq

$ cutadapt -a "A{10}" -q 25 -m 30 -o filteredA10_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
$ cutadapt -a "A{20}" -q 25 -m 30 -o filteredA20_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
$ cutadapt -a "A{30}" -q 25 -m 30 -o filteredA30_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz
$ cutadapt -a "A{100}" -q 25 -m 30 -o filteredA100_c12_A1.fq.gz /home/wangjl/data/scFQ/c12_A1.fq.gz

NofA	reads
10	2930148
20	2942041
30	2943094
100	2985093

这个N越小保留下的reads越少。



(4) 去除5'端接头
$ cutadapt -g AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT -q 25 -m 20 -o dAdapter1_CutA_c16_ROW17.fq ../polyA_fq/CutA_c16_ROW17.fq >c16_ROW17.log
如果是另一端的PCR引物测穿了，需要使用其反向互补序列。







========================================
|-- 建库接头序列 contaminant_list.txt
----------------------------------------

This file contains a list of potential contaminants which are frequently found in high throughput sequencing reactions.  
These are mostly sequences of adapters / primers used in the various sequencing chemistries.
该文件包含高通量测序常见的污染源，通常是各种测序试剂中的adapters 和 primers。

Please DO NOT rely on these sequences to design your own oligos, some of them are truncated at ambiguous positions, and none of them are definitive sequences from the manufacturers so don't blame us if you try to use them and they don't work.
不要依靠这些序列设计自己的oligo，有些在不明确的位置被删减了，这些都不是厂家给定义的序列，所以如果做不出来不要责怪我们。

You can add more sequences to the file by putting one line per entry and specifying a name[tab]sequence.  
If the contaminant you add is likely to be of use to others please consider sending it to the FastQ authors, either via a bug report at www.bioinformatics.bbsrc.ac.uk/bugzilla/ or by directly emailing simon.andrews@bbsrc.ac.uk so other users of the program can benefit.

Illumina Single End Adapter 1  ACACTCTTTCCCTACACGACGCTGTTCCATCT
Illumina Single End Adapter 2  CAAGCAGAAGACGGCATACGAGCTCTTCCGATCT
Illumina Single End PCR Primer 1  AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Single End PCR Primer 2  CAAGCAGAAGACGGCATACGAGCTCTTCCGATCT
Illumina Single End Sequencing Primer  ACACTCTTTCCCTACACGACGCTCTTCCGATCT
	
Illumina Paired End Adapter 1   ACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Paired End Adapter 2   CTCGGCATTCCTGCTGAACCGCTCTTCCGATCT
Illumina Paried End PCR Primer 1   AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Paired End PCR Primer 2   CAAGCAGAAGACGGCATACGAGATCGGTCTCGGCATTCCTGCTGAACCGCTCTTCCGATCT
Illumina Paried End Sequencing Primer 1		ACACTCTTTCCCTACACGACGCTCTTCCGATCT
Illumina Paired End Sequencing Primer 2		CGGTCTCGGCATTCCTACTGAACCGCTCTTCCGATCT


more: https://github.com/csf-ngs/fastqc/blob/master/Contaminants/contaminant_list.txt





========================================
|-- 去除接头 trim_galore: 自动检测adapter的质控软件
----------------------------------------
1.简介

_Trim Galore_ is a wrapper around [Cutadapt](https://github.com/marcelm/cutadapt) and [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) to consistently apply adapter and quality trimming to FastQ files, with extra functionality for RRBS data.

Trim Galore是对FastQC和Cutadapt的包装。适用于所有高通量测序，包括RRBS(Reduced Representation Bisulfite-Seq ), Illumina、Nextera 和smallRNA测序平台的双端和单端数据。主要功能包括两步：
第一步首先去除低质量碱基，然后去除3' 末端的adapter, 如果没有指定具体的adapter，程序会自动检测前1million的序列，然后对比前12-13bp的序列是否符合以下类型的adapter:

Illumina:   AGATCGGAAGAGC
Small RNA:  TGGAATTCTCGG
Nextera:    CTGTCTCTTATA






2. 下载和安装
(1)#不推荐！会在路径提示符前面添加(base)，洁癖的人不要尝试: conda install -c bioconda trim-galore 


(2)[推荐] 下载安装包安装，解压，配置下环境变量就可以使用：https://github.com/FelixKrueger/TrimGalore
$ git clone https://github.com/FelixKrueger/TrimGalore.git
然后里面有二进制文件，就可以用了。
在~/bin下新建软链接：
$ ln -s /home/wangjl/software/TrimGalore/trim_galore
$ trim_galore -v
version 0.6.2


官方安装步骤:
```bash
# Check that cutadapt is installed
cutadapt --version
# Check that FastQC is installed
fastqc -v
# Install Trim Galore
curl -fsSL https://github.com/FelixKrueger/TrimGalore/archive/0.6.0.tar.gz -o trim_galore.tar.gz
tar xvzf trim_galore.tar.gz
# Run Trim Galore
~/TrimGalore-0.6.0/trim_galore











3.使用方法

(1) 双端测序 去接头
echo " trim_galore cut adapters started at $(date)"
trim_galore -q 20 --phred33 --stringency 3 --length 20 -e 0.1 \
            --paired $dir/cmp/01raw_data/$fq1 $dir/cmp/01raw_data/$fq2  \
            --gzip -o $input_data
echo "trim_galore cut adapters finished at $(date)"


## 实例
$ id=SRR13730638
$ trim_galore --quality 20 --phred33 --stringency 3 --length 20 --gzip --fastqc_args "-t 5" --output_dir clean/ --paired raw/${id}_1.fastq raw/${id}_2.fastq

输出文件，注意文件名与单端不一样
$ ls -lth clean/SRR13730638*gz
-rw-r--r--. 1 wangjl jinwf 369M Sep  1 18:24 clean/SRR13730638_2_val_2.fq.gz
-rw-r--r--. 1 wangjl jinwf 332M Sep  1 18:24 clean/SRR13730638_1_val_1.fq.gz




(2) 单端测序 去接头
$ trim_galore --quality 25 --phred33 --stringency 3 --length 30 --gzip  --fastqc_args "-t 15" --output_dir trim_galore xx.fastq

参数解释(部分参数解释见(2))
$ trim_galore --help
--adapter：输入adapter序列。也可以不输入，Trim Galore!会自动寻找可能性最高的平台对应的adapter。自动搜选的平台三个，也直接显式输入这三种平台，即--illumina、--nextera和--small_rna。

--paired：对于双端测序结果，一对reads中，如果有一个被剔除，那么另一个会被同样抛弃，而不管是否达到标准。
--retain_unpaired：对于双端测序结果，一对reads中，如果一个read达到标准，但是对应的另一个要被抛弃，达到标准的read会被单独保存为一个文件。
--gzip和--dont_gzip：清洗后的数据zip打包或者不打包。

-- trim-n : 移除read一端的reads
-e：允许的错误率


实例：
$ trim_galore --quality 20 --phred33 --stringency 3 --length 20 --gzip --fastqc_args "-t 5" --output_dir /home/wangjl/data/apa/190517R/trim_galore/ /home/wangjl/data/apa/fq_files/bc/c16_ROW01_R2.fastq

输出文件:
c16_ROW01_R2_trimmed.fq.gz




(3) 批量模式
echo "Trim galore reads"
# ls *.gz | while read id; do echo "Trim ${id}"; trim_galore --quality 25 --phred33 --stringency 3 --length 30 --gzip  --fastqc_args "-t 15" --output_dir trim_galore $id; done
参数解释
	--quality 25 #设定Phred quality score阈值，默认为20。
	--phred33  #选择-phred33或者-phred64，表示测序平台使用的Phred quality score。
	--stringency 3 #设定可以忍受的前后adapter重叠的碱基数，默认为1（非常苛刻）。可以适度放宽，因为后一个adapter几乎不可能被测序仪读到。
	--length 30 #设定输出reads长度阈值，小于设定值会被抛弃。
	--gzip  #--gzip和--dont_gzip：清洗后的数据zip打包或者不打包。
	--fastqc_args "-t 15" # 线程数量
	--output_dir #输出目录。需要提前建立目录，否则运行会报错。
		-o：输出文件路径
#


实例: 
$ cat SRR_Acc_List.txt | head -n 39 | while read id; do echo $id; 
trim_galore --quality 20 --phred33 --stringency 3 --length 20 --gzip --fastqc_args "-t 5" --output_dir clean/ --paired raw/${id}_1.fastq raw/${id}_2.fastq &
done;





4.质控报告
$ cd /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/trim/galore
$ multiqc *fastqc.zip -o /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/





refer:
https://www.jianshu.com/p/7a3de6b8e503












========================================
比对工具: RNA Mapper
----------------------------------------

Popular short read aligners
http://homer.ucsd.edu/homer/basicTutorial/mapping.html
more: https://en.wikipedia.org/wiki/List_of_sequence_alignment_software#Short-Read_Sequence_Alignment

Most Popular:
bowtie : fast, works well
bowtie2 : fast, can perform local alignments too

Subread - Very fast, (also does splice alignment)
STAR - Extremely fast (also does splice alignment, requires at least 30 Gb memory)
To be honest, I would probably recommend STAR for almost any application at this point if you have the memory (see below)

BWA - Fast, allows indels, commonly used for genome/exome resequencing




========================================
|-- Bowtie2: A fast and sensitive gapped read aligner
----------------------------------------
What is Bowtie?
http://www.cnblogs.com/emanlee/archive/2011/11/12/2246358.html

Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index (based on the Burrows-Wheeler Transform or BWT) to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes. Multiple processors can be used simultaneously to achieve greater alignment speed. Bowtie 2 outputs alignments in SAM format, enabling interoperation with a large number of other tools (e.g. SAMtools, GATK) that use SAM. Bowtie 2 is distributed under the GPLv3 license, and it runs on the command line under Windows, Mac OS X and Linux.

Bowtie 2 is often the first step in pipelines for comparative genomics, including for variation calling, ChIP-seq, RNA-seq, BS-seq. Bowtie 2 and Bowtie (also called "Bowtie 1" here) are also tightly integrated into some tools, including TopHat: a fast splice junction mapper for RNA-seq reads, Cufflinks: a tool for transcriptome assembly and isoform quantitiation from RNA-seq reads, Crossbow: a cloud-enabled software tool for analyzing reseuqncing data, and Myrna: a cloud-enabled software tool for aligning RNA-seq reads and measuring differential gene expression.


http://genomebiology.com/2009/10/3/R25  (paper)
http://bowtie-bio.sourceforge.net/index.shtml (source, bin)
http://www.genome.iastate.edu/bioinfo/resources/manuals/rna_bowtie.txt
http://blog.csdn.net/cherylnatsu/article/details/6801997


官网 http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml


1.安装
https://github.com/BenLangmead/bowtie2


(1) conda安装（不推荐，会有不可测的副作用）
https://anaconda.org/bioconda/bowtie2

$ conda install -c bioconda bowtie2 #需要安装和更新很多包。
# 安装了perl，会导致冲突，又删除了
$ conda remove bowtie2 #卸载失败
$ conda uninstall -c bioconda bowtie2
## 必须同意更新好几个包，才能删除
$ which bowtie2
/usr/bin/which: no bowtie2 in (...



(2) binaries安装（推荐）
http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#building-from-source

1) 安装依赖包
Operating System	/Sync Package List	/Search	/Install
Ubuntu, Mint, Debian	/apt-get update	/apt-cache search tbb	/apt-get install libtbb-dev
Fedora, CentOS	/yum check-update	/yum search tbb	/yum install tbb-devel.x86_64

## yum install tbb-devel.x86_64

$ yum list installed | grep tbb
tbb.x86_64                             4.1-9.20130314.el7              @anaconda
tbb-devel.x86_64                       4.1-9.20130314.el7              @anaconda

2) 下载 binaries package
$ wget https://github.com/BenLangmead/bowtie2/releases/download/v2.3.5.1/bowtie2-2.3.5.1-linux-x86_64.zip
$ unzip bowtie2-2.3.5.1-linux-x86_64.zip 

3) 添加到路径[推荐]
$ vim ~/.bashrc #末尾添加一行
export PATH=/home/wangjl/software/bowtie2-2.3.5.1-linux-x86_64:$PATH
$ source ~/.bashrc

（或者把可执行文件复制到路径中的目录中： bowtie2, bowtie2-align-s, bowtie2-align-l, bowtie2-build, bowtie2-build-s, bowtie2-build-l, bowtie2-inspect, bowtie2-inspect-s and bowtie2-inspect-l.）


4) 检查路径和版本号
$ which bowtie2
~/software/bowtie2-2.3.5.1-linux-x86_64/bowtie2

$ bowtie2 --version
/home/wangjl/software/bowtie2-2.3.5.1-linux-x86_64/bowtie2-align-s version 2.3.5.1
64-bit
Built on 
Wed Apr 17 02:50:12 UTC 2019
Compiler: gcc version 7.3.1 20180303 (Red Hat 7.3.1-5) (GCC) 
Options: -O3 -m64 -msse2 -funroll-loops -g3 -g -O2 -fvisibility=hidden -I/hbb_exe_gc_hardened/include -ffunction-sections -fdata-sections -fstack-protector -D_FORTIFY_SOURCE=2 -fPIE -std=c++98 -DPOPCNT_CAPABILITY -DWITH_TBB -DNO_SPINLOCK -DWITH_QUEUELOCK=1
Sizeof {int, long, long long, void*, size_t, off_t}: {4, 8, 8, 8, 8, 8}






2. 文档 https://github.com/BenLangmead/bowtie2
https://blog.csdn.net/u011262253/article/details/79833969

(1)建立索引
# Building a small index
bowtie2-build example/reference/lambda_virus.fa example/index/lambda_virus
# Building a large index
bowtie2-build --large-index example/reference/lambda_virus.fa example/index/lambda_virus

建立6文件索引：
$ bowtie2-build mm9.fa bowtie2/mm9 --threads 50 #bowtie2/mm9 中mm9是前缀，留空不好查看文件

/home/wangjl/data/ref/human
$ bowtie2-build hg19.fa bowtie2/hg19 --threads 90 #使用90个线程



(2)开始mapping
# Aligning unpaired reads
bowtie2 -x example/index/lambda_virus -U example/reads/longreads.fq
# Aligning paired reads
bowtie2 -x example/index/lambda_virus -1 example/reads/reads_1.fq -2 example/reads/reads_2.fq


我的是单端测序：
$ bowtie2 -p 6 --local -x /home/wangjl/data/ref/mouse/bowtie2/mm9 -U /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/R2_left/c2_ROW01_R2.fastq -S /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bowtie2_mouse/c2_ROW01_R2.sam
 -p 6 使用6个线程
 -S 指定输出文件
 --local	在这种模式下，Bowtie 2不要求整个读取从一端到另一端对齐。相反，为了达到最大可能的对齐分数，可以从末端省略一些字符（“软裁剪”）

$ bowtie2 -p 60 --local -x /home/wangjl/data/ref/human/bowtie2/hg19 -U /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/R2_left/c2_ROW01_R2.fastq -S /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bowtie2_human/c2_ROW01_R2.sam


结论：bowtie2的--local选项，有助于提高比对比率。可能是能去除两端残留的建库index。








========================================
|-- STAR: ultrafast universal RNA-seq aligner 
----------------------------------------

STAR is recommended by the ENCODE project, definitely should be your best option(among Bowtie2,).


RNA-seq比对算法开发了STAR（Spliced Transcripts Alignments to a Reference，STAR）.
该算法使用了未压缩后缀阵列中的连续最大可比对种子搜索，接着种子聚类和缝合过程。

1.paper:https://academic.oup.com/bioinformatics/article/29/1/15/272537
Published: 25 October 2012
STAR被实现为一个单机C++代码。

rna call varients时gatk推荐工具，broad institute都推荐了，还是encode计划时冷泉港内部开发的，特点：超级快速（8min map完6gb的reads）、as支持性好、支持长reads、全转录本、发现嵌合转录本等，有理由看一下。

STAR, 很犀利, ENCODE专属RNA-seq工具. 在准度和时间消耗上, 效果拔群. 因为STAR, 了解一下啥是suffix array. 原来做mapping的, 真的就是ctrl+F的工作... 只是, 这个字符串寻找比较麻烦, 既要容错(insertion/deletion/mismatch), 又要考虑到RNA splicing而导致的genomic gap. 

容错的这个还好, 因为DNA-seq已经打下了夯实基础; 反倒是splicng带来的splicing junction detection问题, 是RNA-seq里专属. 所以在处理DNA-seq和RNA-seq数据做mapping时, 真的不一样. 比如bowtie是unspliced mapper, tophat是spliced mapper, 也难怪bowtie多用于DNA-seq而tophat多用于RNA-seq的mapping步骤. 

STAR 吐槽现有的RNA-seq工具都是DNA-seq工具的延伸, 并非量身打造. 从其算法里的mapping一步来看, 应该属于spliced mapper, 但又不同于把reads打断成k-mer形式. 








2. download and install 安装比对软件 STAR

(1)安装star 2.7：https://github.com/alexdobin/STAR
# Get latest STAR source from releases
wget https://github.com/alexdobin/STAR/archive/2.7.0f.tar.gz
tar -xzf 2.7.0f.tar.gz
cd STAR-2.7.0f

#编译 Compile under Linux
cd STAR/source
make STAR

#然后在~/bin下添加快捷方式
$ ln -s /home/wangjl/data/software/STAR-2.7.0f/source/STAR

#查看版本号
$ STAR --version
2.7.0f


(2)最后使用的是(lab上一致的)老版本2.5.2：
https://github.com/alexdobin/STAR/archive/2.5.2b.tar.gz
https://github.com/alexdobin/STAR

find one on the server:
$ find / -name '*STAR*'
/share/apps/genomics/STAR-2.5.2b

use it directly:
$ cd /home/wangjl/bin/
$ ln -s /share/apps/genomics/STAR-2.5.2b/bin/Linux_x86_64/STAR
$ STAR --version
STAR_2.5.2b




(3)下载参考基因组
1).资源:下载hg19基因组的fasta文件和gtf注释文件
参考基因组 http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/
jinlab: $ ls /data1/hou/RNA/refs/hg19


a1) 下载方式1
axel -n 30 http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz

a2) 下载方法2
axel -n 40 http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz
tar -zxvf chromFa.tar.gz
cd chroms
cat *.fa >hg19.fa



2)下载老鼠mm9基因组
axel -n 20 http://hgdownload.cse.ucsc.edu/goldenPath/mm9/bigZips/mm9.2bit


3)怎么把2bit转变成fasta？使用twoBitToFa工具
https://blog.csdn.net/weixin_40099163/article/details/86151988

mm9.2bit - contains the complete mm9 Mouse Genome
    in the 2bit format.  A utility program, twoBitToFa (available from our src tree), can be used to extract .fa file(s) from this file.  See also:
        http://genome.ucsc.edu/admin/cvs.html - CVS access to the source tree
        http://genome.ucsc.edu/admin/jk-install.html - building the utilities
A pre-compiled version of the command line tool can be found at: http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/

下载
$ axel -n 30 http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/twoBitToFa
$ chmod +x twoBitToFa
添加到~/bin下。

运行该软件：
$ twoBitToFa mm9.2bit mm9.fa


4)下载mm10  http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/
cd /home/wangjl/data/ref/mm10
$ axel -n 80 http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/mm10.2bit
#下载mm10 gtf和bed文件



(4)从UCSC下载gtf注释文件

选择最新的gtf注释文件，人类和小鼠常在http://www.gencodegenes.org下载，植物的可信基因组见http://plants.ensembl.org





(5) 使用genecode的一套fa和gtf文件: Release M25 (GRCm38.p6)
$ cd /home/wangjl/data/ref/mouse_M25
$ wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.annotation.gtf.gz
$ wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/GRCm38.p6.genome.fa.gz

ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.transcripts.fa.gz

$ gunzip *








3.generating genome index 生成索引(很耗时)
STAR command line format: STAR --option1-name option1-value(s)--option2-name option2-value(s)

(1)生成hg19的STAR index（放在 nohup CMD & 中运行的）
STAR --runMode genomeGenerate  \
	--runThreadN 20  \
	--genomeDir /home/wangjl/index/STAR/  \
	--genomeFastaFiles /share/reference/genome/hg19/hg19.fa  \
	--sjdbGTFfile /share/reference/genome/hg19/hg19_ucsc_genes.gtf  \
	--sjdbOverhang 100
# 大服务器上:
STAR --runMode genomeGenerate \
	--runThreadN 50 \
	--genomeDir /home/wangjl/data/ref/human/STAR/  \
	--genomeFastaFiles /home/wangjl/data/ref/human/hg19.fa \
	--sjdbGTFfile /home/wangjl/data/ref/human/hg19_ucsc_genes-20190506.gtf \
	--sjdbOverhang 100
参数解释：
	runThreadN 线程数
	runMode 运行模式，genomeGenerate 选项用来产生index;
	genomeDir 指定生成的index保存的位置
	genomeFastaFiles 输入的参考基因组文件fasta。
	sjdbGTFfile 注释gtf文件
	sjdbOverhang : specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. max(ReadLength)-1 或者默认100.

很费时，占用CPU很严重，建议晚上进行。小心被骂。
Jan 17 21:18:32 ..... started STAR run
Jan 17 21:18:32 ... starting to generate Genome files
Jan 17 21:19:47 ... starting to sort Suffix Array. This may take a long time...
Jan 17 21:20:03 ... sorting Suffix Array chunks and saving them to disk...
Jan 17 21:32:57 ... loading chunks from disk, packing SA...
Jan 17 21:34:13 ... finished generating suffix array
Jan 17 21:34:13 ... generating Suffix Array index
Jan 17 21:37:29 ... completed Suffix Array index
Jan 17 21:37:29 ..... processing annotations GTF
Jan 17 21:37:35 ..... inserting junctions into the genome indices
Jan 17 21:39:49 ... writing Genome to disk ...
Jan 17 21:39:50 ... writing Suffix Array to disk ...
Jan 17 21:40:02 ... writing SAindex to disk
Jan 17 21:40:04 ..... finished successfully

推荐使用好理解的路径名，比如 /home/wangjl/data/ref/hg19/index/star

lab server上本来就有该STAR可用的hg19索引:
/data1/hou/RNA/refs/hg19_ERCC92
/data1/hou/RNA/refs/hg19_ERCC92/index/star


(2)生成小鼠的基因组STAR index（放在 nohup CMD & 中运行的）
STAR --runThreadN 50 --runMode genomeGenerate --genomeDir /home/wangjl/data/ref/mouse/STAR/  --genomeFastaFiles /home/wangjl/data/ref/mouse/mm9.fa --sjdbGTFfile /home/wangjl/data/ref/mouse/mm9_ucsc_genes-20190506.gtf --sjdbOverhang 100
[11:05 - 11:21]


## 193服务器上
$ STAR --runMode genomeGenerate \
	--runThreadN 50 \
	--genomeDir /home/wangjl/data/ref/mouse_M25/index/star/  \
	--genomeFastaFiles /home/wangjl/data/ref/mouse_M25/GRCm38.p6.genome.fa \
	--sjdbGTFfile /home/wangjl/data/ref/mouse_M25/gencode.vM25.annotation.gtf \
	--sjdbOverhang 100
#
[11:52 - 12:07]









4.Run mapping jobs. 

51页手册STARmanual.pdf： https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf
推荐直接用(3)，
多样本考虑(2)，
uniq map低看看(4)


(1) 基本比对，后面还有3个改进版。
test data
[wangjl@nih_jin test3]$ ls -lth /home/wangjl/data/test
total 2.5G
-rwxr-xr-x. 1 wangjl user 287M Jan 17 21:59 c16_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 1.5G Jan 17 21:59 c15_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 100M Jan 17 21:59 c14_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 365M Jan 17 21:59 c13_A1.fa.gz
-rwxr-xr-x. 1 wangjl user 261M Jan 17 21:59 c12_A1.fa.gz

###基本语句to hg19
$ STAR --runThreadN 5  \
	--genomeDir /data1/hou/RNA/refs/hg19_ERCC92/index/star  \
	--readFilesIn /home/wangjl/data/test/c13_A1.fa.gz  \
	--readFilesCommand gunzip -c  \
	--outFileNamePrefix /home/wangjl/data/afterMapping/test3/c14_A1_

Jan 17 22:14:40 ..... started STAR run
Jan 17 22:14:40 ..... loading genome
Jan 17 22:15:10 ..... started mapping
Jan 17 22:17:03 ..... finished successfully
参数解释：
	--runThreadN 线程数
	--genomeDir 基因组index路径
	--readFilesIn 带路径名的fq测序文件
	--readFilesCommand gunzip -c  读取文件的解压命令和参数。也可以使用 --readFilesCommand zcat \
	--outFileNamePrefix 输出文件的前缀，默认是./
	
获得文件: c14_A1_Aligned.out.sam


(2)改进1： 对多个样本的比对，建议添加保留基因组选项（--genomeLoad LoadAndKeep），对于共用一套index的多个比对，能节省很多内存，提高比对的并发数量。
该测序read最长150bp。
$ STAR --runThreadN 10 \
	--genomeDir /data1/hou/RNA/refs/hg19_ERCC92/index/star \
	--readFilesIn /home/wangjl/data/test/c13_A1.fa.gz \
	--readFilesCommand gunzip -c \
	--outFileNamePrefix /home/wangjl/data/afterMapping/test3/c14_A1_ \
	--genomeLoad LoadAndKeep \
	--outSAMtype BAM SortedByCoordinate \
	--sjdbOverhang 149

##
--readFilesIn sample_r1.fq.gz sample_r2.fq.gz \#对于PE序列
--readFilesCommand zcat \#如果是gz文件，也可以这样解压缩
--outBAMsortingThreadN 10 \#输出bam的线程数


Note, if the spike-ins are used, the reference sequence should be augmented with the DNA sequence of the spike-in molecules prior to mapping.
注意：如果有spike-ins，参考序列也应该用spike-in分子DNA序列先扩容。

Note, when UMIs are used, their barcodes should be removed from the read sequence. A common practice is to add the barcode to the read name.
注意：使用UMI时，需要去除barcode。通常做法是把barcode加到read的名字上。



2) https://www.biostars.org/p/260069/
还有人建议，多样本比对时，
--genomeLoad LoadAndExit 载入基因组
	$ STAR --genomeLoad LoadAndExit --genomeDir starIndexDirectoryPath
然后for循环比对
	STAR --genomeLoad LoadAndKeep --genomeDir starIndexDirectoryPath --runThreadN nThreads -readFilesIn /pathToReadFile --outFileNamePrefix prefix
--genomeLoad Remove  移除基因组。
	STAR --genomeLoad Remove --genomeDir starIndexDirectoryPath


LoadAndRemove will automatically remove the index from memory once all STAR jobs using it finishes. 
LoadAndExit will leave the index in memory until you run STAR with --genomeLoad Remove.


## 测试实例: 首尾加载、卸载基因组，中间是若干比对过程。 
$ STAR --genomeLoad LoadAndExit --genomeDir /home/wangjl/data/ref/hg19/index/star/ --outFileNamePrefix map/tmp/_load_
$ STAR --runThreadN 10 \
		--outSAMtype BAM SortedByCoordinate  \
		--genomeDir /home/wangjl/data/ref/hg19/index/star/ \
		--readFilesIn clean/c16ROW01_trimmed.fq.gz \
		--readFilesCommand zcat  \
		--genomeLoad LoadAndKeep  \
		--limitBAMsortRAM 20000000000  \
		--outFileNamePrefix map/tmp/c16ROW01_
$ STAR --genomeLoad Remove --genomeDir /home/wangjl/data/ref/hg19/index/star/ --outFileNamePrefix map/tmp/_rm_






(3)改进2：推荐直接输出bam文件，适合py并发使用。
--outSAMtype BAM SortedByCoordinate 输出格式为BAM并排序过。

$ STAR --runThreadN 30  \
	--outSAMtype BAM SortedByCoordinate  \
	--genomeDir /home/wangjl/data/ref/hg19_mm10_transgenes/starIndex  \
	--readFilesIn /home/wangjl/data/apa/190515/trim_galore/c2_ROW01_R2_trimmed.fq.gz  \
	--readFilesCommand zcat  \
	--genomeLoad LoadAndKeep \  #单一会报错
	--limitBAMsortRAM 20000000000 \ #加入这一行就不报错了
	--outFileNamePrefix  /home/wangjl/data/apa/190515/hg19_mm10/c2_ROW01_
#


如果是双端 reads, 最后三个参数是为了提高比对的 uniq map，降低 too short 百分比:
$ STAR --genomeDir /home/niesy/data/reference/star_hg --runThreadN 10 --readFilesIn ATAC-1-3_L4_F0000570F0001151.R1_val_1.fq.gz ATAC-1-3_L4_F0000570F0001151.R2_val_2.fq.gz --readFilesCommand zcat --outFileNamePrefix test --outSAMtype BAM SortedByCoordinate --outBAMsortingThreadN 10 --outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --outFilterMismatchNmax 2 








## 另一个例子: 双端测序
a) 用 ensembl的数据吧:
$ cd /home/wangjl/data/ref/mm10/ensembl/
$ wget ftp://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/Mus_musculus.GRCm38.102.chr.gtf.gz
$ wget ftp://ftp.ensembl.org/pub/release-102/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz


b) 生成索引
$ STAR --runMode genomeGenerate \
	--runThreadN 50 \
	--genomeDir /home/wangjl/data/ref/mm10/ensembl/STAR/  \
	--genomeFastaFiles /home/wangjl/data/ref/mm10/ensembl/Mus_musculus.GRCm38.dna.primary_assembly.fa \
	--sjdbGTFfile /home/wangjl/data/ref/mm10/ensembl/Mus_musculus.GRCm38.102.chr.gtf \
	--sjdbOverhang 100
# 19:41 - 19:57 = 16min;


c) 批量比对，双端。
$ ls *_1.fastq | tail -n 9 | while read id; do cid=$(basename $id "_1.fastq"); 
echo $cid; 
STAR --runThreadN 30  \
	--outSAMtype BAM SortedByCoordinate  \
	--genomeDir /home/wangjl/data/ref/mm10/ensembl/STAR/  \
	--readFilesIn /home/wangjl/data/apa/20200701Fig/zs/smart_seq2/${cid}_1.fastq /home/wangjl/data/apa/20200701Fig/zs/smart_seq2/${cid}_2.fastq \
	--genomeLoad LoadAndKeep \
	--limitBAMsortRAM 20000000000 \
	--outFileNamePrefix  /home/wangjl/data/apa/20200701Fig/zs/smart_seq2/map/${cid}_
done;







(4) 改进3：如果uniq比对率过低，出现 too short 过高，则可以调整比对参数 
### https://github.com/alexdobin/STAR/issues/169
You can increase the number of mapped reads by relaxing the requirements on the mapped length, e.g.: 
--outFilterScoreMinOverLread 0.3 --outFilterMatchNminOverLread 0.3       
或者下面这个更好:
--outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --outFilterMatchNmin 0
the number of missmaches I controlled by --outFilterMismatchNmax 2 and got my preferred results.


## 参数解释
% of reads unmapped: too short |       21.10%
"Reads too short" doesn't really mean that, it just means "didn't map".

outFilterScoreMinOverLread      0.66
    float: same as outFilterScoreMin, but  normalized to read length (sum of mates' lengths for paired-end reads)


## 效果对比
Uniquely mapped reads % |	63.30%
Uniquely mapped reads % |	65.68%
# STAR 参数加上 --outFilterScoreMinOverLread 0 --outFilterMatchNminOverLread 0 --outFilterMatchNmin 0 \
Uniquely mapped reads % |	80.54%
Uniquely mapped reads % |	83.70%






(5)更多参数
除了上面常用的一些参数外，STAR的可选参数其实非常多.

输出BAM文件时，STAR还可以对BAM进行一些预处理，"--bamRemoveDuplicatesType"用于去重("UniqueIdentical","UniqueIdenticalNotMulti")

如果你希望输出信号文件(Wig格式),那么需要额外增加 --outWigType参数，如 --outWigType wiggle read2, 还可以用 --outWigStrand指定是否将两条链合并(Stranded, Unstranded), 默认 --outWigNorm RPM，也就是用RPM进行标准化，可以选择None.

如果你在建立索引或者比对的时候增加了注释信息，那么STAR还能帮你进行基因计数。参数为 --quantMode, 分为转录本水平(TranscriptomeSAM)和基因水平(GeneCounts)，在计数的时候还允许指定哪些哪些read不参与计数，"IndelSoftclipSingleend"和"Singleend"

对于非链特异性RNA-seq，同时为了保证能和Cufflinks兼容，需要添加 --outSAMstrandField intronMotif在SAM中增加XS属性，并且建议加上 --outFilterIntronMotifsRemoveNoncanonical。如果是链特异性数据，那么就不需要特别的参数，Cufflinks用 --library-type声明类型即可=

















5. 查看输出文件
log、sam、剪切点注释 三类文件，需要注意的是，sam里第五列 uniquely mapping reads的map质量值是255。


$ ls -lth
total 1.2G
-rw-r--r--. 1 wangjl user 1.9K Jan 17 22:17 c14_A1_Log.final.out
-rw-r--r--. 1 wangjl user  22K Jan 17 22:17 c14_A1_Log.out
-rw-r--r--. 1 wangjl user  364 Jan 17 22:17 c14_A1_Log.progress.out
-rw-r--r--. 1 wangjl user 351K Jan 17 22:17 c14_A1_SJ.out.tab
-rw-r--r--. 1 wangjl user 1.2G Jan 17 22:17 c14_A1_Aligned.out.sam

(1)3个log文件
1)
Log.out: 主要的log文件，对排错和debug很重要。
Log.progress.out: 报告该运行的统计结果，比如处理了多少reads，map上的占百分比。改文件每1min更新一次。
Log.final.out: mapping结束后的map统计结果，对质控很重要。对每个read（单个或双端）分别做统计，然后对全部reads汇总、求平均。
注意：STAR把一个paired-end read计为一个read，不像samtools agstat/idxstats是对每个mate分别计数。
大多信息是关于UNIQUE mappers的，不像samtools agstat/idxstats不区分unique or multi-mappers。
每个splicing都在splices数中计数，这和SJ.out.tab中的汇总一致。

mismatch/indel error rates是按照每个碱基统计的，比如 
total number of mismatches/indels in all unique mappers 除以total number of mapped bases.


2)
#目的：(python on Linux)从star结果文件获取Uniq比对reads数和百分比
import subprocess

#要点： 使用id拼接linux命令。建议都用绝对路径。
def doLinuxCMD(id):
    #cmd1
    cmd="grep 'Uniquely mapped reads number' /home/wangjl/data/apa/190517R/hg19/"+id+"_Log.final.out|cut -d '\t' -f2"
    (status, output)=subprocess.getstatusoutput(cmd)
    rs=str(status)+" "+output;
    
    #cmd2
    cmd2="grep 'Uniquely mapped reads %' /home/wangjl/data/apa/190517R/hg19/"+id+"_Log.final.out|cut -d '\t' -f2"
    (status2, output2)=subprocess.getstatusoutput(cmd2)
    rs2=str(status2)+" "+output2;

    return id+" "+rs+" "+rs2 #返回状态码status2=0表示命令正常执行，其他表示异常，需要查看output推测具体原因

#test 测试
doLinuxCMD('c12_ROW02')  #只需要传入细胞名字即可
##'c12_ROW02 0 2187377 0 72.56%'





(2)1个sam文件
Aligned.out.sam - alignments in standard SAM format.

 - 为了使sam结果和下游Cufflinks or StringTie兼容，要设置 --outSAMattrIHstart 0.(默认是1)
run Cufflinks with the library option --library-type options.

For example, 
$ cufflinks ... --library-type fr-firststrand 
should be used for the standard dUTP protocol, including
Illumina's stranded Tru-Seq. This option has to be used only for Cufflinks runs and not for STAR
runs.
In addition, it is recommended to remove the non-canonical junctions for Cufflinks runs using
 --outFilterIntronMotifs RemoveNoncanonical.

 
(3) "SJ.out.tab"存放的高可信的剪切位点，每一列的含义如下
第一列: 染色体
第二列: 内含子起始（以1为基）
第三列: 内含子结束（以1为基）
第四列：所在链，1(+)，2(-)
第五列: 内含子类型，0表示不是下面的任何一种功能，1表示GT/AG, 2表示:GT/AC,3表示GC/AG,4表示GT/GC,5表示AT/GC,6表示GT/AT
第六列: 是否是已知的注释
第七列: 有多少唯一联配支持
第八列: 有多少多重联配支持
第九列: maximum spliced alignment overhang, 这个比较难以翻译，指的是当短读比对到剪切位点时，中间会被分开，另一边能和基因组匹配的数目，例如ACGTACGT----------ACGT，就是4或者8，取决于方向。

控制过滤的参数为 --outSJfilter*系列，其中 --outSJfilterCountUniqueMin3111表示4类内含子唯一匹配的read支持数至少为3,1,1,1, 而 --outSJfilterCountTotalMin3111则表示4类内含子唯一匹配和多重匹配read的支持数和，至少为3,1,1,1。如果你设置的 --outSJfilterReadsUnique，那么上面两者是等价的，当然默认情况下是 All










refer:
STAR:
https://www.cnblogs.com/Dicor/p/4004819.html
http://www.mamicode.com/info-detail-1163133.html
http://www.bio-info-trainee.com/727.html
d:/ STARmanual.pdf







========================================
|-- Run STAR with 4 fastq files / 使用多个PE测序结果 --readFilesIn a.R1.fq,b.R1.fq a.R2.fq,b.R2.fq
----------------------------------------
1. 多输入文件用逗号隔开，R1和R2用空格隔开
You are not using correct separation of the file names. Separate group of paired-end mates by a space and multiple R1/R2 files by commas. Try

--readFilesIn PCa10_S6_L001_R1_001.fastq.gz,PCa10_S7_L001_R1_001.fastq.gz PCa10_S6_L001_R2_001.fastq.gz,PCa10_S7_L001_R2_001.fastq.gz



2. 如果有是 fastq.gz 
$ STAR --runThreadN 30 --genomeDir /srv/STAR_genome/human/ --readFilesIn /../RNA4_1_R1.fastq.gz /../RNA4_1_R2.fastq.gz --readFilesCommand zcat




ref: 
入门课程 https://biocorecrg.github.io/RNAseq_course_2019/alnpractical.html
该问题: https://www.biostars.org/p/229379/





========================================
|-- STAR比对软件的参数 --quantMode 的作用？
----------------------------------------

--quantMode TranscriptomeSAM #同时生成基于转录本的比对文件
    GeneCounts #计数


1. --quantMode GeneCounts 啥用？
https://www.biostars.org/p/462595/

for i in $(raw_data/270,raw_data/272, raw_data/274,raw_data/278C,raw_data/284C); do 
/DataAnalysis/STAR-2.7.5a/bin/Linux_x86_64/./STAR --genomeDir /DataAnalysis/Manoj-data/test-star/SAindex \
    --readFilesIn raw_data/${i}_R1.fastq raw_data/${i}_R2.fastq \
    --runThreadN 8 --outFileNamePrefix aligned/$i. \
    --outSAMtype BAM SortedByCoordinate \
    --quantMode GeneCounts; 
done


2. STAR比对软件的--quantMode TranscriptomeSAM 参数
https://blog.csdn.net/sweet_yemen/article/details/119301918
https://www.cnblogs.com/honeyShi/p/17652655.html

比对时，添加上这个参数，比对的结果文件会增加一个基于转录本比对的结果文件，直接上图

基于转录本比对的结果文件。其前缀是由转录本号+对应的长度组合而成。

相应的，比对结果文件中的信息也是对应转录本号以及比对到转录本上的位置。






========================================
|-- *** 基因表达量定量 (Quantification) -- 使用featureCounts
----------------------------------------

3. 基因表达量定量 (Quantification) -- 使用featureCounts
https://www.cnblogs.com/honeyShi/p/17652655.html

$ featureCounts -v
featureCounts v2.0.2

$ ls -lh map/SRR1124*bam #12个
-rw-rw-r-- 1 wangjl wangjl 2.4G Oct 26 16:24 map/SRR11247521_Aligned.sortedByCoord.out.bam
-rw-rw-r-- 1 wangjl wangjl 1.9G Oct 26 16:24 map/SRR11247522_Aligned.sortedByCoord.out.bam

$ ls -lth /data/wangjl/ref/hg38/gencode/GRCh38.p13.gtf
-rw-rw-r--. 1 wangjl wangjl 905M Dec 20  2022 /data/wangjl/ref/hg38/gencode/GRCh38.p13.gtf


$ mkdir counts

(1) try1: 默认 -t exon -g gene_id 
$ featureCounts \
-a /data/wangjl/ref/hg38/gencode/GRCh38.p13.gtf \
-p \
-T 10 \
-o counts/featureCounts_SRR1124_matrix.txt \
map/SRR1124*.sortedByCoord.out.bam

参数：
-a 输入GTF/GFF基因组注释文件
-p 这个参数是针对paired-end数据
-T 多线程数
-o 输出文件

仅比对到 54%-65%!!
每一行都是 ENSG00000290825.1，不是基因名。



(2) try2: 改为 -t exon -g gene_name
$ featureCounts \
-a /data/wangjl/ref/hg38/gencode/GRCh38.p13.gtf \
-p \
-T 10 \
-o counts/featureCounts_SRR1124_matrix2.txt \
-t exon -g gene_name \
map/SRR1124*.sortedByCoord.out.bam

仅比对到 54%-65%!!
每一行是 FAM138A，基因名。










========================================
|-- STAR: too many too short!
----------------------------------------
1. issues on github:
https://github.com/alexdobin/STAR/issues/577

In your case, the mapping mismatch rate is very high, ~5%, This is typically caused by either
(i) poor sequencing quality - you can check the quality scores with FASTQC tool
(ii) sample RNA divergent from the reference genome

Note that (by default) STAR does not set the limit for the mapped bases, but rather limits the minimum mapped length to ~0.66 of each read length, which on average will be 38b in your case.
The parameters that control this are --outFilterScoreMinOverLread and --outFilterMatchNminOverLread , both 0.66 by default.

最短长度是按照百分比计算的，默认 0.66 倍的序列长度。
150*0.66=99
奇怪，对不上。38/0.66=58 倒着算长度也对不上。



(1) 根据map位置分类
https://github.com/alexdobin/STAR/issues/577

Q: What happens when 1 read maps to 2+ in the reference? Is the read discarded, is the locus with the highest score used, or are all of them used?
A: Such reads will be reported as multi-mappers, unless they map to more than --outFilterMultimapNmax loci (=10 by default).
map 到1个地方的叫 uniq;
map 到 [2,10]的叫 multi;


(2) 
$ cat MDA/H/04_map/H_polyA_Log.out | grep "outFilterScoreMinOverLread"
outFilterScoreMinOverLread        0.66
outFilterScoreMinOverLread        0.66

[wangjl@bio_svr1 chenxi]$ cat MDA/H/04_map/H_polyA_Log.out | grep "outFilterMatchNminOverLread"
outFilterMatchNminOverLread       0.66
outFilterMatchNminOverLread       0.66


(3) STAR solo 
https://www.cnblogs.com/3Dgenome/p/14402741.html

$ STAR --readFilesIn Read_cDNA.fastq Read_barcode.fastq 
	--soloType Droplet 
	--soloCBwhitelist /path/to/737K-august-2016.txt 
	--soloFeatures Gene 
	--outSAMtype BAM SortedByCoordinate 
	--outSAMattributes NH HI CR UR CB UB GX GN









2. 测试

(1) STAR 
老版本:
$ cd /data/jinwf/wangjl/chenxi/MDA/H/
$ cat 04_map/H_polyA_Log.out | grep "outFilter"

$ STAR --runThreadN 5 \
	--genomeDir /home/wangjl/data/ref/hg38/gencode/index/STAR/ \
	--readFilesIn 03_cutPolyA_reads/H_polyA_reads.fq.gz \
	--readFilesCommand zcat \
	--outFileNamePrefix 04_map/H_polyA_ \
	--outSAMtype BAM SortedByCoordinate  \
	--outFilterMultimapNmax 1


v2: try --outFilterMatchNminOverLread 0.13 --outFilterScoreMinOverLread 0.13 保证 20/150=0.133 和qrtPCR一致，也就是 20bp 的引物就认为够特异了。
$ mv 04_map 04_map_v1
$ mkdir 04_map
$ STAR --runThreadN 5 \
	--genomeDir /home/wangjl/data/ref/hg38/gencode/index/STAR/ \
	--readFilesIn 03_cutPolyA_reads/H_polyA_reads.fq.gz \
	--readFilesCommand zcat \
	--outFileNamePrefix 04_map/H_polyA_ \
	--outSAMtype BAM SortedByCoordinate  \
	--outFilterMultimapNmax 1 \
	--outFilterMatchNminOverLread 0.13 \
	--outFilterScoreMinOverLread 0.13
# 11:15-->11:35, 20min;




(2) STAR solo
老版本
$ less B01_STAR_solo/H_Log.out
$ STAR27 --runThreadN 30 \
	--genomeDir /home/wangjl/data/ref/hg38/gencode/index/STAR27/ \
	--readFilesIn raw/H.R2.fastq.gz raw/H.R1.fastq.gz \
	--soloType CB_UMI_Simple --soloCBstart 1 --soloCBlen 8 --soloUMIstart 9 --soloUMIlen 10 --soloBarcodeReadLength 0  \
	--readFilesCommand zcat \
	--outSAMtype BAM SortedByCoordinate \
	--soloCBwhitelist raw/cell_barcode.txt \
	--outSAMattributes CB UB NH HI AS nM \
	--outFileNamePrefix B01_STAR_solo/H_


v2: try --outFilterMatchNminOverLread 0.13 --outFilterScoreMinOverLread 0.13 保证 20/150=0.133 和qrtPCR一致，也就是 20bp 的引物就认为够特异了。
$ mv B01_STAR_solo B01_STAR_solo_v1
$ mkdir B01_STAR_solo

$ STAR27 --runThreadN 30 \
	--genomeDir /home/wangjl/data/ref/hg38/gencode/index/STAR27/ \
	--readFilesIn raw/H.R2.fastq.gz raw/H.R1.fastq.gz \
	--soloType CB_UMI_Simple --soloCBstart 1 --soloCBlen 8 --soloUMIstart 9 --soloUMIlen 10 --soloBarcodeReadLength 0  \
	--readFilesCommand zcat \
	--outSAMtype BAM SortedByCoordinate \
	--soloCBwhitelist raw/cell_barcode.txt \
	--outSAMattributes CB UB NH HI AS nM \
	--outFileNamePrefix B01_STAR_solo/H_ \
	--outFilterMatchNminOverLread 0.13 --outFilterScoreMinOverLread 0.13
#11:25-->12:07, 40min;


查看其日志记录:
$ cat B01_STAR_solo/H_Log.final.out

                                 Started job on |	Jul 15 11:25:27
                             Started mapping on |	Jul 15 11:26:00
                                    Finished on |	Jul 15 12:07:16
       Mapping speed, Million of reads per hour |	256.61

                          Number of input reads |	176488105
                      Average input read length |	150
                                    UNIQUE READS:
                   Uniquely mapped reads number |	79360325
                        Uniquely mapped reads % |	44.97%
                          Average mapped length |	86.91
                       Number of splices: Total |	8710394
            Number of splices: Annotated (sjdb) |	8127654
                       Number of splices: GT/AG |	8294912
                       Number of splices: GC/AG |	38679
                       Number of splices: AT/AC |	3332
               Number of splices: Non-canonical |	373471
                      Mismatch rate per base, % |	2.34%
                         Deletion rate per base |	0.02%
                        Deletion average length |	1.50
                        Insertion rate per base |	0.02%
                       Insertion average length |	1.45
                             MULTI-MAPPING READS:
        Number of reads mapped to multiple loci |	86887151
             % of reads mapped to multiple loci |	49.23%
        Number of reads mapped to too many loci |	9364864
             % of reads mapped to too many loci |	5.31%
                                  UNMAPPED READS:
  Number of reads unmapped: too many mismatches |	0
       % of reads unmapped: too many mismatches |	0.00%
            Number of reads unmapped: too short |	407357
                 % of reads unmapped: too short |	0.23%
                Number of reads unmapped: other |	468408
                     % of reads unmapped: other |	0.27%
                                  CHIMERIC READS:
                       Number of chimeric reads |	0
                            % of chimeric reads |	0.00%

可见， Uniquely mapped reads % 由原来的 16.48%(29M) 上升到了 44.97%(79M)。
too short 减少为 0.23%，但是 % of reads mapped to multiple loci 高达 49.23%。
说明不归为太短的序列，则可能比对到多个位置，还是不能要。
只保留 uniq mapped reads: 44.97%(79M).



总的UMI reads数为:
$ cat B01_STAR_solo/H_Solo.out/Gene/UMIperCellSorted.txt | awk '{sum+=$1};END{print sum}'
28283876=28,283,876=28M











========================================
|-- 多线程版 STAR (snakemake 控制并发数)
----------------------------------------

1. 起点是 SRA 下载的人的PE测序文件
(1) 文件结构
|-raw/ 从GEO/SRA下载的fastq文件

$ ls raw/ | head -n 2
SRR13730617_1.fastq
SRR13730617_2.fastq

$ head SRR_Acc_List.txt 
SRR13730642
SRR13730661
SRR13730617


(2) 创建2个文件
|-config.yaml
|-map.sf

$ cp SRR_Acc_List.txt config.yaml
$ sed -i 's/^/ - /' config.yaml
$ vim config.yaml 
第1行加上: star_index: /home/wangjl/data/ref/hg19/index/star/
第2行加上: samples:


$ cat map.sf
configfile: "config.yaml"
SI=config["samples"]
rule all:
    input: "map/tmp/remove_genome.tmp"

rule load_genome:
	output:"map/tmp/load.tmp"
	params: index=config["star_index"], out="map/tmp/_load_"
	log: "map/tmp/load.tmp.log"
	shell: "touch {output} && \
		STAR --genomeLoad LoadAndExit \
		--genomeDir {params.index}  \
		--outFileNamePrefix  {params.out} >{log} 2>&1"

rule mapping:
	input: "clean/{sample}_1_val_1.fq.gz", "clean/{sample}_2_val_2.fq.gz", "map/tmp/load.tmp"
	output: "map/{sample}_Aligned.sortedByCoord.out.bam"
	params: out="map/{sample}_"
	log: "map/log/{sample}.star.log"
	threads: 30
	shell: "STAR --runThreadN {threads}  \
		--outSAMtype BAM SortedByCoordinate  \
		--genomeDir /home/wangjl/data/ref/hg19/index/star/  \
		--readFilesIn {input[0]} {input[1]} \
		--readFilesCommand zcat  \
		--genomeLoad LoadAndKeep  \
		--limitBAMsortRAM 20000000000  \
		--outFileNamePrefix {params.out} >{log} 2>&1"

rule remove_genome:
	input: expand("map/{sample}_Aligned.sortedByCoord.out.bam", sample=SI)
	output: "map/tmp/remove_genome.tmp"
	params: index=config["star_index"], out="map/tmp/_remove_"
	log: "map/tmp/remove_genome.tmp.log"
	shell: "touch {output} && \
		STAR --genomeLoad Remove \
		--genomeDir {params.index}  \
		--outFileNamePrefix  {params.out} >{log} 2>&1"
#

(3) 测试运行
$ snakemake -s map.sf -j 150 -p -n


(4) 正式运行
$ snakemake -s map.sf -j 150 -p 
## 程序中设定30个cpu核心，外部分配150个cpu核心，也就是一次可以运行 150/30=5个样本。
## 耗时：~0.5h

查看结果
$ ls -lht map/*bam |wc
     40     360    3680
$ ls map/*bam | head
map/SRR13730617_Aligned.sortedByCoord.out.bam
map/SRR13730618_Aligned.sortedByCoord.out.bam
map/SRR13730619_Aligned.sortedByCoord.out.bam

比对率挺高的：
map/SRR13730663_Log.final.out:                        Uniquely mapped reads % | 92.88%
map/SRR13730664_Log.final.out:                        Uniquely mapped reads % | 92.93%







========================================
|-- Mapping QC: 用RSeQC、deepTools 对比对后的转录组数据进行质控(An RNA-seq Quality Control Package) 5'-3'测序覆盖度
----------------------------------------
RSeQC包是一个python软件


1.目的： 
There are many ways to measure the mapping quality, including: amount of reads mapping to rRNA/tRNAs, proportion of uniquely mapping reads, reads mapping across splice junctions, read depth along the transcripts. 
Reference: * RSeQC: quality control of RNA-seq experiments Bioinformatics (2012) 28 (16): 2184-2185. doi: 10.1093/bioinformatics/bts356




2.安装
$ wget -b https://sourceforge.net/projects/rseqc/files/RSeQC-2.6.4.tar.gz
安装报错，需要python2。算了，还是使用conda切换为python2.7环境，然后

$ pip install RSeQC






3.使用(输入排序后的bam格式文件，要先samtools index产生.bai文件)：http://dldcc-web.brc.bcm.edu/lilab/liguow/CGI/rseqc/_build/html/
中文教程： https://www.jianshu.com/p/f9da70fcaf8d
官方文档: http://rseqc.sourceforge.net/

python <RSeQCpath>/geneBody_coverage.py -i input.bam -r genome.bed -o output.txt
python <RSeQCpath>/bam_stat.py -i input.bam -r genome.bed -o output.txt
python <RSeQCpath>/split_bam.py -i input.bam -r rRNAmask.bed -o output.txt


(1) 对bam文件排序和索引
// 排序
$ ls ../R2bam -S | tail
ACTTTGTCAATTAGGA-1.bam
AAAGTCCAGCAGTCTT-1.bam
...

$ ls ../R2bam -S | head -n 5 | while read id; do 
cb=$(basename $id ".bam");
echo $cb;
samtools sort -@ 5 -o ${cb}.sorted.bam ../R2bam/$id;
done;

// 索引
$ ls *bam| while read id; do 
echo $id;
samtools index ${id};
done;




(2)geneBody_coverage.py
参考基因组bed文件下载 https://www.jianshu.com/p/e0676631bd35

为了保证染色体名字一致(主要是chr1和1的区别)，要下载参考基因组fa来源一致的bed文件。
一共三家UCSC，NCBI，ensemble。
比如从UCSC下载hg19的bed文件：
https://genome.ucsc.edu/cgi-bin/hgTables


警告: 
	- bam必须排序和索引过; 
	- 提供的bed文件和bam文件的染色体名字风格要一致: 都是chr1或者1。
#


运行：
$ geneBody_coverage.py -r hg19.refseq.bed  -i Pairend_nonStrandSpecific_36mer_Human_hg19.bam -o output
	-r 参考基因组bed格式[required]
	-i Alignment file in BAM or SAM format
	-o Prefix of output files(s). [required] 必须要设置前缀
	
#查看帮助文档
$ geneBody_coverage.py --help
-i INPUT_FILES, --input=INPUT_FILES
	Input file(s) in BAM format. 
	"-i" takes these input:
	1) a single BAM file. 
	2) "," separated BAM files. 
	3) directory containing one or more bam files. 
	4) plain text file containing the path of one or more bam file (Each row is a BAM file path). 
	All BAM files should be sorted and indexed using samtools.
#

1)单个bam文件
$ geneBody_coverage.py -r /data/jinwf/wangjl/ref/human/hg19_ucsc_genes-20190509.bed  -i /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bamR2Lh/c2_ROW02.sorted.bam  -o /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/mappingQC_human/c2_ROW02

2)多个bam文件时，文件之间用逗号隔开:
$ cd /home/wangjl/data/c1_2019APA/sc/combine/sc_fq/mapping/bamR2Lh/
$ geneBody_coverage.py -r /data/jinwf/wangjl/ref/human/hg19_ucsc_genes-20190506.bed  -i c2_ROW01.sorted.bam,c2_ROW02.sorted.bam  -o ../mappingQC_human/c2_ROW0102



# 如何获取逗号隔开的大量文件名？
$ ls -m *bam ##不好：逗号和空格隔开的
AATGCCATCTTCCCAG-1.sorted.bam, ACTTTCAAGCTTAGTC-1.sorted.bam, AGAAGTAAGTGAGCCA-1.sorted.bam
$ ls -m *bam | xargs echo  | sed 's/ //g' 
AATGCCATCTTCCCAG-1.sorted.bam,ACTTTCAAGCTTAGTC-1.sorted.bam,AGAAGTAAGTGAGCCA-1.sorted.bam,ATCCTATCAGAGCCCT-1.sorted.bam,ATGCATGGTGAAGCTG-1.sorted.bam,GGATGTTCACTAACGT-1.sorted.bam,GTACAACGTCTTCGAA-1.sorted.bam,GTGAGGACACACGTGC-1.sorted.bam,TCCTTCTTCCTGGGAC-1.sorted.bam,TGTTTGTCACCGAATT-1.sorted.bam
## 最好还是使用方法4)，把文件名放到一个文本中




3)输入-i也可以提供包含bam文件的文件夹：
$ geneBody_coverage.py -r /home/wangjl/data/ref/hg19/hg19_ucsc_genes-20190509.bed -i ./ -o ./test_


4)-i 后跟文本文件，文本内容是bam的路径，一行一个bam的路径。
可以多开几个窗口，并行运行。


提示: 运行需要的时间很长，一般要过夜。为防止以外中断，请在tmux内运行。
为了加快速度，可以每5个bam文件名放到一个文本文件中，使用 -i bamFile_2.txt 多开几个窗口并行运行。
最后用R重绘图。







4. 新版本 geneBody_coverage2.py，据说很快！(线条太粗糙，不好)
https://github.com/nf-core/rnaseq/issues/195
BAM -> bigWig -> geneBody_coverage2.py
geneBody_coverage2.py 输入 bigWig 文件。


在Ubuntu上安装:
$ pip3 install RSeQC
Successfully installed RSeQC-4.0.0


$ geneBody_coverage2.py
Usage: geneBody_coverage2.py [options]

Calculate the RNA-seq reads coverage over gene body.
This module uses bigwig file as input.

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -i INPUT_FILE, --input-file=INPUT_FILE
                        Coverage signal file in bigwig format
  -r REF_GENE_MODEL, --refgene=REF_GENE_MODEL
                        Reference gene model in bed format. [required]
  -o OUTPUT_PREFIX, --out-prefix=OUTPUT_PREFIX
                        Prefix of output files(s). [required]
  -t GRAPH_TYPE, --graph-type=GRAPH_TYPE
                        Graphic file type in "pdf", "jpeg", "bmp", "bmp",
                        "tiff" or "png".default=pdf [optional]
#

(1) 怎么 bam to bigWig?
bam -> bedGraph -> bigWig: 见 NGS/文件格式

(2) bigWig 做 gene body 覆盖度

$ geneBody_coverage2.py -i KO_1.bw -r /home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene.bed -o geneBody2/KO_1_
# 17:35 -> 19:39  4min;
# 效果不好。毛刺太多，曲线像台阶一样，太粗糙。
# 还是用原版吧，虽然慢，但是线条细腻。


耗时对比
## 保证bed和bam中的染色体名字一致，都是带chr或不带。
$ cat /home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene.bed | sed 's/^chr//' >/home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene_noChr.bed

$ geneBody_coverage.py -i /home/wangjl/data/ZhangMin/map/ADK_KO_1Aligned.sortedByCoord.out.bam -r /home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene_noChr.bed -o geneBody/KO_1_
# 17:30 -> 19:01  1.5h.

还是用原函数，批量化，可能需要到明天出结果了。











5. 太慢了，有替代品吗？qualimap rnaseq 
https://github.com/nf-core/rnaseq/issues/195

结论：没有中间文件，不方便5'-3'覆盖度的二次绘图。
但给出比对的质控，uniq百分比，比对到不同位置的百分比:
    exonic =  32,876,847 (75.15%)
    intronic = 10,286,263 (23.51%)
    intergenic = 585,148 (1.34%)
    overlapping exon = 8,712,959 (19.92%)


(2)$ qualimap --help
rnaseq           Evaluate RNA-seq alignment data
$ qualimap rnaseq --help
usage: qualimap rnaseq [-a <arg>] -bam <arg> -gtf <arg> [-oc <arg>] [-outdir
       <arg>] [-outfile <arg>] [-outformat <arg>] [-p <arg>] [-pe] [-s]

## 运行
$ qualimap rnaseq -bam map/${id}Aligned.sortedByCoord.out.bam -gtf /data/wangjl/ref/mouse/ensembl/Mus_musculus.GRCm38.102.gtf -outdir QC_map/ -outfile ${id}.report.pdf -outformat PDF \
  --java-mem-size=10G

## try1:
WARNING: out of memory!
Qualimap allows to set RAM size using special argument: --java-mem-size
Check more details using --help command or read the manual. 
加上那个内存语句后就正常了。
18:33->18:38，确实快多了。
效果也不太好，图不太清晰，不方便二次绘制。









6. 尝试 deepTools computeMatrix (~20min)
$ deeptools --version
deeptools 3.5.1


(1) 全长RNA-seq样本，gene body 覆盖程度。(要注意物种！物种小鼠)
-rw-rw-r-- 1 wangjl wangjl 2.7G Jun  2 21:30 /home/wangjl/data/ZhangMin/map/ADK_KO_3Aligned.sortedByCoord.out.bam

$ id="ADK_KO_3"
$ bamCoverage -b map/${id}Aligned.sortedByCoord.out.bam -o QC_map/${id}.bw;  ##必须提前index 10:46->10:53(7min)

参数解释
--binSize 20 分bin的宽度，默认是50
--region chr10 指定区域
-p INT 处理器个数


运行前检查bed是否和样本匹配：物种(human/mouse)、版本(hg19/hg38)、染色体(chr10/10): 
	/home/wangjl/data/ref/human/UCSC/hg38_refseq_whole_gene.noChr.bed
	/home/wangjl/data/ref/human/hg19/hg19_ucsc_genes-20190509.noChr.bed
$ computeMatrix scale-regions  -p 10  \
    -b 2000 -a 2000 \
    -R /home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene_noChr.bed \
    -S QC_map/${id}.bw  \
	--regionBodyLength 5000 \
    --skipZeros -o QC_map/matrix/${id}.body.gz #10:56->11:04(8min)
参数：--numberOfProcessors INT, -p INT 默认1


$ plotHeatmap -m QC_map/matrix/${id}.body.gz -out QC_map/matrix/${id}.body.Heatmap.pdf --plotFileFormat pdf

# try1: 第一次的结果很不好，5'-3'图看着乱七八糟的。检查发现 bam里染色体没有chr。
# try2: 重新生成noChr的bed ref文件，再做热图，还是很烂，发现该用小鼠的基因组gtf。
# try3: 正常。但是感觉基因间区有点短。
# try4: 加入 --regionBodyLength 5000 \
# 这个数据不是太好，说是全长，但是 gene body 区域不平坦。


(2) 对比 (巨慢)
$ geneBody_coverage.py --version
geneBody_coverage.py 4.0.0

$ id="ADK_KO_3"
$ geneBody_coverage.py -r /home/wangjl/data/ref/mouse/UCSC/mm10_refseq_whole_gene_noChr.bed  -i map/${id}Aligned.sortedByCoord.out.bam  -o QC_map/${id}_
10:55->? (3h)




ref:
https://www.jianshu.com/p/2b386dd437d3








========================================
|-- gtfToGenePred: gtf 转为12列 bed 格式
----------------------------------------
1. geneBody_coverage.py 的参数 -r，需要一个bed格式的gene模型文件。
我们需要把下载的 gtf 转为12列 bed 格式
-r REF_GENE_MODEL, --refgene=REF_GENE_MODEL
			Reference gene model in bed format. [required]

http://genomewiki.ucsc.edu/index.php/Genes_in_gtf_or_gff_format


(2) GenePred格式简介
http://www.biotrainee.com/thread-928-1-1.html

GTF的产生和流行有其历史的原因。但是从技术角度来讲，这个文件格式是个非常差劲的格式。

GTF格式非常冗余。以人类转录组为例，Gencode V22的GTF文件为1.2G，压缩之后只有40M。大家知道压缩软件的压缩比是和软件的冗余程度。很少有文件能够压缩到1/30的大小。可见GTF格式多么冗余。GTF格式（及其早期版本GFF等）有很好的替代格式。从信息量上来讲：GTF 等价于 GenePred （Bed12) + Gene_Anno_table。GenePred是Jimmy Kent创建UCSC genome browser的时候建立的文件格式。UCSC的文件格式定义是非常smart的，包括之后我可能会讲到的2bit，bigwig格式。

GTF vs GenePred：
1) 从文件大小上来看，压缩前：GTF（1.2G) >> Genepred(23M) + Gene_Anno_table (2.8M)。压缩后：GTF(40M) >> GenePred(7.8M) +Gene_Anno_table (580K)
2) 从可读性来讲，GTF是以gene interval 为单位（行），每行可以是gene，transcript，exon，intron，utr等各种信息，看起来什么都在里面，很全面，其实可读性非常差，而且容易产生各种错误。GenePred格式是以transcript为单位，每行就是一个transcript，简洁直观。
3) 从程序处理的角度来讲：以GTF文件作为输入的程序，如果换成以GenePred格式为输入，编程的难度会降低一个数量级，运算时间会快很多，代码的可读性强很多。







2. 使用UCSC的 gtfToGenePred 工具从gtf基因组注释文件可以获取reference gene model in bed format。
这种文件除了包含常用的chromosome, start, end, name, score, strand等信息外，最后一列包含了多个extron和intron等的位置，用逗号隔开。
Example：http://dldcc-web.brc.bcm.edu/lilab/liguow/RSeQC/dat/sample.bed

(1) 下载安装
http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/

$ cd ~/bin/
$ wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/gtfToGenePred
$ wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/genePredToBed

$ chmod +x gtfToGenePred #增加执行权限
$ chmod +x genePredToBed #增加执行权限
$ gtfToGenePred -v

$ gtfToGenePred #帮助文档
### kent source version 426 ###
gtfToGenePred - convert a GTF file to a genePred
usage:
   gtfToGenePred gtf genePred

options:
     -genePredExt - create a extended genePred, including frame information and gene name
     -allErrors - skip groups with errors rather than aborting.
		Useful for getting infomation about as many errors as possible.
     -ignoreGroupsWithoutExons - skip groups contain no exons rather than generate an error.
     -infoOut=file - write a file with information on each transcript
     -sourcePrefix=pre - only process entries where the source name has the specified prefix.  May be repeated.
     -impliedStopAfterCds - implied stop codon in after CDS
     -simple    - just check column validity, not hierarchy, resulting genePred may be damaged
     -geneNameAsName2 - if specified, use gene_name for the name2 field instead of gene_id.
     -includeVersion - it gene_version and/or transcript_version attributes exist, include the version in the corresponding identifiers.


(2) 使用
GTF 转换成GenePred: gtfToGenePred -genePredExt -ignoreGroupsWithoutExons -geneNameAsName2 test.gtf test.gpd
genepred格式的坐标是0起始，左闭右开。


$ cd /home/wangjl/data/ref/GRCm39/gencode/
$ gtfToGenePred -genePredExt -ignoreGroupsWithoutExons -geneNameAsName2 gencode.vM28.annotation.gtf gencode.vM28.annotation.gpd
$ head gencode.vM28.annotation.gpd 
ENSMUST00000193812.2    chr1    +       3143475 3144545 3144545 3144545 1       3143475,        3144545,        0       4933401J01Rik   none    none    -1,
ENSMUST00000082908.3    chr1    +       3172238 3172348 3172348 3172348 1       3172238,        3172348,        0       Gm26206 none    none    -1,
ENSMUST00000162897.2    chr1    -       3276123 3286567 3286567 3286567 2       3276123,3283831,        3277540,3286567,        0       Xkr4    none    none    -1,-1,
ENSMUST00000159265.2    chr1    -       3276745 3285855 3285855 3285855 2       3276745,3283661,        3277540,3285855,        0       Xkr4    none    none    -1,-1,
ENSMUST00000070533.5    chr1    -       3284704 3741721 3286244 3741571 3       3284704,3491924,3740774,        3287191,3492124,3741721,        0       Xkr4    cmpl    cmpl    1,2,0,
ENSMUST00000192857.2    chr1    +       3322979 3323459 3323459 3323459 1       3322979,        3323459,        0       Gm18956 none    none    -1,
ENSMUST00000195335.2    chr1    -       3435953 3438772 3438772 3438772 1       3435953,        3438772,        0       Gm37180 none    none    -1,
ENSMUST00000192336.2    chr1    -       3445778 3448011 3448011 3448011 1       3445778,        3448011,        0       Gm37363 none    none    -1,
ENSMUST00000194099.2    chr1    -       3535199 3537508 3537508 3537508 1       3535199,        3537508,        0       Gm37686 none    none    -1,
ENSMUST00000161581.2    chr1    +       3536809 3583776 3583776 3583776 2       3536809,3583627,        3536910,3583776,        0       Gm1992  none    none    -1,-1,
# 142376  2135640 30412952 gencode.vM28.annotation.gpd #删掉吧


$ gtfToGenePred -genePredExt -geneNameAsName2 gencode.vM28.annotation.gtf gencode.vM28.annotation.all.gpd
$ head gencode.vM28.annotation.all.gpd
ENSMUST00000193812.2    chr1    +       3143475 3144545 3144545 3144545 1       3143475,        3144545,        0       4933401J01Rik   none    none    -1,
ENSMUST00000082908.3    chr1    +       3172238 3172348 3172348 3172348 1       3172238,        3172348,        0       Gm26206 none    none    -1,
ENSMUST00000162897.2    chr1    -       3276123 3286567 3286567 3286567 2       3276123,3283831,        3277540,3286567,        0       Xkr4    none    none    -1,-1,
# 142376  2135640 30412952 gencode.vM28.annotation.all.gpd



2) 第12列是 gene name，转bed时不妨把 $1 替换成 $12。不过仅仅算覆盖度不看基因的话也无所谓了。
$ cat gencode.vM28.annotation.all.gpd | awk '{print $12}' | sort | uniq -c | wc
  54456  108912  875275
$ cat gencode.vM28.annotation.all.gpd | awk '{print $1}' | sort | uniq -c | wc
 142376  284752 4139282





(3) 从GenePred文件提取信息就可以得到bed文件
$ awk '{print $2"\t"$4"\t"$5"\t"$1"\t0\t"$3"\t"$6"\t"$7"\t0\t"$8"\t"$9"\t"$10}' gencode.vM28.annotation.all.gpd >  gencode.vM28.annotation.all.bed


$ head gencode.vM28.annotation.all.bed
chr1    3143475 3144545 ENSMUST00000193812.2    0       +       3144545 3144545 0       1       3143475,        3144545,
chr1    3172238 3172348 ENSMUST00000082908.3    0       +       3172348 3172348 0       1       3172238,        3172348,
chr1    3276123 3286567 ENSMUST00000162897.2    0       -       3286567 3286567 0       2       3276123,3283831,        3277540,3286567,
chr1    3276745 3285855 ENSMUST00000159265.2    0       -       3285855 3285855 0       2       3276745,3283661,        3277540,3285855,
chr1    3284704 3741721 ENSMUST00000070533.5    0       -       3286244 3741571 0       3       3284704,3491924,3740774,        3287191,3492124,3741721,
#   142376  1708512 26065973 gencode.vM28.annotation.all.bed

这个结尾的,不知道是否受影响。


经过测试，这个不能用，无法载入。需要重新排序后使用


(4) 排序
$ bedtools sort -i gencode.vM28.annotation.all.bed > gencode.vM28.annotation.sort.all.bed

# 删除中间文件
$ rm gencode.vM28.annotation.all.gpd
$ rm gencode.vM28.annotation.all.bed


# 提供1个 mouse genome bed版本
/home/wangjl/data/ref/GRCm39/gencode/gencode.vM28.annotation.sort.all.bed

经过测试，这个还是不能用，无法载入。原因未知。请使用 3 给出的版本。







3. 排序就是参考这个的。不过这个链接使用的是 gene_name，而不是 ensembl ID。
该版本还使用到了 genePredToBed 工具。
https://github.com/deeptools/pyGenomeTracks/issues/72
So far, we succeeded to convert gtf to bed12 using gtfToGenePred and Genepredtobed as;

(1) pred to bed 
# 直接执行这一句报错 $ genePredToBed gencode.vM28.annotation.all.bed gencode.vM28.annotation.all.bed12
invalid unsigned integer: "ENSMUST00000193812.2"


(2) 按原文从gtf开始执行执行

$ gtfToGenePred gencode.vM28.annotation.gtf vM28.annotation.gp -geneNameAsName2 -ignoreGroupsWithoutExons -genePredExt
$ awk '{ print $12"\t"$2"\t"$3"\t"$4"\t"$5"\t"$6"\t"$7"\t"$8"\t"$9"\t"$10"\t"$11"\t"$12"\t"$13"\t"$14"\t"$15"\t"}' vM28.annotation.gp > vM28.annotation.gene_name.gp
$ genePredToBed vM28.annotation.gene_name.gp vM28.annotation.bed12
$ bedtools sort -i vM28.annotation.bed12 > vM28.annotation.sort.bed12

# 删除中间文件
$ rm vM28.annotation.gp vM28.annotation.gene_name.gp vM28.annotation.bed12


# 查看文件
$ head vM28.annotation.sort.bed12
chr1    3143475 3144545 4933401J01Rik   0       +       3144545 3144545 0       1       1070,   0,
chr1    3172238 3172348 Gm26206 0       +       3172348 3172348 0       1       110,    0,
chr1    3276123 3286567 Xkr4    0       -       3286567 3286567 0       2       1417,2736,      0,7708,
chr1    3276745 3285855 Xkr4    0       -       3285855 3285855 0       2       795,2194,       0,6916,
chr1    3284704 3741721 Xkr4    0       -       3286244 3741571 0       3       2487,200,947,   0,207220,456070,
chr1    3322979 3323459 Gm18956 0       +       3323459 3323459 0       1       480,    0,
chr1    3435953 3438772 Gm37180 0       -       3438772 3438772 0       1       2819,   0,

# 142376  1708512 15865308 /home/wangjl/data/ref/GRCm39/gencode/vM28.annotation.sort.bed12



(3) 提供另一个 mouse genome bed版本
/home/wangjl/data/ref/GRCm39/gencode/vM28.annotation.sort.bed12

查看从 UCSC下载的 mm10 的bed文件
$ wc /home/wangjl/data/ref/mm10/mm10_ucsc_genes-20190510.bed 
  138930  1667160 17376913 /home/wangjl/data/ref/mm10/mm10_ucsc_genes-20190510.bed

$ head /home/wangjl/data/ref/mm10/mm10_ucsc_genes-20190510.bed 
chr1	3073252	3074322	ENSMUST00000193812.1	0	+	3073252	3073252	0	1	1070,	0,
chr1	3102015	3102125	ENSMUST00000082908.1	0	+	3102015	3102015	0	1	110,	0,
chr1	3206522	3215632	ENSMUST00000159265.1	0	-	3206522	3206522	0	2	795,2194,	0,6916,
chr1	3205900	3216344	ENSMUST00000162897.1	0	-	3205900	3205900	0	2	1417,2736,	0,7708,
chr1	3252756	3253236	ENSMUST00000192857.1	0	+	3252756	3252756	0	1	480,	0,






ref:
https://www.jianshu.com/p/3d5a772a79af
https://www.bioinfo-scrounger.com/archives/234/







========================================
|-- flagstat 命令简介
----------------------------------------
统计输入文件的相关数据并将这些数据输出至屏幕显示。
每一项统计数据都由两部分组成，分别是QC pass和QC failed，表示通过QC的reads数据量和未通过QC的reads数量。以“PASS + FAILED”格式显示。

还可以根据samtools的标志位显示相应的内容，但是这里不做讨论。

1. 命令格式：samtools flagstat <in.bam> |<in.sam> | <in.cram>
$ samtools flagstat
Usage: samtools flagstat [options] <in.bam>
      --input-fmt-option OPT[=VAL]
          Specify a single input file format option in the form of OPTION or OPTION=VALUE
  -@, --threads INT
          Number of additional threads to use [0]


2. 实例 
$ samtools flagstat SRR13730656_Aligned.sortedByCoord.out.bam
12981789 + 0 in total (QC-passed reads + QC-failed reads)
1206729 + 0 secondary
0 + 0 supplementary
0 + 0 duplicates
12981789 + 0 mapped (100.00% : N/A)
11775060 + 0 paired in sequencing
5889261 + 0 read1
5885799 + 0 read2
11771402 + 0 properly paired (99.97% : N/A)
11771402 + 0 with itself and mate mapped
3658 + 0 singletons (0.03% : N/A)
0 + 0 with mate mapped to a different chr
0 + 0 with mate mapped to a different chr (mapQ>=5)









========================================
|-- blast: 给定序列，从一个fasta库中查找最相近的序列
----------------------------------------
Wang Zhiwei: 芯片数据，如何根据序列注释出基因名字(gene symbols)？[2020.3.21]
比如: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL18109

hint: https://shengxin.ren/article/442

Me:

1. 找到文章的方法
https://gut.bmj.com/content/63/11/1700.long

Microarray processing and statistical analysis

LncRNA expression profiling was performed using the Agilent human lncRNA+mRNA array V.2.0 platform. After a filtering procedure, 8900 human lncRNAs (annotated by GENCODE(V13) database, lincRNAs from Cabili et al,21 and the University of California Santa Cruz database) were selected for the following analysis (see online supplementary methods). First, quantile normalisation of the microarray data (containing the 8900 lncRNAs and all mRNAs in the microarray) of all 119 paired tumour–normal samples was carried out. Then, the data was log 2-scale transformed. Missing values were imputed using the random Forest unsupervised classification algorithm (see online supplementary methods). The data of the 60 sample pairs in the independent cohort were processed independently in the same way.

(1)只有第2句是如何注释的(GENCODE, Cabili et al, 和UCSC)。注释后过滤，得到8900个lncRNA供后续分析。
After a filtering procedure, 
8900 human lncRNAs 
(annotated by GENCODE(V13) database, lincRNAs from Cabili et al,21 and the University of California Santa Cruz database) 
were selected for the following analysis (see online supplementary methods).



(2)文献21是
Cabili MN, Trapnell C, Goff L, et al. Integrative annotation of human large intergenic noncoding RNAs reveals global properties and specific subclasses. Genes Dev 2011;25:1915–27.


(3)补充材料1 
https://gut.bmj.com/content/gutjnl/suppl/2014/02/12/gutjnl-2013-305806.DC1/gutjnl-2013-305806supp1.pdf

1) Microarray fabrication
30 The Agilent human lncRNA+mRNA Array v2·0 was designed with four identical arrays per slide (4 x
31 180K format). Each array contained probes interrogating about 39,000 human lncRNAs and about
32 32,000 human mRNAs. Each RNA was detected by two probe repeats. The array also contained 4974
33 Agilent control probes


Then, we employed the blast program to map the
40 probes uniquely to the annotated lncRNA sequences, and 8900 lncRNAs with at least one unique 
1 probe were retrieved. For each of the 8900 lncRNAs, t

2)使用pubmed搜索 blast，第一个是网页版
https://blast.ncbi.nlm.nih.gov/Blast.cgi
其中有:
Standalone and API BLAST
Download BLAST
Get BLAST databases and executables

点击进入下载页面
https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastDocs&DOC_TYPE=Download
看到标题: Download BLAST Software and Databases



3)软件下载: ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/
blast+ download:	ftp://ftp.ncbi.nih.gov/blast/executables/blast+/
BLAST+ user manual, https://www.ncbi.nlm.nih.gov/books/NBK279690/
the BLAST Help manual, https://www.ncbi.nlm.nih.gov/books/NBK1762/

$ cd /home/wangjl/data/soft
## wget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.10.0+-x64-linux.tar.gz
$ axel -n 30 ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.10.0+-x64-linux.tar.gz
## 并行下载更快

## 下载很慢，但是我发现我已经安装过
	$ blastn -version
	blastn: 2.6.0+
	Package: blast 2.6.0, build Jan 15 2017 17:12:27
	安装位置:
	$ whereis blastn
	blastn: /usr/bin/blastn
## 版本太老，好像不能下载官方库了。决定还是重新安装吧


解压后，进入bin目录: /data/wangjl/soft/ncbi-blast-2.10.0+/bin

加入到path中；
$ vim ~/.bashrc
末尾加一行并保存:
export PATH="/data/wangjl/soft/ncbi-blast-2.10.0+/bin":$PATH

更新path
$ source ~/.bashrc

查看版本号，已经是最新版了
$ blastp -version
blastp: 2.10.0+
 Package: blast 2.10.0, build Dec  3 2019 18:03:18



4)库
Download the databases you need,(see database section below), or create your own. Start searching.

blast formatdb 使用方法介绍
http://www.cnblogs.com/emanlee/archive/2011/11/18/2254604.html


ftp://ftp.ncbi.nlm.nih.gov/blast/
ftp://ftp.ncbi.nlm.nih.gov/blast/db/


查看有哪些库
$ perl update_blastdb.pl --showall
	Connected to NCBI
	16S_ribosomal_RNA
	18S_fungal_sequences
	28S_fungal_sequences
	Betacoronavirus
	ITS_RefSeq_Fungi
	ITS_eukaryote_sequences
	LSU_eukaryote_rRNA
	LSU_prokaryote_rRNA
	SSU_eukaryote_rRNA
	landmark
	nr 蛋白库
	nt 核酸库
	patnt
	pdbaa
	pdbnt
	ref_euk_rep_genomes
	ref_prok_rep_genomes
	ref_viroids_rep_genomes
	ref_viruses_rep_genomes
	refseq_protein
	refseq_rna
	swissprot
	taxdb
#
Contents of the /blast/db/ directory
The pre-formatted BLAST databases are archived in this directory. The names of these databases and their contents are listed below.
########################
File Name        #  Content Description   
16SMicrobial.tar.gz          #  Bacterial and Archaeal 16S rRNA sequences from BioProjects 33175 and 33117
FASTA/        #  Subdirectory for FASTA formatted sequences
README        #  README for this subdirectory (this file)
Representative_Genomes.*tar.gz        #  Representative bacterial/archaeal genomes database
cdd_delta.tar.gz          #  Conserved Domain Database sequences for use with stand alone deltablast
cloud/          #  Subdirectory of databases for BLAST AMI; see http://1.usa.gov/TJAnEt
env_nr.*tar.gz        #  Protein sequences for metagenomes
env_nt.*tar.gz        #  Nucleotide sequences for metagenomes
est.tar.gz        #  This file requires est_human.*.tar.gz, est_mouse.*.tar.gz, and est_others.*.tar.gz files to function. It contains the est.nal alias so that searches against est (-db est) will include est_human, est_mouse and est_others. 
est_human.*.tar.gz        #  Human subset of the est database from the est division of GenBank, EMBL and DDBJ.
est_mouse.*.tar.gz        #  Mouse subset of the est databasae
est_others.*.tar.gz           #  Non-human and non-mouse subset of the est database
gss.*tar.gz           #  Sequences from the GSS division of GenBank, EMBL, and DDBJ
htgs.*tar.gz          #  Sequences from the HTG division of GenBank, EMBL,and DDBJ
human_genomic.*tar.gz         #  Human RefSeq (NC_) chromosome records with gap adjusted concatenated NT_ contigs
nr.*tar.gz        #  Non-redundant protein sequences from GenPept, Swissprot, PIR, PDF, PDB, and NCBI RefSeq
nt.*tar.gz        #  Partially non-redundant nucleotide sequences from all traditional divisions of GenBank, EMBL, and DDBJ excluding GSS,STS, PAT, EST, HTG, and WGS.
other_genomic.*tar.gz         #  RefSeq chromosome records (NC_) for non-human organisms
pataa.*tar.gz         #  Patent protein sequences
patnt.*tar.gz         #  Patent nucleotide sequences. Both patent databases are directly from the USPTO, or from the EPO/JPO via EMBL/DDBJ
pdbaa.*tar.gz         #  Sequences for the protein structure from the Protein Data Bank
pdbnt.*tar.gz         #  Sequences for the nucleotide structure from the Protein Data Bank. They are NOT the protein coding sequences for the corresponding pdbaa entries.
refseq_genomic.*tar.gz        #  NCBI genomic reference sequences
refseq_protein.*tar.gz        #  NCBI protein reference sequences
refseq_rna.*tar.gz        #  NCBI Transcript reference sequences
sts.*tar.gz           #  Sequences from the STS division of GenBank, EMBL,and DDBJ
swissprot.tar.gz          #  Swiss-Prot sequence database (last major update)
taxdb.tar.gz          #  Additional taxonomy information for the databases listed here providing common and scientific names
tsa_nt.*tar.gz        #  Sequences from the TSA division of GenBank, EMBL,and DDBJ
vector.tar.gz         #  Vector sequences from 2010, see Note 2 in section 4.
wgs.*tar.gz           #  Sequences from Whole Genome Shotgun assemblies





如何下载某个库?
NCBI提供了一个非常智能化的脚本update_blastdb.pl来自动下载所有blast数据库。
$ perl update_blastdb.pl nt

更优化的命令
$ nohup perl update_blastdb.pl --decompress nt >out.log 2>&1 &
自动在后台下载，然后自动解压。（下载到一半断网了，再运行会接着下载，而不会覆盖已经下载好的文件）





(4) blast搜索过程

1) blast构建索引 (makeblastdb)
https://www.ncbi.nlm.nih.gov/books/NBK279688/

如果我们下载的是已经建好索引的数据库，可以省去makeblastdb的过程。

$ makeblastdb -in mature.fa -input_type fasta -dbtype nucl -title miRBase -parse_seqids -out miRBase -logfile File_Name

-in 后接输入文件，你要格式化的fasta序列
-dbtype 后接序列类型，nucl为核酸，prot为蛋白
-title 给数据库起个名，好看~~(不能用在后面搜索时-db的参数)
-parse_seqids 推荐加上，现在有啥原因还没搞清楚
-out 后接数据库名，自己起一个有意义的名字，以后blast+搜索时要用到的-db的参数
-logfile 日志文件，如果没有默认输出到屏幕




2) 资源消耗 
常见的命令参数有下面几个：
	-query <File_In> 要查询的核酸序列
	-db <String> 数据库名字
	-out <File_Out> 输出文件
	-evalue <Real> evalue阈值
	-outfmt <String> 输出的格式

$ blastx -query test.merged.transcript.fasta -db nr -out test.blastx.out

其中fasta文件只有19938行。
可是运行起来耗费了很多资源：
	平均内存消耗：51.45G；峰值：115.37G
	cpu：1个
	运行时间：06:00:24（你敢信？这才是一个小小的test）
所以我强烈推荐用diamond替代blast来做数据库搜索。(??)


3) blast结果解读
每一个合格的序列比对都会给出一个这样的结果（一个query sequence比对到多个就有多个结果）：



如何看懂NCBI BLAST输出结果
http://www.cnblogs.com/emanlee/archive/2011/11/11/2245397.html

解读报告前需要掌握的概念
alignments 代表比对上的两个序列

hits 表示两个序列比对上的片段

Score 比对得分，如果序列匹配上得分，不一样，减分，分值越高，两个序列相似性越高 
E Value 值越小，越可信，相对的一个统计值。 
Length 输入序列的长度 
Identities 一致性，就是两个序列有多少是一样的 
Query 代表输入序列 
Sbjct 代表数据库中的序列




	>AAB70410.1 Similar to Schizosaccharomyces CCAAT-binding factor (gb|U88525).
	EST gb|T04310 comes from this gene [Arabidopsis thaliana]
	Length=208
	 
	 Score = 238 bits (607),  Expect = 7e-76, Method: Compositional matrix adjust.
	 Identities = 116/145 (80%), Positives = 127/145 (88%), Gaps = 2/145 (1%)
	 Frame = +1
	 
	Query  253  FWASQYQEIEQTSDFKNHSLPLARIKKIMKADEDVRMISAEAPVVFARACEMFILELTLR  432
				FW +Q++EIE+T+DFKNHSLPLARIKKIMKADEDVRMISAEAPVVFARACEMFILELTLR
	Sbjct  39   FWENQFKEIEKTTDFKNHSLPLARIKKIMKADEDVRMISAEAPVVFARACEMFILELTLR  98
	 
	Query  433  SWNHTEENKRRTLQKNDIAAAITRNEIFDFLVDIVPREDLKDEVLASIPRGTLPMGAPTE  612
				SWNHTEENKRRTLQKNDIAAA+TR +IFDFLVDIVPREDL+DEVL SIPRGT+P  A
	Sbjct  99   SWNHTEENKRRTLQKNDIAAAVTRTDIFDFLVDIVPREDLRDEVLGSIPRGTVPEAA-AA  157
	 
	Query  613  GLPYYYMQPQHAPQVGAPGMFMGKP  687
				G PY Y+    AP +G PGM MG P
	Sbjct  158  GYPYGYLPAGTAP-IGNPGMVMGNP  181　　

结果解读网上很多，这里不啰嗦了。

以下是我在同样条件下测试的diamond：
	平均内存消耗：11.01G；峰值：12.44G
	cpu：1个（571.17%）也就是会自动占用5-6个cpu
	运行时间：00:26:15
	而且diamond注明了，它的优势是处理>1M 的query，量越大速度越快。

diamond的简单用法：
diamond makedb --in nr.fa -d nr
diamond blastx -d nr -q test.merged.transcript.fasta -o test.matches.m8
但是diamond使用有限制，只能用于比对蛋白数据库。





############
# 实例测试
############
(5) 获得序列文件
1)准备库文件所需的fasta
$ grep -v '^[!^#]' GPL18109_family.soft >GPL18109.txt  #去掉注释行
$ awk '$6!="" {print $0}' GPL18109.txt > GPL18109_seq.txt #只保留有序列的行
$ cat GPL18109_seq.txt | awk 'NR>1{print ">"$1"|"$2"|"$3"|"$4"\n"$6}' > GPL18109.fa #一行转为2行，fasta格式
$ cat GPL18109_seq.txt | awk 'NR>1{print ">"$1"\n"$6}' > GPL18109.fa #一行转为2行，fasta格式
行数 143032

2)建库(时间: 2s)
$ makeblastdb -in GPL18109.fa -input_type fasta -dbtype nucl -title probeDB -parse_seqids -out probeDB -logfile probeDB.log
输出log为
$ cat probeDB.log
	Building a new DB, current time: 03/21/2020 21:51:53
	New DB name:   /data/wangjl/blast_data/probeDB
	New DB title:  probeDB
	Sequence type: Nucleotide
	Keep MBits: T
	Maximum file size: 1000000000B
	Adding sequences from FASTA; added 71516 sequences in 0.626761 seconds.

3)比对
##   tail GPL18109.fa >test.fasta #造数据，并为每一行>后添加前缀AA_
$ blastn -query test.fasta -db probeDB -out test.blastn.out


4)查看比对结果
$ cat test.blastn.out | grep -P "^(Query=|>)"
Query= AA_4064|521|33|Y4-New #这相当于未知序列和编号
>4064   #这相当于库中找到的条目id

Query= AA_4970|518|261|Y5-New
>4970
Query= AA_4677|519|167|Y6-New
>4677
Query= AA_5412|517|57|Y7-New
>5412
Query= AA_4497|519|188|Y8-New
>4497


(6) 准备lincRNA库文件所需的fasta格式，统一用hg38;
1) gencode, Release 33 (GRCh38.p13)
https://www.gencodegenes.org/human/
最相近的似乎是
Long non-coding RNA transcript sequences: Nucleotide sequences of long non-coding RNA transcripts on the reference chromosomes

$ cd /data/wangjl/lincRNADB
$ wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/gencode.v33.lncRNA_transcripts.fa.gz

## 2020-03-21 22:16:33 (822 KB/s) - ‘gencode.v33.lncRNA_transcripts.fa.gz’ saved [14647143]

#行数
$ zcat gencode.v33.lncRNA_transcripts.fa.gz |wc
1137813 1137813 70303664


$ gunzip gencode.v33.lncRNA_transcripts.fa.gz ##/home/wangjl/data/lincRNADB ## 1137813行
$ makeblastdb -in gencode.v33.lncRNA_transcripts.fa -input_type fasta -dbtype nucl -title genCodeDB -parse_seqids -out genCodeDB -logfile genCodeDB.log
## 报错 BLAST Database creation error: Near line 1, the local id is too long.  Its length is 110 but the maximum allowed local id length is 50.  Please find and correct all local ids that are too long. 
## 就是第一行太长了，最多50个字符，而这个已经100个了
$ head gencode.v33.lncRNA_transcripts.fa
>ENST00000473358.1|ENSG00000243485.5|OTTHUMG00000000959.2|OTTHUMT00000002840.1|MIR1302-2HG-202|MIR1302-2HG|712|
GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCCTACCCGTGCTTTCT

大于号开头的共 48438 行;
使用python替换掉这些序列: row后面编号
替换后字典文件:
$ head gencode.v33.lncRNA_transcripts_codeNameMap.txt
Row1    ENST00000473358.1       ENSG00000243485.5       OTTHUMG00000000959.2    OTTHUMT00000002840.1    MIR1302-2HG-202 MIR1302-2HG     712
Row2    ENST00000469289.1       ENSG00000243485.5       OTTHUMG00000000959.2    OTTHUMT00000002841.2    MIR1302-2HG-201 MIR1302-2HG     535

fa文件:
$ head gencode.v33.lncRNA_transcripts_short.fa
>Row1
GTGCACACGGCTCCCATGCGTTGTCTTCCGAGCGTCAGGCCGCCCCTACCCGTGCTTTCT

建库: 2s
$ makeblastdb -in gencode.v33.lncRNA_transcripts_short.fa -input_type fasta -dbtype nucl -title genCodeDB -parse_seqids -out genCodeDB -logfile genCodeDB.log

未知信息的序列:(来自于基因芯片) 143032行
$ head ../blast_data/GPL18109.fa
>12414
TTTACCTCGGTGTCCTACCAGCAAGGGGTCCTGTCTGCCACCATCCTCTATGAGATCCTG
>13811
TCTACAGTCTTGAAGAACGGGTTGAAAACAACAGTGTGCCAAGTCGCTTCTCACCTGAAT


比对: (20s) 
$ blastn -query ../blast_data/GPL18109.fa -db genCodeDB -out GPL18109.blastn.out

提取结果:
$ cat GPL18109.blastn.out | grep -P "^(Query=|ROW)" >GPL18109.blastn.out2 #144384行
$ head GPL18109.blastn.out2
Query= 12414 #这一行是未知序列，下面没有>表示没有在库中查到，***** No hits found *****
Query= 13811
Query= 12228
Query= 33906
Query= 653 #这一行是未知序列
ROW37443       111        5e-25 #这行是查到的结果
Query= 108
ROW29246       111        5e-25 #如果查到多个序列，默认取第一个
ROW29245       111        5e-25
...
Query= 79840  
ROW35254       111        5e-25
ROW40969       84.2       1e-16 #第一个e值最小
ROW31327       84.2       1e-16
ROW16715       84.2       1e-16


原比对结果
	Query= 108
	Length=60
												   Score        E
	Sequences producing significant alignments:   (Bits)     Value
												  
	ROW29246                                       111        5e-25
	ROW29245                                       111        5e-25


	>ROW29246
	Length=298

	 Score = 111 bits (60),  Expect = 5e-25
	 Identities = 60/60 (100%), Gaps = 0/60 (0%)
	 Strand=Plus/Plus

	Query  1    ATGAGCAGTGACTTTCTACTCTCCATCCAGCAGCCAACAATAGACTTCGTCATTCAGATG  60
				||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
	Sbjct  196  ATGAGCAGTGACTTTCTACTCTCCATCCAGCAGCCAACAATAGACTTCGTCATTCAGATG  255


使用python把未知序列和已知序列做一一对应，在库中查到多个的，取第一个，查不到的放弃;
13089 行输出
$ head GPL18109_gencode.csv #第一列是芯片的第一列，后面的是gencode fasta第一行的信息
653,Row37443,ENST00000563906.1,ENSG00000261538.1,OTTHUMG00000175441.1,OTTHUMT00000429982.1,AC096996.2-201,AC096996.2,426
108,Row29246,ENST00000638100.1,ENSG00000283138.1,OTTHUMG00000191673.1,OTTHUMT00000489239.1,AC006207.1-202,AC006207.1,298







2) UCSC 
http://genome.ucsc.edu/cgi-bin/hgTables

TUCP(Transcripts with Unknown Coding Potential)
?


ref:
[1]构建NCBI本地BLAST数据库 (NR NT等) | blastx/diamond使用方法 | blast构建索引 | makeblastdb
https://www.cnblogs.com/leezx/p/6425620.html
[2]五分钟搞定一个芯片的重注释，让那些没有genesymbol的数据再次好用
https://shengxin.ren/article/442






========================================
|-- megablast //todo
----------------------------------------

megablast 采用贪婪式算法，速度较一般blast快，多用于数据量大且序列相似性较高的情况。
http://www.cnblogs.com/emanlee/archive/2011/11/19/2254863.html







========================================
RNA表达量的定量
----------------------------------------

========================================
|-- 定量比对结果 HTseq-count
----------------------------------------
1. 概述
https://www.cnblogs.com/timeisbiggestboss/p/7171535.html

HTSeq:一个用于处理高通量数据（High-throughout sequencing)的python包。
HTSeq包有很多功能类，熟悉python脚本的可以自行编写数据处理脚本。
另外，HTSeq也提供了两个脚本文件能够直接处理数据:htseq-qa(检测数据质量)和htseq-count（reads计数）。
文档：http://htseq.readthedocs.io/

htseq-count用于reads计数的轻便软件。貌似所有能转换为sam格式文件的输出都可以用htseq-count计数。

htseq-count的输入文件
输入为sam/bam格式的文件，如果是paired-end数据必须按照reads名称排序（sort by name）。官方推荐了msort，不过我用起来感觉不是很方便（也可能是使用方法不当），于是我采用了samtools先对bam文件（tophat2的输出结果为bam）排序，再转换为sam。
命令：samtools sort -n file.bam #sort bam by name
      samtools view -h bamfile.bam>samfile.sam
其实 htseq-count 加上 -f bam 参数就可以使用bam格式的输入。





2.下载和安装
https://github.com/htseq/htseq

$ pip -V
pip 9.0.1 from /home/wangjl/software/anoconda3/lib/python3.6/site-packages (python 3.6)

$ pip install htseq
几秒钟后安装好了。

版本号
$ htseq-count -h
...
version 0.11.2.


(2) 下载gtf数据
https://htseq.readthedocs.io/en/release_0.11.1/count.html
Q: I have used a GTF file generated by the Table Browser function of the UCSC Genome Browser, and most reads are counted as ambiguous. Why?
A: In these files, the gene_id attribute incorrectly contains the same value as the transcript_id attribute and hence a different value for each transcript of the same gene. Hence, if a read maps to an exon shared by several transcripts of the same gene, this will appear to htseq-count as and overlap with several genes. Therefore, these GTF files cannot be used as is. Either correct the incorrect gene_id attributes with a suitable script, or use a GTF file from a different source.

从UCSC Table Browser下载的gtf文件不能直接用到htseq-count中，因为gene_id和transcript_id一样，导致同一个gene有不同的value。所以，如果一个read map到多个转录本共享的exon上时，htseq-count就认为和几个基因重叠了。所以，这些gtf文件不能直接使用，要么使用脚本纠正错误的gene_id，或者从不同的途径下载GTF文件。

chr1    hg19_knownGene  exon    13221   14409   0.000000        +       .       gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; 
chr1    hg19_knownGene  exon    11874   12227   0.000000        +       .       gene_id "uc010nxr.1"; transcript_id "uc010nxr.1"; 
chr1    hg19_knownGene  exon    12646   12697   0.000000        +       .       gene_id "uc010nxr.1"; transcript_id "uc010nxr.1"; 
chr1    hg19_knownGene  exon    13221   14409   0.000000        +       .       gene_id "uc010nxr.1"; transcript_id "uc010nxr.1"; 
chr1    hg19_knownGene  start_codon     12190   12192   0.000000        +       .       gene_id "uc010nxq.1"; transcript_id "uc010nxq.1"; 

修改为：
chr1	stdin	transcript	11874	14409	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; 
chr1	stdin	exon	11874	12227	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; exon_number "1"; exon_id "uc001aaa.3.1";
chr1	stdin	exon	12613	12721	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; exon_number "2"; exon_id "uc001aaa.3.2";
chr1	stdin	exon	13221	14409	.	+	.	gene_id "uc001aaa.3"; transcript_id "uc001aaa.3"; exon_number "3"; exon_id "uc001aaa.3.3";
chr1	stdin	transcript	11874	14409	.	+	.	gene_id "DDX11L1"; transcript_id "uc010nxr.1"; 
chr1	stdin	exon	11874	12227	.	+	.	gene_id "DDX11L1"; transcript_id "uc010nxr.1"; exon_number "1"; exon_id "uc010nxr.1.1";


(2.1)怎么修改？







(2.2)
https://www.gencodegenes.org/human/release_30lift37.html
下载gtf文件和Gene symbol吧。

如何修改gtf文件？
https://www.gencodegenes.org/pages/data_format.html
python版： https://github.com/openvax/gtfparse







3. 用法：
(1)常用
$ htseq-count -h
usage: htseq-count [options] alignment_file gff_file
...
<alignment_file> :contains the aligned reads in the SAM format.
	Make sure to use a splicing-aware aligner such as TopHat.
	To read from standard input, use - as <alignment_file>.

$ cd /home/wangjl/data/afterMapping/quantify
$ htseq-count ../c12_A1_Aligned.out.sam /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_geneid.gtf > htseq_c12_A1.sam.count 2>htseq_c12_A1.sam.count.log
$ ls -lth
total 276K
-rw-r--r--. 1 wangjl user 270K Jan 19 16:58 htseq_c12_A1.sam.count
-rw-r--r--. 1 wangjl user 1.8K Jan 19 16:58 htseq_c12_A1.sam.count.log


警告！如果是bam文件，一定要加-f bam ，否则获得的表达数据全是0！


(2)更多参数
usage: htseq-count [options] alignment_file gff_file

-f {sam,bam}  (default: sam)
-r {pos,name}  (default: name) 比对文件的排序方式。PE数据必须按照pos或者name排序，且必须明确指定。SE忽略该设置。
-s {yes,no,reverse}  (default: yes) reads是否匹配到同一条链上。 #此处关于选项-s为我自己的认识，不一定对
    #数据是否来源于链特异性测序，链特异性是指在建库测序时，只测mRNA反转录出的cDNA序列，而不测该cDNA序列反向互补的另一条DNA序列；换句话说就是，链特异性能更准确反映出mRNA的序列信息
    #我们知道在gff/gtf中第7列是+-信息，+表示来源于参考基因组序列正链，-表示参考基因组序列的反向互补链
    #sam/bam文件的第2列是flag信息，也可以看出比对到正链还是负链
    #stranded=no，无链特异性，一条reads通过flag列知道比对到+还是-链后，不管是不是和gff/gtf相匹配，都算是这个feature中的
    #stranded=yes, 且se测序，要和gff/gtf相匹配，才算是这个feature中的
    #stranded=yes, 且pe测序，要和gff/gtf相匹配，才算是这个feature中的
    #stranded=reverse，是yes的相反，这时不是和gff/gtf相匹配了，而是恰好相反，可能源于另一种链特异性，只测cDNA序列反向互补的另一条DNA序列
-a MINAQUAL (default: 10)
    #忽略比对质量低于此值的比对结果
-t feature type 外显子为最小的定义单位，对基因计数，只需要将包含的外显子计数相加即可。 
    #feature type (3rd column in GFF file) to be used, all features of other type are ignored (default, suitable for Ensembl GTF files: exon)
    #没想到这个还能自己设置
-i IDATTR 最终的计数单位，一般为基因。 默认为：gene_id   也可以设置转录本，但由于模型问题，计数效果不佳。
    #GFF attribute to be used as feature ID (default, suitable for Ensembl GTF files: gene_id)
-m {union,intersection-strict,intersection-nonempty} (default: union) 计数模型，统计reads的时候对一些比较特殊的reads定义是否计入。具体说明如官网图所示。


note:
加上 -i gene_name 是不是就可以输出gene symbol了？测试一下，确实可以。
然后使用 gencode.v30lift37.metadata.HGNC 第二列的HGNC基因3.7万个取交集： awk '{print $2}' gencode.v30lift37.metadata.HGNC|sort|uniq -c|wc
37409   74818  586179





4.输出结果
$ ls *count
SRR3286802.count  SRR3286803.count  SRR3286804.count  SRR3286805.count  SRR3286806.count  SRR3286807.count

#基于相同gff/gtf得到的计数文件，行数相同，第一列（基因名）相同
$ wc -l *count
  37889 SRR3286802.count
  37889 SRR3286803.count
  37889 SRR3286804.count
  37889 SRR3286805.count
  37889 SRR3286806.count
  37889 SRR3286807.count

#且最后5列统计了整个计数过程没有使用到的reads
$ tail -n 5 SRR3286802.count
__no_feature    237560
__ambiguous 1846779
__too_low_aQual 0
__not_aligned   1323985
__alignment_not_unique  2015872

####
their alignment did not overlap any gene (no_feature).
their alignment overlapped with more than one gene (ambiguous);
their alignment quality was lower than the user-specified threshold (too_low_aQual);
they did not align at all (not_aligned);
based on the NH tag in the BAM file, they aligned to more than one place in the reference genome (alignment_not_unique);





http://www.bio-info-trainee.com/244.html





========================================
|-- 定量比对结果 featureCounts: a ultrafast and accurate read summarization program
----------------------------------------
1. featurecounts具有count速度快，兼容性好的特点。
http://blog.sciencenet.cn/home.php?mod=space&uid=2609994&do=blog&id=985692

发现Subread是个功能很全面的软件，而且还有相对应的R包Rsubread。二进制包直接可用。
http://www.360doc.com/content/18/0112/01/50153987_721213961.shtml


软件的作者认为其软件的优点在于（我就复制黏贴了）：

It carries out precise and accurate read assignments by taking care of indels, junctions and structural variants in the reads
It takes only half a minute to summarize 20 million reads（真是快。。。）
It supports GTF and SAF format annotation
It supports strand-specific read counting
It can count reads at feature (eg. exon) or meta-feature (eg. gene) level
Highly flexible in counting multi-mapping and multi-overlapping reads. Such reads can be excluded, fully counted or fractionally counted（这点跟HTSeq-count不一样了，其对于多重比对的reads并不是只采用全部丢弃的策略，按照其说法是更加灵活的对待）
It gives users full control on the summarization of paired-end reads, including allowing them to check if both ends are mapped and/or if the fragment length falls within the specified range（可让使用者更加个性化的使用）
Reduce ambuiguity in assigning read pairs by searching features that overlap with both reads from the pair
It allows users to specify whether chimeric fragments should be counted（考虑的有点周到）
Automatically detect input format (SAM or BAM)
Automatically sort paired-end reads. Users can provide either location-sorted or namesorted bams files to featureCounts. Read sorting is implemented on the fly and it only incurs minimal time cost


2. 可用性和实现：featureCounts作为Subread（http://www.sourceforge.net/projects/subread）或Rsubread（http://www.bioconductor.org/packages/release/bioc/html/Rsubread.html）软件包的一部分


(1) github 
https://github.com/ShiLab-Bioinformatics/subread

$ wget https://github.com/ShiLab-Bioinformatics/subread/releases/download/2.0.2/subread-2.0.2-Linux-x86_64.tar.gz

发现了个之前安装的:
$ featureCounts -v
featureCounts v2.0.1





(2) 之前的
https://sourceforge.net/projects/subread/
$ wget -b https://sourceforge.net/projects/subread/files/subread-1.6.0/subread-1.6.0-Linux-x86_64.tar.gz --no-check-certificate
$ tar zxvf subread-1.6.0-Linux-x86_64.tar.gz
$ vim ~/.bashrc 
在该文件结尾增加一行
export PATH=/home/wangjl/software/subread-1.6.0-Linux-x86_64/bin:$PATH

export PATH=/home/wangjl/data/software/subread-1.6.0-Linux-x86_64/bin:$PATH #193 server

保存后加载改文件，使其生效。
$ source ~/.bashrc

$ featureCounts -v
featureCounts v1.6.0





3. 使用：
$ featureCounts -h 参数有点多。

官方简单教程如下：
$featureCounts -T 6 -p -t exon -g gene_id -a ~/annotation/mm10/gencode.vM13.annotation.gtf -o SRR3589959_featureCounts222.txt SRR3589959.bam

主要的参数：
-a 输入GTF/GFF基因组注释文件
-p 这个参数是针对paired-end数据
-F 指定-a注释文件的格式，默认是GTF
-g 从注释文件中提取Meta-features信息用于read count，默认是gene_id
-t 跟-g一样的意思，其是默认将exon作为一个feature
-o 输出文件
-T 多线程数

其他参数介绍只能看文档了，不常用的话也是记不住的，要用时再去翻就行
运行中和运行后有两张图可以看看，主要讲了其运行中的一些信息，如下：




例子：
# include multimapping
<featureCounts_path>/featureCounts -O -M -Q 30 -p -a genome.gtf -o outputfile input.bam
# exclude multimapping
<featureCounts_path>/featureCounts -Q 30 -p -a genome.gtf -o outputfile input.bam



Summarize a single-end read dataset using 5 threads:
$ featureCounts -T 5 -t exon -g gene_id -a annotation.gtf -o counts.txt mapping_results_SE.sam

Summarize a BAM format dataset:
$ featureCounts -t exon -g gene_id -a annotation.gtf -o counts.txt mapping_results_SE.bam

Summarize multiple datasets at the same time:
$ featureCounts -t exon -g gene_id -a annotation.gtf -o counts.txt library1.bam library2.bam library3.bam

Perform strand-specific read counting (use '-s 2' if reversely stranded):
$ featureCounts -s 1 -t exon -g gene_id -a annotation.gtf -o counts.txt mapping_results_SE.bam


多个文件，链特异性，单端测序 
$ featureCounts -s 1 -t exon -g gene_id -a annotation.gtf -o counts.txt library1.bam library2.bam library3.bam


try1：链特异吗？Successfully assigned reads : 2048668 (24.1%) 貌似不是
$ featureCounts -T 50 -s 1 -t exon -g gene_name -a /home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gtf -o FACS.C1.counts.s1.txt `ls map/*bam |head -n3`
使用50个进程 -T 50

try2: 试试另一个数字 Successfully assigned reads : 1208275 (24.0%) 
$ featureCounts -T 50 -s 2 -t exon -g gene_name -a /home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gtf -o FACS.C1.counts.s2.txt `ls map/*bam|head -n 3`


try3: 试试不用链特异呢？Successfully assigned reads : 3659205 (42.6%)
$ featureCounts -T 50 -t exon -g gene_name -a /home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gtf -o FACS.C1.counts.t3.txt `ls map/*bam|head -n 3`

怎么办？



测试：
$ featureCounts -O -M -Q 30 -p -a genome.gtf -o outputfile input.bam
-O Assign reads to all their overlapping meta-features (or features if -f is specified).
-M Multi-mapping reads will also be counted. For a multi-mapping read, all its reported alignments will be counted. The 'NH' tag in BAM/SAM input is used to detect multi-mapping reads.
-Q <int>    The minimum mapping quality score a read must satisfy in order to be counted. For paired-end reads, at least one end should satisfy this criteria. 0 by default.
-p        If specified, fragments (or templates) will be counted instead of reads. This option is only applicable for paired-end reads.
-a <string>   Name of an annotation file. GTF/GFF format by default.
        See -F option for more format information. Inbuilt annotations (SAF format) is available in 'annotation' directory of the package.


$ featureCounts -T 5 -O -M -a /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_geneid.gtf -o /home/wangjl/data/afterMapping/quantify2/fC_c12_A1.sam.count ../c12_A1_Aligned.out.sam 2>fC_c12_A1.sam.count.log

输出文件的解释：
[wangjl@nih_jin quantify2]$ ls -lth
total 19M
-rw-r--r--. 1 wangjl user  19M Jan 19 21:54 fC_c12_A1.sam.count
-rw-r--r--. 1 wangjl user 3.6K Jan 19 21:54 fC_c12_A1.sam.count.log
-rw-r--r--. 1 wangjl user  330 Jan 19 21:54 fC_c12_A1.sam.count.summary


(1).从log也就是屏幕输出可见，输出为
[wangjl@nih_jin quantify2]$ cat *log

==========     _____ _    _ ____  _____  ______          _____
=====         / ____| |  | |  _ \|  __ \|  ____|   /\   |  __ \
  =====      | (___ | |  | | |_) | |__) | |__     /  \  | |  | |
    ====      \___ \| |  | |  _ <|  _  /|  __|   / /\ \ | |  | |
      ====    ____) | |__| | |_) | | \ \| |____ / ____ \| |__| |
==========   |_____/ \____/|____/|_|  \_\______/_/    \_\_____/
  v1.6.0

//  featureCounts setting  \\
||                                                                            ||
||             Input files : 1 SAM file                                       ||
||                           S ../c12_A1_Aligned.out.sam                      ||
||                                                                            ||
||             Output file : /home/wangjl/data/afterMapping/quantify2/fC_ ... ||
||                 Summary : /home/wangjl/data/afterMapping/quantify2/fC_ ... ||
||              Annotation : /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_ge ... ||
||      Dir for temp files : /home/wangjl/data/afterMapping/quantify2         ||
||                                                                            ||
||                 Threads : 5                                                ||
||                   Level : meta-feature level                               ||
||              Paired-end : no                                               ||
||         Strand specific : no                                               ||
||      Multimapping reads : counted                                          ||
|| Multi-overlapping reads : counted                                          ||
||   Min overlapping bases : 1                                                ||
||                                                                            ||
\\  http://subread.sourceforge.net/  //

//  Running  \\
||                                                                            ||
|| Load annotation file /data1/hou/RNA/refs/hg19_ERCC92/UCSC_hg19_geneid. ... ||
||    Features : 742585                                                       ||
||    Meta-features : 28610                                                   ||
||    Chromosomes/contigs : 152                                               ||
||                                                                            ||
|| Process SAM file ../c12_A1_Aligned.out.sam...                              ||
||    Single-end reads are included.                                          ||
||    Assign reads to features...                                             ||
||    Total reads : 3353091                                                   ||
||    Successfully assigned reads : 2732203 (81.5%)                           ||
||    Running time : 0.04 minutes                                             ||
||                                                                            ||
||                         Read assignment finished.                          ||
||                                                                            ||
|| Summary of counting results can be found in file "/home/wangjl/data/after  ||
|| Mapping/quantify2/fC_c12_A1.sam.count.summary"                             ||
||                                                                            ||
\\  http://subread.sourceforge.net/  //

Successfully assigned reads : 2732203 (81.5%)  说明有81.5%定位到基因上了。
其余的为什么没有定位上？请看summary文件。

(2) $ cat *summary
Status  ../c12_A1_Aligned.out.sam
Assigned        2732203
Unassigned_Unmapped     0
Unassigned_MappingQuality       0
Unassigned_Chimera      0
Unassigned_FragmentLength       0
Unassigned_Duplicate    0
Unassigned_MultiMapping 0
Unassigned_Secondary    0
Unassigned_Nonjunction  0
Unassigned_NoFeatures   620888
Unassigned_Overlapping_Length   0
Unassigned_Ambiguity    0

(3)运行速度？没的说，仅仅Running time : 0.04 minutes！比HTseq快了一个数量级。
fC_c12_A1.sam.count 文件包含了很多杂乱的信息，如果想了解每个基因上的count数，则只需要提取出第1列和第7列的信息
$ cut -f 1,7 fC_c12_A1.sam.count |grep -v '^#' >fC_c12_A1.sam.count.lite





Citation
We have published papers on our Subread/Subjunc read aligners and featureCounts read quantifiers.

The Subread aligner: fast, accurate and scalable read mapping by seed-and-vote, Y Liao, GK Smyth, W Shi, Nucleic acids research, 2013 PMID:23558742

featureCounts: an efficient general purpose program for assigning sequence reads to genomic features, Y Liao, GK Smyth, W Shi, Bioinformatics, 2014 PMID:24227677

The R package Rsubread is easier, faster, cheaper and better for alignment and quantification of RNA sequencing reads, Y Liao, GK Smyth, W Shi, Nucleic acids research, 2019 PMID:30783653






========================================
|-- 定量比对结果 RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome //todo1
----------------------------------------
1. 
paper: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-323
https://deweylab.github.io/RSEM/

安装
wget https://github.com/deweylab/RSEM/archive/v1.3.1.tar.gz
mv v1.3.1.tar.gz RSEM-v1.3.1.tar.gz
tar zxvf RSEM-v1.3.1.tar.gz
cd RSEM-1.3.1/
#To compile RSEM, simply run
make
#To install RSEM, simply put the RSEM directory in your environment's PATH variable. Alternatively, run
make install
or 
#make install DESTDIR=/home/my_name prefix=/software #will install RSEM executables to /home/my_name/software/bin.
make install DESTDIR=/home/ prefix=/wangjl #$ ls ~/bin 已经安装了
rsem-calculate-expression #- Estimate gene and isoform expression from RNA-Seq data.


docs: https://github.com/deweylab/RSEM




2. 使用笔记
RSEM，老牌的工具依旧是笔者的第一选择
原创： 老牛哥哥  生信草堂  8月17日


Alignment-based的转录本定量-RSEM
http://www.bioinfo-scrounger.com/archives/482











========================================
|-- 使用Salmon对RNAseq进行直接定量 //todo How?
----------------------------------------
无需mapping，直接对RNA结果fq文件进行定量。
手册：https://combine-lab.github.io/salmon/getting_started/



使用 salmon 直接对 fq 进行定量。
注意：salmon 产生估计的read计数和估计的转录本每百万( transcripts per million (tpm))，
按照我们的经验，后者过于纠正scRNASeq中的长基因的表达，所以推荐使用read计数。

$ cd /home/wangjl/data/afterMapping/quantify2



1. Salmon is a tool for wicked-fast transcript quantification from RNA-seq data. 
官网：https://combine-lab.github.io/salmon/
文档：http://salmon.readthedocs.io/en/latest/salmon.html

$ cd /home/wangjl/software
$ wget -b https://github.com/COMBINE-lab/salmon/releases/download/v0.9.1/Salmon-0.9.1_linux_x86_64.tar.gz
$ tar xzvf Salmon-0.9.1_linux_x86_64.tar.gz
$ vim ~/.bashrc
结尾增加一行：
export PATH=/home/wangjl/software/Salmon-latest_linux_x86_64/bin:$PATH
保存后执行该文件：
$ source ~/.bashrc 

$ salmon -h
Salmon v0.9.1

Usage:  salmon -h|--help or
        salmon -v|--version or
        salmon -c|--cite or
        salmon [--no-version-check] <COMMAND> [-h | options]

Commands:
     index Create a salmon index
     quant Quantify a sample
     swim  Perform super-secret operation
     quantmerge Merge multiple quantifications into a single file

例子1：
$ salmon quant -i salmon_transcript_index -1 reads1.fq.gz -2 reads2.fq.gz -p #threads -l A -g genome.gtf --seqBias --gcBias --posBias


例子2：
#!/bin/bash
for fn in data/DRR0161{25..40};
do
samp=`basename ${fn}`
echo "Processing sample ${samp}"
salmon quant -i athal_index -l A \
         -1 ${fn}/${samp}_1.fastq.gz \
         -2 ${fn}/${samp}_2.fastq.gz \
         -p 8 -o quants/${samp}_quant
done 
其中
-i 是index位置
-l A 是自动判断文库类型（链特异与否）
-p 指定线程
-o 输出文件位置
输入read文件：-r, -1, -2


(1)生成索引
$ salmon index -t athal.fa.gz -i athal_index

找不到人的transcriptome，没法生成索引。
https://github.com/COMBINE-lab/salmon/issues/186
作者留言说怎么下载。

1).表达组 human transcriptome 下载 
https://www.gencodegenes.org/releases/current.html
我提的github issue: https://github.com/COMBINE-lab/salmon/issues/186

$ wget -c ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_27/gencode.v27.transcripts.fa.gz
不要用axel多线程下载，这个网站太敏感，会禁止访问，欧洲人就是没有美国人大气。







========================================
|-- 比对质量质控 deepTools: tools for exploring deep sequencing data
----------------------------------------
1. 基本使用
(1) 安装
https://deeptools.readthedocs.io/en/develop/

deepTools is a suite of python tools particularly developed for the efficient analysis of high-throughput sequencing data, such as ChIP-seq, RNA-seq or MNase-seq.
deeptools是基于Python开发的一套工具，用于处理诸如RNA-seq, ChIP-seq, MNase-seq, ATAC-seq等高通量数据。


工具分为四个模块，当然也可以简单分为两个部分：数据处理和可视化。
	BAM和bigWig文件处理
	质量控制
	热图和其他描述性作图
	其他
#

$ pip3 install deeptools
$ deeptools --version
deeptools 3.5.1



2. 用法 
https://deeptools.readthedocs.io/en/develop/content/list_of_tools.html

(1) bam to bigWig

deeptools提供bamCoverage和bamCompare进行格式转换，为了能够比较不同的样本，需要对先将基因组分成等宽分箱(bin)，统计每个分箱的read数，最后得到描述性统计值。
对于两个样本，描述性统计值可以是两个样本的比率，或是比率的log2值，或者是差值。如果是单个样本，可以用SES方法进行标准化。

ap2_chip_rep1_2_sorted.bam是前期比对得到的BAM文件，得到的bw文件就可以送去IGV/Jbrowse进行可视化。 
bamCoverage的基本用法：
$ bamCoverage -e 170 -bs 10 -b ap2_chip_rep1_2_sorted.bam -o ap2_chip_rep1_2.bw 
参数解释:
--bam BAM file, -b BAM file [必须参数：输入bam文件名] BAM file to process (default: None)
--outFileName FILENAME, -o FILENAME  [必须参数: 输出bw文件名] Output file name. (default: None)

-e/--extendReads和-bs/--binSize即拓展了原来的read长度，且设置分箱的大小。

--filterRNAstrand {forward, reverse}: 仅统计指定正链或负链
--region/-r CHR:START:END: 选取某个区域统计
--smoothLength: 通过使用分箱附近的read对分箱进行平滑化


2) 如果为了其他结果进行比较，还需要进行标准化，deeptools提供了如下参数：
--scaleFactor: 缩放系数
—normalizeUsingRPKMReads: Per Kilobase per Million mapped reads (RPKM)标准化
--normalizeTo1x: 按照1x测序深度(reads per genome coverage, RPGC)进行标准化
--ignoreForNormalization： 指定那些染色体不需要经过标准化

3)如果需要以100为分箱，并且标准化到1x，且仅统计某一条染色体区域的正链（这个正怎么体现的??），输出格式为bedgraph,那么命令行可以这样写
$ bamCoverage -e 170 -bs 100 -of bedgraph -r Chr4:12985884:12997458 --normalizeTo1x 100000000 -b 02-read-alignment/ap2_chip_rep1_1_sorted.bam -o chip.bedgraph
参数解释:
--region CHR:START:END, -r CHR:START:END
	Region of the genome to limit the operation to - this is useful when testing parameters to reduce the computing time. 
	The format is chr:start:end, for example --region chr10 or --region chr10:456700:891000. (default: None)


bamCompare和bamCoverage类似，只不过需要提供两个样本，并且采用SES方法进行标准化，于是多了--ratio参数。



多线程
--numberOfProcessors INT, -p INT
	Number of processors to use. 
	Type "max/2" to use half the maximum number of processors or "max" to use all available processors. (Default: 1)
#





ref:
http://www.360doc.com/content/18/0205/00/19913717_727772433.shtml



========================================
RNAseq分析: 使用 Limma, DEseq2, edgeR, 筛选差异表达基因DEG
----------------------------------------

详情参考本博客专题 R / 分析差异表达基因DEG。






========================================
|-- 使用DEXSeq分析NGS数据中的exon表达差异 //todo
----------------------------------------
1. 官网: http://bioconductor.org/packages/release/bioc/html/DEXSeq.html

Inference of differential exon usage in RNA-Seq 区分RNAseq中差异表达的外显子

The package is focused on finding differential exon usage using RNA-seq exon counts between samples with different experimental designs. It provides functions that allows the user to make the necessary statistical tests based on a model that uses the negative binomial distribution to estimate the variance between biological replicates and generalized linear models for testing. The package also provides functions for the visualization and exploration of the results.
该包聚焦于使用RNAseq的外显子count数，在不同实验设计的样品间，发现差异外显子，
提供一个基于负二项分布的模型，和广义线性模型的检验，估算生物学重复之间的变异，进行必要的统计学检验。
该包也提供对结果的探索和可视化函数。

安装： 
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("DEXSeq")


文档： 
browseVignettes("DEXSeq") #弹出网页包含pdf文档和示例代码。由于端口会变，所以使用服务器Rstudio时看不到这些文件。





2.sam文件转换成counts数据
#第一步：计数

(1)把gtf文件转为gff文件
# 找到DEXSeq提供的python角本的路径。
system.file('python_scripts', package='DEXSeq')
# [1] "/home/wangjl/R/x86_64-pc-linux-gnu-library/3.6/DEXSeq/python_scripts"
#$ python ~/projects/RLib.3.01/DEXSeq/python_scripts/dexseq_prepare_annotation.py Homo_sapiens.GRCh37.75.fixed.gtf DEXSeq.hg19.gene.gff
## # 注意的是，gtf文件必须与mapping的基因组一致，尤其是染色体的名字要一致，注意mapping的bam文件和gtf中是chr1还是1.

> pythonScriptsDir = system.file( "python_scripts", package="DEXSeq" )
> list.files(pythonScriptsDir)
## [1] "dexseq_count.py"              "dexseq_prepare_annotation.py"

官方推荐句子：python /path/to/library/DEXSeq/python_scripts/dexseq_prepare_annotation.py Drosophila_melanogaster.BDGP5.72.gtf Dmel.BDGP5.25.62.DEXSeq.chr.gff


因为一直报错，有人说是版本问题，就改用py2试试。需要重新安装py2的HTSeq包。
$ pip2 install --user HTSeq
$ cd /home/wangjl/data/ref/hg19
$ python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_prepare_annotation.py gencode.v30lift37.basic.annotation.gtf gencode.v30lift37.basic.annotation.gff


这里需要注意的是，gtf文件必须与mapping的基因组一致，尤其是染色体的名字要一致，比如说如果mapping时有chr，gtf文件中的染色体一定也需要有chr。这里运行python是在terminal中，而不是R中。


(2).而后使用dexseq_count.py来计数每个exon上的reads数。 Counting reads
#$ python ~/projectGREEN/RLib.3.01/DEXSeq/python_scripts/dexseq_count.py \
#    -p no -s yes -a 10 -f bam ~/DEXSeq/DEXSeq.hg19.gene.gff bam.file out.counts
#参数-p指出mapping文件是否是pair end文件，默认no。
#参数-s表示是否是stranded，默认为yes。
#-f指输入文件的格式，默认为sam。bam需要安装 pysam包。
# -a to specify the minimum alignment quality，sam文件第五列。跳过低于该值的。

# 在运行计数结束之后，需要检查一下最后四行，看看empty的多不多，如果超过20%，可能需要检查一下mapping的结果
#，当然也可能是计数文件准备错误，比如mapping结果没有index等等。如果以上都不是，那可能是polyA太多了。




$ cd /home/wangjl/data/apa/190705PAS/test/
$ samtools view -h c01_ROW07.bam >c01_ROW07.sam
$ python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_count.py -p no -s yes -a 10 \
/home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gff  c01_ROW07.sam c01_ROW07.counts





(3)批量化计数
/home/wangjl/data/apa/190705PAS/MAPQ255/c01_ROW07.bam 

同步化的syncHeLa:
c01_ROW24
c01_ROW35
c01_ROW31

非同步化的nonSyncHeLa:
c12_ROW10
c12_ROW16
c12_ROW17

$ cat sync.id | while read id; do echo ${id}; samtools view -h /home/wangjl/data/apa/190705PAS/MAPQ255/${id}.bam > ${id}.sam; \
python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_count.py /home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gff  ${id}.sam ${id}.counts; done;

$ cat non.id | while read id; do echo ${id}; samtools view -h /home/wangjl/data/apa/190705PAS/MAPQ255/${id}.bam > ${id}.sam; \
python2 /data/software/bcbio/anaconda/lib/R/library/DEXSeq/python_scripts/dexseq_count.py /home/wangjl/data/ref/hg19/gencode.v30lift37.annotation.gff  ${id}.sam ${id}.counts; done;










3.读入R环境中
# 在R中读入计数数据，需要准备好计数文件，实验设计，以及前面用到的gff文件。
#在这里，我们使用Bioconductor中已有的pasilla数据来示例。
library("DEXSeq")
#1.count file names
inDir <- system.file("extdata", package="pasilla")
countFiles <- list.files(inDir, pattern="fb.txt$", full.names=TRUE)
basename(countFiles)
## [1] "treated1fb.txt"   "treated2fb.txt"   "treated3fb.txt"   "untreated1fb.txt" "untreated2fb.txt" 
## [6] "untreated3fb.txt" "untreated4fb.txt"


#2.gff
gffFile <- list.files(inDir, pattern="gff$", full.names=TRUE) #Dmel.BDGP5.25.62.DEXSeq.chr.gff
##注意，如果是自己的数据的话，比如之前示例使用的是DEXSeq.hg19.gene.gff，这里就用DEXSeq.hg19.gene.gff

##实验设计
sampleTable <- data.frame(row.names=c(paste("treated", 1:3, sep=""), paste("untreated", 1:4, sep="")),
                          condition=rep(c("knockdown", "control"), c(3, 4)))
sampleTable
##            condition
## treated1   knockdown
## treated2   knockdown
## treated3   knockdown
## untreated1   control
## untreated2   control
## untreated3   control
## untreated4   control


dxd <- DEXSeqDataSetFromHTSeq(
  countFiles,
  sampleData=sampleTable,
  design= ~sample + exon + condition:exon,
  flattenedfile=gffFile)
## converting counts to integer mode
dim(dxd) #[1] 70463    14
head(dxd)

#查看矩阵
dim(featureCounts(dxd)) #[1] 70463     7
head( featureCounts(dxd), 5 )


# 第三步：获得差异表达数据
#只需要一步
dxr <- DEXSeq(dxd) #耗时
head(dxr)

## 对于结果dxr，可以直接视为data.frame来操作。也可以使用as.data.frame来转换它。
## 结合使用plotDEXSeq就可以查看自己感兴趣的目标基因中的exon的表达情况。
head(unique(dxr$groupID))
plotDEXSeq(dxr, geneID='FBgn0000014')
plotDEXSeq(dxr, geneID='FBgn0010909')
plotDEXSeq( dxr, "FBgn0010909", displayTranscripts=TRUE, legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 )



#
########################begin 可选的三步法
## 同DESeq一样，它分为三个步骤：normalization, Dispersion estimation 以及 testing for differential exon usage。
dxd2 <- estimateSizeFactors(dxd) #第一步
dxd3 <- estimateDispersions(dxd2) #第二步，耗时。此时可以使用plotDispEsts(dxd)来观察离散情况
plotDispEsts(dxd3)
## Figure 1: Fit Diagnostics
## The initial per-exon dispersion estimates (shown by black points), the fitted 
## mean-dispersion values function (red line), and the shrinked values in blue
dxd4 <- testForDEU(dxd3) #第三步: differential exon usage (DEU)
#dxd5 <- estimateExonFoldChanges(dxd4, fitExptoVar="condition")
##  Error in estimateExonFoldChanges(dxd4, fitExptoVar = "condition") : 
## unused argument (fitExptoVar = "condition")
dxd5 <- estimateExonFoldChanges(dxd4)
dxr2 <- DEXSeqResults(dxd5) #可以使用plotMA(dxr1)来查看结果
head(dxr2)
#
dxr[1:4,1:4]
dxr2[1:4,1:4] #一模一样
########################end




plotMA(dxr2)
mcols(dxr2)$description #描述意义

#  how many exonic regions are significant with a false discovery rate of 10%:
table ( dxr2$padj < 0.1 )
## FALSE  TRUE 
## 42985   233

##may also ask how many genes are affected
table ( tapply( dxr2$padj < 0.1, dxr2$groupID, any ) )
## FALSE  TRUE 
## 5220   166

plotMA( dxr2, cex=0.8 ) ## Figure 2: MA plot
## Mean expression versus log2 fold change plot. Significant hits at an FDR=0.1 are coloured in red.

#查看样本注释信息
sampleAnnotation(dxd)

## 其他技术和实验因素 6


#具体基因外显子的可视化
plotDEXSeq( dxr2, "FBgn0010909", legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 ) ## Figure 3: Fitted expression
## The plot represents the expression estimates from a call to testForDEU(). 
##  Shown in red is the exon that showed significant differential exon usage.

# 每个样品 with normalized count values of each exon in each of the samples.
plotDEXSeq( dxr2, "FBgn0010909", expression=FALSE, norCounts=TRUE, 
            legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 )

# after subtraction of overall changes in gene expression.
plotDEXSeq( dxr2, "FBgn0010909", expression=FALSE, splicing=TRUE,
            legend=TRUE, cex.axis=1.2, cex=1.3, lwd=2 )
#
#把所有的显著性差异的基因的相关图片全部画一遍
DEXSeqHTML( dxr, FDR=0.1, color=c("#FF000080", "#0000FF80") )
getwd()








# 不知道干啥用的
wh = (dxr2$groupID=="FBgn0010909")
stopifnot( sum(dxr2$padj[wh] < formals(plotDEXSeq)$FDR)==1 )


##
# 查看gene
head( geneIDs(dxd) )
## [1] "FBgn0000003" "FBgn0000008" "FBgn0000008" "FBgn0000008" "FBgn0000008" "FBgn0000008"

# 查看外显子
head( exonIDs(dxd) )
## [1] "E001" "E001" "E002" "E003" "E004" "E005"


## Overlap operations
interestingRegion = GRanges( "chr2L", IRanges(start=3872658, end=3875302) )
subsetByOverlaps( x=dxr, ranges=interestingRegion )
#
#This functions could be useful for further downstream analysis.
findOverlaps( query=dxr2, subject=interestingRegion )
## queryLength: 70463 / subjectLength: 1




#
library(ggplot2)
ggplot(as.data.frame(dxr2), aes( log10(exonBaseMean), log2fold_knockdown_control) )+ geom_point()

library(ggplot2)
ggplot(as.data.frame(dxr2), aes( log10(exonBaseMean), dispersion) )+ geom_point()







refer:
1.https://www.plob.org/article/6960.html
2.http://www.bio-info-trainee.com/bioconductor_China/software/DEXSeq.html
3.http://www.biotrainee.com/thread-1220-1-1.html

4. #软件工具#使用DEXSeq分析NGS数据中的exon表达差异 泊客  云生信学生物信息学  2016-10-26




========================================
|-- 加权基因共表达网络分析 (WGCNA, Weighted correlation network analysis)
----------------------------------------
https://mp.weixin.qq.com/s?__biz=MzI5MTcwNjA4NQ==&mid=2247485220&idx=1&sn=007188964e7c43d75dcd0b11b880bbfa&scene=21#wechat_redirect







========================================
MACS(Model-based Analysis of ChIP-Seq) 的安装
----------------------------------------

1. MACS 1.4.2
http://liulab.dfci.harvard.edu/MACS/Download.html

最新版：
https://github.com/macs3-project/MACS/tags

MACS全称Model-based Analysis of ChIP-Seq，最初的设计是用来鉴定转录因子的结合位点，但是它也可以用于其他类型的富集方式测序。


子命令: 
- bdgcmp使用 *_treat_pileup.bdg和 *_control_lambda.bdg计算得分轨(score track)
- bdgpeakcall使用 *_treat_pvalue.bdg 或bdgcmp得到的结果或begGraph文件进行peak calling.bdgbroadcall差不多也是这样子。
- bdgdiff能用来分析4个bedgraph文件,得到treatment1 vs control1, treatment2 vs control2, treatment1 vs control2, treament2 vs control1的得分。
- filterdup：过滤重复，结果是BED文件
- predictd：从比对文件中估计文库大小或d
- randsample： 随机抽样
- pileup：以给延伸大小去堆积(pileup)比对得到的reads。这一步不会有去重和测序深度标准化，你需要预先做这些工作。





2. MACS2
(1)下载和安装
https://pypi.org/project/MACS2/
https://github.com/taoliu/MACS/releases

好像可以用pip安装
$ pip install MACS2 #报错，必须python2.7
## 再试
$ python2 -V ## Python 2.7.5
$ pip2 install --user MACS2 ##Successfully installed MACS2-2.1.2

查看版本号
$ macs2 --version
macs2 2.1.2

2) on Ubuntu, CentOS: (2021.2.22)
$ pip3 install --user MACS2

$ macs2 --version
macs2 2.2.7.1



(2)都是输入bam文件，能输入BED文件吗？可以
The BED format can be found at [UCSC genome browser website](http://genome.ucsc.edu/FAQ/FAQformat#format1).
chr7    127471196  127472363  Pos1  0  +
chr7    127472363  127473530  Pos2  0  -

The essential columns in BED format input are the 1st column "chromosome name", the 2nd "start position", the 3rd "end position", and the 6th, "strand".

1) For BED format, the 6th column of strand information is required by MACS. And please pay attention that the coordinates in BED format is zero-based and half-open (http://genome.ucsc.edu/FAQ/FAQtracks#tracks1).
bed的坐标是从0开始的，前闭后开区间。



(3) 运行命令

macs2 callpeak 是macs2最主要的一个功能，能够利用bam文件寻找chip peak；
macs2 callpeak 使用：
## regular peak calling: $ macs2 callpeak -t ChIP.bam -c Control.bam -f BAM -g hs -n test -B -q 0.01
## broad peak calling: $ macs2 callpeak -t ChIP.bam -c Control.bam --broad -g hs --broad-cutoff 0.1


$ macs2 callpeak -t ChIP.bam -c Control.bam -f BAM -g hs -n test -B -q 0.05

参数解释：
-t/--treatment FILENAME
这是MACS唯一必须的参数，文件可以是 --format 选项指定的任何格式。如果有多个比对文件，可以将它们指定为 -t A B C 。MACS 会将所有这些文件合并在一起。

-c/--control
	control 或 mock(非特异性抗体，如IgG)组
	control: input DNA，没有经过免疫共沉淀处理；
	mock: 1）未使用抗体富集与蛋白结合的DNA片段 2）非特异性抗体，如IgG
-n/--name
	MACS 输出文件名前缀。
-f/--format FORMAT
	声明输入文件的格式，目前 MACS 能够识别的格式有 ELAND、BED、ELANDMULTI、ELANDEXPORT、ELANDMULTIPET（双端测序）、SAM、BAM、BOWTIE、BAMPE、BEDPE。除了BAMPE和BEDPE需要额外声明，其他格式都可以用 AUTO 自动检测。
-g/--gsize
	有效基因组大小(可比对基因组大小)；基因组中有大量重复序列测序测不到，实际上可比对的基因组大小只有原基因组90% 或 70%；人类默认值是– 2.7e9（UCSC human hg18 assembly）
-s/--tsize
	测序读长；如果不设定，MACS 利用输入的 treatment 文件前10个序列自动检测；设定会覆盖自动检测的标签大小。
-q/--qvalue
	q 值（最小的 FDR）的阈值，默认是 0.05 。可以根据结果进行修正，q 值是 p 值经 Benjamini-Hochberg-Yekutieli 修正后的值。
-p/--pvalue
	p 值，如果 -p 设定，MACS2会使用 p 值代替 q 值。
--verbose
	隐藏MACS运行过程信息，设置0；想了解各条染色体peak信息，设置为3或>3的数。
#
https://github.com/taoliu/MACS/











3. 更多参数

(0) MACS 参数解释： 
$ macs2 callpeak
usage: macs2 callpeak [-h] -t TFILE [TFILE ...] [-c [CFILE [CFILE ...]]]
                      [-f {AUTO,BAM,SAM,BED,ELAND,ELANDMULTI,ELANDEXPORT,BOWTIE,BAMPE,BEDPE}]
                      [-g GSIZE] [-s TSIZE] [--keep-dup KEEPDUPLICATES]
                      [--outdir OUTDIR] [-n NAME] [-B] [--verbose VERBOSE]
                      [--trackline] [--SPMR] [--nomodel] [--shift SHIFT]
                      [--extsize EXTSIZE] [--bw BW] [--d-min D_MIN]
                      [-m MFOLD MFOLD] [--fix-bimodal] [-q QVALUE | -p PVALUE]
                      [--scale-to {large,small}] [--down-sample] [--seed SEED]
                      [--tempdir TEMPDIR] [--nolambda] [--slocal SMALLLOCAL]
                      [--llocal LARGELOCAL] [--max-gap MAXGAP]
                      [--min-length MINLEN] [--broad]
                      [--broad-cutoff BROADCUTOFF] [--cutoff-analysis]
                      [--call-summits] [--fe-cutoff FECUTOFF]
                      [--buffer-size BUFFER_SIZE] [--to-large] [--ratio RATIO]
macs2 callpeak: error: the following arguments are required: -t/--treatment

更详细的参数解释
$ macs2 callpeak --help




i)输入文件参数：
-f BAM 输入文件格式，支持bam,sam,bed等
	{AUTO,BAM,SAM,BED,ELAND,ELANDMULTI,ELANDEXPORT,BOWTIE,BAMPE,BEDPE}
	除'BAMPE', 'BEDPE'需要特别声明外，其他格式都可以用 AUTO自动检测。

-g hs 人类基因组
	hs: 2.7e9
	mm: 1.87e9
	ce: 9e7
	dm: 1.2e8
	对于其他物种，则需要自己指定有效基因组的大小，单位为bp。
-t:实验组，IP的数据文件
-c: 对照组

--keep-dup all #要不要去重看你之前的处理，如果前面已经去过可以不用去重。

--nolambda：不考虑peak 候选区域的偏差，使用背景λ作为 localλ。


ii)输出文件参数：
--outdir macs2_result/01/ -n total 文件夹、文件名前缀

-B/--bdg:输出bedgraph格式的文件，输出文件以NAME+'_treat_pileup.bdg' for treatment data, NAME+'_control_lambda.bdg' for local lambda values from control显示。
	以bedGraph格式存放fragment pileup, control lambda, -log10pvalue 和log10qvale.
-B, --bdg   Whether or not to save extended fragment pileup, and local lambda tracks (two files) at every bp into a bedGraph file. DEFAULT: False
	是否输出2个bedGraph文件: 每个bp位置的扩展片段堆叠文件，本地lambda轨道。


--SPMR    If True, MACS will SAVE signal per million reads for fragment pileup profiles. 
	该参数如果为 True，MACS 会为片段重叠谱保存数据 signal per million reads。

	It won't interfere with computing pvalue/qvalue during peak calling, since internally MACS2 keeps using the raw pileup and scaling factors between larger and smaller dataset to calculate statistics measurements. 
	这个不干扰peak calling 过程中的p值/q值的计算，因为内部MACS2使用 raw pileup 和大小数据集之间的缩放因子做统计测试。

	If you plan to use the signal output in bedGraph to call peaks using bdgcmp and bdgpeakcall, you shouldn't use this option because you will end up with different results.
	如果你计划输出这个信号，然后使用 bdgcmp and bdgpeakcall call peak，不要加这个选项，因为你会得到不同的结果。

	However, this option is recommended for displaying normalized pileup tracks across many datasets. 
	然而，在很多数据集中展示标准化后的堆积轨道图时，推荐加该参数。

	Require -B to be set. Default: False
	需要设置-B参数。默认 False。




iii)peak calling 参数
-q/--qvalue 和 -p/--pvalue: q value默认值是0.05，与pvalue不能同时使用。

--nolambda: 不要考虑在峰值候选区域的局部偏差/λ
--nolambda      If True, MACS will use fixed background lambda as local lambda for every peak region. 
	Normally, MACS calculates a dynamic local lambda to reflect the local bias due to the potential chromatin accessibility.
	如果True，对于每个peak区域，macs使用固定的背景 lambda 作为局部lambda。
	通常，macs计算每个动态局部 lambda 来反应可能的染色质可接近性引起的局部偏差。


一般常规是够用的，但是如果你需要看那些更加宽的peak，可以按照官方的建议使用如下参数
--broad  peak有narrow peak和broad peak, 设置时可以call broad peak 的结果文件。
	broad region最大长度是 4d。其中d表示MACS的双峰模型两个peak的距离。结果会得到BED12格式文件，存放着附近高度附近的区域。由于要足够的宽，所以需要专门的参数进行统计学过滤。

--broad-cutoff 和pvalue、以及qvalue相似
	用于过滤 broad得到的peak，默认是q值，如果设置 -p就用p值。

--max-gap MAXGAP  Maximum gap between significant sites to cluster them together. The DEFAULT value is the detected read length/tag size.
	峰最大间隔。默认值时检测到的reads的长度。



iv)Shift 模型参数：
Shifting model arguments:
--nomodel    Whether or not to build the shifting model. 
If True, MACS will not build model. by default it means shifting size = 100, try to set extsize to change it. 
It's highly recommended that while you have many datasets to process and you plan to compare different conditions, aka differential calling, use both 'nomodel' and 'extsize' to make signal files from different datasets comparable. DEFAULT: False



--shift SHIFT   (NOT the legacy --shiftsize option!) The arbitrary shift in bp. 
	Use discretion while setting it other than default value. 
	When NOMODEL is set, MACS will use this value to move cutting ends (5') towards 5'->3' direction then apply EXTSIZE to extend them to fragments. 
	When this value is negative, ends will be moved toward 3'->5' direction. 
	Recommended to keep it as default 0 for ChIP-Seq datasets, or -1 * half of EXTSIZE together with EXTSIZE option for detecting enriched cutting loci such as certain DNAseI-Seq datasets. 
	Note, you can't set values other than 0 if format is BAMPE or BEDPE for paired-end data. DEFAULT: 0.
	shift推荐设置：
		ChIP-Seq数据使用 0 
		或者 -0.5*extsize: detecting enriched cutting loci such as certain DNAseI-Seq datasets
	对于 BAMPE or BEDPE 数据， shift 只能是0.



--extsize EXTSIZE     The arbitrary extension size in bp. 
	When nomodel is true, MACS will use this value as fragment size to extend each read towards 3' end, then pile them up. 
	It's exactly twice the number of obsolete SHIFTSIZE. 
	In previous language, each read is moved 5'->3' direction to middle of fragment by 1/2 d, then extended to both direction with 1/2 d. 
	This is equivalent to say each read is extended towards 5'->3' into a d size fragment. DEFAULT: 200. 
	EXTSIZE and SHIFT can be combined when necessary. Check SHIFT option.

						
--nomodel  这个参数和extsize、shift是配套使用的，有这个参数才可以设置extsize和shift。
--shift  当设置了--nomodel，MACS用这个参数从5' 端移动剪切，然后用--extsize延伸，如果--shift是负值表示从3'端方向移动。
	建议ChIP-seq数据集这个值保持默认值为0，对于检测富集剪切位点如DNAsel数据集设置为EXTSIZE的一半。
--extsize  当设置了nomodel时，MACS会用--extsize这个参数从5'->3'方向扩展reads修复fragments。比如说你的转录因子结合范围200bp，就设置这个参数是200。


-m MFOLD MFOLD, --mfold MFOLD MFOLD
	Select the regions within MFOLD range of high-confidence enrichment ratio against background to build model. 
	选择这些高可信 与背景富集比 区域构建model。

	Fold-enrichment in regions must be lower than upper limit, and higher than the lower limit. Use as "-m 10 30". 
	富集倍数必须比上限低，比下限高。

	This setting is only used while building the shifting model. 
	该设置仅用于构建 shifting model.

	Tweaking it is not recommended. DEFAULT:5 50



v) Post-processing options:
--call-summits  If set, MACS will use a more sophisticated signal processing approach to find subpeak summits in each enriched peak region. DEFAULT: False
	设置这个参数时，MACS 会使用更复杂的 信号处理方法，找子峰。默认不找。


















4. 实战、输出解读
https://www.jianshu.com/p/edfe4ac6b085
macs2 callpeak -t pas.bed -f BED --outdir out_dir2 -n pas_ -g hs -q 0.05

$ macs2 callpeak -t N5_NH_CB_list_filtered_c1.bam --outdir macs2 -n c1_ -g hs --keep-dup all -q 0.05 --nolambda --nomodel --shift 0
$ macs2 callpeak -t N5_NH_CB_list_filtered_c5.bam --outdir macs2 -n c5_ -g hs --keep-dup all -q 0.05 --nolambda --nomodel --shift 0

$ samtools view N5_NH_CB_list_filtered_c5.bam | awk '{print $2}'| sort | uniq -c
3777423 0
4048925 1024
2546823 1040
3038720 16

$ samtools view N5_NH_CB_list_filtered_c5.bam | head

1)后缀为xls的文件是peak的输出结果
# 开头的是注释信息，显示了软件调用的具体命令和参数设置，便于核查；
其他的行记录了peak的区间信息，这里的起始位置采用的是从1开始计数的方式。

$ grep -v '^#' c1__peaks.xls |head
chr     start   end     length  abs_summit  pileup  -log10(pvalue)  fold_enrichment -log10(qvalue)  name
1       14519   14802   284     14671       44      36.5958          11.5829         34.2358        c1__peak_1
1       185318  185528  211     185429      26      16.8188          6.94973         14.6765        c1__peak_2
1       629032  629507  476     629231      12857   41338.7          3309.62         41334          c1__peak_3
1       629870  630087  218     629940      22      13.0267          5.92014         10.9504        c1__peak_4

2) 后缀为narrowpeak的文件是BED6+4格式，内容示意如下。
$ head c1__peaks.narrowPeak 
##我加一行数字
#1      2       3       4               5       6       7        8       9      10
1       14518   14802   c1__peak_1      342     .       11.5829 36.5958 34.2358 152
1       185317  185528  c1__peak_2      146     .       6.94973 16.8188 14.6765 111
1       629031  629507  c1__peak_3      413340  .       3309.62 41338.7 41334   199
1       629869  630087  c1__peak_4      109     .       5.92014 13.0267 10.9504 70

前四列代表peak区间和名称(chr, start, end, peakName)，注意bed格式中起始位置从0开始计数，

第五列代表score,在macs2的输出结果中为 int(-10*log10qvalue) ==> 大概是 10*第9列

第六列strand 用+/- 表示链或者方向。如果是“.”则代表没有指定方向。macs2的输出结果中为.
第七列为 signalvalue， 通常使用fold_enrichment的值，
第八列为-log10(pvalue),
第九列为-log10(qvalue),
第十列为peak的中心，relative summit position to peak start
	即summit距离peak起始位置的距离，对应abs_summit - start。


3)后缀为bed的文件为peak中心，即summit对应的bed文件，内容示意如下
$ head c1__summits.bed 
1       14670   14671   c1__peak_1      34.2358
1       185428  185429  c1__peak_2      14.6765
1       629230  629231  c1__peak_3      41334
1       629939  629940  c1__peak_4      10.9504

最后一列为-log10(qvalue)。









========================================
|-- MACS2 输出文件解读
----------------------------------------
ATAC-seq关心的是在哪切断，断点才是peak的中心，所以使用shift模型，--shift -75或-100
对人细胞系ATAC-seq 数据call peak的参数设置如下：
$ macs2 callpeak -t H1hesc.final.bam -n sample --shift -100 --extsize 200 --nomodel -B --SPMR -g hs --outdir Macs2_out 2> sample.macs2.log


MACS2输出文件解读
$ ls -lh macs2
total 280K
-rw-rw-r-- 1 wangjl wangjl  17K Jun 19 10:36 treat_vs_control-shift100-m_peaks.xls
-rw-rw-r-- 1 wangjl wangjl  14K Jun 19 10:36 treat_vs_control-shift100-m_peaks.narrowPeak
-rw-rw-r-- 1 wangjl wangjl  11K Jun 19 10:36 treat_vs_control-shift100-m_summits.bed
-rw-rw-r-- 1 wangjl wangjl 3.7K Jun 19 10:36 shift100-m.macs2.log
-rw-rw-r-- 1 wangjl wangjl  82K Jun 19 10:36 treat_vs_control-shift100-m_model.r
-rw-rw-r-- 1 wangjl wangjl  21K Jun 19 10:39 treat_vs_control-shift100-m_model.pdf




1.NAME_peaks.xls
包含peak信息的tab分割的文件，前几行会显示callpeak时的命令。

$ grep "^c" macs2/treat_vs_control-shift100-m_peaks.xls|head
自己加的行2        3        4         5       6         7             8                9               10
chr   start      end      length  abs_summit  pileup  -log10(pvalue)  fold_enrichment  -log10(qvalue)  name
chr1  1013306   1013391   86      1013366     3        11.0199        3.98448          4.04898 macs2/treat_vs_control-shift100-m_peak_1
chr1  33037014  33037099  86      33037034    2       8.0082          2.98836          2.16415 macs2/treat_vs_control-shift100-m_peak_2
chr1  37692348  37692433  86      37692351    2       7.03643         2.97555          2.16415 macs2/treat_vs_control-shift100-m_peak_3
chr1  51236156  51236241  86      51236156    2       8.0082          2.98836          2.16415 macs2/treat_vs_control-shift100-m_peak_4
chr1  75786189  75786274  86      75786225    3       11.0199         3.98448          4.04898 macs2/treat_vs_control-shift100-m_peak_5

输出矩阵的列：
1 染色体号
2 peak起始位点
3 peak结束位点
4 peak区域长度
5 peak的峰值位点（summit position）
6 peak 峰值的堆积高度（pileup height at peak summit, -log10(pvalue) for the peak summit）
7 -log10(pvalue)
8 peak的富集倍数（相对于random Poisson distribution with local lambda）
	fold enrichment for this peak summit against random Poisson distribution with local lambda
9 -log10(qvalue)
10 峰编号

Coordinates in XLS is 1-based which is different with BED(0-based) format
XLS里的坐标和bed格式的坐标还不一样，起始坐标需要减1才与narrowPeak的起始坐标一样。

# 注意：XLS文件里面的起始位点和GFF文件相似从1起始，bed文件从0开始




2.NAME_peaks.narrowPeak
*narrowPeak文件是BED6+4格式，可以上传到UCSC genome browser 浏览。输出文件每列信息分别包含：

$ cat macs2/treat_vs_control-shift100-m_peaks.narrowPeak|head
#自己加的一行2          3           4                                             5       6       7       8       9       10
chr1    1013305      1013391      macs2/treat_vs_control-shift100-m_peak_1        40      .       3.98448 11.0199 4.04898 60
chr1    33037013     33037099     macs2/treat_vs_control-shift100-m_peak_2        21      .       2.98836 8.0082  2.16415 20
chr1    37692347     37692433     macs2/treat_vs_control-shift100-m_peak_3        21      .       2.97555 7.03643 2.16415 3
chr1    51236155     51236241     macs2/treat_vs_control-shift100-m_peak_4        21      .       2.98836 8.0082  2.16415 0
chr1    75786188     75786274     macs2/treat_vs_control-shift100-m_peak_5        40      .       3.98448 11.0199 4.04898 36

1；染色体号
2：peak起始位点
3：结束位点
4：peak name
5：int(-10*log10qvalue)  ==> 大概是10*第9列。
6 ：正负链，一般全是点号。
7：fold change
8：-log10pvalue
9：-log10qvalue
10：峰位与peak起点的距离：relative summit position to peak start 
	==> 可以根据 第2列 + 第10列 算出来峰尖(summit)坐标。
	就是上文xml中的 abs_summit - start







3. NAME_summits.bed (找motif用这个文件)

BED格式的文件，包含peak的summits位置，第5列是-log10qvalue。如果想找motif，推荐使用此文件。
Remove the beginning track line if you want to analyze it by other tools.???

$ cat macs2/treat_vs_control-shift100-m_summits.bed|head
#我加的  2               3              4                                               5
chr1    1013365         1013366         macs2/treat_vs_control-shift100-m_peak_1        4.04898
chr1    33037033        33037034        macs2/treat_vs_control-shift100-m_peak_2        2.16415
chr1    37692350        37692351        macs2/treat_vs_control-shift100-m_peak_3        2.16415
chr1    51236155        51236156        macs2/treat_vs_control-shift100-m_peak_4        2.16415

1 chr 
2 summits的1位
3 summits的2位
4 name 
5 -log10qvalue  <==





4. .bdg
bedGraph格式，可以导入UCSC或者转换为bigwig格式。

两种bfg文件：
- treat_pileup: 实验组bedGraph 文件
- and control_lambda: 对照组bedGraph 文件




5. NAME_peaks.broadPeak
BED6+3格式与narrowPeak类似，只是没有第10列。


6. NAME_peaks.gappedPeak
BED12+3格式，存放broad region 和 narrow peaks，可以使用UCSC genome browser查看。


7. NAME_model.r
R程序，运行后在当前文件夹下，生成基于输入数据产生的模型图片
$ Rscript NAME_model.r






========================================
|-- macs2 bdgdiff: 不同条件下的差异peak分析
----------------------------------------
通过bdgdiff子命令来进行差异peak分析， 该命令不需要基于已有的peak calling结果，只需要输入每个样本对应的bedGraph格式的文件。

需要注意的是，该命令只针对两个样本间的差异peak进行设计，适用于没有生物学重复的情况。

https://github.com/macs3-project/MACS/wiki/Call-differential-binding-events


1. 第一步 Generate pileup tracks using callpeak module

$ macs2 callpeak -B -t cond1_ChIP.bam -c cond1_Control.bam -n cond1 --nomodel --extsize 120
$ macs2 callpeak -B -t cond2_ChIP.bam -c cond2_Control.bam -n cond2 --nomodel --extsize 120

在运行这一步的时候，会输出每个样本过滤之后的reads数目，示意如下

$ egrep "tags after filtering in treatment|tags after filtering in control" cond1_peaks.xls
 # tags after filtering in treatment: 19291269
 # tags after filtering in control: 12914669

$ egrep "tags after filtering in treatment|tags after filtering in control" cond2_peaks.xls
 # tags after filtering in treatment: 19962431
 # tags after filtering in control: 14444786




2. 第二步

$ macs2 bdgdiff \
	--t1 cond1_treat_pileup.bdg --c1 cond1_control_lambda.bdg \
	--t2 cond2_treat_pileup.bdg --c2 cond2_control_lambda.bdg \
	--d1 12914669 --d2 14444786 -g 60 -l 120 --o-prefix diff_c1_vs_c2

其中-d1和-d2的值就是第二步运行时输出的reads数目，-o参数指定输出文件的前缀。运行成功后，会产生3个文件
diff_c1_vs_c2_c3.0_cond1.bed
diff_c1_vs_c2_c3.0_cond2.bed
diff_c1_vs_c2_c3.0_common.bed






========================================
----------------------------------------





========================================
----------------------------------------








========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------








========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------


