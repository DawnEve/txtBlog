shell-in-seq


好的脚本事半功倍，并充满了简洁美。
好的脚本值得收藏，并反复研究。


另一个笔记: http://www.biomooc.com/linux/linux-freq.html

能解释 shell 语句的网站: https://explainshell.com/



========================================
服务器之间传输文件 / 远程拷贝时保留软链接: 推荐使用 rsync -avl (GPT未验证)
----------------------------------------
默认scp复制软连接指向的文件，而不是软连接本身。
如果你只想复制符号链接，而不复制它指向的文件，可以先打包再传输，或者使用 rsync 更加灵活。


文件迁移前，先看原位置内是否有软连接：find /path/to/ -type l
	$ find /data3/wangjl/scPolyA-seq2/chenxi/MDA -type l
	/data3/wangjl/scPolyA-seq2/chenxi/MDA/raw/SUSTC-20220617-L-01-2022-06-211426



1. 使用 scp 复制符号链接本身
(1)压缩包含符号链接的目录
$ tar -czf archive.tar.gz /path/to/directory
(2) 使用 scp 传输压缩文件
$ scp archive.tar.gz user@remote:/path/to/destination
(3) 第三步：在目标服务器上解压文件
$ tar -xzf archive.tar.gz -C /path/to/extract
这样会保留符号链接本身，而不会复制其指向的文件。




2. 使用 rsync 复制符号链接
rsync 是一种比 scp 更加灵活的工具，支持直接复制符号链接本身。

##rsync -avz -e ssh -l user@remote:/path/to/source /path/to/destination

$ rsync -avl src_dir user@host:dest_dir
参数解释：
	-v, --verbose               increase verbosity
	-a, --archive               archive mode; equals -rlptgoD (no -H,-A,-X)
	-e, --rsh=COMMAND           specify the remote shell to use
	-z, --compress              compress file data during the transfer

	-l, --links 	#When symlinks are encountered, recreate the symlink on the destination.
		-l 保留软连接本身
	-L, --copy-links 	#When symlinks are encountered, the item that they point to (the referent) is copied, rather than the symlink.
		-L 则复制文件，目标位置是文件副本，而不是软连接本身。
	--safe-links: 忽略指向源路径目录树以外的链接（这是默认行为）。

(2) rsync -avl 和 rsync -avzP 的区别？
首先，都保留软连接。
-z (compress)：在传输过程中压缩文件，以减少带宽使用，适合网络传输。
-P (partial & progress)：显示进度，并在传输中断时保留部分传输的文件，便于下次继续传输。
	代价是，速度减慢了。

总结
* 使用 rsync -avl 适合本地或快速网络传输，注重文件属性的完整保留。
* 使用 rsync -avzP 适合远程传输或带宽有限的情况，注重减少传输时间和显示进度信息。






========================================
使用find和grep做关键词文件文本搜索，高亮显示
----------------------------------------
1.关键词搜索
$ find . -name "*sh"|xargs grep "hg19_mm10" --color=auto

$ ls *txt | xargs grep "hg19_mm10" --color=auto



2.使用正则表达式搜索
$ find . -name "*sh"|xargs grep "drop.*seq" --color=auto
##./procedure/1208/mef_filterBAM_20180322.sh:# Using drop-seq tools to split bam -> hg19, mm10, and calculate species 
##./procedure/1208/mef_filterBAM_20180322.sh:	java -jar /home/hou/data/apa/p2/procedure/1208/Drop-seq_tools-1.13/jar/dropseq.jar FilterBAM INPUT=$files OUTPUT=hg19/${files%%.*}_$TargetFile.bam REF_SOFT_MATCHED_RETAINED=HUMAN





========================================
查看fastq/sam/bam 文件的常用语句
----------------------------------------

1.查看某一个reads，并显示匹配行后的三行
示例：$ zcat pbmc8k_S1_L007_R1_001.fastq.gz | grep -A3 ST-K00126:314:HFYL2BBXX:7:2103:14996:4725
解释：
 - gzip, gunzip, zcat - compress or expand files
 - grep, egrep, fgrep - print lines matching a pattern
	-A NUM, --after-context=NUM
              Print NUM lines of trailing context after matching lines.  Places a line containing a group separator (described under --group-separator) between contiguous groups of matches.  With the -o or --only-matching option,  this
              has no effect and a warning is given.

实例：
$ zcat ../B116/B116_S1_L005_R1_001.fastq.gz | grep -A3 E00500:97:H7NHJCCXY:5:1209:21978:71946



2.预览bam文件
$ samtools view possorted_genome_bam.bam|less

id  NH
E00500:97:H7NHJCCXY:5:1209:21978:71946 3
E00500:97:H7NHJCCXY:5:1212:18213:37928 4
E00500:97:H7NHJCCXY:5:2116:9516:13334 7
E00500:97:H7NHJCCXY:5:1113:27285:35713 8


3.查找bam文件中匹配的行：
$ samtools view possorted_genome_bam.bam|grep E00500:97:H7NHJCCXY:5:1209:21978:71946



4.查找bam文件的第五列的值的分布情况：
$ samtools view possorted_genome_bam.bam| head -n 10000000|cut -f5|sort|uniq -c
1097428 0
  80075 1
8496404 255
 326093 3







========================================
单行 if 语句怎么加分号?(判断sam文件是否只有0行)
----------------------------------------
# if 语句
if [ `samtools view 01_fq_cb/small_bam/GCATATGG.bam| wc -l` == '0' ] 
then
    echo "eq";
fi


# 单行if
if [ `samtools view 01_fq_cb/small_bam/GCATATGG.bam| wc -l` == '0' ]; then    echo "eq"; fi







========================================
求匹配某关键词的文件大小总和
----------------------------------------

可以使用管道
ls -l | grep 20130606 | awk '{c += $5}END{print c/1024/1024 "MB"}'

如果文件数量不那么多可以使用
du -m *20130606* | awk '{c+=$1}END{print c}'

这两个命令显示的单位是MB，如果要显示GB可以print c 的c再除以一个1024


========================================
cat合并不同lane的同一个样品数据为单个fastq文件
----------------------------------------

cat S294_05B_CHG011307-Mix2-40sc-9_L006_R2.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L007_R2.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L008_R2.fastq.gz >../column9_R2.fastq.gz

cat S294_05B_CHG011307-Mix2-40sc-9_L006_R1.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L007_R1.fastq.gz S294_05B_CHG011307-Mix2-40sc-9_L008_R1.fastq.gz >../column9_R1.fastq.gz


========================================
每隔4行显示一行(查看fastq文件的序列): sed /awk /perl 等实现
----------------------------------------
目的：显示fastq文件的序列，也就是第二行。此后，每隔四行显示一行。


1. sed 命令

$ seq 100|sed '2~4!d'  ## d是删除，!d是不删除。
$ seq 100|sed -n '2~4p'


实例
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '{n;p;n;n}'
或者
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '2~4p'

如果要打印行号：
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '2~4{=;p}'


# 解释
first~step: Match every step'th line starting with line first.  

For example, ``sed -n 1~2p'' will print all the odd-numbered lines in the input stream, and the address 2~5 will match every fifth line, starting with the second.  first can be zero; in this case, sed operates as if it were equal to step.  (This  is an extension.)




2. awk 命令 

$ seq 100 | awk '(NR%4==2)'
$ seq 100 | awk 'NR%4==2'


$ zcat /path/to/CRCSZ05N_S1_L001_R1_001.fastq.gz | head -n 100 | awk 'NR%4==2' 
解释: 
在awk中行号为NR，是从1开始计数的。NR%4余数为2就是第二行，默认动作是打印出来。


如果要打印行号 
$ zcat /data4/public_data/10x_colon_RNA/CRCSZ04_mergeAllData/mergedRawData2/CRCSZ05N_S1_L001_R1_001.fastq.gz | head -n 100 | awk 'NR%4==2{print NR"\t"$0}' 
2	ANTCTCCGTTCTTCATTTTGTCATTGGATTAAATATAATATTTTTTTTTTTTTTTTTTATTAATATTTTTTAAAAGAATTATTTTTATATTTAAAATGCCTATTAATTTTATTTAACAAAAATAAAATATTTTTTTTTTTATATCTTTAA
6	TNTCGCCGTAGGCAGTATATGTAATTCTTTATAGATATTATTTTTTTTTTTTTTTTTTTTTTTTTACTTGATTTCTTTGATTATCATGGCCGAGGCACAATGCACCACCACAAACTCTGCCTTTCTGTTGTTAATTGTCAGATTCACTGT







========================================
shell循环高级练习
----------------------------------------
1. 批量解压缩
ls *.gz|while read id ; do gunzip $id;done




2. 批量对bam sort and index
$ cat cids2.txt | while read id;do 
	echo $id; 
	samtools index ${id}.rmdup.bam
done;



# sort
$ echo syncHeLa normalHeLa B0 B1 | xargs -n 1|while read id; do 
	echo $id;
	samtools sort -o ${id}.sort.bam ${id}.bam;
done;

# index
$ echo syncHeLa normalHeLa B0 B1 | xargs -n 1|while read id; do 
	echo $id;
	samtools index ${id}.sort.bam;
done;





3. $(basename)函数：去掉文件路径，只要文件名
$ ls ../R2bam/*bam | head
../R2bam/AAACCCAGTCCAATCA-1.bam
../R2bam/AAACCCAGTCTGTCCT-1.bam
../R2bam/AAACGCTCATGGACAG-1.bam

$ ls ../R2bam/*bam | while read id; do 
	bc=$(basename $id ".bam"); 
	echo $bc; 
done





4. 从配置文件获取文件id，文件路径，拼凑shell命令
$ cat config
a1 a1_R1.fq a1_R2.fq
a2 a2_R1.fq a2_R2.fq
b3 b3_R1.fq b3_R2.fq

## 使用shell拼凑命令
$ cat config | while read id; do arr=($id); R1=${arr[1]}; R2=${arr[2]}; echo "~bowtie2_fake" ${arr[0]} $R1 $R2; done;
~bowtie2_fake a1 a1_R1.fq a1_R2.fq
~bowtie2_fake a2 a2_R1.fq a2_R2.fq
~bowtie2_fake b3 b3_R1.fq b3_R2.fq




refer: https://www.jianshu.com/p/f9da70fcaf8d






========================================
新建文件夹，如果有就删除再建
----------------------------------------

TargetFile=mm10
if [ ! -d $TargetFile ]; then ##如果有同名文件怎么办？ todo
    mkdir $TargetFile
else
      rm -rf $TargetFile
      mkdir $TargetFile
fi








========================================
下载大量测序结果，SRA 公共数据
----------------------------------------

1. 下载 SRA 数据
$ cat SRR_Acc_List.txt
SRR3290193
SRR3290196
SRR3290199


$ cat SRR_Acc_List.txt| while read id;
do echo $id;
fasterq-dump --split-files -e 10 $id;
done





2.可以在ftp的README中找到具体下载地址，写入文本文件，使用awk批处理下载这些数据。
上面chrX_data.tar.gz是这些文件的chrX上reads的子集。

$ cat list.txt
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188245/ERR188245_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188428/ERR188428_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188337/ERR188337_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188401/ERR188401_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188257/ERR188257_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188383/ERR188383_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR204/ERR204916/ERR204916_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188234/ERR188234_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188273/ERR188273_1.fastq.gz
ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR188/ERR188454/ERR188454_1.fastq.gz
...


$ awk '{print "wget -b "$1}' list.txt | bash
估计要下载几个小时。









========================================
|-- 压缩文件，并检查再加压后md5sum值是否和之前相同(snakemake 流程)
----------------------------------------
目的： 安全删除fastq文件，只保留压缩后的fq.gz文件前，检查是否可以解压恢复源文件。

主版本3大步骤: main.sf
	压缩前的 md5sum m1
	压缩后的 md5sum m2
	压缩后再解压的 md5sum m3
	比较m3是否等于m1的值
就按照单文件写，如果是双端，可以当不同的文件名。

精简版本：simple.sf 
	仅仅计算文件的md5sum值
	多线程方式。



1. 批量压缩文件

(1) 准备几个测试文件
$ ls -lth raw
total 16K
-rw-rw-r-- 1 wangjl wangjl 146 Aug 26 15:19 b2.fastq
-rw-rw-r-- 1 wangjl wangjl  88 Aug 26 15:18 b1.fastq
-rw-rw-r-- 1 wangjl wangjl 367 Aug 26 15:18 a2.fastq
-rw-rw-r-- 1 wangjl wangjl 367 Aug 26 15:18 a1.fastq

(2) 压缩前的 md5sum 

$ md5sum raw/a1.fastq
b3f8495caec755f7f5369d59babf5f49  raw/a1.fastq

$ ls raw/* | head -n 1| xargs md5sum
b3f8495caec755f7f5369d59babf5f49  raw/a1.fastq

以上两者等价。
而后者又可以轻易扩展成多个，多开几个终端就是手动多进程了。

$ ls raw/* | head -n 2| xargs md5sum
b3f8495caec755f7f5369d59babf5f49  raw/a1.fastq
03425794de6268e314ed4fc70659de37  raw/a2.fastq


工作目录
$ config.yaml
samples:
 - a1.fastq
 - a2.fastq
 - b1.fastq
 - b2.fastq


脚本目录:
$ cat main.sf
configfile: "config.yaml"

SI=config["samples"]

# step1: get origin md5sum
rule all:
	input:
		"md5/origin.md5",
		#expand("gz/{sample}.gz", sample=SI)

rule getMd5_pre:
	input: "raw/{sample}"
	output: temp( "temp/{sample}.md5")
	shell: "md5sum {input} > {output}"
rule getMd5:
	input: expand("temp/{sample}.md5", sample=SI)
	output: "md5/origin.md5"
	shell: "cat {input} > {output}"

工作目录中运行:
$ snakemake -s /home/wangjl/data/soft/snakemakeWorkflow/shell/demo1/main.sf -j 4 -p

输出: 
$ cat md5/origin.md5 
b3f8495caec755f7f5369d59babf5f49  raw/a1.fastq
03425794de6268e314ed4fc70659de37  raw/a2.fastq
4db446117a68a13458f06854257c4252  raw/b1.fastq
02ed6baad2521532dea5c7fe192fca06  raw/b2.fastq




(3) 压缩文件

$ gzip tmp.txt  # 产生 tmp.txt.gz 但是删除源文件

$ gzip -c tmp.txt >tmp.txt.gz # 保留源文件


rule gz:
	input: "raw/{sample}"
	output: "gz/{sample}.gz"
	shell: "gzip -c {input} > {output}"
# 新增目标 expand("gz/{sample}.gz", sample=SI),

$ ls -lth gz/
total 16K
-rw-rw-r-- 1 wangjl wangjl 114 Aug 26 15:48 b2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 195 Aug 26 15:48 a2.fastq.gz
-rw-rw-r-- 1 wangjl wangjl 182 Aug 26 15:48 a1.fastq.gz
-rw-rw-r-- 1 wangjl wangjl  88 Aug 26 15:48 b1.fastq.gz


(3) 计算压缩后的md5sum

$ ls *.fastq.gz | xargs md5sum >result.md5


rule md5_gz_pre:
	input: "gz/{sample}.gz"
	output: temp( "temp/{sample}.gz.md5")
	shell: "md5sum {input} > {output}"

rule md5_gz:
	input: expand("temp/{sample}.gz.md5", sample=SI)
	output: "md5/gz.md5"
	shell: "cat {input} > {output}"

# 新增目标 "md5/gz.md5"

$ cat md5/gz.md5 
fca41c04c97f0ca9586962375b6960d7  gz/a1.fastq.gz
86c251b27b0040470d91865b26e980ae  gz/a2.fastq.gz
589741ef69d496d3a5899eef0a9bc443  gz/b1.fastq.gz
548f97cf0dae2c508f35855a6fd1e2c6  gz/b2.fastq.gz



(4) 再解压后的md5值

$ gunzip -c tmp.txt.gz | md5sum
85b93a57a9eb790ebce2b2ba9cf687c2  -

$ md5sum tmp.txt
85b93a57a9eb790ebce2b2ba9cf687c2  tmp.txt

确实一样，但是怎么把 文件名放进去呢？
$ echo `gunzip -c tmp.txt.gz | md5sum | awk '{print $1}'` "tmp.txt.gz"
85b93a57a9eb790ebce2b2ba9cf687c2 tmp.txt.gz


rule unzip_md5_pre:
	input: "gz/{sample}.gz"
	output: temp( "temp/{sample}.unzip.md5")
	shell: "echo `gunzip -c {input} | md5sum | awk '{{print $1}}'`\"  raw/\"{wildcards.sample} > {output}"

rule unzip_md5:
	input: expand("temp/{sample}.unzip.md5", sample=SI)
	output: "md5/unzip.md5"
	shell: "cat {input} > {output}"
# 新增目标: "md5/unzip.md5"

$ cat md5/unzip.md5 
b3f8495caec755f7f5369d59babf5f49	raw/a1.fastq
03425794de6268e314ed4fc70659de37	raw/a2.fastq
4db446117a68a13458f06854257c4252	raw/b1.fastq
02ed6baad2521532dea5c7fe192fca06	raw/b2.fastq




(5) 检查解压后的md5值是否和压缩前一致

就是比较2个文件
$ diff md5/origin.md5 md5/unzip.md5

1)使用脚本逐行判断
file1="md5/origin.md5"
file2="md5/unzip.md5"

line=`cat $file1|wc -l`

for ((i=1;i<=$line;i++));do
	line1=`awk 'NR=="'$i'"{print $1}' $file1`
	line2=`awk 'NR=="'$i'"{print $1}' $file2`
	fName=`awk 'NR=="'$i'"{print $2}' $file1`
if [ $line1 == $line2 ];then
	echo -e "${i}\t${fName}\tOK"
else
	echo -e "${i}\t${fName}\tdifferent"
fi
done


2) 使用md5判断

m1=`md5sum md5/unzip.md5 | cut -d ' ' -f 1`;
m2=`md5sum md5/unzip.md5 | cut -d ' ' -f 1`;
if [ $m1 == $m2 ];then echo 'same'; else echo 'diff'; fi


rule confirm:
	input: "md5/origin.md5", "md5/unzip.md5"
	output: "md5/confirm.txt"
	shell: "m1=`md5sum {input[0]} | cut -d ' ' -f 1`; \
		m2=`md5sum {input[1]} | cut -d ' ' -f 1`; \
		if [ $m1 == $m2 ];then echo 'same'; else echo 'diff'; fi > {output} && cat {output}"
# 新增目标： "md5/confirm.txt"


$ snakemake -s /home/wangjl/data/soft/snakemakeWorkflow/shell/demo1/main.sf -j 4 -p





2. 精简版本：simple.sf 
   仅仅计算文件解压后的md5sum值，可选计算解压后的md5值。
   多线程方式。

$ vim simple.sf
configfile: "config.yaml"

SI=config["samples"]

rule all:
        input:
                "md5/origin.md5",
                #"md5/gz.md5",
                "md5/unzip.md5",

include: "rules/01_origin_md5.sf"
#include: "rules/03_md5_of_gz.sf"
include: "rules/04_md5_of_unzip.sf"

$ snakemake -s /home/wangjl/data/soft/snakemakeWorkflow/shell/demo1/simple.sf -j 4 -p
输出原始文件的md5，和压缩文件解压后的md5。
然后手动比较第一列的md5是否一致
$ awk '{print $1}' md5/origin.md5 | md5sum
$ awk '{print $1}' md5/unzip.md5 | md5sum




github: https://github.com/DawnEve/snakemakeWorkflow/tree/master/shell/demo1








========================================
******** 脚本欣赏 ********
----------------------------------------


========================================
脚本欣赏1：复制部分文件到指定文件夹(for循环变量从文件读取、字符串的截取)
----------------------------------------
#!/bin/bash
# Using drop-seq tools to split bam -> hg19, mm10, and calculate species 
# human 108052
# version 0.1

bamFiles=`cat mouse_cells.txt` #先引用本地文件
# echo $bamFiles

cd /home/hou/data/apa/p2/procedure/1208/mef/demultiplex/R2/trim_galore/hg19_mm10/mm10 #接下来都在这个目录下

# mm10_clean

# TargetFile=mm10
# if [ ! -d $TargetFile ]; then
#     mkdir $TargetFile
# else
#       rm -rf $TargetFile
#       mkdir $TargetFile
# fi

#循环内复制文件，从当前mm10目录下把部分bam文件复制到mm10_clean下。
for files in $bamFiles
do
	cp $files*.bam mm10_clean/${files%%_*}.bam
	# echo $files*.bam 
	echo "Finish " $files #复制一个提醒一次
done



========================================
脚本欣赏2：移动文件(shell数组、循环)
----------------------------------------
#!/bin/bash


file_list=( "c12_B4" "c12_C3" "c12_C4" "c12_D3" "c12_E2" "c12_E3" "c13_A5" "c13_C1" "c13_C3" "c13_C4" "c13_D5" "c13_E4" "c13_F1" "c13_G1" "c13_H4" "c13_H5" "c14_C1" "c14_C3" "c14_G2" "c15_A2" "c15_C1" "c15_C5" "c16_A2" "c16_A4" "c16_C3" "c16_D4" "c19_A1" "c19_C1" "c19_G3"  )

for f in ${file_list[@]}
do 
    mv $f*.gz needstar
done



========================================
使用管道输入STAR文件名
----------------------------------------
./procedure/1208/reame.sh:

ls *.gz |while read id;do STAR --runThreadN 12 --outSAMtype BAM SortedByCoordinate --genomeDir /home/hou/data/RNA/refs/hg19_mm10_transgenes_reference/starIndex --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  hg19_mm10/${id%%.*}_; done




========================================
使用star比对，并统计结果到文件
----------------------------------------

基本句型：
1. ls xx | while read id; do xxCMD; done
比如：$ ls *csv|while read id;do  ls -lt $id; done





#!/bin/bash
##########################
## c3
cd /home/hou/data/apa/p2/CHG011307/combine/c3/trimed && mkdir -p star/hg19 star/mm10

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/hg19_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/hg19/${id%%.*}_ ; done

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/mm10_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/mm10/${id%%.*}_ ; done

ls star/mm10/*_Log.final.out | while read id; do  echo $id >> star/mm10/mm10_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/mm10/mm10_mapping_ratio ; done

ls star/hg19/*_Log.final.out | while read id; do  echo $id >> star/hg19/hg19_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/hg19/hg19_mapping_ratio ; done

##########################
## c6
cd /home/hou/data/apa/p2/CHG011307/combine/c6/trimed && mkdir -p star/hg19 star/mm10

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/hg19_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/hg19/${id%%.*}_ ; done

##run together
ls *.gz |while read id;do STAR --runThreadN 24 --genomeDir /home/hou/data/RNA/refs/mm10_ERCC92/index/star --readFilesIn $id --readFilesCommand zcat --outFileNamePrefix  star/mm10/${id%%.*}_ ; done

ls star/mm10/*_Log.final.out | while read id; do  echo $id >> star/mm10/mm10_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/mm10/mm10_mapping_ratio ; done

ls star/hg19/*_Log.final.out | while read id; do  echo $id >> star/hg19/hg19_mapping_ratio; grep Uniquely $id | awk -F '|' '{print $2}' | awk '{print $1}' >> star/hg19/hg19_mapping_ratio ; done






========================================
多行变一行，不留空格
----------------------------------------

造一个测试文件
$ cat test.txt 
@This is a book.
AATAAA
+
Last Line.


1. 采用awk
# 换行符变成空格
$ awk BEGIN{RS=EOF}'{gsub(/\n/," ");print}' test.txt
@This is a book. AATAAA + Last Line.

# 换行符变成空字符呢？
$ awk BEGIN{RS=EOF}'{gsub(/\n/,"");print}' test.txt
@This is a book.AATAAA+Last Line.

说明：awk默认将记录分隔符（record separator即RS）设置为\n，此行代码将RS设置为EOF（文件结束），也就是把文件视为一个记录，然后通过gsub函数将\n替换成空格，最后输出。



更简单的形式，
$ awk '{printf $0}' test.txt  #但是行结尾没有换行符，导致和下一行命令提示在一行了。
#加上行结尾换行
$ awk '{printf $0}END{print}' test.txt
@This is a book.AATAAA+Last Line.Last Line.




2. 采用sed

$ sed ':a ; N;s/\n// ; t a ; ' test.txt
@This is a book.AATAAA+Last Line.

$ sed ':a;N;s/\n//;ta;' test.txt #不留空格效果一样
@This is a book.AATAAA+Last Line.

说明：sed默认只按行处理，N可以让其读入下一行，再对\n进行替换，这样就可以将两行并做一行。
但是怎么将所有行并作一行呢？可以采用sed的跳转功能。
:a 在代码开始处设置一个标记a，在代码执行到结尾处时利用跳转命令t a重新跳转到标号a处，重新执行代码，这样就可以递归的将所有行合并成一行。

$ sed ':b;N;s/\n//;t b;' test.txt
@This is a book.AATAAA+Last Line.


3. cat test.txt | xargs
@This is a book. AATAAA + Last Line. #不好，换行符变成了空格
说明：这可能是最简单的一种方式。不符合预期效果



refer: https://blog.csdn.net/hjxhjh/article/details/17264739


========================================
批量 重命名 (改名字)
----------------------------------------
1. 后缀名 由 .gz 改为 fastq.gz

for file in ` ls | grep .gz`
do
  newfile=`echo $file | sed 's/gz$/fastq.gz/' `
  echo $file $newfile;
  mv $file $newfile
done




ref:
https://blog.csdn.net/xuleisdjn/article/details/79024586



========================================
临时文件夹 /tmp/ 的使用，及生成随机文件名
----------------------------------------

1. 遇到一个报错
(1) 错误: samtools sort 的临时文件夹 -T 不能创建

Error in rule sort_to_bam:
    jobid: 46      
    output: map/3.sort.bam 
    log: map/3.sort.bam.log (check log file(s) for error message)
    shell:                 
        samtools sort -@ 2 -O bam -T /tmp/map/3_tmp -o map/3.sort.bam map/3_bwa.sam >map/3.sort.bam.log 2>&1
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)
$ less map/3.sort.bam.log 说打不开临时文件 /tmp/map/3_tmp_000.bam 类似的，找不到了。


难道是两层文件夹不能用？
$ samtools sort -@ 2 -O bam -T /tmp/map/3_tmp -o tmp_3.sort.bam map/3_bwa.sam
[E::hts_open_format] Failed to open file /tmp/map/3_tmp.0000.bam
samtools sort: failed to create temporary file "/tmp/map/3_tmp.0000.bam": No such file or directory
[E::hts_open_format] Failed to open file /tmp/map/3_tmp.0001.bam
samtools sort: failed to create temporary file "/tmp/map/3_tmp.0001.bam": No such file or directory



(2)试试一层文件夹
$ samtools sort -@ 2 -O bam -T /tmp/map_3_tmp -o tmp_3.sort.bam map/3_bwa.sam

$ ls -lth /tmp/ | head
total 2.0G
-rw-rw-r-- 1 wangjl  wangjl  195M Aug 27 10:25 map_3_tmp.0009.bam
-rw-rw-r-- 1 wangjl  wangjl  195M Aug 27 10:25 map_3_tmp.0008.bam
-rw-rw-r-- 1 wangjl  wangjl  196M Aug 27 10:23 map_3_tmp.0006.bam
-rw-rw-r-- 1 wangjl  wangjl  195M Aug 27 10:23 map_3_tmp.0007.bam
-rw-rw-r-- 1 wangjl  wangjl  197M Aug 27 10:20 map_3_tmp.0004.bam
-rw-rw-r-- 1 wangjl  wangjl  196M Aug 27 10:20 map_3_tmp.0005.bam
-rw-rw-r-- 1 wangjl  wangjl  196M Aug 27 10:18 map_3_tmp.0002.bam
-rw-rw-r-- 1 wangjl  wangjl  197M Aug 27 10:18 map_3_tmp.0003.bam
-rw-rw-r-- 1 wangjl  wangjl  197M Aug 27 10:15 map_3_tmp.0001.bam

没问题了，说明就是文件夹 /tmp/map/ 不存在。
该测试表明 sakemake 只能在工作目录创建目录，之外的地方需要自己搞定。




(3) 怎么在 /tmp/ 下创建文件夹呢？
直接创建文件肯定不行，首先太乱，其次可能会覆盖其他进程的临时文件。

看看其他都是怎么使用的？
$ ls -lth /tmp/| grep -v "bam$" | head
total 2.3G
drwx------ 2 wangjl  wangjl  4.0K Aug 27 09:51 tmpterg0dlwsnakemake-runtime-source-cache
drwxrwxrwt 2 root    root    4.0K Aug 27 00:09 rstudio-rsession
drwxr-xr-x 2 wangjl  wangjl  4.0K Aug 26 23:12 hsperfdata_wangjl
drwx------ 2 wangjl  wangjl  4.0K Aug 14 16:15 Rtmptp5h2l
drwx------ 2 wangjl  wangjl  4.0K Aug 14 16:09 RtmpwCeDth
drwx------ 2 wangjl  wangjl  4.0K Aug 14 11:34 ssh-LXkAbrhkbhYb

都是新建 乱码目录，然后在目录中操作。
$ echo "xx" | md5sum
102f5037fe6474019fe947b4977bb2a5  -

$ echo "xx" | sha1sum 
ce3af7760f1289d02bf6a7ad19f3214c4e5c7c2e  -


$ echo Sanke_`date |md5sum | cut -c 1-6`
Sanke_61c202

$ echo Sanke_`date |sha1sum | cut -c 1-6`
Sanke_89bafe





(4) /tmp/ 的文件会自动删除
https://blog.csdn.net/chao199512/article/details/113920887

每次开机会被自动清空。
每隔一段时间，会自动清空n天每修改和访问过的文件。
我没找到哪个文件 //todo


1) ## find 了一下36天前的文件删除了
$ find /tmp/ -type f -atime +10

$ find /tmp/ -type f -atime +1 2>/dev/null 
/tmp/.X12-lock
/tmp/.X11-lock
/tmp/tmpaddon
/tmp/.X10-lock
/tmp/hsperfdata_wangjl/11468

$ find /tmp/ -type f -atime +35 2>/dev/null 
/tmp/.X12-lock
/tmp/.X11-lock
/tmp/tmpaddon
/tmp/.X10-lock
$ find /tmp/ -type f -atime +36 2>/dev/null




2) 从/var/log/cron 日志中发现，服务器除了调用用户的计划任务外，还会执行系统自己的，比如：
/etc/cron.d/       3个 anacron  popularity-contest  sysstat
/etc/cron.daily/   16个
/etc/cron.hourly/  0个
/etc/cron.monthly/ 1个 0anacron
/etc/cron.weekly/  3个 0anacron  man-db  update-notifier-common











========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



