线性代数2

国内课本之外的内容




========================================
数学教程
----------------------------------------
MIT 的线性代数
https://www.bilibili.com/video/BV1ix411f7Yp?p=2


数学博客 https://wangliangster.github.io/#/AI/ML/leastsquares




========================================
|-- 矩阵的列意义: 向量的线性组合
----------------------------------------
方程组
x+y=3
x-2y=-3

写成 列向量 形式
x[1 1]T + y[1 -2]T=[3 -3]T

x=1, y=2




========================================
|-- 高斯消元法解线性方程组: 矩阵行变换相当于左乘一个变换矩阵
----------------------------------------
1. 比如矩阵A=
[1 1]
[1 -2]

(1) 第一次行变换
第二行减去第一行，得到U1=
[1 1]
[0 -3]

相当于A左乘一个矩阵P1=
[1 0]
[-1 1]

P1.A=U1



(2) 第二次行变换
U1第二行除以-3，得到U2=
[1 1]
[0 1]

相当于U1左乘以矩阵P2=
[1 0]
[0 -1/3]


(3) 第三次变换
U2的 r1-r2,得到U3=
[1 0]
[0 1]

相当于U2左乘矩阵P3=
[1 -1]
[0 1]


(4)总结 
P3.U2=U3=E 
P3.(P2.U1)=E 矩阵乘法有结合律，所以括号可以去掉。但是矩阵乘法没有交换律！ 
P3.P2.P1.A=E

P=P3.P2.P1= 自己演算
[2/3 1/3]
[1/3 -1/3]

P.A=E

就是越后期的行变换矩阵，越在更左边。





2. 怎么交换列呢？
右乘一个矩阵。




3. 矩阵A左乘以置换矩阵P得到单位矩阵E，怎么把单位矩阵E变回A矩阵呢？
如果P^-1.P=E, 则 称P^-1是P的逆矩阵。
根据定义，由 P.A=E 则 A^-1=P

注意:
P.A=E 
P.E=A^-1

这就揭示了逆矩阵的一种解法: 对于增广矩阵 A|E 做行变换P，当A变为单位矩阵E时，后面的E就变成了A^-1






## 英语名词
permutation matrix 置换矩阵
identity matrix 单位矩阵





========================================
|-- 矩阵乘法和矩阵的逆
----------------------------------------
1. 定义 A.B=C 

(1) 从维度理解
Amxn . Bnxp = Cmxp
相乘的两个矩阵的内标必须一致才能相乘。


(2) 从B的角度理解
A的全部行，乘以
	B的第1列，是C的第1列；
	B的第2列，是C的第2列；
	B的第3列，是C的第3列；
	...
#
也就是C的列，是A的列向量的线性组合。 // 见下例(4)



(3) 从A的角度理解
A的第1行，是C的第1行；
A的第2行，是C的第2行；

rows of C is combinnation of rows of B.



(4) columnA x rowB
[2]
[3]
[4] . [1 6]
=
[2 12]
[3 16]
[4 24]

A .B =
A矩阵的第1行. B矩阵的第1列 + 
A矩阵的第2行. B矩阵的第2列 + 
...






2. 分块矩阵 

矩阵相乘，可以矩阵分块后，在继续相乘。





3. 矩阵的逆
A^-1.A=I 单位矩阵;

性质 左逆也是右逆 也就是 A^-1.A=A.A^-1=I;

有逆的矩阵，也叫非奇异矩阵。


(1) 有些矩阵为什么不可逆呢？
A=
[1 3]
[2 6]

真实情况是 A.B =C 的列是A的列的组合，如果一个列是[1 0]T了，另一列不可能是[0 1]T了。
A.[3 -1]T=[0 0]T


从方程的角度看:
A.x=0 有非零解了，则A矩阵就不可逆了。
反证法 A如果可逆，则 x=A^-1.0=0，这与x有非零解矛盾。所以A不可逆。


从行列式的角度看: det(A)=3-3=0





(2) 如何求逆矩阵呢？
[1 3] [a c] [1 0]
[2 7].[b d]=[0 1]

可以分解为2个列
[1 3] [a] [1]
[2 7].[b]=[0]
和 
[1 3] [c] [0]
[2 7].[d]=[1]
就是同时解2个二元一次方程组.


Gauss-Jordan法
[1 3] | [1 0]
[2 7] | [0 1]
同时变换，当左侧变为单位矩阵时，右侧就是A矩阵的逆。
=r2-2*r1
[1 3 1 0]
[0 1 -2 1]
=r1-3*r2
[1 0 7 -3]
[0 1 -2 1]
所以求出来逆矩阵 
[7 -3]
[-2 1]




(3) 原理是什么呢？
对于扩充矩阵 [A E]，E为单位矩阵，则左乘一个矩阵P相当于对矩阵[A E]做行变换
P.[A E]=[P.A P.E]
如果P.A=E, 则P=A^-1，同时后半部分 P.E=A^-1
=[E A^-1]




========================================
|-- 4. 矩阵的LU分解 A=L.U
----------------------------------------
A.A^-1=I

1. LU分解 
L 是下三角矩阵，U是上三角矩阵。

A=
[2 1]
[8 7]
对A进行行变换P1得到上三角矩阵 U, 也即是 P1.A=U
U=
[2 1]
[0 3]

则P1=
[1 0]
[-4 1]

如果想写成 A=L.U的形式，那么显然 L=P1^-1=
[1 0]
[4 1]
L正好是一个下三角矩阵。


过程:
[ 1 0 1 0]
[-4 1 0 1]
=r2+4*r1
[1 0 1 0]
[0 1 4 1]





2. LDU分解
A=L.U
如果把U继续分解为一个只保留U主对角线的对角矩阵 乘以一个上三角矩阵U2的形式呢？

U=
[2 1]
[0 3]

D=
[2 0]
[0 3]

则 U=diag(U).U2
[2 0] [1 0.5]
[0 3].[0 1]

U2=diag(U)^-1.U




3. 对于3x3的情况会更复杂

（原文用E表示行消元矩阵，我感觉可能和单位矩阵混淆，底下使用P。）

P32.P31.P21.A=U (只消元不移动行的位置)
依次左乘其逆
P31.P21.A=P32^-1.U
P21.A=P31^-1.P32^-1.U
A=P21^-1.P31^-1.P32^-1.U
简写 A=L.U，L就是这些消元变换的逆的乘积。

如果没有行互换，就能得到下三角矩阵L。

(2) 一个nxn的矩阵，没有0元素，消元法获得上三角矩阵，需要多少计算操作？
假设n=100,
第1列，需要99行变为0，需要99*100次计算；
第2列，需要98行变为0，需要98*99次计算；
...
第k列，需要100-k行变为0，需要 (100-k)*(100-k+1)次计算；
第99列，需要1行变为0，需要1*2次计算；

小计: 1*2 + 2*3 +..+n*(n+1)+.+ 99*100=(1^2+2^2+...+99^2)+(1+2+...+99)
抽象化: (1^2+...+n^2)+(1+...+n), 大概是 n^3 /3 (积分的思想)




(3) 什么时候要行互换呢？
就是主对角线上出现0元素的时候。

permutation matrix: 置换矩阵 
P1=
[0 1 0]
[1 0 0]
[0 0 1]

E是单位矩阵，则
P1.E=
[0 1 0]
[1 0 0]
[0 0 1]

就互换了第一行和第二行。


行置换举着的性质: 逆矩阵等于其转置。
A=matrix(c(0, 1, 0,1, 0, 0,0, 0, 1), nrow=3, byrow = T );A
t(A) - solve(A) # 全是0
t(A)  %*% A # 单位矩阵






========================================
|-- 5. 转置、置换、向量空间
----------------------------------------

如果有行互换，则添加一个互换矩阵 P.A=L.U
这样就能保证L是下三角，U是上三角。

P 是单位矩阵，但是行交换过的。用于把A重新排序，保证消元时主对角线上元素不为0.

P有多少种呢？ 视频说是 n! 种。
我认为是: 两两互换操作 choose(n,2)=n!/2!/(n-2)!

性质: P^-1=P^T, 所以 P^T.P=I; 其中I为单位矩阵





2. 对称矩阵 symmetry matrix: A^T=A

怎么构建对称矩阵呢？ 矩阵R是任意矩阵，R^T.R 就是对称矩阵。

因为 (A.B)^T=B^T.A^T;
所以 (R^T.R)^T=R^T.R 所以确实是对称矩阵。





3. 向量空间 Vector spaces

空间，就是一堆向量。一般要满足一些关系：对数乘和加法两种算法是封闭的。也就是线性组合仍然在该组合内。
  也就是 X属于该空间，则a*X也属于该空间，其中a是实数。
  X和Y属于该空间，则a*X+b*Y也属于该空间，其中a和b是实数。

例如:
R^2: 表示二维real vectors.


任何空间都要包含0，否则无法计算 a-a 
子空间也必须包含原点0，子空间也是空间，也要对线性组合封闭。

反例: 不是向量空间的例子
所有向量都是正的。
计算数字0乘以某向量=0，不在该空间。

0向量自己也是一个子空间。满足数乘和加法封闭。




(2) 矩阵A的列，构成列空间，记作C(A)





========================================
|-- 列向量和零空间
----------------------------------------
1. 子空间，是大空间的子集。
性质：也必然过原点。

比如R^3的子空间，可以是一个过原点的平面P，或者过原点的直线L。

取并集 P U L = all vectors in P or L or both;
这并不一定是子空间，因为可能合成一个不在直线和平面内的向量，导致不封闭。

取交集呢？P and L=0 如果L不在P内，或者 L如果L在P内。结果肯定是子空间。




(2). 推广一下：子空间的交集也是空间，虽然可能小一点。
已知子空间S和T。则S与T的交集一定是子空间。S与T的并集不一定是子空间。





2. 矩阵的列向量组成的向量
A=
[1 1 2]
[2 1 3]
[3 1 4]
[4 1 5]
矩阵A的列向量组成的空间是子空间吗？是，是R^4的子空间。
有多大呢？显然3个向量无法充满整个R^4空间。

A.x=b 总是有解吗？ 4个方程3个未知数。
这个就是说使用A的列向量(3个)A=[a1 a2 a3]，一定能经过线性组合 (系数x=[x1 x2 x3]T)，得到任意向量b=[y1 y2 y3 y4]吗？

什么样的b能有解呢？或者A的列向量能表达什么范围内的向量呢？
也就是只有b在A的列向量空间内，x才有解。


进一步的，A的列向量互相独立吗？或者能互相被表示吗？
如果某一列能被其他列线性表示，则该列可以被扔掉。
比如A的第三列，就是前两列的和，可以被丢弃掉，而不影响A的空间范围。
前面2列就是主向量。



(2). 零向量，零空间Null space.
A.x=0
接上文，A是4x3矩阵，x是3x1的，结果是4*1.

也就是A的3个列向量，其权重x=[1 1 -1]T，怎么组合得到0向量？
实际上，x=[c c c]T 都能组合成0向量。
这时的x是一个过原点的直线。

如果 A.v=0, A.w=0, 然后有 A.(v+w)=A.v+A.w=0，也就是该直线上的点的线性组合也在直线上。
所以该方程A.x=0的解 x构成一个空间，叫做解空间。



(3). 但是如果b!=0时呢？
A.x=b 解不能构成一个空间，因为此时 x!=0.

解构成的集合是不通过原点的面或直线。







3. 有两种构建空间的方法：
- 告诉几个column vector，然后线性组合成一个空间
- 告诉满足的条件 A.x=0，然后x的解构成一个空间。







## 英文名词
pivot vector 主向量







========================================
|--7. A.x=0 主变量、特解
----------------------------------------
线性方程组 A.x=0
其中 A=
[1 2 2 2]
[2 4 6 8]
[3 6 8 10]
第一行加上第二行，等于第三行。

消元法: 做行变换，不改变方程的解
A-->r2-2r1; r3-3r1;
[1 2 2 2]
[0 0 2 4]
[0 0 2 4]
-->r3-r2; r1-r2; r2/2;
[1 2 0 -2]
[0 0 1 2]
[0 0 0 0]
=U 上三角矩阵，每行第一个非零元素所在的列都消元为0.





2. R=reduced row echelon(梯形) form.
阶梯化后，每行第一个非0元素归一化，该元素上下列都化为0.
只做行变换，相当于对原方程组的每一行的操作。


rank(A)=2; A的秩是2.

U的第1、3列叫做 pivot columns；
U的第2、4列叫做 free columns；

A.x=0 的同解方程 U.x=0;
x1+2x2 -2x4=0
x3+2x4=0

解就是 
x1=-2x2+2x4
x2=x2 
x3=   -2x4;
x4=   x4;

写成向量形式 X=[-2 1 0 0]T*k1 + [2 0 -2 1]T*k2

可以理解为，特解就是对A的列向量进行组合的基本方式：
[-2 1 0 0]T，-2第一列+第二列=0.
[2 0 -2 1]T，2*第一列-2*第三列+1*第四列 =0；
这是2个特解。
就是给自由变量中的一个赋值1，其他给0. 当然还可以扩大任意倍。然后计算主变量的值。主变量+自由变量 = 获得特解。
特解的自由组合，就是通解。系数就是对自由变量扩大任意倍数(特解中自由变量一般是1)。


rank(A)=2，就是pivot variable=ran(A)=2个，其他是free variable=4-rank(A)=2个.
主变量，自由变量。



回顾过程:
消元
求特解
组合得通解。





例题: 把B=A^T再做一次。4x3;
B=
[1 2 3]
[2 4 6]
[2 6 8]
[2 8 10]
->r4-r3; r3-r2; r2-2*r1, 
[1 2 3]
[0 0 0]
[0 2 2]
[0 2 2]
->r4-r3; r1-r3; r3/2; r2和r3交换
[1 0 1]
[0 1 1]
[0 0 0]
[0 0 0] = R
= 可以把R写成分块矩阵的形式，第一个是单位矩阵，接着是自由变量部分
[I F]
[0 0]

那么方程组 R.x=0 可以写成
[I F] [-F]
[0 0].[I]=0
也就是X=[-F I]T
本题来看，-F=-[1 1]T, I=[1] 只有一阶的单位矩阵。


行秩=列秩。所以还是rank=2列 pivot variables, 自由变量 3-r=1列.
同解方程组 
x1 + x3=0
  x2+x3=0
所以:
x1=-x3;
x2=-x3;
x3=x3;
写成向量形式
x=[-1 -1 1]T

检查: 对B矩阵的列向量进行组合，-1*col1-col2+col3=0

写成零空间 null space 就是 
x=k*[-1 -1 1]T，k是任意实数。就是一条过原点的直线。





========================================
|-- 8. Ax=b 可解性，及解的结构
----------------------------------------
1. 增广矩阵 augmented matrix 
还是使用上次的矩阵A，不过右侧不是0向量了，是一个向量b=[b1 b2 b3]T

线性方程组 A.x=b
其中 A=
[1 2 2 2]
[2 4 6 8]
[3 6 8 10]

增广矩阵 A_bar=
[1 2 2 2 b1]
[2 4 6 8 b2]
[3 6 8 10 b3]
-> r2-2r1; r3-3r1;
[1 2 2 2 b1]
[0 0 2 4 b2-2*b1]
[0 0 2 4 b3-3*b1]
->r3-r2; r1-r2; 0.5*r2;
[1 2 0 -2 3*b1-b2]
[0 0 1 2 0.5*b2-b1]
[0 0 0 0 -b1-b2+b3]

有解的充要条件是，矩阵A和增广矩阵A_bar的秩相等。
也就是 -b1-b2+b3=0

假设b=[1 5 6]T
则 A_bar->
[1 2 0 -2 -2]
[0 0 1 2 1.5]
[0 0 0 0 0]


A.x=b 可解，当且仅当 b在A的列向量构成的空间中。

另一种描述，A的行组合为0时，b的这种组合也必须为0.





2. 怎么找到 A.x=b 的全部解呢？
(1) 找到一个特解
set all free variables to 0;
solve A.x=b for pivot variables;


本例中，自由变量就是x2=0，x4=0，然后求出来主变量
x1=-2
x3=1.5
一个特解是 Xp=[-2 0 1.5 0]T

(2) 找到A.x=0的全解Xn
则A.x=b的全解为 Xc=Xn+Xp

证明也简单， 
已知A.xn=0， A.xp=b;
则 A.(xn+xp)=A.xn+A.xp=0+b=b;

Xc= [-2 1 0 0]T*k1 + [2 0 -2 1]T*k2 + [-2 0 1.5 0]T


这是一个经过原点的平面(子空间N(A))，平移到过Xp点。
平移后不过原点，已经不是空间了。








3. 列满秩。r=n;
如果矩阵Amxn列满秩，则没有自由变量，都是pivot variable。
相当于方程组个数多于未知数个数，独立约束条件至少和未知数个数相等。
这时只有0解或1个解。

列满秩就不可能组合成0向量，也即是A.x=0无解（零空间只包含0向量），也就是方程组A.x=b只有特解。

A=
[1 3]
[2 1]
[6 1]
[5 1]







4. 行满秩呢？r=m;
消元时，每一个行都有一个pivot variable;
剩下的自由变量个数为 n-r =n-m个。

A= 做行变换
[1 2 6 5]
[3 1 1 1]
->
[1 0 F]
[0 1 F]

左边是单位矩阵，右侧是自由部分的矩阵。





5. 如果r=m=n, 则矩阵可逆。
这时 A.x=b 总是有1个解。







A的秩决定着方程A.x=b的解的个数。


总结一下，Amxn.x=b的解的情况, 其中 r=rank(A):
A有m行，n列

1. r=m=n, R=I, 对于A.x=b只有1个解；
2. r=m<n, 也即是方程少于未知数个数
R=[I F], 应该是有无数个解；

3. r=n<m, 也就是方程多于未知数个数，方程有冗余的，
R=[I 0]T, 应该是0个或1个解。

4. 如果r<m,r<n，也就是行或列都有冗余，
R=
[I F]
[0 0]
则0个或无穷多个解。








========================================
|-- 9. 线性相关性、基和维数
----------------------------------------
1. 线性相关
一组向量，至少一个可以被其他线性组合出来，则这组向量线性相关。


2. 线性无关
一组向量，任何一个都不能被其他向量线性表示。都不是多余的。

定义：
列向量x1,x2,...,xn,系数c1,...,cn;
c1x1+...+cnxn=0 只有0解，则称他们互相独立，或者线性无关。
也就是列向量x1,...,xn构成的矩阵A，A.x=0 只有0解。

如果有0向量，则肯定线性相关。
比如 v1=[2,3], v2=[0,0]，则 0*v1+6*v2=0, 可见有非零解x=[0,6]。不独立。


对于二维平面，只能由2个独立向量，第三个肯定不独立。





3. spanning space 生成空间
A矩阵的各个列向量，生成一个生成空间。
该空间包含的，是A所有列向量的线性组合。







4. basis 基 
空间的基，就是最少的能表示空间所有向量的向量，要满足2个条件:
- 互相独立 they are independent;
- 能生成整个空间 they span the space.

二维空间，只需要2个基，就能表示所有该空间内的所有向量。


对于R^3，一个基是单位矩阵的列:
[1 0 0]
[0 1 0]
[0 0 1]

一个空间有无数的基。
基的判定条件: 行列式不为0，或者可逆，或者满秩。


比如这三列构成的矩阵 B=
1 2 3
1 2 4
2 5 8

det(B) #-1
solve(B)
# 4 1 -2
# 0 -2 1
# -1 1 0

qr(B)$rank #3





5. dimension of space 维数 
R^3 需要3个基向量。维数就是 3。








========================================
|-- 当前进度 
----------------------------------------

https://www.bilibili.com/video/BV1ix411f7Yp?p=2  2020.9.13 over;
3  2020.9.18 over;
4  2020.9.20 over; 画质太渣了
5  2020.9.22 over;
6  2020.9.29 over;
7  2020.10.2 over;
8  2020.10.3 over;

https://www.bilibili.com/video/BV1ix411f7Yp?p=9 2020.9.;
2.36
21.36




========================================
****** 矩阵专题 (附带R语言代码) ******
----------------------------------------

对矩阵进行特征值分解、奇异值分解、LU分解、QR分解。



# 左逆也是右逆，R验证
A=matrix(c(1,2,2,2,1,2,2,2,1), nrow=3, byrow = T ); A
B=solve(A);B
round(A %*% B)
round(B %*% A)

# svd分解
rs=svd(A)
# A=U.diag().V^T;
rs$u %*% diag(rs$d) %*% t(rs$v)








========================================
|-- 专题: 矩阵的3大关系(等价/相似/合同)
----------------------------------------
1. 等价
(1)定义: 若A经过有限次的初等变换化为B，就说A和B等价。
(包括初等行变换、初等列变换)
有限次。


(2) 判别法
除了定义法，还经常使用判别法
A和B同型，则A和B等价。 <==> r(A)=r(B)






2. 矩阵相似 
(1) 定义: A和B为n阶矩阵，如果存在可逆的矩阵P，使得 P^-1.A.P=B，则称A和B相似。
记作 A~B。


(2) 矩阵相似的性质
性质1:
1) A~A
2) A~B, 则 B~A;
3) A~B, B~C, 则 A~C;


性质2: A~B，则 r(A)=r(B); 反之不成立。
反例:
A=
|1 0|
|0 2|

B=
|1 -1|
|0 1|

满足 r(A)=r(B)=2;
但是特征值不等
|A-L.E|=0, L1=1, L2=2;
|B-L.E|=0, L1=L2=1;
相似矩阵的特征值是相等的。
所以这里A和B不相似。


性质3: A~B, 则 |A-L.E|=|B-L.E|; 反之不成立。

性质4: A~B, 则 f(A)~f(B);

性质5: A~B, 则 
	1) tr(A)=tr(B); # 因为矩阵的迹等于特征值的乘积；相似矩阵的特征值相同。
	2) |A|=|B|; #因为矩阵的行列式，等于特征值的乘积。
	3) A^T ~ B^T;
	4) 若 A、B可逆，则 A^-1 ~ B^-1, 还有 A*~B*; 
#


(3) 矩阵相似的判别法 (diffucult)

1) 矩阵A和B相似 A ~ B, 则 A和B特征值相同，但是反之不成立。

定理: |A-L.E|=|B-L.E| (就是特征值相同)，且A和B都可以对角化，则A~B。

证明: |A-L.E|=|B-L.E| 可知 A和B特征值相同。设为L1, ..., Ln;
因为A和B可以对交化，则存在可逆矩阵P1和P2，使得
 P1^-1.A.P1=diag(L1,..,Ln)
 P2^-1.B.P2=diag(L1,..,Ln)
所以 P1^-1.A.P1=P2^-1.B.P2, 左乘 P2,右乘P2^-1，得
P2.P1^-1.A.P1.P2^-1=P2.P2^-1.B.P2.P2^-1=B
B=(P2.P1^-1).A.(P1.P2^-1)=(P1.P2^-1)^-1.A.(P1.P2^-1) = P^-1.A.P
其中 P=P1.P2^-1; 怎么证明P可逆呢？因为P1，P2可逆，所以P可逆。  //todo
根据定义 A~B。证毕。





3. 合同 
(1) 矩阵合同的概念，产生于二次型。

f = X^T.A.X = Y^T.(P^T.A.P).Y
其中，A是对称矩阵，而变换后为标准二次型，也就是 P^T.A.P 为对角矩阵 diag(L1,...,Ln)。
等号两边相等，所以叫合同。

定义: A和B为n阶矩阵，若存在可逆矩阵P，使得 P^T.A.P=B，则称A和B合同。记作 A≌B (全等三角形符号)


(2) 判别法
A^T=A, B^T=B, 对称是前提条件！！一定要先保证！
对称矩阵合同 <==> 特征值的正、负、0个数相同。 有专业术语说是正负惯性系数相同。

---> 特征值相同是矩阵相似要求的，矩阵合同不要求特征值相同。

例1: A=
[0 1]
[1 0]
B=diag(-1,4)
判断A和B的关系?
求特征值即可，
|A-L.E|=
|-L 1|
|1 -L|
=(1-L)
|1 1|
|0 -L-1|=-(1-L)(1+L)=0
L1=1, L2=-1;

|B-L.E|=
|-1-L 0|
|0 4-L|=-(1+L)(4-L)=0
L1=-1, L2=4; #这个其实不用算了，对角矩阵的特征值就是对角线元素！

特征值不同，所以不相似。
但是惯性系数相同，所以合同。
















4. 矩阵等价、相似、合同 三种关系的关系 
https://blog.csdn.net/huangmingleiluo/article/details/104211738
https://blog.csdn.net/qq_36468195/article/details/89604688

## 矩阵等价、相似和合同之间的区别：
1)等价，相似和合同三者都是等价关系。
2)矩阵相似或合同必等价，反之不一定成立。
3)矩阵等价，只需满足两矩阵之间可以通过一系列可逆变换，也即若干可逆矩阵相乘得到。
4)矩阵相似，则存在可逆矩阵P使得，AP=PB。
5)矩阵合同，则存在可逆矩阵P使得，P^TAP=B。
6)当上述矩阵P是正交矩阵时，即PT=P(-1)，则有A，B之间既满足相似，又满足合同关系。


## 数学定义
如果A和B矩阵等秩，则等价。存在矩阵P和Q，使得 PAQ=B，则称A,B等价。

A,B均为n阶方阵，若存在可逆矩阵P,使B=P*(-1)AP,则A,B相似。

A,B均为n阶矩阵，若存在可逆矩阵P,使B=P(T)AP,则A,B合同。
	矩阵合同相当于对一个矩阵实施一系列对应的初等行列变换，
	当且仅当P为正交矩阵即P*(-1)=P(T)，才有矩阵相似与合同等价。但是A,B合同，可以直接推出A,B等价。
#



矩阵相似满足两个条件
 - 两个矩阵特征值一样
 - 若特征值相同①一个可对角化一个不可则不相似②两个都可对角化一定相似

矩阵合同满足两个条件
 - A的转置等于A B的转置等于B
 - AB的特征值正负个数一致
#









========================================
|-- 专题: 特征根综合练习
----------------------------------------

1. 回顾 
L1*L2*...*Ln=det(A), 矩阵特征值的乘积等于行列式。

r(A)=n  <==> Li!=0, 1<=i<=n;


Q: 如果前r个特征值不为0，后面都是0，也就是 L1!=0, ..., Lr!=0, L(r+1)=L(r+2)=...=Ln=0,
因为|A|=0，所以 r(A)<n; 
那么 r(A)=r 是否成立？不成立。可举反例，再探究原因。

A: 
反例: A=
|0 1 -1|
|0 0 1|
|0 0 2|
计算行列式 |A-L.E|=
|-L 1 -1|
|0 -L 1|
|0 0 2-L|
=L^2*(2-L)=0
所以 L1=L2=0, L3=2;
有1个特征值不是0，但是秩 r(A)=2.


why? 因为 A 不可对角化。


证明: 当A可对角化，有r个特征值不为0，其余都是0，则 r(A)=r.




例: A3x3，A^T=A,  A^2=2A, r(A)=2, E-A ~ B, 求矩阵B。
解:
1) 令 AX=L.X, 由 A^2=2*A, 则 (A^2-2A)X=(L^2-2*L)X=0, 
因为X!=0, 所以 L(L-2)=0, L1=0, L2=2;

2) 因为 A^T=A, 则A可对角化。
r(A)=2，则A的3个特征值中2个不能是0，则L=2是2重，也就是L2=L3=2;
E-A 的特征值，可以由A的特征值算出来 L1=1-0=1, L2=L3=1-2=-1;

所以 E-A ~ diag(1, -1, -1);







========================================
|-- 矩阵的QR分解 [正交性]
----------------------------------------

矩阵的QR分解和LU分解的目的都是为了便于矩阵计算。

0. 前置知识：

正交矩阵：若n阶方阵A满足 A^T.A=E, 则称A为正交矩阵，简称 正交阵 （复数域上称为酉矩阵）
	- A是正交矩阵的充要条件：A的列(行)向量都是单位向量，且两两正交。
#



一个矩阵如果满足i>j+1时aij=0，则将这个矩阵成为上Hessenberg阵。上Hessenberg阵。
x x x x x
x x x x x
0 x x x x
0 0 x x x
0 0 0 x x


QR分解为矩阵分解的一种，在解决矩阵特征值计算和最小二乘问题中有很大的作用。

QR分解定理: 任意的一个满秩实(复)矩阵A，都可唯一的分解为A=QR,其中Q为正交矩阵，R为正对角元的上三角矩阵




1. 定义: A=Q.R

将矩阵分解成一个正规正交矩阵Q与上三角形矩阵R，所以称为QR分解法。
该算法对对称矩阵和非对称矩阵都适用。

QR（正交三角）分解法是求一般矩阵全部特征值的最有效并广泛应用的方法，一般矩阵先经过正交相似变化成为Hessenberg矩阵，然后再应用QR方法求特征值和特征向量。
它是将矩阵分解成一个正规正交矩阵Q与上三角形矩阵R，所以称为QR分解法，与此正规正交矩阵的通用符号Q有关。

如果实（复）非奇异矩阵A能够化成正交（酉）矩阵Q与实（复）非奇异上三角矩阵R的乘积，即A=QR，则称其为A的QR分解。



计算QR分解的方法一共有三种：
	Gram–Schmidt Orthogonalization: 有数值不稳定的缺点，可以用改进的Gram-Schmidt方法
	Householder Triangularization: 数值稳定，适用于稠密矩阵: https://www.cnblogs.com/caimagic/p/12202884.html
	Givens Rotations: 数值稳定，适用于稀疏矩阵
# https://www.zhihu.com/question/23905796/answer/528875727


纠正一下，其实A=QR分解并不要求A是一个列满秩矩阵，只是当A不是列满秩时，QR分解不唯一。



用施密特正交计算方法, 只需通过Gram-Schmidt过程得到A的标准正交矩阵Q，很快速的求取出R.
https://www.jianshu.com/p/13a81c5b4b9d






2. QR分解

步骤:
- 写出矩阵A的列向量;
- 把列向量按照施密特正交化得到正交向量组(q1, q2, ...),由此构成的矩阵为正交矩阵Q;
- 把矩阵A列向量表示成向量组(q1,q2,...)的线性组合，则系数就是矩阵R;
- 得到矩阵的QR分解为: A=Q.R

性质
由于Q是正交矩阵，所以 Q^T.Q=E
Q^T.A=Q^T.Q.R=R;
所以 R=Q^T.A
也就是把矩阵A化为正交矩阵Q后，可以直接计算出R。




(1)例题: 对矩阵A做QR分解，A=
|0 3 1|
|0 4 -2|
|2 1 2|

1) 记 A=[x1 x2 x3]，其中 x1=[0 0 2]T, x2=[3 4 1]T, x3=[1 -2 2]T;

2) 做施密特正交化 
y1=x1=[0 0 2]T
y2=x2-<x2,y1>/<y1,y1> *y1=x2-1/2 *y1=[3 4 0]T
y3=x3-<x3,y2>/<y2,y2>*y2 -<x3,y1>/<y1,y1>*y1=x3+1/5*y2-y1=[8/5, -6/5, 0]T

3)单位化
e1=y1/||y1||=1/2*y1=[0 0 1]T
e2=1/5*y2=[3/5 4/5 0]T
e3=1/2*y3=[4/5 -3/5 0]T

4) 再看x和e的关系
x1=y1=2e1
x2=1/2*y1+y2=e1+5e2
x3=y1-1/5*y2+y3=2e1-e2+2e3;

因为 最开始的设定 A=[x1, x2, x3]=[2e1, e1+5e2, 2e1-e2+2e3]=[e1, e2, e3] . R
这就是 Q 和 R 矩阵。
- Q是正交矩阵: 正交化，单位化求的
- A是对角矩阵: 就是 xi=t.ei 的系数

5) 所以 A=Q.R 的分解为
Q=[e1 e2 e3]=
[0 3/5 4/5]
[0 4/5 -3/5]
[1 0 0]

上三角矩阵 R=
[2 1 2]
[0 5 -1]
[0 0 2]





(2)R语言代码实现 QR 分解

#默认是按照 列 填充矩阵的
A=matrix(c(0,3,1,0,4,-2,2,1,2), nrow=3, byrow = T);A
solve(A) #求A^-1

eigen(A) #特征值和特征向量
# 特征根是复数...


# QR 分解
qrresult <- qr(A)
qrresult
Q=qr.Q(qrresult);Q #Q矩阵
#     [,1] [,2] [,3]
#[1,]    0 -0.6 -0.8
#[2,]    0 -0.8  0.6
#[3,]   -1  0.0  0.0
R=qr.R(qrresult);R #R矩阵
#     [,1] [,2] [,3]
#[1,]   -2   -1   -2
#[2,]    0   -5    1
#[3,]    0    0   -2
qr.X(qrresult) #还原矩阵

> Q %*% R #还原矩阵
     [,1] [,2] [,3]
[1,]    0    3    1
[2,]    0    4   -2
[3,]    2    1    2

R的结果和上面手算的符合相反，这也合理。毕竟单位向量可以固定直线，但无法说直线的2个方向哪个更好。












3. QR 分解的应用

QR 分解常用于求解A的特征值、A的逆，线性最小二乘法等问题.

QR分解的实际计算有很多方法，例如 ivens旋转，Householder变换以及Gram-Schmidt正交化等等。每一种方法都有其优点和不足。

对于非方阵的mxn(m≥n)阶矩阵A也可能存在QR分解。这时Q为m*m阶的正交矩阵，R为m*n阶上三角矩阵。这时的QR分解不是完整的(方阵)，因此称为约化QR分解(对于列满秩矩阵A必存在约化QR分解)。同时也可以通过扩充矩阵A为方阵或者对矩阵R补零，可以得到完全QR分解。


(1) 使用QR分解求 特征根

QR算法求矩阵全部特征值的基本思想是利用矩阵的QR分解通过迭代格式
  Ak=Qk.Rk 
  A(k+1)=Rk.Qk #反向相乘
通过若干迭代，将A=A1化成相似的上三角阵，从而其对角线上就是原矩阵A的全部特征值。
证明过程: https://www.cnblogs.com/chenying99/articles/4967960.html


## R 代码演示如下:
A=matrix(c(5, -3, 2, 6,-4,4,4,-4,5), nrow=3, byrow = T);A
#     [,1] [,2] [,3]
#[1,]    5   -3    2
#[2,]    6   -4    4
#[3,]    4   -4    5

> eigen(A)
eigen() decomposition
$values
[1] 3 2 1

$vectors
          [,1]          [,2]       [,3]
[1,] 0.3333333 -7.071068e-01 -0.4082483
[2,] 0.6666667 -7.071068e-01 -0.8164966
[3,] 0.6666667 -6.106227e-16 -0.4082483


# 开始迭代，把A化为上三角矩阵
qrresult <- qr(A)
Q=qr.Q(qrresult);Q
R=qr.R(qrresult);R

# 反复多次反向相乘，A2=R.Q 的对角线为原矩阵A的特征值
for(i in seq(1,10)){
  A2=R %*% Q;A2
  qrresult <- qr(A2)
  Q=qr.Q(qrresult);Q
  R=qr.R(qrresult);R
};A2
#              [,1]          [,2]        [,3]
#[1,]  2.988494e+00 -1.011336e+00 -12.0220481
#[2,] -1.128289e-02  2.011544e+00  -1.8628214
#[3,]  7.440532e-06 -7.612464e-06   0.9999624

> round(A2) #该上三角矩阵的对角线上，就是原矩阵A的全部特征值
#     [,1] [,2] [,3]
#[1,]    3   -1  -12
#[2,]    0    2   -2
#[3,]    0    0    1



==> 特征向量怎么求?怎么用软件求? //todo
A.x=L.x

L=3时, (A-3*E).x=0 就是求解 齐次线性方程组的解。
好像没有现成的函数可用。
只能手工做行变换，化为行最简形式
[2 0 -1]
[0 1 -1]
[0 0 0]
通解为 x=k[1/2 1 1]^T


求解 (A-2*E).x=0 通解 x2=[1 1 0]^T

求解 (A-1*E).x=0 通解 x3=[1 2 1]^T
B=(A-1*diag(rep(1,3)));B
det(B)
matrix(c(5/2,  -3/2, 0, 
         3, -2, 0,
         -2, 2, -1),byrow = T, nrow=3) %*% B
输出:
     [,1] [,2] [,3]
[1,]    1    0   -1
[2,]    0    1   -2
[3,]    0    0    0

矩阵形式
A2=matrix(c(1,2,2,
         1,1,0,
         1,2,1), nrow = 3);A2
apply(A2, 2, function(x){
  x/sqrt(sum(x**2))
})
## 
          [,1]      [,2]      [,3]
[1,] 0.3333333 0.7071068 0.4082483
[2,] 0.6666667 0.7071068 0.8164966
[3,] 0.6666667 0.0000000 0.4082483
还是符号不一致，每个特征向量都有系数k，可以是任意值，包括+-1。







(2) 解线性最小二乘法
矩阵的QR分解是指，可以将矩阵A分级成一个正交阵Q和一个上三角矩阵R的乘积。实际中，QR分解经常被用来解线性最小二乘问题。
我们利用QR分解可以同时得到Ax=b的最小二乘解和剩余 //how?

A.x=b, 其中 A为 nxk 的矩阵，x为 kx1 列向量，b为 nx1 列向量。
如果 n>k(方程个数大于未知数个数)，这个方程称为 Over Determined System。
这样的方程时没有精确解的，只能根据某个规则，给出最优解。

比如 给出拟合值 A.x 和 真实值 b 的差 A.x-b，为了防止负负得正，要进行平方，这就是二乘(平方)的意思。




1) 证明过程: https://www.jianshu.com/p/811d17ed3d81
A.x=b 在 x=(A^T.A)^-1.A^T.b 时损失函数取最小值

# least square problem
f(x)=A.x - b 为线性方程时，线性最小二乘问题为 min||A.x-b||^2 的x值是多少?
展开 h(x) = ||A.x-b||^2 = (A.x-b)^T*(A.x-b)
h(x)=(Ax)^T.(Ax) - b^T.Ax - (Ax)^T.b + b^T.b
求偏导有  dh(x)/dx=A^T.A.x - A^T.b
当导数为0时，得到损失函数值为最小值，因此
	A^T.A.x=A^T.b 
也就是 x=(A^T.A)^-1.A^T.b

拟合值 y = b2= A.x= A.(A^T.A)^-1.A^T.b = A.(A^T.A)^-1.A^T.b





2) 使用QR分解求值该值
min||A.x-b||^2 取最小值时 x=(A^T.A)^-1.A^T.b
由于涉及矩阵求逆，计算上比较困难，采用QR分解能简化求值过程:

首先对A进行QR分解，A=Q.R, 其中 Q.Q^T=Q^T=E, R为上三角矩阵;

x=(A^T.A)^-1.A^T.b
(A^T.A).x=A^T.b
(QR)^T.(QR).x=(QR)^T.b
R^T.Q^T.Q.R.x=R^T.Q^T.b
R^T.R.x=R^T.Q^T.b
R^T.(R.x - Q^T.b)=0 
# 接下来怎么算？两边同时左乘 R^T 的逆? 上三角矩阵的逆一定存在吗？怎么证明?
假设 R 可逆:
R.x=Q^T.b
x=R^-1.Q^T.b

其中 R 为上三角矩阵，求逆相对容易很多，规避了直接对 (A^T.A) 求逆复杂度高的问题。




3) 我的例子
# 准备数据
df1=data.frame(
  x1=iris$Sepal.Length,
  x2=iris$Sepal.Width,
  x3=iris$Petal.Length,
  y=iris$Petal.Width
)
head(df1)
#plot(y~x1+x2+x3, data=df1)

#i 使用 lm() 多元线性回归
model=lm(y~x1+x2+x3, data=df1)
summary(model)
#abline(model, col="red")

> model
Call:
lm(formula = y ~ x1 + x2 + x3, data = df1)

Coefficients:
(Intercept)           x1           x2           x3  
    -0.2403      -0.2073       0.2228       0.5241  


# 复原矩阵
# model$qr
qr.Q(model$qr) %*%
  qr.R(model$qr)



#ii 根据公式算，使用QR分解法
model$qr
qr.Q(model$qr) %*%
qr.R(model$qr)
#
b=df1[, "y",drop=F]
head(b)
#
A=df1[,-4, drop=F]
A$i=1; #添加一列常数项，最后多出来的是截距项
head(A)

rs1=qr(A)
qr.Q(rs1) %*%
qr.R(rs1)


x=solve(qr.R(rs1)) %*% t(qr.Q(rs1)) %*% as.matrix(b)
> x
            y
x1 -0.2072661
x2  0.2228285
x3  0.5240831
i  -0.2403074 #截距项


# 模型拟合值 A.x=b
b1=fitted(model, x) #现有函数
b2=as.matrix(A) %*% as.matrix(x) #公式
table(b1-b2<1e-10) # all T


# 求残差:
res1=qr.resid(rs1, as.matrix(b)) #现有函数
res2=b - as.matrix(A) %*% as.matrix(x) #公式
table( res1 - res2 <1e-10 )  # all T


# 问题: 如果 Ax=b 的b有很多个，比如1万个，怎么加速求残差过程？
Seurat 4 中是这么做的:
先对A矩阵求一次 QR 分解，然后对每个b循环
	根据 QR 分解求出最优x值 x=solve(qr.R(rs1)) %*% t(qr.Q(rs1)) %*% as.matrix(b)
	残差就是 resid = b - as.matrix(A) %*% as.matrix(x)
# 这样，比直接使用1万次lm()拟合节省了1万-1次A矩阵的 QR 分解过程，从而节省了大量时间。





# trouble shooting: 
最开始发现：系数差别很大，并且，公式算的没有截距项。
为什么差别这么大? 
	* 没有截距怎么办？Ax=b 需要给A矩阵添加一列常数项1，就有截距项了。
	* 考虑截距后，系数和 lm() 一致了。



4) 难道 最小二乘法 还不止一种？

# 这个图就是 y 方向的距离: 公式是按照 y2-y 算的距离，印象中高中的最小二乘法是按照 点到直线的距离 计算的，为什么不同呢？
https://zhuanlan.zhihu.com/p/128083562
最小二乘法公式应该就是让y方向距离最小，而不是点到直线的垂直距离和最小，图没错。要让垂直距离最小要用全面最小二乘法。从第一个公式(Xw-y)^2就看出来了(y值的差的平方)








ref:
https://blog.csdn.net/Jakob_Hu/article/details/90901054
https://zhuanlan.zhihu.com/p/84415000
https://wangliangster.github.io/#/AI/ML/leastsquares

https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#





========================================
|-- 奇异值分解(singular value decomposition, SVD) [对称阵的 奇异值==特征值的绝对值]
----------------------------------------

以Ax = b为例，x是m维向量，b是n维向量，m,n可以相等也可以不相等，表示矩阵可以将一个向量线性变换到另一个向量，这样一个线性变换的作用可以包含旋转、缩放和投影三种类型的效应。

奇异值分解正是对线性变换这三种效应的一个析构。

而特征值分解其实是对旋转缩放两种效应的归并。（有投影效应的矩阵不是方阵，没有特征值）
https://www.zhihu.com/question/19666954/answer/89140627

对于一个矩阵X，其奇异值的平方和 X.X^T 对应的特征值相等。





1.将矩阵分解为奇异向量(singular vector)和 奇异值(singular value).

特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个N * M的矩阵就不可能是方阵，我们怎样才能描述这样普通的矩阵呢的重要特征呢？奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法。


2.
通过奇异值分解，我们会得到一些与特征分解相同类型的信息。然而，奇异值分解有更广泛的应用。
>> 每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。
例如，非方阵的矩阵没有特征分解，这时我们只能使用奇异值分解。

在PCA中，使用奇异值分解SVD来代替特征分解，得到特征值和特征向量。




3. 先回顾特征值分解：针对方阵
(1)已知方阵Anxn，根据特征值的定义 
A.x1=L1.x1; 
A.x2=L2.x2; 
...
A.xn=Ln.xn; 
相加得
A.(x1,x2,...,xn)=L1.x1+L2.x2+...+Ln.xn=(x1,x2,...,xn)*diag(L1,...,Ln)
记 W=(x1, x2, ..., xn)，则
A.W=W*diag(L1,...,Ln)

如果W可逆，则 A=W*diag(L1,...,Ln)*W^-1;

(2) 对矩阵W做施密特正交化，归一化，得到正交矩阵W, 
||W||=1, 
W^T.W=E，所以 W^-1=W^T;

所以，上述(1)中的等式可以写成 A=W*diag(L1,...,Ln)*W^T;







4. 对于普通矩阵 Amxn , 假设能写成 Amxn=U.B.V^T的形式，
B是除了主对角线外都是0的矩阵MxN，我们把B的主对角线上的元素叫做奇异值。

其中 U是MxM的，V是NxN的，而且U和V都是酉矩阵。也就是U^T.U=E, V^T.V=E;

(0) 补充 酉矩阵
https://blog.csdn.net/qq_38048756/article/details/102710075

1)酉矩阵（unitary matrix）定义
若n阶复矩阵A满足 AH.A=A.AH=E, 则称A为酉矩阵，记之为A∈UN×N。其中，AH是A的共轭转置。

2)性质
如果A是酉矩阵
i)A^−1=A^H
i)A^−1也是酉矩阵；
i)det(A)=1; （det表示矩阵的行列式）
i)充分条件是它的n个列向量是两两正交的单位向量。

3)共轭转置
首先将A中的每个元素aij取共轭得bij，将新得到的由bij组成的新m*n型矩阵记为矩阵B，再对矩阵B作普通转置得到B^T，即为A的共轭转置矩阵：B^T=A^H

对于矩阵部分的内容在深度学习（花书）第二章线性代数中有一些介绍，如果遇到问题可以在第二章进行寻找。






下面我们将求出U和V，并给出B是对角阵的证明。

(1) 求出左侧的U矩阵
A.A^T 是一个MxM的方矩，求其特征值: (A.A^T).Ui=Lambdai.Ui, 把其M个特征值张成向量U=(u1, ..., uM)。
就是我们SVD公式里面的U矩阵了。一般我们将U中的每个特征向量叫做A的左奇异向量。

(2) 求出右侧的V矩阵
A^T.A 是一个NxN的方矩，求其特征值: (A^T.A).Vi=Lambdai.Vi, 把其N个特征值张成向量V=(v1, ..., vM)。
就是我们SVD公式里面的V矩阵了。一般我们将V中的每个特征向量叫做A的右奇异向量。

(3) 接着求对角矩阵B
U和V我们都求出来了，现在就剩下奇异值矩阵B没有求出了。由于B除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值σ就可以了。

由 A=U.B.V^T，左乘向量V，由V^T.V=E得 A.V=U.B.V^T.V=U.B，
把U和V按向量展开，记作对角矩阵B=diag(sigma1, sigma2, ...)
A.(v1,...,vi)=(u1,...,ui).diag(sigma1,...,sigmai)
A.vi=ui.sigmai, 所以 sigmai=A.vi/ui;

这样我们可以求出我们的每个奇异值，进而求出奇异值矩阵B。

(4) 补充证明:
上面还有一个问题没有讲，就是我们说AT.A的特征向量组成的就是我们SVD中的V矩阵，而AAT的特征向量组成的就是我们SVD中的U矩阵，这有什么根据吗？这个其实很容易证明，我们以V矩阵的证明为例。

A=U.B.VT 两边取转置，A^T=V.B^T.U^T，

1)左乘原始式子，使用 UT.U=E，
A^T.A=V.B^T.U^T.U.B.VT=V.B^T.E.B.VT=V.(B^T.B).VT
而之前证明了对于正交矩阵V有: A^T.A=V.diag(L1,...,La).V^T
所以 V.(B^T.B).VT=V.diag(L1,...,La).V^T，也就是 B^T.B=diag(L1,...,Lv)

可以看出AT.A的特征向量组成的的确就是我们SVD中的V矩阵。
可见B^T.B是对角矩阵，且奇异值的平方等于 B^T.B新矩阵的特征值: diag(sigma1^2,...,sigmav^2)=diag(L1,...,Lv)


2) 同理右乘原始式子，可以得到
A.A^T=U.B.VT.V.B^T.U^T=U.B.E.B^T.U^T=U.(B.B^T).U^T
由特征值的性质得 A.A^T=U.diag(L1,...,Lu).U^T
所以 U.(B.B^T).U^T=U.diag(L1,...,Lu).U^T，也就是 B.B^T=diag(L1,...,Lu)

可以看出A.AT的特征向量组成的的确就是我们SVD中的U矩阵。
可见B.B^T是对角矩阵，且奇异值的平方等于 B.B^T 新矩阵的特征值: diag(sigma1^2,...,sigmau^2)=diag(L1,...,Lu)

3) 综合1)和2) 的结论，我们知道 diag(L1,...Lv)和diag(L1,...,Lu)的非零部分是一一相等的，不等的部分都是0。

B=
[1 0 0]
[0 2 0]

B^T=
[1 0]
[0 2]
[0 0]

乘积
B^T.B=diag(1,4,0)
B.B^T=diag(1,4)

我们的特征值矩阵等于奇异值矩阵的平方:
	sigmai^2=Lambdai
	sigmai=sqrt(Lambdai)

我们可以不用σi=Avi/ui来计算奇异值，也可以通过求出AT.A的特征值取平方根来求奇异值。






5. 例题

奇异值矩阵中也是按照从大到小排列。

例1: 对矩阵进行奇异值分解 A=
[0 1]
[1 1]
[1 0]

A^T=
[0 1 1]
[1 1 0]

(1)A.A^T=
[1 1 0]
[1 2 1]
[0 1 1]
|A.A^T -L.E|=0，得
L1=1, u1=[-1 0 1]T;
L2=3, u2=[1 2 1]T;
L3=0, u3=[-1 1 -1]T;
归一化得
U=(u1/|u1|, u2/|u2|, u3/|u3|)=
[-1/sqrt(2), 1/sqrt(6), -1/sqrt(3)]
[0, 2/sqrt(6), 1/sqrt(3)]
[1/sqrt(2), 1/sqrt(6), -1/sqrt(3)]

(2) A^T.A=
[2 1]
[1 2]
|A^T.A-L.E|=0，得
L1=1, v1=[-1 1];
L2=3, v2=[1 1];
归一化得
V=(v1/|v1|, v2/|v2|)=
[-1/sqrt(2), 1/sqrt(2)]
[1/sqrt(2), 1/sqrt(2)]

(3) 用σi=sqrt(λi)直接求出奇异值为1和sqrt(3)
奇异矩阵B=
[1 0]
[0 sqrt(3)]
[0 0]

(4) 最终得到A的奇异值分解为：

A=U.B.V^T=
[-1/sqrt(2), 1/sqrt(6), -1/sqrt(3)]
[0, 2/sqrt(6), 1/sqrt(3)]
[1/sqrt(2), 1/sqrt(6), -1/sqrt(3)]
*
[1 0]
[0 sqrt(3)]
[0 0]
*
[-1/sqrt(2), 1/sqrt(2)]
[1/sqrt(2), 1/sqrt(2)]



(5) 使用R验证一下
U=matrix( c(-1/sqrt(2), 1/sqrt(6), -1/sqrt(3),
0, 2/sqrt(6), 1/sqrt(3),
1/sqrt(2), 1/sqrt(6), -1/sqrt(3)), byrow = T, nrow = 3);U
#            [,1]      [,2]       [,3]
#[1,] -0.7071068 0.4082483 -0.5773503
#[2,]  0.0000000 0.8164966  0.5773503
#[3,]  0.7071068 0.4082483 -0.5773503

V=matrix(c(-1/sqrt(2), 1/sqrt(2),
           1/sqrt(2), 1/sqrt(2)), byrow = T, nrow=2);V
#           [,1]      [,2]
#[1,] -0.7071068 0.7071068
#[2,]  0.7071068 0.7071068

B=diag(x=c(1,sqrt(3)), nrow=3, ncol=2);B
#           [,1]      [,2]
#[1,] -0.7071068 0.7071068
#[2,]  0.7071068 0.7071068

A=round(U %*% B %*% t(V) ); A
#      [,1] [,2]
# [1,]    1    0
# [2,]    1    1
# [3,]    0    1





而用R直接进行SVD分解呢？
> A2=svd(A);A2
$d
[1] 1.732051 1.000000

$u
           [,1]          [,2]
[1,] -0.4082483  7.071068e-01
[2,] -0.8164966 -1.110223e-16
[3,] -0.4082483 -7.071068e-01

$v
           [,1]       [,2]
[1,] -0.7071068  0.7071068
[2,] -0.7071068 -0.7071068

## 这里T是list，注意这里的U和V是矩阵，D是向量，想要恢复原矩阵，需要：
> attach(A2)
> round(u %*% diag(d) %*% t(v))
     [,1] [,2]
[1,]    1    0
[2,]    1    1
[3,]    0    1
> detach(A2)



#---> 感觉不对，u为什么不是方阵?
> A2=svd(A, nu=3);A2$u
           [,1]          [,2]       [,3]
[1,] -0.4082483  7.071068e-01  0.5773503
[2,] -0.8164966 -1.110223e-16 -0.5773503
[3,] -0.4082483 -7.071068e-01  0.5773503





6. SVD的意义
对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。

Am×n=Um×m . Bm×n . VTn×n ≈ Um×k . Bk×k . VTk×n

其中k要比n小很多，也就是一个大的矩阵A可以用三个小的矩阵Um×k,Bk×k,VTk×n来表示。
如下图所示，现在我们的矩阵A只需要灰色的部分的三个小矩阵就可以近似描述了。

由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。
也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。
同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。
下面我们就对SVD用于PCA降维做一个介绍。


1)SVD用于PCA
在主成分分析（PCA）原理总结中，我们讲到要用PCA降维，需要找到样本协方差矩阵XTX的最大的d个特征向量，然后用这最大的d个特征向量张成的矩阵来做低维投影降维。可以看出，在这个过程中需要先求出协方差矩阵XTX，当样本数多样本特征数也多的时候，这个计算量是很大的。

注意到我们的SVD也可以得到协方差矩阵XTX最大的d个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵XTX，也能求出我们的右奇异矩阵V。也就是说，我们的PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是我们我们认为的暴力特征分解。


2)另一方面，注意到PCA仅仅使用了我们SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？

假设我们的样本是m×n的矩阵X，如果我们通过SVD找到了矩阵XXT最大的d个特征向量张成的m×d维矩阵U，则我们如果进行如下处理：

X′d×n=UTd×mXm×n

可以得到一个d×n的矩阵X‘,这个矩阵和我们原来的m×n维样本矩阵X相比，行数从m减到了d，可见对行数进行了压缩。也就是说，左奇异矩阵可以用于行数的压缩。相对的，右奇异矩阵可以用于列数即特征维度的压缩，也就是我们的PCA降维。　




7.SVD小结　
SVD作为一个很基本的算法，在很多机器学习算法中都有它的身影，特别是在现在的大数据时代，由于SVD可以实现并行化，因此更是大展身手。SVD的原理不难，只要有基本的线性代数知识就可以理解，实现也很简单因此值得仔细的研究。

当然，SVD的缺点是分解出的矩阵解释性往往不强，有点黑盒子的味道，不过这不影响它的使用。


(2) SVD分解的应用

1).降维
通过上面的式子很容易看出，原来矩阵A的特征有n维。而经过SVD分解之后，完全可以用前r个非零奇异值对应的奇异向量表示矩阵A的主要特征。这样，就天然起到了降维的作用。

2).压缩
还是看上面的式子，再结合第三部分的图，也很容易看出，经过SVD分解以后，要表示原来的大矩阵A，我们只需要存U，Σ，V三个较小的矩阵的即可。而这三个较小矩阵的规模，加起来也远远小于原有矩阵A。这样，就天然起到了压缩的作用。






8. 问题
(1)我想问下现在的降维技术我知道的有，奇异值分解，非负矩阵分解，典型关联分析，这三种在使用中哪种更有优势，运行速度更快？
首先，典型关联分析主要是做关联分析的，降维的话不算它最重要的目的。

当特征维度很高的时候，非负矩阵分解一般比奇异值分解快一些，毕竟只有两个矩阵要求。

当特征维度不高的时候，速度差不多，这时推荐奇异值分解，因为非负矩阵分解是基于损失函数得到的近似矩阵。而奇异值分解是完全确定的矩阵。









ref:
1. https://www.cnblogs.com/pinard/p/6251584.html
	https://zhuanlan.zhihu.com/p/31386807
2. https://blog.csdn.net/Jakob_Hu/article/details/91841729




========================================
|-- 矩阵的LU分解 [特征值与特征向量] //todo
----------------------------------------

LU分解的主要用途包括求解矩阵的逆，求解线性方程组等


1. LU分解

(1) 
假定我们能把矩阵A写成下列两个矩阵相乘的形式：A=LU，其中L为下三角矩阵，U为上三角矩阵。

这样我们可以把线性方程组Ax= b写成 Ax= (LU)x = L(Ux) = b。
令Ux = y，则原线性方程组Ax = b可首先求解向量y，使Ly = b，然后求解 Ux = y，从而达到求解线性方程组Ax= b的目的。


LU分解的用处: 分解为 L 和 U 之后，先解 Ly=b 再解 Ux=y 会变得简单许多。
Solving Ax=b becomes LUx=b :
1). find A=LU
2). solve Ly=b
3). solve Ux=y


(2) 高斯消元法: 不允许行交换
回顾我们手工求解这个线性方程组的做法，首先将矩阵 A 行之间进行加减，将 A 矩阵转化为一个上三角矩阵U，然后从下往上将未知数一个一个求解出来, 这就是高斯消元法。
Gaussian Elimination transform a linear system into an upper triangular one by applying linear transformations on the left. It is triangular triangularization.

换个角度看：将 A 做高斯消元得到 U 可以看成是对 A 连续做基础行变换: Lm−1.…L2.L1.A=U

L is unit lower-triangular: L 的对角线上的元素全为1！

实际上，矩阵行变换等价于左乘一个单位矩阵，（因为行变换操作可以直接体现在左乘的单位矩阵上），我们将左乘的单位矩阵最终等价变换后得到的矩阵命名为 M ,显然， M 是一个下三角矩阵，U是一个上三角矩阵。
M.A=U
M和L互为逆矩阵，L=M^-1, 则 A=L.U



意义:https://blog.csdn.net/weixin_30748995/article/details/96204183

LU分解在本质上是高斯消元法的一种表达形式，我们只需要将消元过程中的消元乘数写在相应的位置就可以得到L，使用这种方式可以减少消元的操作步骤，且使得消元思路清晰




(3) 高斯消元法: 允许行交换
由于要确保 A 转化后的第一行的第一项不为 0, 第二行的第二项不为 0, 第三行…… 因此， A 前面应该再加上一个行与行之间进行交换的矩阵 P。
因此，LU 分解的公式又可以写成：PA = LU

P 是 permutation matrix, L 是 lower triangular matrix, U 是 upper triangular matrix. 
有些书将置换矩阵 P 放置在等号右边。
比如这个矩阵在线计算器( https://matrixcalc.org/en/ )默认的就是将 P 放置在矩阵 L 的前面。不妨利用这个工具测试一下 LU 分解的正确性。


R语言中就是A = PLU( 而不是PA = LU ): http://www.cocoachina.com/articles/112034
library(Matrix)
A <- matrix(c(4, 3, -2, 5, 2, -4, 6, 1, -1, 2, -5, 6, 3, 5, -2, -3), nrow = 4);A
B <- matrix(c(16.9, -14, 25, 9.4), nrow = 4);B

luA <- lu(A)
elu <- expand(luA) # 这个就是对A的LU分解， A=P.L.U #其中P是的作用是行交换
(L <- elu$L)
(U <- elu$U)
(P <- elu$P)
#[1,] . . | .
#[2,] . . . |
#[3,] . | . .
#[4,] | . . .

#
# A=P.L.U
# A.x=B, P.L.Ux=B
# 记y=U.x, P.L.y=B, y=(P.L)^-1.B=L^-1 . P^-1 .B
# 然后求解 x=u^-1.y
(Y <- solve(L) %*% solve(P) %*% B)
(X <- solve(U) %*% Y)
#4 x 1 Matrix of class "dgeMatrix"
#      [,1]
#[1,]  4.5
#[2,]  1.6
#[3,] -3.8
#[4,] -2.7

# check if A.x=b;
A %*% X -B








(4)
如果 A 是对称正定矩阵，L 和 U 正好是互为转置，差别在于对应的行存在倍数关系，这种那个倍数关系可以用对角线矩阵来实现，所以， LU 分解就可以写成：A=L.D.L^T

将对角线矩阵均摊到两边，公式可以转化为： A=L.D.L^T=(LD^0.5).(D^0.5.L^T)=U^T.U
其中，矩阵 U 是上三角矩阵，这个就是 Cholesky decomposition，这个分解方法有点像求实数的平方根。同时，该分解方法的计算量只有 LU 分解的一半。










========================================
第七部分 线性空间
----------------------------------------




========================================
|-- 向量空间
----------------------------------------
1. 定义 
设V是非空的n维向量集合，如果集合V对于向量的加法和数乘运算满足以下条件
1) 对于任意的a,b属于V, 有a+b属于V;
2) 对于任意的a属于V，Lambda属于R，有Lambda.a属于V;
则称V为向量空间。

封闭：集合中的任意两个向量经过向量的加法和数乘运算后得到的向量仍是集合中的向量。
所以向量空间的定义也可以表述为: 对向量的加法和数乘运算封闭的非空向量集合。


(2)
齐次线性方程组的解的全体 S={x| Ax=0} 是向量空间。
非齐次线性方程组的解的全体 S_bar={x| Ax=b} 不是向量空间。
证明:因为非齐次不封闭。
对于任意的ita1, ita2 属于S_bar, L属于R，有
A(ita1+ita2)=Aita1+Aita2=b+b=2*b != b, 对加法和数乘不封闭。



例1: 证明等价的向量组生成的向量空间相等。
证明: 设向量组a1,...,am与向量组b1,...,bs等价，且
V1={x=L1.a1+...+Lm.am|L1,...,Lm属于R}
V2={x=mu1.b1+...+mus.bs|mu1,...,mus属于R}
V1和V2是向量空间，证明V1=V2.

设 x属于V1，则x可由a1,...,am线性表示。
因为a1,..,am可由b1,...,bs线性表示，因而x可由b1,..,bs线性表示，所以x属于V2.
也就是说 x属于V1，则x属于V2，因此 V1属于V2。
同理可证 V2属于V1.
所以V1=V2




2. 向量空间的基和维数
(1) 定义: 设V为向量空间，如果
1) 在V中有r个向量a1,...,ar线性无关;
2) V中任意一个向量a可由向量组a1,...,ar线性表示,
则称a1,...,ar为向量空间V的一个 基， 
r称为向量空间V的 维数，
并称V是r维向量空间。


只含有一个零向量的集合{0}也是一个向量空间，这个向量空间没有基，规定它的维数为零，并称之为零维向量空间。


(2)类比概念
向量空间V  <=> 向量组 
向量空间V的基  <=> 最大无关组 
V的维数 <=> 向量组的秩
所以，向量空间V的基不唯一，但是维数是唯一确定的。

设V是r维向量空间，则V中任意r个线性无关的向量都是V的一个基。


(3) 类比方程组
齐次线性方程组 Ax=0，r(A)=r，则自由变量个数为 n-r 个。
基础解系 e1,...,e(n-r)
任意解可以表示为 e=k1.e1+...+k(n-r).e(n-r), k1,...,k(n-r)属于R;

从而基础解系e1,...,e(n-r)就是解空间S={x|Ax=0}的一个基，所以解空间S是n-r维向量空间。
解空间S可以表示为 S={x=k1.e1+...+k(n-r).e(n-r) | k1,...,k(n-r)属于R}


如果r(A)=n时，方程组Ax=0只有0解，因而没有基础解系。此时解空间S只含有一个零向量，为零维向量空间。





3. 基变换公式，坐标变换公式
(1) 某个基下的坐标

(2) 存在可逆矩阵P和Q，p^-1=Q, 使得n维向量空间V的2个基a1,...,an和b1,...,bn能互相线性表示
(b1,...,bn)=(a1,...,an).P (I)
及 
(a1,...,an)=(b1,...,bn).Q (II)
则称以上2个公式为基变换公式。
P称为由基a1,...,an到基b1,...,bn的 过渡矩阵。
Q称为由基b1,...,bn到基a1,...,an的 过渡矩阵。



(3) 设 n维向量空间V的向量a，
在基a1,...,an下的坐标是[x1 ... xn]T, 
在基b1,...,bn下的坐标是[y1 ... yn]T, 
如果这两个基满足关系(I)和(II)，则有
a=(a1, a2, ..., an).[x1 x2 ... xn]T; 
及
a=(b1, b2, ..., bn).[y1 y2 ... yn]T= (a1,...,an).P.[y1 y2 ... yn]T

由于a在基下的表示是唯一的，所以的到 坐标变换公式:
[x1 x2 ... xn]T = P.[y1 y2 ... yn]T
或 
P^-1.[x1 x2 ... xn]T = [y1 y2 ... yn]T = Q.[x1 x2 ... xn]T (III)





========================================
|-- 基、维数与坐标 //todo
----------------------------------------









========================================
|-- 线性空间的同构 //todo
----------------------------------------






========================================
|-- 线性变换 //todo
----------------------------------------







========================================
***************
----------------------------------------



========================================
线性代数与R语言
----------------------------------------
https://blog.csdn.net/hnu_lb/article/details/38419405

1. 矩阵的运算
(1)矩阵的相乘
A %*% B



(2) 求行列式
> A=matrix( c(2,1, 1,
1, 2, 1,
1, 1, 2), nrow=3, byrow = T);A

     [,1] [,2] [,3]
[1,]    2    1    1
[2,]    1    2    1
[3,]    1    1    2

> det(A) #求矩阵的行列式
[1] 4




2.矩阵的逆
> set.seed(1)
> A=matrix(rnorm(9),nrow=3,ncol=3)
> A
           [,1]       [,2]      [,3]
[1,] -0.6264538  1.5952808 0.4874291
[2,]  0.1836433  0.3295078 0.7383247
[3,] -0.8356286 -0.8204684 0.5757814

> solve(A)
            [,1]        [,2]       [,3]
[1,] -0.50015875  0.82896132 -0.6395669
[2,]  0.45439113 -0.02930499 -0.3470881
[3,] -0.07838637  1.16130885  0.3139817

# 验证
> round(A %*% solve(A))
> round( solve(A) %*% A )
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1



solve()函数也可以用来求解方程组ax=b。
> A=matrix( c(1, 1, 3, 12,
            1, 1, -5, -12,
            2, 1, -1, 1), nrow=3, byrow = T);A

     [,1] [,2] [,3] [,4]
[1,]    1    1    3   12
[2,]    1    1   -5  -12
[3,]    2    1   -1    1

> solve(A[,1:3], A[,4])
[1] 1 2 3




3. 矩阵的特征值和特征向量
矩阵A的谱分解如下：A=UΛU’，其中U的列为A的特征值所对应的特征向量，在R中可以用eigen()函数得到U和Λ。
> A=matrix( c(2,1, 1,
1, 2, 1,
1, 1, 2), nrow=3, byrow = T);A

     [,1] [,2] [,3]
[1,]    2    1    1
[2,]    1    2    1
[3,]    1    1    2

> det(A) #求矩阵的行列式
[1] 4

> Aeigen=eigen(A)
> Aeigen
eigen() decomposition
$values # 特征值
[1] 4 1 1

$vectors
           [,1]       [,2]       [,3]
[1,] -0.5773503  0.0000000  0.8164966
[2,] -0.5773503 -0.7071068 -0.4082483
[3,] -0.5773503  0.7071068 -0.4082483


> Aeigen$vectors %*% diag(Aeigen$values) %*% solve(Aeigen$vectors)
     [,1] [,2] [,3]
[1,]    2    1    1
[2,]    1    2    1
[3,]    1    1    2




## 奇异值分解
> A2=svd(A);A2
$d
[1] 4 1 1

$u
           [,1]       [,2]          [,3]
[1,] -0.5773503  0.8164966 -5.041791e-17
[2,] -0.5773503 -0.4082483 -7.071068e-01
[3,] -0.5773503 -0.4082483  7.071068e-01

$v
           [,1]       [,2]       [,3]
[1,] -0.5773503  0.8164966  0.0000000
[2,] -0.5773503 -0.4082483 -0.7071068
[3,] -0.5773503 -0.4082483  0.7071068



> A2$u%*%diag(A2$d)%*%t(A2$v)
     [,1] [,2] [,3]
[1,]    2    1    1
[2,]    1    2    1
[3,]    1    1    2








4. 矩阵如何转为对角矩阵? 
# 矩阵A
A=matrix(c(0,-1,1,-1,0,1,1,1,0), nrow=3)
A
# 特征值和特征向量
eigen(A)

# P^-1.A.P=diag(L1,L2,L3),则P，我算出来的和eigen(A)不很一致:
P=matrix( c( c(-1,-1,1)/sqrt(3), c(1,0,1)/sqrt(2), c(-0.5,1,0.5)/sqrt(0.5**2*2+1) ),nrow=3 )
P
solve(P) %*% A %*% P

# 该转换矩阵本身是正交矩阵，正交矩阵
# 正交矩阵 P^T.P=E，且都是规范化的。
t(P) %*% P
apply(P, 1, function(x){sum(x^2)}) #行向量是单位向量
apply(P, 2, function(x){sum(x^2)}) #列向量也是单位向量

# 正交矩阵的转置和逆相等
solve(P) - t(P)




========================================
左乘旋转矩阵的几何意义：旋转坐标轴
----------------------------------------
1. 2D 坐标的旋转
一个坐标A1(x,y)逆时针旋转alpha角度到A2(x',y'), 满足[x',y']T=A.[x,y]T时，坐标变换矩阵 A=
|cosA, -sinA|
|sinA, cosA|

推导方式:
使用极坐标辅助推导。A和B到原点的距离为r。
假设A1和原点角度为beta，则x=r*cos(b), y=r*sin(a);
则A2的角度为alpha+beta;
x'=r*cos(a+b)=r*(cos(a)*cos(b)-sin(a)*sin(b))=x*cos(a)-y*sin(a);
y'=r*sin(a+b)=r*(sin(a)*cos(b)+cos(a)*sin(b))=x*sin(a)+y*cos(a);
写成矩阵形式，[x',y']T=A.[x,y]T，可知矩阵A 如上。


只要给出旋转角度，计算出A矩阵，然后使用A矩阵分别左乘每一个点，就能计算出这个点旋转后的点坐标 这样我们就可以通过矩阵变换坐标了。
而旋转坐标系alpha度，相当于每个点旋转-alpha度。


查看A矩阵，如果alpha=90度，则A的形式为
|0, -1|
|1, 0|




(1) 而交换坐标轴和上文并不一样，不能简单的说是旋转角度，需要区别对待。
[y,x]T=A.[x,y]T 时，矩阵A=
|0 1|
|1 0|




2. 3D坐标的旋转






ref:
坐标轴的旋转矩阵: https://blog.csdn.net/TOM_00001/article/details/62054572







========================================
矩阵求导
----------------------------------------
见 ML in action, P308 附录B，有半页。


矩阵求导（Matrix Derivative）也称作矩阵微分（Matrix Differential），在机器学习、图像处理、最优化等领域的公式推导中经常用到。


布局约定（Layout conventions）
布局（Layout）：在矩阵求导中有两种布局，分别为分母布局(denominator layout)和分子布局(numerator layout)。这两种不同布局的求导规则是不一样的。

通过观察，发现分子布局和分母布局刚好是转置关系。
分子布局下与原来的分子相同，而分母布局下差一个转置。
本文采用分子布局。



1. 对向量和矩阵的求导，并不比常规的求导更难，只是要清楚这里的概念和定义。

向量函数(也就是函数组成的向量) y=[y1 y2 ... yn]T, 关于向量 x=[x1 x2 ... xn]T 的导数，记作 dy/dx=
[dy1/dx1 dy2/dx2 ... dyn/dxn]
...
[dyn/dx1 dyn/dx2 ... dyn/dxn]
这个矩阵叫做 Jacobian 矩阵。



例: 列向量 A= [ sin(x)-y,  sin(3*x)-4*y]T，
(1) A对标量x求导，得到另一个向量 [cos(x), 3*cos(3*x)]T 
(2) 如果A要对另一个向量求导，会得到一个矩阵，比如B=[x y z]T
dA/dB=
[cosx 3*cos(3x)]
[-1 -4]
[0 0]





2. 矩阵关于向量 x 的导数
很复杂，没看懂

一个 m × n 的矩阵 Y 对 一个 p × 1 的向量x求导，结果应该是一个 m × n × p 的矩阵.




//todo
维度分析




ref:
https://www.jianshu.com/p/6b64b7ee6ec2
https://blog.csdn.net/xidianliutingting/article/details/51673207


里奇微积分(Ricci Calculus)：一种计算向量求导，矩阵求导，张量求导的简单方法
https://zhuanlan.zhihu.com/p/63176747


矩阵求导术（上）
https://zhuanlan.zhihu.com/p/24709748


学习机器学习，遇到关于矩阵（矩阵求导，矩阵范数求平方之类)的
https://www.zhihu.com/question/338548610



========================================
卷积与反卷积 计算细节 //todo
----------------------------------------
https://zhuanlan.zhihu.com/p/29119239









========================================
非负矩阵分解 (Non-negative Matrix Factorization, NMF)
----------------------------------------
factorization [ˌfæktəraɪˈzeɪʃən] n. [数] 因子分解；[数] 因式分解


1. 名词解释
(1) NMF 是什么

非负矩阵分解(Non-negative Matrix Factorization, NMF)本质上说是一种矩阵分解的方法，对于任意给定的一个非负矩阵V，NMF算法能够寻找到一个非负矩阵W和一个非负矩阵H，使得 V≈W*H成立 ，从而将一个非负的矩阵分解为左右两个非负矩阵的乘积。

(2) NMF的特点？
NMF最重要的特点是非负性约束。矩阵分解的方法有很多种，如奇异值分解 (singular value decomposition, SVD) 、独立成分分析 (independent component analysis, ICA) 、主成分分析 (principal component analysis, PCA) 等。这些方法的共同特点是，即使初始矩阵 V 元素是非负的，分解出来的因子 W 和 H 中的元素往往含有负值元素。从计算科学的角度来看，分解出来的因子 W 和 H 中的元素含有负值元素并没有问题， 但负值元素通常是无法解释的。NMF约束了原始矩阵V和分解矩阵W、H的非负性，这就意味着只能通过特征的相加来实现原始矩阵V的还原，最终导致的结果是：

- 非负性会引发稀疏
- 非负性会使计算过程进入部分分解
- NMF生成的分量是没有顺序的

对比一下PCA与NMF分解图像的效果：
* PCA分解之后，每个主成分（PC）都会保留与其他PC不正交的全局特征，并且PC保留的特征是递减的。
* NMF分解之后，每个因子保留的都是局部特征，它们的权重是基本平等的。通过这张图可以看出，很多因子能与面部特征一一对应起来，例如鼻子、眼睛、嘴巴都能找到相应的因子。



NMF 和其他矩阵分解技术（例如奇异值分解（SVD）和主成分分析（PCA））在提取数据重要特征的核心目标上有共同之处。然而，NMF 有其独特的特点：
- 非负性：NMF 的一个显著特征是其非负性要求，即分解得到的矩阵 W 和 H 的所有元素都必须是非负的。这使得 NMF 特别适合处理如图像、文本等自然数据，这些数据通常只包含非负值。
- 部分数据表示：NMF 倾向于提供部分数据表示，意味着每个原始数据点可以被看作是少量基础成分的加权和。这与 PCA 等方法提供的全局数据表示不同。
- 解释性：由于非负性的约束，NMF 分解的结果通常更易于解释。在 NMF 中，原始数据集可以被视为一系列基本特征的组合，其中每个特征都有明确的物理或实际意义。




2. 公式推导
(1) 根据定义 V ≈ W.H
   H 
W  V
满足:  V(mxn) ≈ W(mxL).H(Lxn)
其中 
    W = [Wml] s.t. Wml>=0
    H = [Hln] s.t. Hln>=0
如果 L 较小，则相当于对原矩阵的维度做了压缩。
如果 L 较大，则相当于对原矩阵做了 特征提取。

就是求 V 和 W.H 的最小化距离：
* 可以求 欧氏距离 Euclidean Distance
    E(W,H) = 1/2 || V-WH ||^2 = 0.5 * 累加(i,j,   Vij - [WH]ij)^2

* 或者 相对熵 KL-Divergence
    D(V||WH) =累加(i,j,  Vij*log(Vij/[WH]ij) - Vij + [WH]ij)


(2) 理解
NMF是一个很厉害的算法，但其实质是加权和，我们可以在原理上等效为基本的线性方程：

y = a0 + a1x1 + a2x2 + ... + anxn

y构成了原矩阵中的元素，a系列值是权重，x系列变量是特征。

矩阵乘法中特征用列向量表示，权重系数用行向量表示，所以成了图中所看到的样子。




(3) 怎么获取 W 和 H？
Two different multiplicative algorithms for non-negative matrix factorization are analyzed and one algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence.

1) Multiplicative Update Rules

锁定一个矩阵，然后更新另一个矩阵
- 求 W(k+1) 使得 f(W(k+1), Hk) <= f(Wk; Hk)
- 求 H(k+1) 使得 f(W(k+1), H(k+1)) <= f(W(k+1); Hk)

2) Projected Gradient Descent

3) Sparse NMF









ref:
https://www.bilibili.com/video/BV1EK411X75s/?spm_id_from=333.337.search-card.all.click
视频中用到的论文和资料：
OG 算法原文：https://www.semanticscholar.org/paper/6fb07b90b7fd2785ffec0da1069e75c53f7313c2
Projected Gradient Methods 原文：https://doi.org/10.1162/neco.2007.19.10.2756
NON-NEGATIVE SPARSE CODING (视频中的第一个 sparse NMF 算法）原文：https://doi.org/10.48550/arXiv.cs/0202009
Sparseness constraints (视频中的第二个 sparse NMF 算法）原文：https://www.semanticscholar.org/paper/ede3af3637977988b8cbb330c294183e919b7d5a
NMF Github 代码：https://github.com/MingSheng92/NMF/blob/173de9c17108e39d9801db219277f402543b09fb/NMF.py

非负矩阵分解（NMF）迭代公式推导证明 https://zhuanlan.zhihu.com/p/340774022

非负矩阵分解算法综述 http://oa.ee.tsinghua.edu.cn/~zhangyujin/Download-Paper/C151=Dzxb-08.pdf





========================================
----------------------------------------





========================================
----------------------------------------





========================================
----------------------------------------
