transformer






========================================
Transformer 环境配置
----------------------------------------
Transformer的优势在于，它可以不受梯度消失的影响，能够保留任意长的长期记忆。
而RNN的记忆窗口很短；
LSTM和GRU虽然解决了一部分梯度消失的问题，但是它们的记忆窗口也是有限的。

Recurrent neural networks (RNN) are also capable of looking at previous inputs too. But the power of the attention mechanism is that it doesn't suffer from short term memory. RNNs have a shorter window to reference from, so when the story gets longer, RNNs can't access words generated earlier in the sequence. This is still true for Gated Recurrent Units (GRU) and Long-short Term Memory (LSTM) networks, although they do a bigger capacity to achieve longer-term memory, therefore, having a longer window to reference from. The attention mechanism, in theory, and given enough compute resources, have an 「infinite」 window to reference from, therefore being capable of using the entire context of the story while generating the text.



1. 用什么环境？
(1) 概述
- py 哪个版本？ >3.6
- 哪个框架/包？
	TensorFlow：由Google开发的开源机器学习框架，它提供了高级API如Keras，可以简化构建和训练神经网络的过程。
	PyTorch：由Facebook开发的开源机器学习库，它以动态计算图和易用性而闻名。
	Transformers：由Hugging Face团队开发的一个专门针对NLP任务的库，它包含了多种预训练模型，包括Transformer及其变体。
- 哪个编辑器/IDE？
	IDE（集成开发环境）
	Visual Studio Code (VS Code)：轻量级但功能强大的编辑器，支持多种编程语言，拥有丰富的插件生态。
	Jupyter Notebook：适合进行数据分析和机器学习的交互式开发环境，可以逐步执行代码并查看结果。
	PyCharm：专为Python开发设计的IDE，提供了强大的代码分析和智能提示功能，适合大型项目开发。
	Google Colab：基于云端的Jupyter Notebook环境，允许你使用免费的GPU和TPU资源进行机器学习实验。



(2) 安装和配置
1)确保你安装了Python和pip（Python包管理器），然后可以通过pip安装所需的包，例如：
$ pip3 install tensorflow
直接安装GPU版本：
$ pip3 install tensorflow[and-cuda]

$ pip3 install torch
$ pip3 install transformers

2)如果你选择使用Jupyter Notebook，可以通过pip安装并启动：
pip3 install notebook
jupyter notebook

3)对于TensorFlow或PyTorch，你可能还需要安装对应的GPU版本，以利用图形处理单元加速计算。

4) 选择建议
如果你是初学者，Jupyter Notebook可以帮助你逐步学习和实验。

如果你打算进行更复杂的项目开发，PyCharm或VS Code可能更适合，因为它们提供了更好的项目管理和调试工具。
Transformers库与TensorFlow和PyTorch都兼容，因此你可以根据自己的喜好选择。
最后，选择哪个工具取决于你的个人习惯和项目需求。大多数深度学习任务在这些工具上都能完成，关键是找到最适合你的工作流程。


For win11:
 ERROR: Could not find a version that satisfies the requirement tensorflow-intel==2.13.1; platform_system == "Windows" (from tensorflow) (from versions: 0.0.1, 2.10.0.dev20220728, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0)
ERROR: No matching distribution found for tensorflow-intel==2.13.1; platform_system == "Windows" (from tensorflow)


For CentOS7.9:
$ pip list| wc
    154     308    5517
$ which pip
~/soft/python3/python-3.10.14/bin/pip

$ pip install tensorflow

版本：
$ pip list| grep -i tensorflow
tensorflow                   2.16.1
tensorflow-io-gcs-filesystem 0.36.0

$ pip list| grep -i torch
torch                        2.2.2

$ pip list| grep -i keras
keras                        3.3.2

$ pip list| grep -i transformers
transformers                 4.40.0



(3) 启动
$ cd /home/wangjl/data/web/docs/code/learnDL
$ jupyter notebook --no-browser --port 8201 --ip=10.10.117.158

开放端口
$ sudo iptables -I INPUT -p tcp --dport 8201 -j ACCEPT

访问：
http://localhost:8201
http://127.0.0.1:8201/?token=6bfbeb60ba9cb9a55fcbe56fcd06dfd37ff5e800d21b4193

查询登陆方式：
$ jupyter notebook list
Currently running servers:
http://10.10.117.158:8201/?token=6bfbeb60ba9cb9a55fcbe56fcd06dfd37ff5e800d21b4193 :: /picb/jinlab/wangjl/web/docs/code/learnDL










========================================
怎么学 Transformer？教程、教材
----------------------------------------
- 入门教程
- 深入书籍
- 深入教程
- 项目
- 源码阅读


1. 问Kimi：Transformer 怎么学？
Transformer是一种深度学习模型，它在自然语言处理（NLP）领域非常流行，尤其是在机器翻译任务中。要学习Transformer模型，你可以按照以下步骤进行：

1)基础知识：首先，你需要对深度学习和自然语言处理有基本的了解。这包括神经网络的工作原理、词嵌入（word embeddings）、序列到序列（seq2seq）模型等。

2)理解注意力机制：Transformer的核心是注意力机制（Attention Mechanism），它允许模型在处理序列时关注不同的部分。理解注意力机制的数学原理和它如何工作是非常重要的。

3)阅读原始论文：阅读Vaswani等人的原始论文《Attention Is All You Need》可以帮助你深入理解Transformer架构的细节。

4)学习模型架构：了解Transformer的各个组成部分，包括编码器（Encoder）、解码器（Decoder）以及它们是如何交互的。

5)编码实践：通过实践来加深理解。使用Python和深度学习框架（如TensorFlow或PyTorch）来实现一个简单的Transformer模型。
**

6)研究变体和优化：了解Transformer的不同变体，如BERT（Bidirectional Encoder Representations from Transformers）、GPT（Generative Pre-trained Transformer）等，以及它们是如何对原始模型进行优化的。

7)项目实践：参与一个实际的NLP项目，如机器翻译、文本摘要、问答系统等，将Transformer模型应用到实际问题中。

8)持续学习：NLP和深度学习领域在不断进步，持续关注最新的研究和技术进展是非常重要的。

9)社区参与：加入相关的在线论坛和社区，如Reddit、Stack Overflow、GitHub等，与其他学习者和专家交流经验。

10)高级主题：一旦你对基础知识有了扎实的掌握，可以探索更高级的主题，如模型压缩、多模态学习、长序列处理等。

记住，学习深度学习模型是一个逐步的过程，需要时间和实践来逐渐掌握。不要急于求成，而应该一步一步地建立起你的知识体系。





2. 入门教程
(1) 8个资料
1) B站强推！一口气学完AI万能模型【Transformer】入门到精通 ...
	https://www.bilibili.com/video/BV1f1421S7t6/?vd_source=826befc4ac0d0fa3b98deaa3efc7f800
2) 【学习】从零入门Transformer_transformer零基础-CSDN博客
	https://blog.csdn.net/wqwqqwqw1231/article/details/115179188
3) Transformer从零详细解读(可能是你见过最通俗易懂的讲解 ...
	https://www.bilibili.com/video/BV1Di4y1c7Zm/?vd_source=826befc4ac0d0fa3b98deaa3efc7f800
4) 目前B站最完整的【Transformer从入门到精通】讲解，居然 ...
	https://www.bilibili.com/video/BV16M411C77T/
5) Hello! · Transformers快速入门
	https://transformers.run/
6) Transformers 快速入门教程 - Transformers 官方中文文档教程
	https://transformers-doc.100sta.com/docs/transformers/v4.31.0/zh/quicktour
7) Transformers 快速入门 | PyTorch
	https://pytorchchina.com/2020/02/29/transformers-quick-start/
8) 保姆级教程：图解Transformer - Jack Cui
	https://cuijiahua.com/blog/2021/01/dl-basics-3.html
9) 保姆级！从零开始学习Transformer模型教程！（文末送书 ...


(2)学习Transformer的入门教程有很多资源，以下是一些推荐的教程和资源：

1)基础概念和架构理解：
从RNN和NLP的Seq2Seq模型开始理解，因为Transformer最初是作为Seq2Seq模型的一个改进而提出的。
理解word embedding、Position encoding和Self-Attention机制，这些都是构建Transformer模型的关键组件。

2)编码实践：
PyTorch是一个推荐的框架来开始学习Transformer，因为它有丰富的文档和社区支持。
通过实践编码来构建Encoder和Decoder，理解每个组件的功能和它们是如何协同工作的。

3)视频教程：
B站上有一系列完整的Transformer从入门到精通的视频教程，适合通过视频学习的用户。

4)官方文档和快速入门：
Transformers库由Hugging Face开发，官方提供了快速入门教程，帮助用户理解如何使用该库。

5)图解教程：
如果你更喜欢图解和直观的解释，可以查看Jack Cui的保姆级教程，它通过图解的方式帮助你理解Transformer。

6)实战项目：
通过构建语言模型和其他NLP任务来实践Transformer的使用，这有助于加深理解并提高解决实际问题的能力。
7)持续学习：
随着对Transformer模型的理解加深，可以逐步学习更高级的主题，如模型优化、压缩和多模态学习。
8)社区参与：
加入相关的在线论坛和社区，如Reddit、Stack Overflow、GitHub，与其他学习者和专家交流经验。

这些资源将帮助你从基础概念开始，逐步深入到Transformer模型的实现和应用。记住，实践是学习的关键，因此尽可能多地编码和参与项目实践。










100. 疑难问题

(1)学 Transformer  需要学习 keras 吗？
学习Transformer模型不一定需要先学习Keras，但Keras可以作为一个非常有用的工具来简化你的学习过程，尤其是如果你之前没有太多深度学习框架的使用经验。以下是Keras的一些优势，以及为什么它可能对你学习Transformer有帮助：

1)易用性：Keras是TensorFlow的一个高级API，它以简洁和易用而著称。Keras提供了一种快速构建和训练深度学习模型的方式。

2)快速原型开发：Keras允许你快速实验不同的模型架构和超参数，这对于学习和探索Transformer模型非常有帮助。

3)预训练模型：Keras提供了许多预训练模型，这些模型可以直接用于迁移学习，或者作为新模型的起点。

4)社区支持：Keras有一个活跃的社区，你可以找到大量的教程、文档和论坛讨论，这些都是学习过程中的宝贵资源。

5)与TensorFlow的兼容性：Keras完全兼容TensorFlow，这意味着你可以在Keras中构建模型，然后轻松地将其转换为TensorFlow的低级API进行更深入的定制和优化。

6)跨平台：Keras支持多种后端，包括TensorFlow、CNTK和Theano，这为你提供了灵活性，可以根据需要选择或切换不同的后端。

然而，如果你选择使用PyTorch来学习Transformer，那么你可能不需要使用Keras，因为PyTorch本身提供了一套完整的工具和API来构建和训练模型。PyTorch的动态计算图也非常强大，对于某些类型的模型和研究工作来说可能更加灵活。

总之，学习Keras可以为你提供一个平滑的学习曲线，特别是在你刚开始接触深度学习和Transformer模型时。但随着你对深度学习的深入理解，你可能会发现直接使用TensorFlow或PyTorch的低级API提供了更多的控制和灵活性。无论你选择哪种工具，关键是理解背后的原理和概念，这样你就可以根据项目需求灵活选择最合适的工具。









========================================
|-- 核心概念、数学原理
----------------------------------------

1. 相关概念

input、embedding、encoder、decoder、linear+softmax、output
而在这其中真正属于 transformer 的就是  encoder 与 decoder 这两部分。
实际上呢，这两个东西还是会被分开用的像 BERT 用的就是 Encoder，而 GPT 用的就是 Decoder，不是必须两个一起用的。


(1) Encoder-Decoder 架构
Transformer的结构也采用了 Encoder-Decoder 架构。但其结构更加复杂，论文中Encoder层由6个Encoder堆叠在一起，Decoder层也一样。


- Self-attention层
Encoder包含两层，一个Self-attention层(「Multi-Head Attention」)和一个前馈神经网络层(「feed forward」)，Self-attention层能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。

Decoder也包含Encoder提到的两层网络，但是在这两层中间还有一层Attention层，帮助当前节点获取到当前需要关注的重点内容。



(2) Encoder层

- Embedding层
- Self-attention层
- Multi-Headed Attention: "多头"注意力
	有多个Query/Key/Value权重矩阵集 (Transformer使用八个注意力头)。
- The Residuals and Layer normalization
	一个encoder中的细节：在每个encoder中都有一个残差连接，并且都跟随着一个Layer Normalization（层-归一化）步骤。

这几乎就是Encoder的全部。
Encoder就是用来给input一个比较好的embedding，使用self-attention来使一个词的embedding包含了上下文的信息，而不是简单的查look-up table。Transformer使用了多层(6层)的Encoder是为了把握一些高阶的信息。



(3) Decoder层

从更高的角度来看，Transformer的Decoder作用和普通seq2seq一样：
	从<Start>开始，基于之前的Decoder输出，以及Encoder输出的带注意力权重的embedding，来预测下一个词的概率分布。


- Masked Multi-Head Attention

和Encoder一样，Decoder先经过embedding+positional encoding之后得到了一个embedding，输入到multi-head attention中。

和前面不同的是，Decoder的self-attention层其实是「masked」 multi-head attention。mask表示掩码，它对某些值进行掩盖。这是为了防止Decoder在计算某个词的attention权重时“看到”这个词后面的词语。

Since the decoder is auto-regressive and generates the sequence word by word, you need to prevent it from conditioning to future tokens. For example, when computing attention scores on the word "am", you should not have access to the word "fine", because that word is a future word that was generated after. The word "am" should only have access to itself and the words before it. This is true for all other words, where they can only attend to previous words.

「Look-head mask」 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的「解码」输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。

那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为 「-inf」 。把这个矩阵加在每一个序列上，就可以达到我们的目的：

加上-inf的目的是，做softmax之后-inf会变成0：

这个mask是Decoder中self-attention和Encoder中的self-attention唯一有区别的地方。



- 第二个Multi-head Attention -- 普通attention

For this layer, the encoder's outputs are keys and values, and the first multi-headed attention layer outputs are the queries. This process matches the encoder's input to the decoder's input, allowing the decoder to decide which encoder input is relevant to put a focus on.





(4) 重要参数
在 Attention 机制中，Q、K、V 分别代表 Query（查询）、Key（键）、Value（值）。

Q：指的是query，相当于decoder的内容
K：指的是key，相当于encoder的内容
V：指的是value，相当于encoder的内容

Q、K、V 是 Attention 机制中的核心概念，它们分别代表查询、键和值，用于计算注意力权重和生成上下文向量，从而帮助模型关注到最相关的信息。








4. 数学原理

学完这个教程，小白也能构建Transformer，DeepMind科学家推荐 https://baijiahao.baidu.com/s?id=1755605580933785244&wfr=spider&for=pc

传送门：
https://e2eml.school/transformers.html#softmax
更多教程：
https://e2eml.school/blog.html






ref:
https://www.jianshu.com/p/052902aa9ee4

如何从浅入深理解transformer？ https://www.zhihu.com/question/471328838/answer/3429214821

深度学习算法面试常见基础问题（3）transformer基础 https://zhuanlan.zhihu.com/p/628614623



original paper:
Attention is All you need: https://arxiv.org/abs/1706.03762

Illustrated Guide to Transformers- Step by Step Explanation: https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0


Self-Attention 与 Transformer: https://www.6aiq.com/article/1584719677724













========================================
|-- 背景：文字->字向量的转换: Word2Vec //todo
----------------------------------------
1. 得到的 Xembedding  的维度是 [batch size, sequence length, embedding dimension]
(1) batch size 就是 batch 的大小，这里只有一句话，所以 batch size 为 1，sequence length 是句子的长度，一共 7 个字，所以输入的数据维度是 [1, 7]。
(2) embedding dimension 的大小由 Word2Vec 算法决定，Tranformer 采用 512 长度的字向量。

所以，“我们为什么工作”的 Xembedding  的维度是 [1, 7, 512]。

输入的我们为什么工作，可以用一个矩阵来简化表示：[batch size, seq.length, embed.dim]
	行数是 seq.length：第一行第一个字符，第2行第2个字符，...
	列数是 embedding dimension=hidden dimension，就是一个字符的编码位数




========================================
|-- 1. 位置嵌入（positienal encoding）
----------------------------------------
位置信息很重要，比如：“吃饭没”，“没吃饭” 差异很大。

Tranformer 采用的是 sin-cos 规则，使用了 sin 和 cos 函数的线性变换来提供给模型位置信息
	PE(pos, 2i) = sin(pos/10000^(2i/dmodel)
	PE(pos, 2i+1) = cos(pos/10000^(2i/dmodel)

上式中 pos 指的是句中字的位置，取值范围是 [0, max sequence length)，i 指的是字嵌入的维度, 取值范围是 [0, embedding dimension)。 就是 embedding dimension 的大小。

上面有 sin 和 cos 一组公式，也就是对应着 embedding dimension 维度的一组奇数和偶数的序号的维度，从而产生不同的周期性变化。


(1) 可以用代码，简单看下效果。

# 导入依赖库
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
 
def get_positional_encoding(max_seq_len, embed_dim):
    # 初始化一个positional encoding
    # embed_dim: 字嵌入的维度
    # max_seq_len: 最大的序列长度
    positional_encoding = np.array([
        [pos / np.power(10000, 2 * i / embed_dim) for i in range(embed_dim)]
        if pos != 0 else np.zeros(embed_dim) for pos in range(max_seq_len)])
    positional_encoding[1:, 0::2] = np.sin(positional_encoding[1:, 0::2])  # dim 2i 偶数
    positional_encoding[1:, 1::2] = np.cos(positional_encoding[1:, 1::2])  # dim 2i+1 奇数
    # 归一化, 用位置嵌入的每一行除以它的模长
    # denominator = np.sqrt(np.sum(position_enc**2, axis=1, keepdims=True))
    # position_enc = position_enc / (denominator + 1e-8)
    return positional_encoding
    
positional_encoding = get_positional_encoding(max_seq_len=100, embed_dim=16)

plt.figure(figsize=(10,10))
sns.heatmap(positional_encoding)
plt.title("Sinusoidal Function")
plt.xlabel("hidden dimension")
plt.ylabel("sequence length")

可以看到，位置嵌入在 embedding dimension （也是hidden dimension ）维度上随着维度序号增大，周期变化会越来越慢，而产生一种包含位置信息的纹理。


就这样，产生独一的纹理位置信息，模型从而学到位置之间的依赖关系和自然语言的时序特性。

最后，将 Xembedding  和 位置嵌入 相加，送给下一层。


Xembedding=EmbeddingLookup(x) + PositionalEncoding, Xembedding ∈ R^(batch size*seq.len*embed.dim)  (eq.2)





========================================
|-- 2. 自注意力层（self attention mechanism） //todo
----------------------------------------
1. Q,K,V矩阵
分配三个权重，Xembedding在WQ, WK, WV ∈ R^(embed.dim.*embed.dim) 上线性映射后，形成三个矩阵，为Q，K，V，和线性变换之前维度一致。

Q=Linear(Xembedding)=Xembedding.WQ
K=Linear(Xembedding)=Xembedding.WK
V=Linear(Xembedding)=Xembedding.WV  (eq.3)




2. 多头注意力机制
(1) 超参数h
首先定义一个超参数h，也就是head的数量，注意 embedding dimension 必须整除于h.
	因为我们要把 embedding dimension 分割成 h 份 -- 按列。
	head size = embedding dimension / num of heads.

分割后Q，K，V的维度为[batch size, sequence length, h, embedding dimension/h]
之后我们把Q，K，V中的sequence length, h进行一下转置，为了方便后续的计算：
转置后的Q，K，V的维度为[batch size, h, sequence length, embedding dimension/h]

多头的意义在于，Q.K^T得到的矩阵就叫注意力矩阵，它可以表示每个字与其他字的相似程度。因为，向量的点积值越大，说明两个向量越接近。

(2) 注意力矩阵
[batch size, h,     [batch size, h,       [batch size, h,
len, embed.dim/h] .  embed.dim/h, len] =   len, len]

6行3列 . 3行6列 = 6行6列  -->沿着最后一个维度做 softmax 归一化


注意力矩阵的第一行就是指的是第一个字与这六个字的相关性，第二行同理。

	Attention(Q, K, V) = softmax( Q.K^T / sqrt(dk) ).V  (eq.4)

上式就是自注意力机制，我们先求 Q.K^T，也就是求出注意力矩阵，然后用注意力矩阵给V矩阵加权，
sqrt(dk)是为了把注意力矩阵变成标准正态分布，使得 softmax 归一化之后的结果更加稳定，以便于反向传播的时候获取平衡的梯度。


(3) 对V矩阵加权
注意力矩阵的行和为1。
注意力矩阵的作用就是一个注意力权重的概率分布，我们使用注意力矩阵的权重给V进行加权。

6行6列  .  6行3列 = 6行3列

解释：注意力矩阵取第一行（和为1）依次点乘V的列，
	矩阵V的每一列表示每个字向量的数学表示，
	从而使每个字向量都包含当前句子内所有字向量的信息。
注意：点成后V向量的维度不变，还是 [batch size, h, sequence length, embedding dimension/h]


需要注意的是，在上面 self attention 的计算过程中，我们通常使用 mini batch ，也就是一次计算多句话，上文举例只用了一个句子。

每个句子的长度是不一样的，需要按照最长的句子的长度统一处理。对于短的句子，进行 Padding 操作，一般我们用 0 来进行填充。

行：有效句长区域
列：有效句长区域
超过行、列的部分是无效区域，需要masking




========================================
|-- 3. 残差链接和层归一化 //todo current
----------------------------------------
加入了残差设计和层归一化操作，目的是为了防止梯度消失，加快收敛。

1. 





ref: https://cuijiahua.com/blog/2021/01/dl-basics-3.html



========================================
|-- Encoder-Decoder 结构: 编码器-解码器 //todo
----------------------------------------
Transformer 中抛弃了传统的 CNN 和 RNN，整个网络结构完全由 Attention 机制组成，并且采用了 6 层 Encoder-Decoder 结构。















========================================
py 深度学习包 入门实例
----------------------------------------

$ nvidia-smi
Mon Apr 22 19:32:41 2024       
NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2

显示：CUDA 12.2


tf 和 cuda 版本对应关系: https://blog.csdn.net/FL1768317420/article/details/134840200
	CUDA 12.2 对应 tensorflow-2.15.0




========================================
|-- Pytorch：入门实例 //好安装，推荐使用
----------------------------------------

查看显卡设备:

Python 3.7.17 (default, Feb  7 2024, 03:28:24) 
>>> import torch
>>> torch.cuda.is_available() #是否支持显卡
True
>>> torch.cuda.get_device_name(0) #显卡设备名称
'NVIDIA A800 80GB PCIe'

import torch.cuda
device = torch.device("cuda")
memory_size = torch.cuda.get_device_properties(device).total_memory
print("显存大小:", memory_size)  #84987740160
frequency = torch.cuda.get_device_properties(device).clock_rate
	报错: AttributeError: 'torch._C._CudaDeviceProperties' object has no attribute 'clock_rate'
print("核心频率:", frequency)

>>> tmp=torch.cuda.get_device_properties(device)
>>> tmp.multi_processor_count
108
>>> tmp.major
8
>>> tmp.minor
0



1. 无意义实例
我们以最简单的网络定义来学习pytorch的基本使用方法，我们接下来要定义一个神经网络，包括一个输入层，一个隐藏层，一个输出层，这些层都是线性的，给隐藏层添加一个激活函数Relu，给输出层添加一个Sigmoid函数

import torch
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(1, 32)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(32, 1)
        self.Sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.Sigmoid(x)
        return x


(2)模型编译
我们在之前的机器学习文章中反复提到过，模型的训练是怎么进行的呢，要有一个损失函数与优化方法，我们接下来看看在pytorch中怎么定义这些

import torch.optim as optim

# 实例化模型对象
model = SimpleNet()
# 定义损失函数
criterion = nn.MSELoss()
 
# 定义优化器
learning_rate = 0.01
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

我们上面创建的神经网络是一个类，所以我们实例化一个对象model，然后定义损失函数为mse，优化器为随机梯度下降并设置学习率


(3) 模型训练
# 创建随机输入数据和目标数据
input_data = torch.randn((100, 1))  # 100个样本，每个样本有1个特征
target_data = torch.randn((100, 1))  # 100个样本，每个样本有1个目标值
 
# 训练模型
epochs = 100
 
for epoch in range(epochs):
    # 前向传播
    output = model(input_data)
 
    # 计算损失
    loss = criterion(output, target_data)
 
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

以上步骤是先创建了一些随机样本，作为模型的训练集，然后定义训练轮次为100次，然后前向传播数据集，计算损失，再优化，如此反复

(4) 输入格式
关于输入格式是很多人在实战中容易出现问题的，对于pytorch创建的神经网络，我们的输入内容是一个torch张量，怎么创建呢

data = torch.Tensor([[1], [2], [3]])

很简单对吧，上面这个例子创建了一个torch张量，有三组数据，每组数据有1个特征
我们可以把这个数据输入到训练好的模型中，得到输出结果，如果输出不是torch张量，代码就会报错

prediction = model(data)
print(prediction)


可以看到模型输出了三个预测值
注意，这个任务本身没有意义，因为我们的训练集是随机生成的，这里主要学习框架的使用方法



torch 版手写数字识别：https://blog.csdn.net/a1105425455/article/details/117791839





========================================
|-- Keras: 入门实例 (tensorflow 2.16.2) //TF花了2天才安装好
----------------------------------------
我们在这里把和上面相同的神经网络结构使用keras框架实现一遍

查看显卡设备:

>>> import tensorflow as tf
>>> tf.config.list_physical_devices('GPU')
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

一直报错，重装 py3.10才可用。
https://www.tensorflow.org/install/pip
	需要 Python 3.9–3.11，我装的是 3.7.


1. 无意义实例
(1) 模型定义
from keras.models import Sequential
from keras.layers import Dense
 
 
model = Sequential([
    Dense(32, input_dim=1, activation='relu'),
    Dense(1, activation='sigmoid')
])

注意这里也是一层输入层，一层隐藏层，一层输出层，和pytorch一样，输入层是隐式的，我们的输入数据就是输入层，上述代码定义了一个隐藏层，输入维度是1，输出维度是32，还定义了一个输出层，输入维度是32，输出维度是1，和pytorch环节的模型结构是一样的 


a) => 报错:
Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7

$ find / -name libnvinfer.so.7
$ #没找到


尝试安装 tensorrt:
$ pip install nvidia-tensorrt

$ pip list | grep -i tensor
tensorrt                     8.6.1.post1

##没执行: pip install --upgrade "nvidia-tensorrt<8.0" #vidia-tensorrt and tensorrt are the same.



b) 报错2:
ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'OpenSSL 1.0.2k-fips  26 Jan 2017'. See: https://github.com/urllib3/urllib3/issues/2168

$ pip list | grep -i urllib3
urllib3                      2.0.7

降级： https://github.com/explosion/spaCy/discussions/12750
##pip3 install urllib3==1.26.6 #不用老子版本
$ pip3 install urllib3==1.26.18
$ pip list | grep -i urllib3
urllib3                      1.26.18


c) 报错3: Could not load dynamic library 'libcudart.so.11.0';

$ locate libcudart
/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so
/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so.12
/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so.12.2.53
/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart_static.a

删了重新安装：
$ pip uninstall tensorflow #删除 2.11.0
$ pip list | grep -i tensor
再重新安装:
$ pip install tensorflow

不行，那就糊弄一下：
$ ln -s /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so ~/.local/lib/libcudart.so.11.0

$ vim ~/.bashrc
# 添加一行:
export LD_LIBRARY_PATH=~/.local/lib/:$LD_LIBRARY_PATH


ok了。



(2) 模型编译
那么在Keras中模型又是怎么编译的呢

model.compile(loss='mse', optimizer='sgd')

非常简单，只需要这一行代码 ，设置损失函数为mse，优化器为随机梯度下降


(3) 模型训练
模型的训练也非常简单

# 创建随机输入数据和目标数据
import numpy as np
input_data = np.random.randn(100, 1)  # 100个样本，每个样本有10个特征
target_data = np.random.randn(100, 1)  # 100个样本，每个样本有5个目标值

# 训练模型
model.fit(input_data, target_data, epochs=100)

因为我们已经编译好了损失函数和优化器，在fit里只需要输入数据，输出数据和训练轮次这些参数就可以训练了



(4) 输入格式
对于Keras模型的输入，我们要把它转化为numpy数组，不然会报错

data = np.array([[1], [2], [3]])

prediction = model(data)
print(prediction)

输出:
tf.Tensor(
[[0.13990225]
 [0.07553781]
 [0.03909488]], shape=(3, 1), dtype=float32)
#

可以看到，同样的任务，Keras的代码量小很多







========================================
----------------------------------------



========================================
----------------------------------------







========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------







========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------








