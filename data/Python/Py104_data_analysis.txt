Python104-packages(under jupyter notebook and python shell)
提示: 本文数据都是随手编造的，仅用于演示相应Python包的使用。



## for 193 server
$ docker run --rm -d --name notebook -p 18888:8888 -v "${PWD}":/home/jovyan/work jupyter/datascience-notebook
$ docker logs --tail 3 notebook




$ pip3 install pandas -i https://pypi.douban.com/simple/





========================================
Python数据分析: 教程与资料
----------------------------------------
1.官方及英文资料
https://www.data.gov/
教程: https://github.com/jrjohansson/scientific-python-lectures


https://github.com/DawnEve/pydata-book
Python 数据分析的底层基石 Numpy； Python 数据清洗大杀器 Pandas。


(2) 中文博客资料
https://www.itrhx.com/categories/Python-数据分析/






2. 视频资源
(1)Python数据预处理（一）一抽取多源数据文本信息
https://www.imooc.com/learn/1105

本课程由数据预处理整个流程到综合实战。包括：Anaconda、Sublime、Pywin32、数据采集、数据集成、缺失值处理、正则、jieba分词、NLTK、词袋模型、数据抽样、特征词抽取、文本向量化、gensim、数据降维、numpy、scipy、pandas、matplotlib、seaborn、Xgboost等核心技术。

(2)Python数据预处理（二）- 清洗文本数据
https://www.imooc.com/learn/1122

本课介绍数据预处理过程体系，包括数据类型与采集、文本转化与抽取、数据集成与规约、中文分词、数据清洗、特征提取与变换、特征向量化、特征降维、特征选择、可视化、词典模型、TF-IDF向量模型、主题模型等。


(3)
https://www.youtube.com/watch?v=To3YL92HZyc&list=PLXO45tsB95cKKyC45gatc8wEc3Ue7BlI4


(4)
Google在线深度学习神器Colab: https://www.jianshu.com/p/81eae79ee78b











========================================
Numpy基本功能、矩阵操作
----------------------------------------
官方教程： https://numpy.org/devdocs/user/quickstart.html
中文版：https://www.jianshu.com/p/a260a8c43e44

Numpy + Pandas 是python进行数据分析的基石


1.因为基于C写的Numpy，所以速度快，比Python自身运算要快很多。
而且矩阵的计算也做了优化，比原生Python快十倍。

NumPy 官网 http://www.numpy.org/
教程 https://numpy.org/doc/stable/
源代码：https://github.com/numpy/numpy
NumPy is the fundamental package for scientific computing with Python. It contains among other things:

a powerful N-dimensional array object 
sophisticated (broadcasting) functions
tools for integrating C/C++ and Fortran code
useful linear algebra, Fourier transform, and random number capabilities

除了科学计算，还可以任意定义数据，方便和数据库整合。Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.

(2) 安装
pip install Numpy
pip install Pandas 

测试
import numpy as np
import pandas as pd

pd.test()
#报错 ImportError: Need pytest>=3.0 to run tests

安装 
$ pip install --user pytest

再测试，又报错：
$ pip install -U --user setuptools

不管了，先用吧。








2. 使用

(1) 入门
import numpy as np

#pd.test()
array=np.array([
    [1,2,3],
    [4,5,6]])
print(array)
#[[1 2 3]
#[4 5 6]]

array.size #6
array.shape  #(2, 3)
array.ndim #2



(2) 用np创建array
NumPy中创建数组的方式有若干种。最简单的，可以直接利用Python中常规的list和tuple进行创建。

a=np.array([1,23,4])
a # array([ 1, 23,  4])

b=np.array((1,23,4)) #但是 b=np.array(1,23,4) 是错的，会报错 


#dtype 定义数据的位数，越高占的空间也越多
a=np.array([1,23,4],dtype=np.int)
print(a.dtype) #np.array([1,23,4])

a=np.array([1,23,4],dtype=np.float)
print(a.dtype) #float64

a=np.zeros((3,4));a #3行4列的0矩阵。
np.ones((2,4)) #2行4列的1矩阵


创建等差数列
np.arange(10,20) #array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])
np.arange(10,20,2) #等差是2 array([10, 12, 14, 16, 18])


#np.arange返回的不是list，而是array
np.arange( 10, 30, 5 ) #从10到29，以5为间隔
#array([10, 15, 20, 25])

np.arange(12).reshape(3,4) #3行4列
#array([[ 0,  1,  2,  3],
#       [ 4,  5,  6,  7],
#       [ 8,  9, 10, 11]])

np.linspace(1,10,5) #把1-10之间给出5个数字，平均间隔相同 
#array([ 1.  ,  3.25,  5.5 ,  7.75, 10.  ])
#该函数常用于为函数画点图
x = np.linspace( 0, 2*np.pi, 100 )
# useful to evaluate function at lots of points
f = np.sin(x)



(3)numpy的IO
读取csv文件
$ cat score.csv 
1,2,3,4,5
10,20,30,40,50

score=np.genfromtxt("/home/wangjl/score.csv",delimiter=",")
score
# array([[ 1.,  2.,  3.,  4.,  5.],
#        [10., 20., 30., 40., 50.]])


保存结果， todo?

还是用pandas读写文件比较方便。



==>实例2: 使用 numpy 读写矩阵文件。
$ python3
# 设定工作目录
>>> import os
>>> os.chdir("/data/wangjl/rmarkdown_demo/bookdown-demo-main")
>>> os.getcwd()
'/data/wangjl/rmarkdown_demo/bookdown-demo-main'

# 从文件读出矩阵
>>> import numpy as np
>>> dat=np.loadtxt("backup/dims.data.txt",delimiter=" ", skiprows=1, usecols=[x for x in range(1,11)])
>>> print(dat.shape)
(2638, 10)

# 把矩阵写入文件
print(result.shape) #(2638, 2)
np.savetxt("backup/umap_output.py.txt", result)








3.矩阵运算

(1)加减乘除
a=np.array([10,20,40,30])
b=np.arange(4)
print(a,b) #[10 20 40 30] [0 1 2 3]

#减法
c=a-b
print(c) #[10 19 38 27]

b**2 #array([0, 1, 4, 9]) #乘方

10*np.sin(a) #三角函数 
#array([-5.44021111,  9.12945251,  7.4511316 , -9.88031624])

a<35 #布尔运算 array([ True,  True, False,  True])


(2)矩阵乘法
* 表示按照元素相乘。
a*b #array([ 0, 20, 80, 90])

使用@(python>=3.5)或者dot函数表示矩阵乘积
A = np.array( [[1,1],
             [0,1]] )
B = np.array( [[2,0],
             [3,4]] )
A@B #或者 A.dot(B)
#array([[5, 4],
#       [3, 4]])


(3)+= and *=直接修改已有矩阵，而不是创建一个新的
a = np.ones((2,3), dtype=int)
b = np.random.random((2,3))
a *= 3
a
#array([[3, 3, 3],
#       [3, 3, 3]])
b += a
b
#array([[3.57371054, 3.01654088, 3.23986455],
#      [3.4613754 , 3.11336145, 3.40659914]])
a +=b #报错，b不会自动转为整数，因为会丢失精度【upcasting向上转型问题】
a+b #这样不会报错
b +=a #int->float向下转型无所谓，因为不丢失任何精度。


向下转型没问题，越来越细，占用越来越多空间
a=np.ones(3,dtype=np.int32);print(a)
b=np.linspace(0,np.pi, 3); print(b)
#[1 1 1]
#[0.         1.57079633 3.14159265]
b.dtype #dtype('float64')
c=a+b;print(c); #[1.         2.57079633 4.14159265]
c.dtype #dtype('float64')
d=np.exp(c*1j);print(d)
d.dtype #dtype('complex128')


(4)很多像sum等计算一元类的是以ndarray类的方法提供的
a = np.random.random((2,3))
a.sum()
a.min()
a.max()
a.mean()

还可以指定轴参数axis：
b = np.arange(12).reshape(3,4)
b
# array([[ 0,  1,  2,  3],
#        [ 4,  5,  6,  7],
#        [ 8,  9, 10, 11]])
print( b.sum() ) #总和 66
print( b.sum(axis=0) ) #列求和 array([12, 15, 18, 21])
b.sum(axis=1) #行求和 array([ 6, 22, 38])
b.cumsum(axis=1) #每一行的累加
# array([[ 0,  1,  3,  6],
#        [ 4,  9, 15, 22],
#        [ 8, 17, 27, 38]])




(5)通用函数
NumPy提供很多数学函数，如sin, cos, and exp，统称通用函数“universal functions”(ufunc)。作用于每个元素，输出为array。
>>> B = np.arange(3)
>>> B
array([0, 1, 2])
>>> np.exp(B)
array([ 1.        ,  2.71828183,  7.3890561 ])
>>> np.sqrt(B)
array([ 0.        ,  1.        ,  1.41421356])
>>> C = np.array([2., -1., 4.])
>>> np.add(B, C)
array([ 2.,  0.,  6.])






========================================
|-- numpy 索引、切片、迭代(Indexing, Slicing and Iterating)
----------------------------------------
1. 一维数组可以像list和其他python序列一样索引、切片、迭代。

a=np.arange(10)**3;a
# array([  0,   1,   8,  27,  64, 125, 216, 343, 512, 729])

a[2] #8 第一个下标是0
a[2:5] # 有头2无尾5 array([ 8, 27, 64])

a[:6:2] = -1000 # equivalent to a[0:6:2] = -1000; from start to position 6, exclusive, set every 2nd element to -1000
a
# array([-1000,     1, -1000,    27, -1000,   125,   216,   343,   512,
         729])
a[ : :-1]  # reversed a		 
# array([  729,   512,   343,   216,   125, -1000,    27, -1000,     1,
       -1000])
for i in a:
     print(i**(1/3.))




2. 多维数组，则每个轴可以有一个index。
>>> def f(x,y):
...     return 10*x+y
...
>>> b = np.fromfunction(f,(5,4),dtype=int)
>>> b
array([[ 0,  1,  2,  3],
       [10, 11, 12, 13],
       [20, 21, 22, 23],
       [30, 31, 32, 33],
       [40, 41, 42, 43]])
>>> b[2,3]
23
>>> b[0:5, 1]            # each row in the second column of b
array([ 1, 11, 21, 31, 41])
>>> b[ : ,1]             # equivalent to the previous example
array([ 1, 11, 21, 31, 41])
>>> b[1:3, : ]            # each column in the second and third row of b
array([[10, 11, 12, 13],
       [20, 21, 22, 23]])

#当维度少于实际维度时，则认为剩余维度为全部。
b[-1]     # the last row. Equivalent to b[-1,:]，或者用三个点 b[-1,...]
# array([40, 41, 42, 43])

三个点可以表达任意维度的全部：
 x[1,2,...] is equivalent to x[1,2,:,:,:],
 x[...,3] to x[:,:,:,:,3] and
 x[4,...,5,:] to x[4,:,:,5,:].





3. 迭代
对多维数组的迭代是相对于第一维度的。
for row in b:
     print(row)
# [0 1 2 3]
# [10 11 12 13]
# [20 21 22 23]
# [30 31 32 33]
# [40 41 42 43]

如果相对每一个元素做运算，则可以使用flat属性展开一个array
for element in b.flat:
     print(element)
# 0
# 1
# 2
# 3
# 10
# 11








========================================
|-- numpy 数据类型转换：字符串 <--> 数字
----------------------------------------

1. 字符串转数字

import numpy as np
x=np.array(['3.1', "6.2"])
print("x=", x)
y=x.astype(np.float)
print("y=", y)

输出 
x= ['3.1' '6.2']
y= [3.1 6.2]





2.







========================================
Scipy: high-level scientific computing(教程)
----------------------------------------
https://docs.scipy.org/doc/numpy/user/quickstart.html

SciPy 官网：https://www.scipy.org/
SciPy 源代码：https://github.com/scipy/scipy


1.Scipy简介
SciPy (pronounced “Sigh Pie”) 是一个开源软件，用于数学、科研、工程计算。
依赖NumPy提供的方便快速的N维数组操作。
SciPy库建立在NumPy数组上，提供用户友好的、高效的数值路径：数据整合和优化。
它们在主流系统上都能跑，好安装，免费。
NumPy and SciPy使用方便，收到世界顶级科学家和工程师的信赖。
If you need to manipulate numbers on a computer and display or publish the results, give SciPy a try!

scipy包含致力于科学计算中常见问题的各个工具箱。它的不同子模块相应于不同的应用。像插值，积分，优化，图像处理，统计，特殊函数等等。

(1) 安装 
$ pip install scipy
## Successfully installed numpy-1.21.2 scipy-1.7.1


$ pip install Pandas
$ pip install matplotlib
 


(2) 引用
文件输入和输出scipy.io
线性代数操作scipy.linalg
快速傅里叶变换scipy.fftpack
优化器scipy.optimize
统计工具scipy.stats

因为兼容等历史原因，scipy命名空间本身有很多numpy导入的函数(尝试 scipy.cos 就是 np.cos)。
建议任何时候都不要使用 import scipy，而要使用：
from scipy import stats
或
import scipy.io as spio



3. Scipy课程
http://www.scipy-lectures.org/
每个课程1-2h，由浅入深。

SciPy Tutorial
https://docs.scipy.org/doc/scipy/reference/tutorial/index.html
https://docs.scipy.org/doc/scipy/reference/tutorial/
https://docs.scipy.org/doc/scipy/reference/

https://scipy-cookbook.readthedocs.io/index.html


中文：
https://www.jianshu.com/p/1a3db06e786d

Scipy有很多子模块可以应对不同的应用，例如插值运算，优化算法、图像处理、数学统计等。
https://blog.csdn.net/q583501947/article/details/76735870













========================================
|-- scipy 产生稀疏矩阵
----------------------------------------
1. 例子: 转化为稀疏矩阵

(1) 产生对角矩阵
import numpy as np
matrix=np.eye(6)
print(matrix)

输出
[[1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1.]]

(2) 转为 稀疏矩阵
from scipy import sparse
sparse_matrix=sparse.csr_matrix(matrix)
print(sparse_matrix)

输出：
  (0, 0)	1.0
  (1, 1)	1.0
  (2, 2)	1.0
  (3, 3)	1.0
  (4, 4)	1.0
  (5, 5)	1.0
















========================================
pandas: 数据清洗 powerful Python data analysis toolkit (df可保存到excel文件中)
----------------------------------------
1.简介
官网： http://pandas.pydata.org/
文档：http://pandas.pydata.org/pandas-docs/stable/
Cookbook: http://pandas.pydata.org/pandas-docs/stable/cookbook.html#cookbook

10min入门 https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html
中文：https://www.yiibai.com/pandas/python_pandas_series.html


安装
$ conda install pandas
或
$ pip install pandas
$ pip install --upgrade pandas


pandas 解决什么问题？
python善于数据整理和预处理，不太擅长数据分析和建模，pandas弥补了这一点。
对于线性拟合和panel回归之外的建模，请使用statsmodels and scikit-learn包。
距离Python成为统计建模环境的一类公民还很远，我们正在努力。


pandas包含的数据类型：Series和DataFrame。
 - Series：一维数组，与Numpy中的一维array类似。二者与Python基本的数据结构List也很相近。Series如今能保存不同种数据类型，字符串、boolean值、数字等都能保存在Series中。
 - Time- Series：以时间为索引的Series。
 - DataFrame：二维的表格型数据结构。很多功能与R中的data.frame类似。可以将DataFrame理解为Series的容器。
 - Panel ：三维的数组，可以理解为DataFrame的容器。

我们重点学习DataFrame






2. 10分钟入门
http://pandas.pydata.org/pandas-docs/stable/10min.html
首先导入pandas库，一般都会用到numpy库，所以我们先导入备用：

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt #可能画图

(1)创建对象
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html

nlist=['artical 1','a20','a30','a40','a50']
plist=[10,20,30,40,50]
clist=['评论1','评论2','评论3','评论4','评论5']

df=pd.DataFrame( data={'name':nlist, 'price':plist, 'comment':clist} )
#print(df.dtypes)
df
## 结果
# 	name	price	comment
# 0	artical 1	10	评论1
# 1	a20	20	评论2
# 2	a30	30	评论3
# 3	a40	40	评论4
# 4	a50	50	评论5

#转置行和列 https://blog.csdn.net/u013817676/article/details/94861359
df2=df.stack();
#print(df2) #将df格式从表格形式转化成了花括号结构
df3=df2.unstack(0) #行列转置，将第二行的列索引转化成行索引
df3
# 	   0	1	2	3	4
# name	artical 1	a20	a30	a40	a50
# price	10	20	30	40	50
# comment	评论1	评论2	评论3	评论4	评论5

### 最简单的转置
df.T





(2)用pandas筛选annovar结果文件
ex=pd.read_csv("EX23_.hg19_multianno.csv", na_values=["."]) #读取csv文件，并填充空值
ex

ex.shape #(628993, 88)

ex.dtypes

ex=ex.fillna(0) #用0填充na值

#测试语句
tmp=ex["1000g2015aug_all"].iloc[50:60]
print(tmp)
tmp<0.01

#filter1:
print( ex["Func.refGene"].value_counts() ) #对某一列进行计数
ex2=ex[ex["Func.refGene"].isin(['exonic',"exonic;splicing","splicing"])] #对一列进行筛选
ex2.shape #(25347, 88)

#filter2:
ex3=ex2[ (ex2["1000g2015aug_all"]<0.01) & (ex2['1000g2015aug_eas']<0.01) & (ex2['esp6500siv2_all']<0.01) \
        & (ex2['ExAC_ALL']<0.01)& (ex2['ExAC_EAS']<0.01)] #对一列进行筛选
ex3.shape #(1484, 88)

#filter?:是不是要挑出来 非同意突变
ex3["ExonicFunc.refGene"].value_counts()

#过滤后的结果，保存到csv中
ex3.to_csv("EX23_.hg19_multianno.filter1_2.csv")





(3) 保存到excel的sheet中
1)
writer=pd.ExcelWriter('titanic.xlsx')
#
df.to_excel(writer, sheet_name="titanic")
#
df2=df.loc[0:100,]
df2.to_excel(writer, sheet_name="titanic2")
#
writer.save()




2)
# 保存到excel中 pip install openpyxl
with pd.ExcelWriter('aaa.xlsx') as writer:
    df.to_excel(writer, sheet_name="aaa")

# 简洁形式
df=pd.DataFrame(data={'name':nlist, 'price':plist})
df.to_excel(xlsName, sheet_name=sheet_name)

#更复杂的形式：合并2个数据框
#df1 = pd.DataFrame(nlist,columns=['product'])
#df2 = pd.DataFrame(plist,columns=['price'])
#df3 = pd.concat([df1,df2],axis=1,ignore_index=False)
#df3.to_excel('aaa.xlsx',sheet_name='aaa')









3.python:pandas 合并多个DataFrame
https://www.jianshu.com/p/5ecea164cec6

http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html
 - merge 左右合并
 - join
 

 - append 上下合并
 - concat





========================================
|-- pandas 读取文件(文本/excel/mysql)、数据结构(Series/DataFrame)
----------------------------------------
视频:
https://www.bilibili.com/video/BV1UJ411A7Fs?p=1 从入门到实践(todo:3,)
https://www.bilibili.com/video/BV16E411G7sd?p=1 简介快速入门(done)

英文视频:
https://www.bilibili.com/video/BV1zA411b7Ly?from=search&seid=7102611799471868216




使用jupyter演示。
首先引入库
import pandas as pd;



1. 读取数据
数据类型	说明	pandas读取方法
csv/tsv/txt 逗号、tab分割的纯文本文件 pd.read_csv()
excel	微软xls/xlsx文件 pd.read_excel()
mysql 关系型数据库 	pd.read_sql()


例1: 读纯文本文件
fpath="xx.csv"
ratings=pd.read_csv(fpath); #读取csv文件 help(pd.read_csv)查看更多参数
	# sep="\t" 指定分隔符为tab，默认为逗号;
	# header=None, 没有标题行
	
	# index_col=0, # 将第一列作为行名字
	
	# names=['pdate', 'pv', 'uv'] 给出列名
ratings.head();#查看前几行

ratings.shape #查看行列数
ratings.columns #查看列名

ratings.index #查看索引列

ratings.dtypes #查看每一列数据类型


例2: 读取excel
#excel文件和pandas的交互读写，主要使用到pandas中的两个函数,一个是pd.ExcelFile函数,一个是to_excel函数
(1)######
fpath="xx.xlsx"
pvuv=pd.read.excel(fpath);

(2)######
import pandas as pd
#使用pandas读取excel文件
xls_file=pd.ExcelFile('./data/workbook.xls')
xls_file.sheet_names#显示出读入excel文件中的表名字
table1=xls_file.parse('first_sheet')
table2=xls_file.parse('second_sheet')

xlsx_file=pd.ExcelFile("./demo.xlsx")
x1=xlsx_file.parse(0)
x2=xlsx_file.parse(1)

#excel文件的写出
#data.to_excel("abc.xlsx",sheet_name="abc",index=False,header=True)  #该条语句会运行失败，原因在于写入的对象是np数组而不是DataFrame对象,只有DataFrame对象才能使用to_excel方法。

DataFrame(data).to_excel("abc.xlsx",sheet_name="123",index=False,header=True)







例3: 读取mysql
import pymysql
conn=pymysql.connect(
	host='127.0.0.1',
	user='root',
	password="123456",
	database='test',
	charset='utf8'
)

mysql_page=pd.read_sql('select * from tableName', con=conn)
mysql_page







2. pandas的数据结构: 
DataFrame 二维数据，整个表格，多行多列;
Series 一维数据，一行或一列；

竖着为 df.columns;
横着为 df.index;

(1) Series例子
例1: a=pd.Series([1,2,3,4,5]); 默认三个参数 data,index默认是从0开始的编号,dtype
a
# 0    1
# 1    2
# 2    3
# 3    4
# 4    5
# dtype: int64

加更多参数
a=pd.Series([1,2,3,4,5], index=['a','b','c','d','e'], dtype=float);
a
# a    1.0
# b    2.0
# c    3.0
# d    4.0
# e    5.0
# dtype: float64



例2: 也可以使用numpy作为输入
import numpy as np;
a=np.arange(5);
pd.Series(a)
结果同例1。


例3: 传入字典
dic={'name':'Lee', 'gender':'M', 'age':18}
pd.Series(dic) #key变索引，value变data; 如果这里再次设置index则会覆盖字典的key
# name      Lee
# gender      M
# age        18
# dtype: object


例4: 当一个值，却多个索引时
pd.Series(5, [0,1,10])
# 0     5
# 1     5
# 10    5
# dtype: int64

#pd.Series([3,5], [0,1,10,100]) 报错 Length of passed values is 2, index implies 4





(2) DataFrame数据结构，和R的data.frame类似

例1: 创建数据框
a=np.random.randint(0,10,(2,3)) #2行3列
pd.DataFrame(a, index=['a', 'b'], columns=['x','y','z']) #定义index行名，columns列名
#	x	y	z
#a	0	5	1
#b	6	3	2


例2: 传入字典 
population={'beijing':1000, 'shanghai':1200,'guangzhou':999}
a=pd.Series(population)
b=pd.DataFrame(a) ##直接传入字典会报错；
#传入index=population.keys()输出也是不正确的; pd.DataFrame(population, index=population.keys())
print(a)
b
# beijing      1000
# shanghai     1200
# guangzhou     999
# dtype: int64
#
#            0
# beijing	1000
# shanghai	1200
# guangzhou	999

pd.DataFrame(a, columns=['population']) #可以修改列名
# 	   population
# beijing	1000
# shanghai	1200
# guangzhou	999

#如果非要传入字典，也可构建字典形式的参数:
pd.DataFrame( {'data2':population} )
# 	      data2
# beijing	1000
# guangzhou	999
# shanghai	1200

#再补充一个json数据，key要一致，则构建两列数据框：
gdp={'beijing':1, 'shanghai':1.5,'guangzhou':3}
pd.DataFrame( {'population':population, 'gdp':gdp} )
#	population	gdp
#beijing	1000	1.0
#guangzhou	999	3.0
#shanghai	1200	1.5

#还可以添加一个常数列
pd.DataFrame( {'population':population, 'gdp':gdp, 'country':'China'} )
#	   population	gdp	country
# beijing	1000	1.0	China
# guangzhou	999	3.0	China
# shanghai	1200	1.5	China






3. 属性
a=pd.DataFrame( {'population':population, 'gdp':gdp} )
a
#	population	gdp
#beijing	1000	1.0
#guangzhou	999	3.0
#shanghai	1200	1.5

a.values
# array([[1.00e+03, 1.00e+00],
#        [9.99e+02, 3.00e+00],
#        [1.20e+03, 1.50e+00]])

a.index #获取行名
# Index(['beijing', 'guangzhou', 'shanghai'], dtype='object')

a.columns #获取列名
# Index(['population', 'gdp'], dtype='object')

a.shape #(3, 2) 3行2列

a.size #几个元素 6

a.dtypes #每列数据的类型
# population      int64
# gdp           float64
# dtype: object






========================================
|-- 索引、切片、赋值、新增列/行、对行列排序
----------------------------------------
数据接上文
a
# 	   population	gdp
# beijing	1000	1.0
# guangzhou	999	3.0
# shanghai	1200	1.5


1. 如何取一列? 直接使用[]
a['gdp'] #取列名为gdp的一列
# beijing      1.0
# guangzhou    3.0
# shanghai     1.5
# Name: gdp, dtype: float64

或者使用点号
a.gdp #同上




2. 如何取出一行 loc[]
(1)
a.loc['beijing'] #注意是方括号，不是圆括号！
# population    1000.0
# gdp              1.0
# Name: beijing, dtype: float64


#取多行数据
a.loc[ ['beijing', 'guangzhou'] ] #注意输入的是数组形式
#	  population	gdp
# beijing	1000	1.0
# guangzhou	999	3.0

#如果不是数组呢
a.loc[ 'beijing', 'guangzhou' ]
# 报错 'the label [guangzhou] is not in the [index]' 说明第二个参数要是列
a.loc[ 'beijing', 'population' ] #1000 精确返回 beijing行 population列的数据



(2) 切片的方式取出多行，冒号
a.loc[ 'beijing':'guangzhou' ]
# 	  population	gdp
# beijing	1000	1.0
# guangzhou	999	3.0


(3)还可以使用数字编号指定行
a.iloc[0] #第一行
# population    1000.0
# gdp              1.0
# Name: beijing, dtype: float64

a.iloc[1] #第二行
# population    999.0
# gdp             3.0
# Name: guangzhou, dtype: float64

a.iloc[[0,2]] #第1,3行
# 	   population	gdp
# beijing	1000	1.0
# shanghai	1200	1.5




3. 取出一个具体的数值，行列交叉
(1)
a.loc[ 'shanghai', 'gdp' ] #1.5
a.iloc[ 2, 1] #1.5

(2) 按照numpy数组形式取值
print(type(a.values)) #<class 'numpy.ndarray'> 转为numpy类
a.values[2,1] #1.5 
a.values[2][1] #1.5


(3) iloc切片方式
a.iloc[:2, :] #前0,1行，全部列
#	population	gdp
#beijing	1000	1.0
#guangzhou	999	3.0


#切片a:b是一个[a,b)区间
t1=[10,1,2,3,4,5,6,7]
print( t1[0:4])
t1[1:4] # 左闭右开区间



4. 对某一列按条件筛选
a.gdp>1
# beijing      False
# guangzhou     True
# shanghai      True
# Name: gdp, dtype: bool

a[a.gdp>1]
# 	  population	gdp
# guangzhou	999	3.0
# shanghai	1200	1.5

a[a.gdp==1.5]
#	  population	gdp
# shanghai	1200	1.5




5. 赋值
(1)修改某个值，就是使用loc，iloc等定位到元素，然后就可以赋值了
a.iloc[1,1] #3
a.iloc[1,1]=3.2 #赋值

a
#	  population	gdp
# beijing	1000	1.0
# guangzhou	999	3.2 #这里已经修改了
# shanghai	1200	1.5






6. 新增行或列
(1) 新增一列
先新建一个Series，行标题index和原来一样
s=pd.Series([10,20,30], index=['beijing', 'shanghai', 'guangzhou'])
s
# beijing      10
# shanghai     20
# guangzhou    30
# dtype: int64


a['cName']=s; #指定列名
a
# 	  population	gdp	cName
# beijing	1000	1.0	10
# guangzhou	999	3.2	30
# shanghai	1200	1.5	20



(2) 新增一行
a.loc['zhengzhou'] = [1300,0.8,9]
a
# 	  population	gdp	cName
# beijing	1000.0	1.0	10.0
# guangzhou	999.0	3.2	30.0
# shanghai	1200.0	1.5	20.0
# zhengzhou	1300.0	0.8	9.0

(3) 通过合并2个数据框，新增一行或多行
df1=pd.DataFrame([888,0.9,9.5]).T #默认是列向量，变为行向量;
df1.columns = a.columns # 修改df1的column和a的一致
df1.index=['nj'] #修改行名
df1
#   population	gdp	cName
# nj	888.0	0.9	9.5


# 把两个dataframe合并，需要设置 ignore_index=True
#pd.concat([a,df1],ignore_index=True) #ignore_index就会损失掉index
pd.concat([a,df1])
#	  population	gdp	cName
# beijing	1000.0	1.0	10.0
# guangzhou	999.0	3.2	30.0
# shanghai	1200.0	1.5	20.0
# zhengzhou	1300.0	0.8	9.0
# nj	888.0	0.9	9.5





7. 对行列排序 
(1)# 加载数据
print(flights.shape)
flights.head()
##	year	month	passengers
#0	1949	January	112
#1	1949	February	118
#2	1949	March	132


# 交叉表，x=年,y=月，中间数字是乘客人数
df=flights.pivot('month', 'year', 'passengers')
print(df)
# year       1949  1950  1951  1952  1953  1954  1955  1956  1957  1958  1959  \
# month                                                                         
# April       129   135   163   181   235   227   269   313   348   348   396   
# August      148   170   199   242   272   293   347   405   467   505   559 

(2)# 行的顺序不对
df.index
# Index(['April', 'August', 'December', 'February', 'January', 'July', 'June',

# 怎么调整月份顺序
df2=df.reindex(['January','February','March','April','May','June','July',
                'August','September','October','November','December'])
print(df2)
ax2=sns.heatmap(df2)


plt.figure(figsize=(8,5))
sns.heatmap(df2, annot=True, fmt='d') #字体格式d，显示比较正常



(3) # 对列进行倒序
df2.columns
# Int64Index([1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959,

df2.columns[::-1] #Int64Index([1960, 1959, 1958, 1957, 1956, 1955, 1954, 1953, 1952, 1951, 1950,

df3=df2.loc[:, df2.columns[::-1]]
df3

# year	1960	1959	1958	1957	1956	1955	1954	1953	1952	1951	1950	1949
# month												
# January	417	360	340	315	284	242	204	196	171	145	115	112
# February	391	342	318	301	277	233	188	196	180	150	126	118

sns.heatmap(df3, cmap="YlGnBu")








8. pandas 数据排序.sort_index()和.sort_values()
官网推荐使用df.sort_values()这种方法


(1) df. sort_values()
作用：既可以根据列数据，也可根据行数据排序。

注意：必须指定by参数，即必须指定哪几行或哪几列；无法根据index名和columns名排序（由.sort_index()执行）
DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')
axis：{0 or ‘index’, 1 or ‘columns’}, default 0，默认按照列排序，即纵向排序；如果为1，则是横向排序。

实例
按照某一列排序
res = df.sort_values(by='A', ascending=False)

按b列升序排序
df.sort_values(by='b') #等同于df.sort_values(by='b',axis=0)

按照多列排序: 先按B列降序，再按A列升序排序
df.sort_values(by=['B','A'],axis=0,ascending=[False,True]) 

按行3升序排列
df.sort_values(by=3,axis=1) #必须指定axis=1

按行3升序，行0降排列
df.sort_values(by=[3,0],axis=1,ascending=[True,False])



(2) df. sort_index()
作用：默认根据行标签对所有行排序，或根据列标签对所有列排序，或根据指定某列或某几列对行排序。
注意：df. sort_index()可以完成和df. sort_values()完全相同的功能，但python更推荐用只用df. sort_index()对“根据行标签”和“根据列标签”排序，其他排序方式用df.sort_values()。

实例
默认按“行标签”升序排列（推荐）
df.sort_index() #默认按“行标签”升序排序，或df.sort_index(axis=0, ascending=True)

按“列标签”升序排列（推荐）
df.sort_index(axis=1) #按“列标签”升序排序



9. 删除一列或几列

# 直接按列名删除
df2 = df.drop(['B', 'C'], axis=1)







ref:
https://www.bilibili.com/video/BV16E411G7sd?p=5
https://blog.csdn.net/littlehaes/article/details/103902073















========================================
|-- 数据查看、统计计数
----------------------------------------
#先构建数据
dates=pd.date_range(start="2020-1-1", periods=6)
dates
#DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',
#               '2020-01-05', '2020-01-06'],
#              dtype='datetime64[ns]', freq='D')

df=pd.DataFrame(np.random.randint(0,10,(6,4)), index=dates, columns=['A','B','C','D'])
df
# 	           A	B	C	D
# 2020-01-01	5	4	7	2
# 2020-01-02	7	6	9	2
# 2020-01-03	0	6	6	8
# 2020-01-04	9	2	5	5
# 2020-01-05	8	2	5	6
# 2020-01-06	8	3	4	9

1. 查看每一列的数据的特征
df.describe() #统计角度
#           A              B          C         D
# count	6.000000	6.000000	6.000000	6.000000
# mean	6.166667	3.833333	6.000000	5.333333
# std	3.311596	1.834848	1.788854	2.943920
# min	0.000000	2.000000	4.000000	2.000000
# 25%	5.500000	2.250000	5.000000	2.750000
# 50%	7.500000	3.500000	5.500000	5.500000
# 75%	8.000000	5.500000	6.750000	7.500000
# max	9.000000	6.000000	9.000000	9.000000


df.info() #对每一类的描述(内存角度)
# <class 'pandas.core.frame.DataFrame'>
# DatetimeIndex: 6 entries, 2020-01-01 to 2020-01-06 #对索引的描述
# Freq: D
# Data columns (total 4 columns): #对每一列的数据表述
# A    6 non-null int64
# B    6 non-null int64
# C    6 non-null int64
# D    6 non-null int64
# dtypes: int64(4)
# memory usage: 240.0 bytes



2. 查看前几行、后几行
df.head() #默认前5行
df.head(2)

df.tail() #末尾5行



3. 转置行和列
df.T


4. 排序
df.sort_index(axis=1, ascending=False) #按照列名排序，倒序
# 	           D	C	B	A
# 2020-01-01	2	7	4	5
# 2020-01-02	2	9	6	7
# 2020-01-03	8	6	6	0
# 2020-01-04	5	5	2	9
# 2020-01-05	6	5	2	8
# 2020-01-06	9	4	3	8

df.sort_index(axis=0, ascending=False) #按照行名排序，倒序
#               A	B	C	D
# 2020-01-06	8	3	4	9
# 2020-01-05	8	2	5	6
# 2020-01-04	9	2	5	5
# 2020-01-03	0	6	6	8
# 2020-01-02	7	6	9	2
# 2020-01-01	5	4	7	2


df.sort_values('C') #按照"C"列排序
#          A	B	C	D
# 2020-01-06	8	3	4	9
# 2020-01-04	9	2	5	5
# 2020-01-05	8	2	5	6
# 2020-01-03	0	6	6	8
# 2020-01-01	5	4	7	2
# 2020-01-02	7	6	9	2





5. 统计计数

(1) 类似 R 中的 table(df1$velocity_genes)

adata.var.groupby("velocity_genes")["velocity_genes"].count()
输出:
velocity_genes
False    17562
True      1804
Name: velocity_genes, dtype: int64


(2) 
data.obs.groupby("sample0")["sample0"].count()
输出:
sample0
DSS1    6119
DSS2    7769
WT1       21
WT2      376
Name: sample0, dtype: int64







========================================
|-- pandas 的计算、缺失值处理
----------------------------------------

1. 矩阵的运算
(1)和常量的加减法
a=pd.DataFrame([1,2,3])
a #3行1列的列向量
#      0
#0	1
#1	2
#2	3

a+7 #向量每个元素都增加7
# 	0
# 0	8
# 1	9
# 2	10

a.add(b) #结果同上。


(2) 矩阵和矩阵的加减法
b=pd.DataFrame([4,5,6])
a+b #就是对应元素相加
#	0
#0	5
#1	7
#2	9


(3) 矩阵的乘法
c=pd.DataFrame(np.random.randint(10, size=(1,3))) #1行3列
c
#	0	1	2
#0	7	1	3


d=a@c
d
# 	0	1	2
# 0	7	1	3
# 1	14	2	6
# 2	21	3	9
或者 a.dot(c)


(4)再次验证加法add()方法:
# 矩阵相加，如果维度不同，则缺失部分补0；如果直接使用+，则维度不同的地方直接NaN
a+d
#  	0	1	2
# 0	8	NaN	NaN
# 1	16	NaN	NaN
# 2	24	NaN	NaN

a.add(d, fill_value=0)
#      0	1	2
# 0	8	1.0	3.0
# 1	16	2.0	6.0
# 2	24	3.0	9.0







2. 缺失值的处理
创建一个有缺失值的矩阵
# pd.DataFrame( np.arange(9).reshape([3,3]) )
d.iloc[:2,2]=np.nan
d
#	 0	1	2
#0	7	1	NaN
#1	14	2	NaN
#2	21	3	9.0

(1) 直接丢掉含缺失值的行
d.dropna()
#	0	1	2
#2	21	3	9.0

(2) 直接丢掉含缺失值的列
d.dropna(axis=1)
#	0	1
# 0	7	1
# 1	14	2
# 2	21	3


(3) 如果只丢掉全部都是NaN的列呢？有一个NaN还可以接受
d.dropna(axis=1, how="all") #any默认, all
则返回值和原矩阵一行，因为没有全是NaN的列。


(4) 如果想挽救缺失值呢，NaN都填充为0
d.fillna(0)
#	0	1	2
# 0	7	1	0.0
# 1	14	2	0.0
# 2	21	3	9.0
注意: 原始的矩阵d并没有被改变！


(5) 查找缺失值
houseprice.isnull() #元素级别的判断，把对应的所有元素的位置都列出来，元素为空或者NA就显示True，否则就是False
houseprice.isnull().any() #列级别的判断，只要该列有为空或者NA的元素，就为True，否则False
missing=houseprice.columns[houseprice.isnull().any()].tolist() #将为空或者NA的列找出来
houseprice[missing].isnull().sum()　#将列中为空或者NA的个数统计出来

houseprice[['LotFrontage','Alley']][houseprice['Alley'].isnull()==True]  #从LotFrontage 和Alley 列中进行选择行，选择Alley中数据为空的行。主要用来看两个列的关联程度，是不是大多同时为空。

houseprice['Fireplaces'][houseprice['FireplaceQu'].isnull()==True].describe()   #对筛选出来的数据做一个描述，比如一共多少行，均值、方差、最小值、最大值等等。




ref:
https://www.cnblogs.com/gczr/p/6761613.html




========================================
|-- 合并(concat)和对齐(merge)
----------------------------------------
1. concat就是简单合并
a=pd.DataFrame(np.zeros([3,4]), columns=['a', 'b', 'c', 'd']);
b=pd.DataFrame(np.ones([3,4]), columns=['a', 'b', 'c', 'd']);
print(a)
b
#      a    b    c    d
# 0  0.0  0.0  0.0  0.0
# 1  0.0  0.0  0.0  0.0
# 2  0.0  0.0  0.0  0.0
# 
# 	 a	b	c	d
# 0	1.0	1.0	1.0	1.0
# 1	1.0	1.0	1.0	1.0
# 2	1.0	1.0	1.0	1.0


(1) 垂直合并
pd.concat([a,b])
#	a	b	c	d
#0	0.0	0.0	0.0	0.0
#1	0.0	0.0	0.0	0.0
#2	0.0	0.0	0.0	0.0
#0	1.0	1.0	1.0	1.0
#1	1.0	1.0	1.0	1.0
#2	1.0	1.0	1.0	1.0


#如果不想要行标题，设置ignore_index参数，行标题变成递增的编号
pd.concat([a,b], ignore_index=True)


(2) 水平合并
pd.concat([a,b], axis=1)
#	a	b	c	d	a	b	c	d
# 0	0.0	0.0	0.0	0.0	1.0	1.0	1.0	1.0
# 1	0.0	0.0	0.0	0.0	1.0	1.0	1.0	1.0
# 2	0.0	0.0	0.0	0.0	1.0	1.0	1.0	1.0



(3) 如果待合并的矩阵的行名和列名不一致，怎么合并？
a2=pd.DataFrame(np.zeros([3,4]),index=[0,1,2], columns=['a', 'b', 'c', 'd']);
b2=pd.DataFrame(np.ones([3,4]),index=[1,2,3], columns=['c', 'd','E','F']);
print(a2)
b2
#     a    b    c    d
#0  0.0  0.0  0.0  0.0
#1  0.0  0.0  0.0  0.0
#2  0.0  0.0  0.0  0.0
#
#	c	d	E	F
#1	1.0	1.0	1.0	1.0
#2	1.0	1.0	1.0	1.0
#3	1.0	1.0	1.0	1.0

pd.concat([a2,b2], axis=1) #水平合并(水平不做调整)
# 	a	b	c	d	c	d	E	F #可见，列名重复了，列名仅仅是简单平凑一起。
# 0	0.0	0.0	0.0	0.0	NaN	NaN	NaN	NaN
# 1	0.0	0.0	0.0	0.0	1.0	1.0	1.0	1.0
# 2	0.0	0.0	0.0	0.0	1.0	1.0	1.0	1.0
# 3	NaN	NaN	NaN	NaN	1.0	1.0	1.0	1.0
#行名做了整合，相同的行名只出现一次，没有数据的填充NaN;



(4) 
c=pd.Series([1,2,3,4], index=['a', 'b', 'c', 'd'])
c
# a    1
# b    2
# c    3
# d    4
# dtype: int64

a2.append(c, ignore_index=True) #底部新增一行
# 	a	b	c	d
# 0	0.0	0.0	0.0	0.0
# 1	0.0	0.0	0.0	0.0
# 2	0.0	0.0	0.0	0.0
# 3	1.0	2.0	3.0	4.0
#注意 a2 本身没有被改变







2. merge的用法:按照某一列合并
(1)
a3=pd.DataFrame([[2,0],[3,1]], columns=['A','B'])
a3
#	A	B
#0	2	0
#1	3	1

b3=pd.DataFrame([[20,0],[30,1]], columns=['C','B'])
b3
#	C	B
#0	20	0
#1	30	1

pd.merge(a3,b3) #相当于中间列B是一个桥梁中介，类似于sql中的where a.id=b.uid;
#	A	B	C
#0	2	0	20
#1	3	1	30







========================================
|-- 分组(groupby)、数据透视表(stack/pivot)
----------------------------------------
1. 分组查看
df=pd.DataFrame({'key':list('ABCCBA'),
	'data1':range(6),
	'data2':range(20,26)
})
df.iloc[0,1]=4 #修改一个值
df

# 	key	data1	data2
# 0	A	4	20
# 1	B	1	21
# 2	C	2	22
# 3	C	3	23
# 4	B	4	24
# 5	A	5	25

(1) 
df.groupby('key') #<pandas.core.groupby.groupby.DataFrameGroupBy object at 0x7faac8f64fd0>
df.groupby('key').sum()
#	data1	data2
#key		
#A	9	45
#B	5	45
#C	5	45

df.groupby('key').mean()
# 	data1	data2
# key		
# A	4.5	22.5
# B	2.5	22.5
# C	2.5	22.5

df.groupby('key')['data1'].mean() #某一列的均值
#      key
# A    4.5
# B    2.5
# C    2.5
# Name: data1, dtype: float64


df.groupby('key')[['data1']].mean() #返回数据框的形式
# 	data1
# key	
# A	4.5
# B	2.5
# C	2.5



(2) 自定义统计函数
def func1(x):
	x['data1'] /= x['data1'].sum();
	return x;
df.groupby('key').apply(func1)
# 	key	data1	data2
# 0	A	0.444444	20
# 1	B	0.200000	21
# 2	C	0.400000	22
# 3	C	0.600000	23
# 4	B	0.800000	24
# 5	A	0.555556	25
#确实已经做了改变;







2. 数据透视表: stack/pivot

(1) 数据的格式

长格式: mysql方便存储，方便新增一行；不适合直接画图;
sample gene expression
1 1 10
1 2 200
1 3 99
2 1 20
2 2 9
2 3 106


宽格式: 人容易读懂，很容易画二维图
	  sample1 sample2
gene1   10      20
gene2   200      9
gene3   99      106

本文目的: 为了方便作图，如何把mysql中的长格式变为宽的二维交叉格式呢？
这个过程叫重塑或者透视。


melt 融化成长的;
cast 重塑成宽的。常用函数为 stack得到二维数据，pivot得到透视表。



(1)导入数据集
import seaborn as sns;

#titannic=sns.lead_dataset('titanic'); #报错，不起作用

#这是git clone到服务器上的文件
titanic = pd.read_csv("http://y.biomooc.com/wangjl/docs/seaborn-data/titanic.csv")
print(titanic.shape) #(891, 15)
titanic.head()
# survived  pclass	sex	age	sibsp	parch	fare  embarked	class	who adult_male deck	embark_town	alive	alone
# 0	0	3	male	22.0	1	0	7.2500	S	Third	man	True	NaN	Southampton	no	False
# 1	1	1	female	38.0	1	0	71.2833	C	First	woman	False	C	Cherbourg	yes	False
# 2	1	3	female	26.0	0	0	7.9250	S	Third	woman	False	NaN	Southampton	yes	True
# 3	1	1	female	35.0	1	0	53.1000	S	First	woman	False	C	Southampton	yes	False
# 4	0	3	male	35.0	0	0	8.0500	S	Third	man	True	NaN	Southampton	no	True


(2) 查看不同等级仓，不同性别 乘客的生存情况
titanic.pivot_table('survived', index='sex', columns='class')
# class    First	  Second	Third
# sex			
# female  0.968085	0.921053	0.500000
# male    0.368852	0.157407	0.135447

发现女性存活率高于男性，且随着仓位升高而升高。















========================================
|-- 时间日期处理: pd.to_datetime() 时间戳 to 日期和时间
----------------------------------------
pd转为Timestamp过程中有2个大坑!!: 一个输入时间戳单位要设置为s; 一个是转为时间日期为标准时区，和东八区少了8小时。



1. 时间戳 先变为 时间格式Timestamp，然后就能获取任何想要的年月日、时分秒了
import numpy as np
import pandas as pd

help(pd.to_datetime)


(1)一般的10位时间戳的单位是秒，但是pd.to_datetime()默认的输入是ms，所以要指定参数unit='s'
#unit : string, default 'ns' 
#         unit of the arg (D,s,ms,us,ns) denote the unit, which is an
#         integer or float number. This will be based off the origin.
#         Example, with unit='ms' and origin='unix' (the default), this
#         would calculate the number of milliseconds to the unix epoch start.


print(pd.to_datetime( 1590232237*1e9) ) #默认是纳秒
print(pd.to_datetime( 1590232237*1e3, unit='ms')) #指定输入的是 毫秒


pd.to_datetime( 1590232237, unit='s') # 时间戳 to 日期 。unit='s' 指定输入的是 秒

###
#2020-05-23 11:10:37
#2020-05-23 11:10:37
#Timestamp('2020-05-23 11:10:37')


(2) 除了输入时间戳，还可以输入年月日格式、年月日时分秒格式
# format : string, default None
#         strftime to parse time, eg "%d/%m/%Y", note that "%f" will parse
#         all the way up to nanoseconds.

pd.to_datetime( '23/05/2020', format='%d/%m/%Y') #format='%d/%m/%Y' 指定输入格式是 日/月/年
# Timestamp('2020-05-23 00:00:00')

d2=pd.to_datetime( '23/05/2020 9:28:34', format='%d/%m/%Y %H:%M:%S')
d2
# Timestamp('2020-05-23 09:28:34')


(3) 从Timestamp获取更多细分项目
print(d2.hour) #9
print(d2.minute) #28
print(d2.second) #34

print(d2.year) #2020
print(d2.month) #5
print(d2.day) #23



2. 批量化 获取
(1) 制造时间戳表格
import time
now = int(time.time())     # 1533952277

# make data
import random
arr0=[]
arr=[]
np.random.seed(1)
for n in range(now-1000000, now+10000, 20000):
    n2=n+ round(random.random()*100)
    timeArray = time.localtime(n2)
    otherStyleTime = time.strftime("%Y-%m-%d %H:%M:%S", timeArray)
    arr0.append(n2)
    arr.append(otherStyleTime)

# write
fw=open('test.txt','w')
for i in range(len(arr)):
    arr2=[str(i), str(arr0[i]), arr[i]]
    fw.write(','.join(arr2)+'\n')
fw.close()
print('==end==, len=', len(arr0))

## read using np
df=pd.read_csv('test.txt', header=None)
df.columns=['user_id', 'item_id', 'time_stamp']
df.iloc[0:10,]

# 	user_id	item_id	time_stamp
# 0	0	1590236777	2020-05-23 20:26:17
# 1	1	1590256792	2020-05-24 01:59:52
# 2	2	1590276724	2020-05-24 07:32:04


(2) 有一个坑: time.time()如果直接使用pd.to_datetime方法来转化的话,会与当前的北京时间相差8小时. 所以一般需要做修正.
df2=df.copy()
df2['new_ts']=pd.to_datetime(df2['item_id'], unit='s')

df2['new_ts2']=pd.to_datetime(df2['item_id']+8*3600, unit='s')
df2.iloc[0:10,]

# 	user_id	item_id	time_stamp	new_ts	new_ts2
# 0	0	1590236777	2020-05-23 20:26:17	2020-05-23 12:26:17	2020-05-23 20:26:17
# 1	1	1590256792	2020-05-24 01:59:52	2020-05-23 17:59:52	2020-05-24 01:59:52


## 再来
df3=df.copy() #深度拷贝
df3['new_ts2']=pd.to_datetime(df3['item_id']+8*3600, unit='s')
# df3.iloc[0:10,]
tmp=df3['new_ts2'].dt
tmp #<pandas.core.indexes.accessors.DatetimeProperties object at 0x7f6525882fd0>

# df3['day']=df3['new_ts2'].dt.day #简写
df3['year']=tmp.year
df3['month']=tmp.month
df3['day']=tmp.day
df3['hour']=tmp.hour
df3['minute']=tmp.minute
df3['second']=tmp.second
df3.iloc[0:10,]

# 	user_id	item_id	time_stamp	new_ts2	year	month	day	hour	minute	second
# 0	0	1590236777	2020-05-23 20:26:17	2020-05-23 20:26:17	2020	5	23	20	26	17
# 1	1	1590256792	2020-05-24 01:59:52	2020-05-24 01:59:52	2020	5	24	1	59	52



3. 产生一串时间：起点由origin设置，第一个参数是origin的增量[0,1,2,3]，增量单位unit(D天，s秒)
arr=pd.to_datetime([0, 1, 2, 3], unit='s', origin=pd.Timestamp('2020-6-4 10:53:33'))
print(arr)
arr.second
##
# DatetimeIndex(['2020-06-04 10:53:33', '2020-06-04 10:53:34',
#                '2020-06-04 10:53:35', '2020-06-04 10:53:36'],
#               dtype='datetime64[ns]', freq=None)
# Int64Index([33, 34, 35, 36], dtype='int64')



========================================
使用 pandas 对表格进行过滤：按一列或多列，保留或者去除
----------------------------------------
0. 文件读写
(1) 读取文件
import pandas as pd
import numpy as np

# 这是个文本文件，使用tab分隔符
data_filename="/home/marigold/data/web/docs/excel/root/NGS194-23.result.SNV.xls"

snp=pd.read_csv(data_filename, sep="\t")
snp

(2) 保存文件
snp_final.to_csv("/home/wangjl/data/web/docs/excel/root/NGS194-23.result.SNV-v2.csv")






1. 按某一列保留或删除

(1) relative 列是 某些值 的行保留
snp1=snp[ snp['relative'].isin([ "药物相关", "1762V" ])  ]
snp1

(2) ExonicFunc.refGene 列去除 不是同义突变 的行
pandas没有 isnotin 函数，只能自己实现:

# 实现方法1: not in synonymous 列表
snp2a=snp[snp['ExonicFunc.refGene'].map( lambda x: x not in ['-', 'synonymous SNV'] )]


(3) 按正则表达式，保留某一列含有 hae 的行
# 带 hae 的项
arr=tuple(snp['COSMIC_OCCURENCE'].unique())
arr2=[]
for i in range(len(arr)):
	if re.search("hae", arr[i]):
		arr2.append(arr[i])

# COSMIC_OCCURENCE 列：取出包含 hae, 不区分大小写;
snp3a = snp2a[ snp2a["COSMIC_OCCURENCE"].isin(arr2) ]
snp3.shape #(38, 28)


(4) 能否直接使用正则表达式筛选？
t1 = snp3 = snp[ snp["COSMIC_OCCURENCE"].str.contains("hae") ]
t1.shape #(38, 28)

t2 = snp3 = snp[ snp["COSMIC_OCCURENCE"].str.contains("hae", case=False) ] #忽略大小写
注意：contains 方法名前还有一个 ".str"

str.contains()：包含一个特定的字符串
	参数na：缺少值NaN处理
	参数case：大小写的处理
	参数regex：使用正则表达式模式

可以通过str.contains() 的参数na来指定替换NaN结果的值。
print(df_nan['name'].str.contains('li', na=False))

默认情况下，区分大小写。如果参数case为False，则case被忽略。
print(df['name'].str.contains('LI', case=False))

果参数regex为False，则确定是否包含第一个参数的字符串本身。
print(df['name'].str.contains('i.e', regex=False))
例如，如果要判断是否包含正则表达式的特殊字符，例如?,., 则需要设置regex = False。当然，也可以指定一个正则表达式模式，以转义\? 等特殊字符。

str.contain("M|r")这是包含M或r

str.contains() 等同于 re.search()，并且可以在flags参数中指定正则表达式标志。
str.endswith()：以特定字符串结尾
str.startswith()：以特定的字符串开头
str.match()：匹配正则表达式模式
要提取部分匹配的行，可以使用 pandas 转字符串后使用字符串方法 str.xxx() ，根据指定条件提取的字符串方法。









2. 按多列保留或去除

(1) 去掉: 有rs号(也就是不是-)，且CLNSIG是-
思路: 使用 data[] 按列取值，条件之间使用 & 表示 逻辑运算“且”，最后整体取“否”使用~
snp2d=snp2c[ ~( (snp2c['avsnp150']!='-') & (snp2c["CLNSIG"]=='-') )]






3. 按某一列合并表格：集合求并集
(1) 把三个筛选结果合并，不能有重复
开始前添加唯一编号列 index，
取筛选后的表格的index列求集合并集（|），按该列取子集，保证不重复，
最后去掉 index 辅助列

def mergeResult(snp1, snp2, snp3):
    print(snp1.shape[0], snp2.shape[0], snp3.shape[0])
    # 求并集
    index_keep = set(snp1["index"]) | set(snp2["index"]) | set(snp3["index"])
    snp_final = snp[snp['index'].isin(index_keep)]
    # 去掉最后的 index 列：axis=1 是按列
    snp_final=snp_final.drop(['index'], axis=1)
    print(len(index_keep))
    return snp_final

snp_final = mergeResult(snp1, snp2, snp3)
snp_final.shape







4. 逻辑运算 (AND, OR, NOT) 中提取行
使用Pandas从多个条件（AND，OR，NOT）中提取行的方法。

有以下2点需要注意：
	* &, |, ~ 的使用（and、or、not的错误）
	* 使用比较运算符时，请将每个条件括在括号中。

print(df['age'] < 35)
print((df['age'] < 35) & ~(df['state'] == 'NY'))
df_or = df[(df['age'] < 20) | (df['point'] > 90)]

运算符的优先级是NOT（~），AND（&），OR（|）。因此，结果因顺序而异。















========================================
np: 使用SVD分解压缩图像
----------------------------------------

1. 读取图片

import os
#os.chdir(r'D:\xampp\htdocs\txtBlog.py\data\jupyter')
os.chdir(r'G:\ML_MachineLearning\SVD')
os.getcwd()


# pip install Pillow -i "https://pypi.doubanio.com/simple/"
from PIL import Image

import numpy as np
import matplotlib.pyplot as plt
import os
# 读取图片
def getImage(fname):
    A=Image.open(fname, 'r')
    print(A)
    
    # 显示图片
    plt.figure("dog")
    plt.axis('off')# 去掉坐标轴
    plt.imshow(A)
    plt.show()
    return A
# test
img=getImage('dustbin/leaves2.png')





2. 使用SVD前k个奇异值压缩图像，并可视化

# 复原图像
def restore(sigma, u, v, K): #奇异值，左特征向量，右特征向量，前k个
    #print('--> restore K=', K)
    m=len(u)
    n=len(v[0])
    a=np.zeros((m,n))
    for k in range(K+1):
        for i in range(m):
            a[i] += sigma[k] * u[i][k] * v[k]
    b=a.astype('uint8') #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html
    #Image.fromarray(b).save('svd_'+str(K)+'.png')
    return Image.fromarray(b)
    
# 图片的rgb进行svd分解
def img2SVD(img):
    output_path=r".\dustbin"
    if not os.path.exists(output_path):
        os.mkdir(output_path)
    a0=np.array(img)
    a=a0[0:200, 0:300, :] #高2、宽3
    
    print('input image array: ', np.shape(a))
    # 获取rgb通道的svd分解
    K=40
    u_r,sigma_r, v_r=np.linalg.svd(a[:,:,0])
    u_g,sigma_g, v_g=np.linalg.svd(a[:,:,1])
    u_b,sigma_b, v_b=np.linalg.svd(a[:,:,2])
    
    plt.figure(figsize=(15,8), facecolor='w') # 宽、高
    
    for k in range(1,K+1):
        #print('img2SVD K=', K)
        R=restore(sigma_r, u_r, v_r, k)
        G=restore(sigma_g, u_g, v_g, k)
        B=restore(sigma_b, u_b, v_b, k)
        I=np.stack((R,G,B), axis=2)
        
        Image.fromarray(I).save(output_path+'\svd_'+str(K)+'.jpg')
        
        if k<=24:
            plt.subplot(4,6,k) #4行 6列
            plt.imshow(I)
            plt.axis('off')
            plt.title('sigma number:%d' % k)
    plt.suptitle('SVD and image decomposition', fontsize=20)
    plt.tight_layout(0.3, rect=(0,0,1,0.92))
# test
img2SVD(img)
#print('==end==')




========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------

