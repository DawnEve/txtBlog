webSpider


https://www.xuexi.cn/f327a4d94808a4b05942e48fd8036939/b2e5aa79be613aed1f01d261c4a2ae17.html


基于Tornado的异步爬虫， 线程池，代理ip：
https://python-web-guide.readthedocs.io/zh/latest/python-note/index.html



========================================
python爬虫大纲与教程
----------------------------------------
1.爬虫主要由以下几个功能部分组成
(1).访问网址，抓取数据: 使用urllib.request 内建模块, 第三方库requests, Scrapy框架等；
(2).解析数据(难点): 正则模块re、Xpath、Beautiful Soup等
(3).存储数据: 文件存储(txt/csv/xlsx等)、数据库存储



难点：
多线程抓取
保存到数据库
XPath解析数据 https://blog.csdn.net/u013332124/article/details/80621638
	lxml.etree 
	lxml.html
高级解析器： 经典的 BeautifulSoup 网页解析库 
cookie模拟登陆:
	神奇的 fake-useragent 这个随机生成各种 User-Agent 的库 


requests 比 urllib 人性化。
使用 selenium  吧 ，好用多了。


关于Requests库
Requests库官方的介绍有这么一句话:Requests 唯一的一个非转基因的 Python HTTP 库,人类可以安全享用。这句话直接并霸气地宣示了 Requests 库是 python 最好的一个HTTP库。
requests.get()获取html网页的主要方法，对应于http的get.
requests.post() 向html网页提交post请求的方法，对应于http的post.



常见的网页解析方法:
正则表达式使用比较困难,学习成本较高
BeautifulSoup 性能较慢,相对于 Xpath 较难,在某些特定场景下有用
Xpath 使用简单,速度快(Xpath是lxml里面的一种),是入门最好的选择



关于解析神器 XpathXpath 
即为 XML 路径语言(XML Path Language),它是一种用来确定 XML 文档中某部分位置的语言。Xpath 基于 XML 的树状结构,提供在数据结构树中找寻节点的能力。
Xpath解析网页的流程:
1.首先通过Requests库获取网页数据
2.通过网页解析,得到想要的数据或者新的链接
3.网页解析可以通过 Xpath 或者其它解析工具进行,Xpath 在是一个非常好用的网页解析工具







2.工具
(1)jupyter notebook
(2)Colab: https://colab.research.google.com
https://www.jianshu.com/p/81eae79ee78b


爬虫框架更好用：




3.爬虫教程
32个Python爬虫项目
https://www.jianshu.com/p/39d4b15c05ee

http://www.runoob.com/w3cnote/python-spider-intro.html

Python小爬虫——贴吧图片的爬取
https://www.cnblogs.com/Axi8/p/5757270.html

Python爬虫原理-简单版、复杂版、并发版
https://www.cnblogs.com/sss4/p/7809821.html



4.XPath精要
https://www.cnblogs.com/giserliu/p/4399778.html
附录：基本XPATH语法介绍，详细请参考XPath的官方文档

XPATH基本上是用一种类似目录树的方法来描述在XML文档中的路径。比如用“/”来作为上下层级间的分隔。第一个“/”表示文档的根节点（注意，不是指文档最外层的tag节点，而是指文档本身）。比如对于一个HTML文件来说，最外层的节点应该是”/html”。

同样的，“..”和“.”分别被用来表示父节点和本节点。

XPATH返回的不一定就是唯一的节点，而是符合条件的所有节点。比如在HTML文档里使用“/html/head/scrpt”就会把head里的所有script节点都取出来。

为了缩小定位范围，往往还需要增加过滤条件。过滤的方法就是用[]把过滤条件加上。比如在HTML文档里使用“/html/body/div[@id='main']”，即可取出body里id为main的div节点。

其中@id表示属性id，类似的还可以使用如@name, @value, @href, @src, @class….处理属性判断。

而 函数text()的意思则是取得节点包含的文本。比如：<div>hello<p>world</p>< /div>中，用"div[text()='hello']"即可取得这个div，而world则是p的text()。

函数position()的意思是取得节点的位置。比如"li[position()=2]"表示取得第二个li节点，它也可以被省略为"li[2]"。

不过要注意的是数字定位和过滤 条件的顺序。比如"ul/li[5][@name='hello']"表示取ul下第五项li，并且其name必须是hello，否则返回空。而如果用 "ul/li[@name='hello'][5]"的意思就不同，它表示寻找ul下第五个name为"hello"的li节点。

此外，*可以代替所有的节点名，比如用"/html/body/*/span"可以取出body下第二级的所有span，而不管它上一级是div还是p或是其它什么东东。

而 "descendant::"前缀可以指代任意多层的中间节点，它也可以被省略成一个“/”。比如在整个HTML文档中查找id为“leftmenu”的 div，可以用"/descendant::div[@id='leftmenu']"，也可以简单地使用"//div[@id='leftmenu']"。

至于"following-sibling::"前缀就如其名所说，表示同一层的下一个节点。"following-sibling::*"就是任意下一个节点，而"following-sibling::ul"就是下一个ul节点。






========================================
|-- 脚本登录后下载内容：登录获取 token 保存到 cookie 中再次请求
----------------------------------------
1. 学习的脚本
(1) 定义一个类

class bcl2fq(object):
    """
        自动化数据分拆工具
    """

    def __init__(self):
        """
        初始化,获取运行参数
        """
        #指定参数
        # ...
        if 'barcode-mismatches' not in self.args:
            self.args['barcode-mismatches']='1'
        self.query = '''
            bcl2fastq -r %(loading-threads)s -p %(processing-threads)s -w %(writing-threads)s %(no-lane-splitting)s \
                --barcode-mismatches %(barcode-mismatches)s \
                --runfolder-dir %(runfolder-dir)s \
                --sample-sheet  %(sample-sheet)s \
                --output-dir    %(output-dir)s
        '''
    
    # 执行linux命令
    def execute(self,workingDir="."):
        '''
            本地运行bcl2fastq
        '''
        start = time.time()
        try:
            query = self.query % self.args
            print(query)
            # 得到一个临时文件对象， 调用close后，此文件从磁盘删除
            #out_temp = tempfile.TemporaryFile(mode='w+')
            # 获取临时文件的文件号
            #fileno = out_temp.fileno()
            sub = subprocess.Popen(query,shell=True,cwd=os.path.expanduser(workingDir),stdout=subprocess.PIPE,stderr=subprocess.STDOUT)
            while sub.poll() is None:
                line = sub.stdout.readline()
                line = line.strip()
                if line:
                    print(line)
                if sub.returncode == 0:
                    print('Demultiplex Run Succeed')
            #sub.wait()
            #out_temp.seek(0)
            #print(out_temp.read())
        except Exception as e:
            print((str(e)))
        #finally:
            #if out_temp:
            #    out_temp.close()
        stop  = time.time()
        print(('Run time:'+str(round((stop-start),3))+' seconds'))


(3) 运行主体
    def process(self):
        ''' 
            1、从web登录 系统，获取Session ID之后下载SampleSheet 
            2、本地运行bcl2fastq，拆分数据
            3、Web发送请求，更新系统信息，标记已经拆分过数据的Sample
            4、远程退出账户，完成操作
        '''
        #构造Session
        session = requests.Session()
        #在session中发送登录请求，此后这个session里就存储了cookie
        #可以用print(session.cookies.get_dict())查看
        res = session.post(
            self.url_login,
            self.data,
            verify  = self.verify,
            headers = self.headers
        )
        state = json.loads(res.content)
        if state['status'] and state['status']=='SUCCESS':
            #发送访问请求
            print('成功登录系统')
            res = session.get(
                self.url_down+'?id='+self.args['id'], 
                verify  = self.verify,
                headers = self.headers
            )
            print((res.headers))
            if (res.headers['Content-Disposition']) and res.headers['Content-Disposition']=='attachment;fileName=SampleSheet.csv':
                if 'sample-sheet' not in self.args:
                    self.args['sample-sheet']=self.args['runfolder-dir']+os.sep+'SampleSheet.csv'
                with open(self.args['sample-sheet'], "wb") as code:
                    code.write(res.content)

                if os.path.exists(self.args['sample-sheet']):
                    self.execute()
                    result = self.args['output-dir']+os.sep+'Stats'+os.sep+'Stats.json'
                    if(os.path.exists(result)):
                        with open(result) as file_object:
                            data = json.load(file_object)
                            ps   = {
                                'sampleRunId':self.args['id'],
                                'state':'true',
                                'stat':json.dumps(data)
                            }
                            print(ps)
                            res = session.post(
                                self.url_update,
                                ps,
                                verify  = self.verify,
                                headers = self.headers
                            )
        res = session.get(
            self.url_logout,
            verify  = self.verify,
            headers = self.headers
        )
        state = json.loads(res.content)
        if state['status'] and state['status']=='SUCCESS':
            print('成功退出系统')

(4) 程序启动
if __name__=="__main__":
    bcl2fq = bcl2fq() #类的实例化，执行类中 __init__ 方法
    bcl2fq.process()







2. 以上代码中 值得学习的点

(1) 登录后请求资源，并下载
import requests
session = requests.Session()
session #<requests.sessions.Session at 0x7f71ac126c90>

res = session.post(
       "https://192.168.1.3:8443/doLogin",
        {
            "userName": "xxx",
            "userPass": "20191124"
        },
        verify  = False,
        headers = {
            'User-agent': "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36"
        }
    )

输出: /home/wangjl/soft/python3/lib/python3.7/site-packages/urllib3/connectionpool.py:1102: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.1.3'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings
#  InsecureRequestWarning,


import json
state = json.loads(res.content)
state['status'] #'SUCCESS'

print((res.headers))
# {'Set-Cookie': 'JSESSIONID=abccb0f6-1ba0-4fbc-9742-8ac82108ce65; Path=/; HttpOnly, rememberMe=deleteMe; Path=/; Max-Age=0; Expires=Thu, 24-Aug-2023 08:06:16 GMT', 'Content-Type': 'application/json;charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Date': 'Fri, 25 Aug 2023 08:06:16 GMT'}

# 验证信息已经保存到 session 中了
print(session.cookies.get_dict())
# {'JSESSIONID': 'abccb0f6-1ba0-4fbc-9742-8ac82108ce65'}


# 再次请求资源
res2 = session.get(
        "https://192.168.1.3:8443/sample/genSamplesheet?id=230823", 
        verify  = False,
        headers = {
            'User-agent': "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36"
        }
    )

print((res2.headers))
# {'Cache-Control': 'no-cache, no-store, must-revalidate', 'Pragma': 'no-cache', 'Expires': '0', 'Content-Disposition': 'attachment;fileName=SampleSheet.csv', 'Transfer-Encoding': 'chunked', 'Date': 'Fri, 25 Aug 2023 08:06:16 GMT'}

res2.headers['Content-Disposition']
# 'attachment;fileName=SampleSheet.csv'


res.content #二进制
b'{"result":{"userTheme":"theme_1","userEmail":"demo@sliverworkspace.com","updateTime":"2023-08-25 16:06:16","userName":"sliverworkspace","userId":"100000001","roleNames":["super_admin"],"token":"abccb0f6-1ba0-4fbc-9742-8ac82108ce65"},"code":200,"message":"Login Success","status":"SUCCESS","timestamp":"2023-08-25 16:06:16.508"}'


ctt = res2.content
ctt
print(ctt.decode()) #解码

输出结果：
"[Header]"
"IEMFileVersion","5"
"Experiment Name","fusion_data"
"Date","2023/8/25"
"Workflow","GenerateFASTQ"
"Application","NextSeq FASTQ Only"
"Instrument Type","NextSeq 550"
"Assay","TruSeq HT"
"Index Adapters","TruSight Tumor 15 genes"
"Chemistry","Amplicon"
"[Manifests]"
"A","TST_15-A-Manifest.txt"
"[Reads]"
"151"
"151"
"[Settings]"
"[Data]"
"Sample_ID","Sample_Name","Sample_Plate","Sample_Well","I7_Index_ID","index","I5_Index_ID","index2","Manifest","Sample_Project"
"RTR314-23","RTR314-23",,,"93","TAACCGGT","93","ATCGTCTC",,"fusion_data"
"RTR315-23","RTR315-23",,,"94","AACCGTTC","94","CTAGCTCA",,"fusion_data"
"RTR316-23","RTR316-23",,,"95","TGGTACAG","95","TCGAGAGT",,"fusion_data"
"RTR317-23","RTR317-23",,,"96","ATATGCGC","96","ACGATCAG",,"fusion_data"
"RTR318-23","RTR318-23",,,"1","CTGATCGT","1","GCGCATAT",,"fusion_data"
"RTR319-23","RTR319-23",,,"2","ACTCTCGA","2","CTGTACCA",,"fusion_data"
"RTR320-23","RTR320-23",,,"3","TGAGCTAG","3","GAACGGTT",,"fusion_data"
"RHNGS19-23","RHNGS19-23",,,"4","GAGACGAT","4","ACCGGTTA",,"fusion_data"
"NGS218-23","NGS218-23",,,"81","CGACGTTA","81","CTCTGGAT",,"SNV1_tumor_only"
"NGS219-23","NGS219-23",,,"82","TACGCCTT","82","GCTACTCT",,"SNV1_tumor_normal"
"NGS220-23","NGS220-23",,,"83","CCGTAAGA","83","AGAGTCCA",,"SNV1_tumor_only"
"RTT314-23","RTT314-23",,,"84","ATCACACG","84","GTAGCGTA",,"SNV1_tumor_only"
"RTT315-23","RTT315-23",,,"85","CACCTGTT","85","AGGATAGC",,"SNV1_tumor_only"
"RTT316-23","RTT316-23",,,"86","CTTCGACT","86","GATCTTGC",,"SNV1_tumor_normal"
"RTT317-23","RTT317-23",,,"87","TGCTTCCA","87","CGATCGAT",,"SNV1_tumor_only"
"RTT318-23","RTT318-23",,,"88","AGAACGAG","88","ATTAGCCG",,"SNV1_tumor_only"
"RTT319-23","RTT319-23",,,"89","GTTCTCGT","89","TGTTCCGT",,"SNV1_tumor_only"
"RTT320-23","RTT320-23",,,"90","TCAGGCTT","90","ATCATGCG",,"SNV1_tumor_only"
"NGS219-23NC","NGS219-23NC",,,"91","CCTTGTAG","91","CCTTGGAA",,"NC"
"RTT316-23NC","RTT316-23NC",,,"92","GAACATCG","92","TCGACAAG",,"NC"






(2) 参数拼凑，命名参数？

常规1：类似C风格的 sprintf("hello, %s", "world") 替换
    "this is %s %s" % ("apple", "tree")
    输出：'this is apple tree'
常规2： f word，前面加上f，中间使用花括号括着变量
    str1="apple"
    str2="tree"
    f"this is {str1} {str2}"
    输出：'this is apple tree'

不常规：可以无视顺序，使用字典顺填充
    args = {
        'str1': "apple",
        'str2': "tree"
    }
    "this is %(str2)s %(str1)s" % args
    输出：'this is tree apple'



## 实例
query = '''
    bcl2fastq -r %(loading-threads)s -p %(processing-threads)s -w %(writing-threads)s %(no-lane-splitting)s \
        --barcode-mismatches %(barcode-mismatches)s \
        --runfolder-dir %(runfolder-dir)s \
        --sample-sheet  %(sample-sheet)s \
        --output-dir    %(output-dir)s
'''

args = {
    "runfolder-dir": "/home/xx",
    "input-dir": "/home/xx/Data/Intensities/BaseCalls/",
    "output-dir": "/home/xx/Data/Intensities/BaseCalls/",
    "sample-sheet": "/home/xx/SampleSheet.csv",
    "no-lane-splitting": "",
    "loading-threads": "8",
    "processing-threads":"8",
    "writing-threads": '8',
    "barcode-mismatches": "1"
}

query2 = query % args
print(query2)

输出:
bcl2fastq -r 8 -p 8 -w 8          --barcode-mismatches 1         --runfolder-dir /home/xx         --sample-sheet  /home/xx/SampleSheet.csv         --output-dir    /home/xx/Data/Intensities/BaseCalls/





(3) py 多进程运行 linux 命令，并获取 退出状态码：阻塞主进程
import subprocess,os

workingDir="."
try:
    query = "echo 1 && sleep 2  && echo 2 && sleep 2  && echo 3 && sleep 2"
    print(query)
    # 得到一个临时文件对象， 调用close后，此文件从磁盘删除
    #out_temp = tempfile.TemporaryFile(mode='w+')
    # 获取临时文件的文件号
    #fileno = out_temp.fileno()
    sub = subprocess.Popen(query, shell=True,cwd=os.path.expanduser(workingDir),stdout=subprocess.PIPE,stderr=subprocess.STDOUT)
    while sub.poll() is None:
        line = sub.stdout.readline()
        line = line.strip()
        if line:
            print(line)
    # returncode 不应该在 while 循环中，否则永远不会输出
    if sub.returncode == 0:
        print('Demultiplex Run Succeed')
    #sub.wait()
    #out_temp.seek(0)
    #print(out_temp.read())
except Exception as e:
    print((str(e)))

print("main process")


输出: 说明阻塞主线程了！
echo 1 && sleep 2  && echo 2 && sleep 2  && echo 3 && sleep 2
b'1'
b'2'
b'3'
Demultiplex Run Succeed
main process 0










========================================
【重点】requests包获取网页
----------------------------------------

import requests

response  = requests.get("https://www.baidu.com")
print(type(response))
print(response.status_code)
print(type(response.text))
print(response.text)
print(response.cookies)
print(response.content)
print(response.content.decode("utf-8"))


关于请求乱码问题：
    不管是通过response.content.decode("utf-8)的方式
    还是通过response.encoding="utf-8"的方式都可以避免乱码的问题发生
	
请求：
	GET请求：URL查询字符串传递数据，通常我们会通过httpbin.org/get?key=val方式传递
	data = {
		"name":"zhaofan",
		"age":22
	}
	response = requests.get("http://httpbin.org/get",params=data)

	注：如果字典中的参数为None则不会添加到url上

	解析json:
			requests里面集成的json其实就是执行了json.loads()方法，两者的结果是一样的

	获取二进制数据：
			在上面提到了response.content，这样获取的数据是二进制数据，同样的这个方法
			也可以用于下载图片以及视频资源

	添加请求头（headers）：
			headers = {

				"User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
			}
			response =requests.get("https://www.zhihu.com",headers=headers)

	基本POST请求：

			通过在发送post请求时添加一个data参数，这个data参数可以通过字典构造成，这样
			对于发送post请求就非常方便。

	响应：
		可以通过response获得很多属性
		 response.status_code 状态码
		 response.headers  请求头
		 response.cookies  cookie
		 response.url
		 response.history

requests高级用法
	文件上传
		files= {"files":open("git.jpeg","rb")}
		response = requests.post("http://httpbin.org/post",files=files)
	获取cookie
		for key,value in response.cookies.items():
		 print(key+"="+value)
	会话维持
		s = requests.Session()
		s.get("http://httpbin.org/cookies/set/number/123456")
		response = s.get("http://httpbin.org/cookies")
	证书验证
		现在的很多网站都是https的方式访问，所以这个时候就涉及到证书的问题
		利用：verify=False
		response = requests.get("https://www.12306.cn",verify=False)
	代理设置
		proxies= {
			"http":"http://127.0.0.1:9999",
			"https":"http://127.0.0.1:8888"
		}
		response  = requests.get("https://www.baidu.com",proxies=proxies)
		如果代理需要设置账户名和密码,只需要将字典更改为如下：
		proxies = {
		"http":"http://user:password@127.0.0.1:9999"
		}
		如果你的代理是通过sokces这种方式则需要pip install "requests[socks]"
		proxies= {
		"http":"socks5://127.0.0.1:9999",
		"https":"sockes5://127.0.0.1:8888"
		}
	超时设置
		通过timeout参数可以设置超时的时间
	认证设置（碰到需要认证的网站）
		from requests.auth import HTTPBasicAuth
		response = requests.get("http://120.27.34.24:9001/",auth=HTTPBasicAuth("user","123"))

		或者
		response = requests.get("http://120.27.34.24:9001/",auth=("user","123"))
	异常处理



refer:
https://www.cnblogs.com/wanmudong/p/8073822.html





========================================
实例: 下载视频的简单爬虫：[requests,re,file]
----------------------------------------
#下载视频

import re
import requests

#发出请求
respose=requests.get('http://www.xiaohuar.com/v/')
#print(respose.status_code)# 响应的状态码 200
#print(respose.content)  #返回字节信息
#print(respose.text)  #返回文本内容

#正则解析内容
urls=re.findall(r'class="items".*?href="(.*?)"',respose.text,re.S)  #re.S 把文本信息转换成1行匹配
#print(urls)
url=urls[5]

#发送请求2
result=requests.get(url)
#解析出视频链接
mp4_url=re.findall(r'id="media".*?src="(.*?)"',result.text,re.S)[0]
print('mp4_url:',mp4_url)

#发送请求3
video=requests.get(mp4_url)
#写入二进制文件
with open('a.mp4','wb') as f:
    f.write(video.content)
 
print("end")





========================================
实例: 解析有道词典html元素 [lxml.etree,xpath,print]
----------------------------------------
1. 实例
from lxml import etree

html = '<table id="table1" cellspacing="0px"> <tr><th>编号</th><th>姓名</th><th>年龄</th></tr><tr><td>1</td><td>张三</td><td>11</td></tr><tr><td>2</td><td>李四</td><td>12</td></tr><tr><td>3</td><td>王五</td><td>13</td></tr><tr><td>4</td><td>马六</td><td>14</td></tr></table>'
content2 = etree.HTML(html)

#第一次解析
rows = content2.xpath('//table[@id="table1"]/tr')[1:]
for row in rows:
	#第二次解析
    id = row.xpath('./td[1]/text()')[0]
    name = row.xpath('./td[2]/text()')[0]
    age = row.xpath('./td[3]/text()')[0]
    print(id, name, age)

#1 张三 11
#2 李四 12
#3 王五 13
#4 马六 14






2. 解析网易词典，获取所有词条
(1) xpath需要抓取的部分
<div id="phrsListTab" class="trans-wrapper clearfix">
	<h2 class="wordbook-js">
		<span class="keyword">some</span>
		<div class="baav">
			<span class="pronounce">英
				<span class="phonetic">[səm; sʌm]</span>
				<a href="#" title="真人发音" class="sp dictvoice voice-js log-js" data-rel="some&type=1" data-4log="dict.basic.ec.uk.voice"></a>
			</span>
			<span class="pronounce">美
				<span class="phonetic">[səm,sʌm]</span>
				<a href="#" title="真人发音" class="sp dictvoice voice-js log-js" data-rel="some&type=2" data-4log="dict.basic.ec.us.voice"></a>
			</span>
		</div>
	</h2>

	<div class="trans-container">
		<ul>
			<li>det. 一些；某些；好些；少量；某个；算不上；大约；至少有一点</li>
			<li>adj. 某个的；某些的；显著的；大量的；至少有一个的</li>
			<li>pron. （数量不确切时用）有些人，有些事物；部分</li>
			<li>adv. 大约；稍微；（非正式）在某种程度上</li>
			<li>n. (Some) （美、印）索梅（人名）</li>
		</ul>
	</div>
</div>

(2) 代码
import time, requests
from lxml import etree

def getHTML(word):
    url="http://dict.youdao.com/w/{}/#keyfrom=dict2.top".format(word)
    res=requests.get(url)
    return res.text

def open_url(word):
    html=getHTML(word)
    return html
# test
r=open_url('some')
print(r)

tree=etree.HTML(r)
tree


rs=tree.xpath('//*[@id="phrsListTab"]/div/ul/li/text()')
rs

rs2=tree.xpath('//div[@id="phrsListTab"]//li/text()') #同样结果
rs2

输出结果： 
['det. 一些；某些；好些；少量；某个；算不上；大约；至少有一点',
 'adj. 某个的；某些的；显著的；大量的；至少有一个的',
 'pron. （数量不确切时用）有些人，有些事物；部分',
 'adv. 大约；稍微；（非正式）在某种程度上',
 'n. (Some) （美、印）索梅（人名）']
#





========================================
实例: 下载英文小说的简单爬虫: [requests,re,file]
----------------------------------------
1.
def downLoad(url, pageNumber=93):
    #url="https://bluenovels.net/married-by-morning-the-hathaways-4-page-%d/"
    import time, re;

    fw=open('a0011.html', 'w', encoding='utf-8')

    for page in range(1,pageNumber+1):
        if page % 10==0 or pageNumber-page<=5:
            print(page)
        #1. 发出请求
        url2=url % page
        respose=requests.get(url2)
        assert respose.status_code==200,'Error: requests.get'
        #print(respose.text)

        #2. get text
        rs=re.findall(r'page-detail-content-text.*?\><p>(.*?)\<\/p\>\<div',respose.text,re.S)[0]

        #3. save to file
        fw.write("="*20 + '<br>Chapter: '+str(page)+'<br>'+'='*20+'<br>')
        fw.write(rs + "<br>"*2+url2)
        fw.write('<br>'*3)

        #4. pause
        time.sleep(0.1)
    fw.close()
#








========================================
实例: 下载英文小说的爬虫: [requests,bs4,file] 202410
----------------------------------------

import requests
from bs4 import BeautifulSoup
import re


def getAllURL(url):
    response  = requests.get(home_url)
    soup=BeautifulSoup(response.text,"html.parser")
    patterns=soup.find_all("a", attrs={'href': re.compile("^/novel/ancient-strengthening-technique/")} )
    print( len(patterns) )
    return patterns

home_url="http://163.dawneve.cc/novel/ancient-strengthening-technique/"
urls = getAllURL(home_url) #2489


urls[0] #<a href="/novel/ancient-strengthening-technique/ast-chapter-1"> Chapter 1 - Qing Clan, Qing Shui </a>
base_url="http://163.dawneve.cc/"
url=base_url + urls[0].attrs.get("href")
url #'http://163.dawneve.cc//novel/ancient-strengthening-technique/ast-chapter-1'
urls[0].text #标题 ' Chapter 1 - Qing Clan, Qing Shui '



def getContent(url):
    response  = requests.get(url)
    soup=BeautifulSoup(response.text,"html.parser")

    # 网站可能改版了修改了这个id
    patterns=soup.find_all("div", attrs={'id': "chapterContent"} )
    if len(patterns)==0:
        patterns=soup.find_all("div", attrs={'id': "chapter-content"} )
    patterns=patterns[0]

    ##
    text=patterns.find_all("p")
    texts = ""
    for i in range( len(text) ):
        str2 = "" if text[i].string == None else text[i].string
        if str2!="":
            if str2.startswith("[") or str2.startswith("TL: Bluefire") or str2.startswith("Editor: Ziltch"):
                texts += str2 + "\n"
            else:
                texts += str2 + "\n\n"
    return(texts)
texts = getContent(url)


fw=open('xxx001.txt', 'w',errors='ignore')
import time

base_url="http://163.dawneve.cc/"

for i in range(len(urls)):
    url=base_url + urls[i].attrs.get("href")
    print(i, url)

    title=urls[i].text
    texts = getContent(url)

    #save
    fw.write("\n"+"="*20 + "\n# ")
    if not texts.startswith("Chapter"):
        fw.write(title + "\n\n")
    fw.write(texts)
    fw.write("\nurl:" +str(url)+ "\n\n\n\n")

fw.close()
print('==end==')







========================================
实例: 下载中文小说的简单爬虫: [urllib,bs4,file]
----------------------------------------
2. 中文小说

#import requests
from bs4 import BeautifulSoup
import urllib.request

url0="https://www.kquanben.com/xiaoshuo/20544/109065%s.html"

def getContent(url):
    html = urllib.request.urlopen(url).read()
    html = html.decode('gbk')
    # 2.
    soup=BeautifulSoup(html,"lxml")
    #3.
    patterns=soup.find_all("div",attrs={'id':"content"})
    title=soup.find_all("h1")[1].string
    return title,patterns

fw=open('xxx001.txt', 'w',errors='ignore')
import time

for i in range(33,51):
    url=url0 % str(i);
    print(i, url)
    title, text=getContent(url)
    
    fw.write("\n"+"="*20+title+"\n")
    
    for s in text[0].strings:
        fw.write(s)
    
    fw.write("\nurl:" +str(url)+ "\n\n")
    time.sleep(0.2)

fw.close()
print('==end==')







========================================
实例: 下载微信文章的简单爬虫: [requests,re,file]
----------------------------------------
#下载微信文章

import re
import requests

#发出请求
respose=requests.get('https://mp.weixin.qq.com/s?__biz=MzI0NDcxNzc5Mg==&mid=2247485084&idx=1&sn=70239065aaa1f59610dab9894df7f745')
assert respose.status_code==200,'Error: requests.get'

#print(respose.text)

title=re.findall(r'msg_title = "(.*?)"',respose.text,re.S)[0]
print('title=',title)

msg_desc=re.findall(r'msg_desc = "(.*?)"',respose.text,re.S)[0]
print('msg_desc=',msg_desc)

rs=re.findall(r'id="js_content"\>(.*?)\<\/div\>',respose.text,re.S)[0]
#print('rs=', rs)

#写入文件
with open('/home/wangjl/web/en.html','w') as f:
    f.write('<meta http-equiv="Content-Type" content="text/html; charset=utf-8">')
    f.write(rs)

print("end===")






========================================
实例: 获取自己的外网IP: [requests, re, print]
----------------------------------------
import requests
import re

def get_ip_by_ip138():
    response = requests.get("http://2018.ip138.com/ic.asp")
    ip = re.search(r"\[\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\]",response.content.decode(errors='ignore')).group(0)
    return ip

print("本机的ip地址为:",get_ip_by_ip138())

# 本机的ip地址为: [116.7.234.239]





========================================
【难点】 网页内容解析: XPath
----------------------------------------
对 XPath 的理解是很多高级 XML 应用的基础。


1.使用场景
在运行豆瓣爬虫时，由于页面太复杂，用正则解析太繁琐，而使用xpath则很简洁。

#豆瓣电影
import requests
from lxml import html
url='https://movie.douban.com/' #需要爬数据的网址
page=requests.Session().get(url)

tree=html.fromstring(page.text)
#print(page.text)

# xpath:  https://www.cnblogs.com/giserliu/p/4399778.html
#result=tree.xpath('//td[@class="title"]//a/text()') #获取需要的数据[失败，可能是豆瓣改版了]
result=tree.xpath('//ul/li[@class="ui-slide-item"]') #该怎么获取电影名字？

#输出结果
print('len=',len(result))
print(result[0])

#end
print("==end==")





2.学习 xpath 
XPath 就是一个用来查找xml节点的路径语言，一个路径字符串语法
XPath和re的关系，其实跟jQuery和原生js的关系差不多的，前者更简洁。

http://www.w3school.com.cn/xpath/index.asp

什么是 XPath?
	XPath 使用路径表达式在 XML 文档中进行导航
	XPath 包含一个标准函数库
	XPath 是 XSLT 中的主要元素
	XPath 是一个 W3C 标准





(1)Node节点：
在 XPath 中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档节点（或称为根节点）。

请看下面这个 XML 文档：
<?xml version="1.0" encoding="ISO-8859-1"?>
<bookstore>

<book>
  <title lang="en">Harry Potter</title>
  <author>J K. Rowling</author> 
  <year>2005</year>
  <price>29.99</price>
</book>

</bookstore>

上面的XML文档中的节点例子：
	<bookstore> （文档节点）
	<author>J K. Rowling</author> （元素节点）
	lang="en" （属性节点） 


基本值（或称原子值，Atomic value）是无父或无子的节点。
基本值的例子：
J K. Rowling
"en"

项目（Item）是基本值或者节点。

同胞（Sibling）,拥有相同的父的节点

先辈（Ancestor）,某节点的父、父的父，等等。
	上例中，title 元素的先辈是 book 元素和 bookstore 元素。

后代（Descendant），某个节点的子，子的子，等等。
	上例中，bookstore 的后代是 book、title、author、year 以及 price 元素：






(2)XPath 语法
XPath 使用路径表达式来选取 XML 文档中的节点或节点集。节点是通过沿着路径 (path) 或者步 (steps) 来选取的。

$ cat bookstore.xml
<?xml version="1.0" encoding="ISO-8859-1"?>

<bookstore>

<book>
  <title lang="eng">Harry Potter</title>
  <price>29.99</price>
</book>

<book>
  <title lang="eng">Learning XML</title>
  <price>39.95</price>
</book>

</bookstore>


Python解析xml的框架 XML_Demo.py
import requests
from lxml import html

#1.请求数据
url='http://y.biomooc.com/wangjl/bookstore.xml' #需要爬数据的网址
page=requests.Session().get(url)

#2.解析数据，与匹配
# https://blog.csdn.net/liaoningxinmin/article/details/80377252
tree=html.fromstring(page.content)
#################
#XPath start
#################
tree.xpath('//@lang') #['eng', 'eng'] 

#后面就模仿最后一句话，开始写XPath





下面列出了最有用的路径表达式：
表达式	描述
nodename	选取此节点的所有子节点。
/	从根节点选取。
//	从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。
.	选取当前节点。
..	选取当前节点的父节点。
@	选取属性。





谓语（Predicates）
谓语用来查找某个特定的节点或者包含某个指定的值的节点。

谓语被嵌在方括号中。
实例
在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：

路径表达式	结果
/bookstore/book[1]	选取属于 bookstore 子元素的第一个 book 元素。
/bookstore/book[last()]	选取属于 bookstore 子元素的最后一个 book 元素。
/bookstore/book[last()-1]	选取属于 bookstore 子元素的倒数第二个 book 元素。
/bookstore/book[position()<3]	选取最前面的两个属于 bookstore 元素的子元素的 book 元素。
//title[@lang]	选取所有拥有名为 lang 的属性的 title 元素。
//title[@lang='eng']	选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。


在python中还是要用//开头的，注意单双引号的交替。
>>tree.xpath('//title[@lang="eng"]') #2个元素

选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。
>>tree.xpath('//bookstore/book[price>35]') #1个元素

选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。
>>tree.xpath('//bookstore/book[price>35]/title')





选取未知节点
XPath 通配符可用来选取未知的 XML 元素。

通配符	描述
*	匹配任何元素节点。
@*	匹配任何属性节点。
node()	匹配任何类型的节点。


路径表达式	结果
/bookstore/*	选取 bookstore 元素的所有子元素。
//*	选取文档中的所有元素。
//title[@*]	选取所有带有属性的 title 元素。





选取若干路径
通过在路径表达式中使用“|”运算符，您可以选取若干个路径。相当于 或 。

实例
在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：

路径表达式	结果
//book/title | //book/price	选取 book 元素的所有 title 和 price 元素。
//title | //price	选取文档中的所有 title 和 price 元素。
/bookstore/book/title | //price	选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。






(3)XPath 轴
轴可定义相对于当前节点的节点集。

轴名称	结果
ancestor	选取当前节点的所有先辈（父、祖父等）。
ancestor-or-self	选取当前节点的所有先辈（父、祖父等）以及当前节点本身。
attribute	选取当前节点的所有属性。
child	选取当前节点的所有子元素。
descendant	选取当前节点的所有后代元素（子、孙等）。
descendant-or-self	选取当前节点的所有后代元素（子、孙等）以及当前节点本身。
following	选取文档中当前节点的结束标签之后的所有节点。
namespace	选取当前节点的所有命名空间节点。
parent	选取当前节点的父节点。
preceding	选取文档中当前节点的开始标签之前的所有节点。
preceding-sibling	选取当前节点之前的所有同级节点。
self	选取当前节点。





路径表达式是从一个XML节点（当前的上下文节点）到另一个节点、或一组节点的书面步骤顺序。
这些步骤以“/”字符分开，每一步有三个构成成分：  /step1/step2/step3/
 - 轴描述（用最直接的方式接近目标节点）
 - 节点测试（用于筛选节点位置和名称）
 - 节点描述（用于筛选节点的属性和子节点特征）
一般情况下，我们使用简写后的语法。虽然完整的轴描述是一种更加贴近人类语言，利用自然语言的单词和语法来书写的描述方式，但是相比之下也更加啰嗦。


[换个描述再说一遍] 步（step）包括：
 - 轴（axis）: 定义所选节点与当前节点之间的树关系
 - 节点测试（node-test）:识别某个轴内部的节点
 - 零个或者更多谓语（predicate）:更深入地提炼所选的节点集
步的语法：
	轴名称::节点测试[谓语]


实例
例子	结果
child::book	选取所有属于当前节点的子元素的 book 节点。
attribute::lang	选取当前节点的 lang 属性。
child::*	选取当前节点的所有子元素。
attribute::*	选取当前节点的所有属性。
child::text()	选取当前节点的所有文本子节点。
child::node()	选取当前节点的所有子节点。
descendant::book	选取当前节点的所有 book 后代。
ancestor::book	选择当前节点的所有 book 先辈。
ancestor-or-self::book	选取当前节点的所有 book 先辈以及当前节点（如果此节点是 book 节点）
child::*/child::price	选取当前节点的所有 price 孙节点。

没搞懂，什么意思？







(4)XPath 运算符
XPath 表达式可返回节点集、字符串、逻辑值以及数字。
http://www.w3school.com.cn/xpath/xpath_operators.asp

运算符	描述	实例	返回值
|	计算两个节点集	//book | //cd	返回所有拥有 book 和 cd 元素的节点集
+	加法	6 + 4	10
-	减法	6 - 4	2
*	乘法	6 * 4	24
div	除法	8 div 4	2
=	等于	price=9.80	如果 price 是 9.80，则返回 true。否则返回 false。







(5)XPath 标准函数
XPath 含有超过 100 个内建的函数。这些函数用于字符串值、数值，日期和时间比较、节点和 QName 处理、序列处理、逻辑值等等。
http://www.w3school.com.cn/xpath/xpath_functions.asp

. starts-with函数
获取以xxx开头的元素 
例子：xpath('//div[starts-with(@class,"test")]')

contains函数
获取包含xxx的元素 
例子：xpath('//div[contains(@id,"test")]')

and
与的关系 
例子：xpath('//div[contains(@id,"test") and contains(@id,"title")]')

text()函数
例子1：xpath('//div[contains(text(),"test")]') 
例子2：xpath('//div[@id="test"]/text()')









3.实例学习 XPath
(0)抓取原文 test.html

<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
    <title>xpath test</title>
</head>
<body>
<div price="99.8">
    <div>
        <ul>
            <li>时间</li>
            <li>地点</li>
            <li>任务</li>
        </ul>
    </div>
    <div id='testid' data-h="first">
        <h2>这里是个小标题</h2>
        <ol>
            <li data="one">1</li>
            <li data="two">2</li>
            <li data="three">3</li>
        </ol>
        <ul>
            <li code="84">84</li>
            <li code="104">104</li>
            <li code="223">223</li>
        </ul>
    </div>
    <div>
        <h3>这里是H3的内容
            <a href="http://www.baidu.com">百度一下</a>
            <ul>
                <li>test1</li>
                <li>test2</li>
            </ul>
        </h3>
    </div>
    <div id="go">
        <ul>
            <li>1</li>
            <li>2</li>
            <li>3</li>
            <li>4</li>
            <li>5</li>
            <li>6</li>
            <li>7</li>
            <li>8</li>
            <li>9</li>
            <li>10</li>
        </ul>
    </div>
</div>
</body>
</html>


(1)爬虫代码主体
version1.1
import requests
from lxml import html

#1.请求数据
url='http://y.biomooc.com/wangjl/test.html' #需要爬数据的网址
page=requests.Session().get(url)

#2.解析数据
page2=page.text
page3=page2.encode('ISO-8859-1').decode(page.apparent_encoding)
#print(page3) #不乱码了

#3.匹配
tree=html.fromstring(page3)
#################
#XPath start
#################




version1.2
import requests
from lxml import html

#1.请求数据
url='http://y.biomooc.com/wangjl/test.html' #需要爬数据的网址
page=requests.Session().get(url)

#2.解析数据，与匹配
tree=html.fromstring(page.content)
#################
#XPath start
#################
tree.xpath('//@code') #['84', '104', '223']




(2)xpath抓取实例
1)# 匹配包含某属性的所有的属性值//@lang
>>tree.xpath('//@code') #['84', '104', '223']


2)使用|或，设置多个条件
>>tree.xpath('//div[@id="testid"]/h2/text() | //li[@data]/text()') #多个匹配条件
#['这里是个小标题', '1', '2', '3']



3)轴Axes: child 选取当前节点的所有子元素
>>tree.xpath('//div[@id="testid"]/child::ul/li/text()') #child子节点定位
# ['84', '104', '223']

>>tree.xpath('//div[@id="testid"]/child::*') #child::*当前节点的所有子元素
#[<Element h2 at 0x7f1014644958>, <Element ol at 0x7f1014644f48>, <Element ul at 0x7f101464c1d8>]

#定位某节点下为ol的子节点下的所有节点
>>tree.xpath('//div[@id="testid"]/child::ol/child::*/text()') 
#['1', '2', '3']



轴Axes: attribute 选取当前节点的所有属性
>>tree.xpath('//div/attribute::id') #attribute定位id属性值
# ['testid', 'go']

>>tree.xpath('//div[@id="testid"]/attribute::*') #定位当前节点的所有属性
#['testid', 'first']



轴Axes: ancestor 父辈元素 / ancestor-or-self：父辈元素及当前元素
>>tree.xpath('//div[@id="testid"]/ancestor::div/@price') #定位父辈div元素的price属性
#['99.8']

>>tree.xpath('//div[@id="testid"]/ancestor::div') #所有父辈div元素
#[<Element div at 0x7f101464c278>]
>>tree.xpath('//div[@id="testid"]/ancestor-or-self::div') #所有父辈及当前节点div元素
#[<Element div at 0x7f101464c278>, <Element div at 0x7f10146cc5e8>]



轴Axes: descendant 后代 / descendant-or-self：后代及当前节点本身
同上。



轴Axes: following 选取文档中当前节点的结束标签之后的所有节点
#定位testid之后不包含id属性的div标签下所有的li中第一个li的text属性
>>tree.xpath('//div[@id="testid"]/following::div[not(@id)]/.//li[1]/text()')
#['test1']


namespace：选取当前节点的所有命名空间节点
>>tree.xpath('//div[@id="testid"]/namespace::*') #选取命名空间节点
#[('xml', 'http://www.w3.org/XML/1998/namespace')]


parent：选取当前节点的父节点
#选取data值为one的父节点的子节点中最后一个节点的值
>>tree.xpath('//li[@data="one"]/parent::ol/li[last()]/text()') 
#['3']
#注意这里的用法，parent::父节点的名字，也就是从父节点、父父节点等中找ol。




preceding：选取文档中当前节点的开始标签之前的所有节点
>>tree.xpath('//div[@id="testid"]/preceding::div/ul/li[1]/text()')[0] 
#'时间'

##下面这两条可以看到其顺序是靠近testid节点的优先
>>tree.xpath('//div[@id="testid"]/preceding::li[1]/text()')[0]
#'任务'
>>tree.xpath('//div[@id="testid"]/preceding::li[3]/text()')[0]
#'时间'




preceding-sibling：选取当前节点之前的所有同级节点
#记住只能是同级节点
>>tree.xpath('//div[@id="testid"]/preceding-sibling::div/ul/li[2]/text()')[0]
#'地点'
>>tree.xpath('//div[@id="testid"]/preceding-sibling::li') #这里返回的就是空的了
#[] 因为没有同级别的li




self：选取当前节点
#选取带id属性值的div中包含data-h属性的标签的所有属性值
>>tree.xpath('//div[@id]/self::div[@data-h]/attribute::*') 
#['testid', 'first']



组合拳
#定位id值为testid下的ol下的li属性值data为two的父元素ol的兄弟前节点h2的text值
>>tree.xpath('//*[@id="testid"]/ol/li[@data="two"]/parent::ol/preceding-sibling::h2/text()')[0] 
#这里是个小标题





4)position定位
>>tree.xpath('//*[@id="testid"]/ol/li[position()=2]/text()')[0] 
#'2'



5)条件
#定位所有h2标签中text值为`这里是个小标题`
>>tree.xpath(u'//h2[text()="这里是个小标题"]/text()')[0]
#这里是个小标题




6)函数
count：统计
>>tree.xpath('count(//li[@data])') #节点统计
#3.0


concat：字符串连接
>>tree.xpath('concat(//li[@data="one"]/text(),//li[@data="three"]/text())')
#13


string：解析当前节点下的字符
#string只能解析匹配到的第一个节点下的值，也就是作用于list时只匹配第一个
>>tree.xpath('string(//li)') 
#时间


local-name：解析节点名称
>>tree.xpath('local-name(//*[@id="testid"])') #local-name解析节点名称
#div



contains(string1,string2)：如果 string1 包含 string2，则返回 true，否则返回 false
>>tree.xpath('//h3[contains(text(),"H3")]/a/text()')[0] #使用字符内容来辅助定位
#百度一下

##一记组合拳
#匹配带有href属性的a标签的先辈节点中的div，其兄弟节点中前一个div节点下ul下li中text属性包含“务”字的节点的值
>>tree.xpath(u'//a[@href]/ancestor::div/preceding::div/ul/li[contains(text(),"务")]/text()')[0] 
#任务




not：布尔值（否）
>>tree.xpath('count(//li[not(@data)])') #不包含data属性的li标签统计
#18.0


string-length：返回指定字符串的长度
#string-length函数+local-name函数定位节点名长度小于2的元素
>>tree.xpath('//*[string-length(local-name())<2]/text()')[0] 
#百度一下




组合拳2
#contains函数+local-name函数定位节点名包含di的元素
>>tree.xpath('//div[@id="testid"]/following::div[contains(local-name(),"di")]') 
#[<Element div at 0x7f101464cbd8>, <Element div at 0x7f101464c688>]



or：多条件匹配
>>tree.xpath('//li[@data="one" or @code="84"]/text()') #or匹配多个条件
#['1', '84']
#也可使用|
>>tree.xpath('//li[@data="one"]/text() | //li[@code="84"]/text()') #|匹配多个条件
#['1', '84']


组合拳3：floor + div除法 + ceiling
#position定位+last+div除法，选取中间两个
>>tree.xpath('//div[@id="go"]/ul/li[position()=floor(last() div 2+0.5) or position()=ceiling(last() div 2+0.5)]/text()') 
#['5', '6']
######//todo 没看懂？？


组合拳4隔行定位：position+mod取余
#position+取余运算隔行定位
>>tree.xpath('//div[@id="go"]/ul/li[position()=((position() mod 2)=0)]/text()') 
#['2', '4', '6', '8', '10']


starts-with：以。。开始
#starts-with定位属性值以8开头的li元素
>>tree.xpath('//li[starts-with(@code,"8")]/text()')[0]
#'84'






7)数值比较
<：小于
#所有li的code属性小于200的节点
>>tree.xpath('//li[@code<200]/text()')
#['84', '104']


div：对某两个节点的属性值做除法
>>tree.xpath('//div[@id="testid"]/ul/li[3]/@code div //div[@id="testid"]/ul/li[1]/@code')
#2.6547619047619047


组合拳4：根据节点下的某一节点数量定位
#选取所有ul下li节点数大于5的ul节点
>>tree.xpath('//ul[count(li)>5]/li/text()')
# ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']



8)将对象还原为字符串
s = tree.xpath('//*[@id="testid"]')[0] #使用xpath定位一个节点
#<Element div at 0x7f10146cc5e8>

from lxml import etree
etree.tostring(s)  #还原这个对象为html字符串
#'<div id="testid">\n\t\t<h2>&#213;&#226;&#192;&#239;&#202;&#199;&#184;&#246;&#208;&#161;&#177;&#234;&#204;&#226;</h2>\n\t\t<ol>\n\t\t\t<li data="one">1</li>\n\t\t\t<li data="two">2</li>\n\t\t\t<li data="three">3</li>\n\t\t</ol>\n\t\t<ul>\n\t\t\t<li code="84">84</li>\n\t\t\t<li code="104">104</li>\n\t\t\t<li code="223">223</li>\n\t\t</ul>\n\t</div>\n\t'



9)选取一个属性中的多个值
举例：<div class="mp-city-list-container mp-privince-city" mp-role="provinceCityList">
选择这个div的方案网上有说用and的，但是似乎只能针对不同的属性的单个值
本次使用contains

from lxml import etree
html = '<div class="mp-city-list-container mp-privince-city" mp-role="provinceCityList"></div>'
content2 = etree.HTML(html)

#解析
rs=content2.xpath('//div[contains(@class,"mp-city-list-container mp-privince-city")]')
etree.tostring(rs[0])

#当然也可以直接选取其属性的第二个值
content2.xpath('//div[contains(@class,"mp-privince-city")]')
#重点是class需要添加一个@符号






















========================================
爬虫实例：获取豆瓣电影 正在热映电影 名字和时长[requests, XPath, print]
----------------------------------------
import requests
from lxml import html


#1.请求数据
url='https://movie.douban.com/' #需要爬数据的网址
page=requests.Session().get(url)


#2.解析数据
tree=html.fromstring(page.text)
#print(page.text) #网页内容

#该怎么获取电影名字？
# xpath教程:  https://www.cnblogs.com/giserliu/p/4399778.html
#result=tree.xpath('//td[@class="title"]//a/text()') #获取需要的数据[失败，可能是豆瓣改版了]
#result=tree.xpath('//ul/li[@class="ui-slide-item"]/attribute::data-title') #hit25 class="a",but no class="a b"
result=tree.xpath('//ul/li[contains(@class,"ui-slide-item")]/attribute::data-title')#hit 32 items 

duration=tree.xpath('//ul/li[contains(@class,"ui-slide-item")]/attribute::data-duration')#时长



#3.输出结果
print('正在热映电影 ',len(result),'部：')
for i in range(len(result)):
    print(i+1, result[i],duration[i],sep=" / ")

#4. end
print("==end==")



#输出结果
#正在热映电影  32 部：
#1 / 宝贝儿 / 96分钟
#2 / 无双 無雙 / 130分钟
#...


========================================
【大坑】Python解决抓取内容乱码问题（decode和encode解码）
----------------------------------------

1.字符串在Python内部的表示是unicode编码，在做编码转换时，通常需要以unicode作为中间编码，即先将其他编码的字符串解码（decode）成unicode，再从unicode编码（encode）成另一种编码。

decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(‘gb2312’)，表示将gb2312编码的字符串str1转换成unicode编码。
encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(‘utf-8’)，表示将unicode编码的字符串str2转换成utf-8编码。

decode中写的就是想抓取的网页的编码，encode即自己想设置的编码



Python3的 默认编码 为Unicode，测试如下：
# Python3
import sys
sys.getdefaultencoding() #'utf-8'



怎么知道原始网页用的什么编码方式呢？
(1)查看网页源代码，找到：
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
(2)F12检查元素，查看Response Headers中的：
	Content-Type: text/plain;charset=UTF-8
(3)代码中直接查询，见本文 实例
	print(2,req.encoding) #ISO-8859-1 #response内容的编码






2.比如网页源文件中发现是 gb2312 编码的。

2.1 用 urllib 库
#!/usr/bin/env python
# -*- coding:utf-8 -*-
import urllib

page=urllib.request.urlopen(r'http://nhxy.zjxu.edu.cn/')
res=page.read()

#write to file/screen
#print(res.decode("utf-8")) 
print(res.decode("gb2312"))#查网页源代码知道的编码




2.2 用 requests 库
import requests
from lxml import html

#1.请求数据
url='http://nhxy.zjxu.edu.cn/' #需要爬数据的网址
page=requests.Session().get(url)
print(req.encoding) #ISO-8859-1
print(page.apparent_encoding) #'GB2312'

#2.解析数据
page2=page.text
page3=page2.encode('ISO-8859-1').decode(page.apparent_encoding)
print(page3) #不乱码了







3.乱码实例（文件是vim保存到主机上的utf8编码的）
#爬虫，应对乱码
import requests
from lxml import html

#1.请求数据
url='http://y.biomooc.com/wangjl/test.html' #需要爬数据的网址
page=requests.Session().get(url)

############### 解决编码问题
#req=page
#print(1,req.headers['content-type']) #text/html
#print(2,req.encoding) #ISO-8859-1 #response内容的编码
#print(3,req.apparent_encoding) #utf-8 #response headers里设置的编码
#print(4,requests.utils.get_encodings_from_content(req.text)) #['utf-8'] #response返回的html header标签里设置的编码
#返回的内容是采用‘ISO-8859-1’，所以出现了乱码，而实际上我们应该采用‘utf-8’编码
#总结：当response编码是‘ISO-8859-1’，我们应该首先查找response header设置的编码；
#text=req.text
# 转换编码,否则会导致输出乱码
#text2 = text.encode('ISO-8859-1').decode(req.apparent_encoding)
#print(text2)
################
#print(page.text)

#2.解析数据
page2=page.text
page3=page2.encode('ISO-8859-1').decode(page.apparent_encoding)
print(page3) #不乱码了

#可以匹配了
#tree=html.fromstring(page3)
#rs=tree.xpath('//html')

print("==End==")




原文：https://blog.csdn.net/w_linux/article/details/78370218 


========================================
实例: python抓取百度贴吧的图片并保存(遇到过乱码) [urllib.request, re, urllib.request.urlretrieve]
----------------------------------------
https://www.cnblogs.com/to-creat/p/6438288.html

1.简易版
import urllib.request

def getHtml(url):
    page=urllib.request.urlopen(url)
    html=page.read()
    return html


html=getHtml('http://y.biomooc.com/wangjl/test.html')
print(html.decode("utf-8")) #为什么？
#getHtml('https://www.baidu.com').decode("utf-8")

#如果"utf-8"报错：UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc8 in position 0
#则改为"gb18030"
print(html.decode("gb18030"))



其中：
Urllib 模块提供了读取web页面数据的接口，我们可以像读取本地文件一样读取www和ftp上的数据。首先，我们定义了一个getHtml()函数:
urllib.urlopen()方法用于打开一个URL地址。
read()方法用于读取URL上的数据，向getHtml()函数传递一个网址，并把整个页面下载下来。执行程序就会把整个网页打印输出。





2.下载图片到本地版本
#coding=utf-8
import urllib
import re

#fun:获取html代码
def getHtml(url):
    page = urllib.request.urlopen(url)
    html = page.read()
    return html

#fun:用正则解析出图片url
def getImg(html):
    reg = r'src="(.+?\.jpg)" pic_ext' #匹配图片的文件名
    #reg = r'src="(.+?\.jpg)" pic_ext="(.+?)"' #"(.*?)"
    #imgre = re.compile(reg)
    return re.findall(reg,html)

#1.获取html
html = getHtml("http://tieba.baidu.com/p/2460150866")
#print(html.decode('utf-8'))

#2.解析出图片url
#python3这句代码 .decode('utf-8') ，解决报错问题 TypeError: cannot use a string pattern on a bytes-like object
imglist=getImg(html.decode('utf-8'))
print(len(imglist), imglist[0])
#print(imglist)

#3.保存图片到文件夹中
i=0
for img in imglist:
    #这里的核心是用到了urllib.urlretrieve()方法，直接将远程数据下载到本地。
    urllib.request.urlretrieve(imglist[i],'/home/wangjl/web/tmp/baidu_'+str(i)+'.jpg') #request.urlretrieve(jpg_link, path) 
    i+=1
    if i%10==0: #进度条
        print(i, 'done.')

print('==end==')




========================================
实例：10行代码爬取全国所有A股/港股/新三板上市公司信息
----------------------------------------
https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&mid=2247487728&idx=1&sn=f019272798059f475c98990f62d9e562

本文知识点：  
	Table型表格抓取
	DataFrame.read_html函数使用
	MySQL数据库存储
	Navicat数据库的使用


1. table格式的表格，可以利用pandas模块里的read_html函数方便快捷地抓取下来。
$ conda install pandas #jupyter中引入失败
$ pip install pandas -i http://pypi.douban.com/simple --trusted-host pypi.douban.com #重新安装
## https://blog.csdn.net/u012421976/article/details/78329843
这些源安装超级慢，所以我推荐大家使用豆瓣的源

pip install matplotlib -i http://pypi.douban.com/simple --trusted-host pypi.douban.com
pip install numpy -i http://pypi.douban.com/simple --trusted-host pypi.douban.com
pip install pandas -i http://pypi.douban.com/simple --trusted-host pypi.douban.com
pip install seaborn scipy  -i http://pypi.douban.com/simple --trusted-host pypi.douban.com



2.getStockInfo.py
import pandas as pd
import csv

for i in range(1,178):  # 爬取全部177页数据
    url = 'http://s.askci.com/stock/a/?reportTime=2017-12-31&pageNum=%s' % (str(i))
    tb = pd.read_html(url)[3] #经观察发现所需表格是网页中第4个表格，故为[3]
    tb.to_csv(r'1.csv', mode='a', encoding='utf_8_sig', header=1, index=0)
    print('第'+str(i)+'页抓取完成')


不到1min结束。比采用正则表达式、xpath这类常规方法要省心省力地多。
比人工复制粘贴也快的多。



========================================
【难点】多线程、多线程之间的通信
----------------------------------------
1.队列 Queue 能实现子进程之间的通信



2.主进程和子进程之间怎么通信？



========================================
实例：多线程下载ZINC小分子数据库(防止直接用wget并发太大被屏蔽)[requests,shell,file]
----------------------------------------

#1.下载csv文件：
#http://zinc15.docking.org/catalogs/fda/substances/
# (1) 关键词搜索 fda/specsnp/molportnp等   http://zinc15.docking.org/catalogs/
# (2) 点击右侧的 Browse Substances， 
# (3) 点击中间的下载按钮，选择 csv格式的。      
#        
#2.shell 语句处理成单列，并去掉第一行表头
#$ awk -F"," '{print $1}' fda-substances.csv >fda.id
#$ sed -i '1d' fda.id
#
#3. 用python多线程下载，同时下载好的id记录到done文件中
#


################
# on linux only.
# v0.2
################ 

## 任务：从文件读取一行小分子 id，作为参数传给下载进程们。
# 多线程下载 多文件
## 一个进程记录数据，一个进程池 众多线程下载和保存数据。
## 读、写进程之间用queue通信。

#问题： 为什么不能在子进程中完整保存呢？因为没有flush文件。

import os
import re
import requests


############################
##part 1 配置文件:
############################
os.chdir('/home/wangjl/Download/py/') #设置项目目录
print(os.getcwd())

#需要新建文件夹 xx，并在文件夹同级放xx.id文件
pname="molportnp";
#"fda"  #"specsnp" #项目名字project name




############################
#part 2 工作函数
############################
#fun: 根据id下载文件
def download(id):
    #发出请求
    # "http://zinc15.docking.org/substances/ZINC000000000018.sdf"
    rs=requests.get("http://zinc15.docking.org/substances/"+id+".sdf")
    #写入二进制文件
    with open(pname+"/"+id+'.sdf','wb') as f:
        f.write(rs.content)
    return id;
#download("ZINC000000001368")



############################
#part 3 多线程
############################
import time,multiprocessing,os,random,re,sys
from multiprocessing import Queue
from multiprocessing import Process

#start time
start=time.time()
print('='*10, "Begin of main process", os.getpid(), "[child pid by parent ppid]")

# 读和处理数据
def worker(id):
    #一个很耗时的计算
    rs=download(id) #该函数在上一个cell定义的
    q.put(rs) #结果输出到管道
    #print(id+" done.")

#保存的线程1个
def worker_out():
    i=0
    with open(pname+'.done', 'w') as f:
        while True:
            i+=1
            if i%200==0: #进度条
                #pass 
                print(str(i)+" files have been done.  Elapse = "+ str(time.time()-start) +'s' )
            
            rs=q.get() #waite while q is empty
            #print(rs)
            f.write(rs+"\n") #写入文件
            f.flush() #刷新缓存，一次性输出到文件


# 主进程
if __name__ == '__main__':
    q=Queue(30) #会超标，但是不会超出太多
    
    # 声明进程池对象
    pool = multiprocessing.Pool(processes = 12)

    #文件读取，分配任务给进程
    fr=open(pname+".id",'r')

    # 向进程池中提交任务
    i=0
    for lineR in fr.readlines():
        i+=1
        if i>1000:
            pass;
            #break; #测试用语句

        line=lineR.strip()
        #print('>>>>>>to worker:',line)
        #arr=re.split(' ',line)
        #print("start new process", line) #任务是一次发送完的
        pool.apply_async( worker,args=(line,) )
    fr.close() #关闭文件

    #分完任务，开始启动保存进程
    pOut = Process(target=worker_out)
    pOut.start()

    #等待读进程结束
    pool.close()
    pool.join()

    #主线程查看队列，决定是否关掉写循环
    while not q.empty():
        time.sleep(1)#每一秒检查一次队列是否为空

    pOut.terminate(); #终止死循环

print(time.time()-start,'s', '='*10, "End of main process", os.getpid())


========================================
实例：多线程下载沪深股市行情信息（上市公司/csv）[urllib.request.urlopen, shell, file]
----------------------------------------

1. 多线程下载信息
# 多线程，下载股票数据

################
# on linux only.
# v0.2
################ 

## 任务：从爬虫获取股票id，作为参数传给下载进程们。
# 多线程下载 多文件
## 一个进程记录数据，一个进程池 众多线程下载和保存数据。
## 读、写进程之间用queue通信。


import urllib.request
import re
 
##def downback(a,b,c):
##    ''''
##    a:已经下载的数据块
##    b:数据块的大小
##    c:远程文件的大小
##   '''
##    per = 100.0 * a * b / c
##    if per > 100 :
##        per = 100
##    print('%.2f%%' % per)
stock_CodeUrl = 'http://quote.eastmoney.com/stocklist.html'

 
# 获取股票代码列表
def urlTolist(url):
    allCodeList = []
    html = urllib.request.urlopen(url).read()
    html = html.decode('gbk')
    s = r'<li><a target="_blank" href="http://quote.eastmoney.com/\S\S(.*?).html">'
    pat = re.compile(s)
    code = pat.findall(html)
    for item in code:
        if item[0] == '6' or item[0] == '3' or item[0] == '0':
            allCodeList.append(item)
    return allCodeList
 
 
allCodelist = urlTolist(stock_CodeUrl)
len(allCodelist) #3689


import os
import re
import requests


############################
##part 1 配置文件:
############################
os.chdir('/home/wangjl/web/all_stock_data') #设置项目目录
print(os.getcwd())

#需要新建文件夹 xx
pname="20181210";


with open(pname+'.id','w') as f:
    for code in allCodelist:
        f.write(code+"\n")

############################
#part 2 工作函数
############################
def getHtml(url):
    page=urllib.request.urlopen(url)
    html=page.read().decode("gb18030")
    return html
#fun: 根据id下载文件
def download(code):
    if code[0] == '6':
        url = 'http://quotes.money.163.com/service/chddata.html?code=0' + code + \
              '&end=20181210&fields=TCLOSE;HIGH;LOW;TOPEN;LCLOSE;CHG;PCHG;TURNOVER;VOTURNOVER;VATURNOVER;TCAP;MCAP'
    else:
        url = 'http://quotes.money.163.com/service/chddata.html?code=1' + code + \
              '&end=20181210&fields=TCLOSE;HIGH;LOW;TOPEN;LCLOSE;CHG;PCHG;TURNOVER;VOTURNOVER;VATURNOVER;TCAP;MCAP'
    #发出请求,写入二进制文件
    html=getHtml(url)
    with open( pname+"/"+code + '.csv','w') as f:
        f.write(html)
    return code;

download("600000")

print("==2end=============")

############################
#part 3 多线程
############################
import time,multiprocessing,os,random,re,sys
from multiprocessing import Queue
from multiprocessing import Process

#start time
start=time.time()
print('='*10, "Begin of main process", os.getpid(), "[child pid by parent ppid]")

# 读和处理数据
def worker(id):
    #一个很耗时的计算
    rs=download(id) #该函数在上一个cell定义的
    q.put(rs) #结果输出到管道
    #print(id+" done.")

#保存的线程1个
def worker_out():
    i=0
    with open(pname+'.done', 'w') as f:
        while True:
            i+=1
            if i%200==0: #进度条
                #pass 
                print(str(i)+" files have been done.  Elapse = "+ str(time.time()-start) +'s' )
            
            rs=q.get() #waite while q is empty
            #print(rs)
            f.write(rs+"\n") #写入文件
            f.flush() #刷新缓存，一次性输出到文件


# 主进程
if __name__ == '__main__':
    q=Queue(200) #会超标，但是不会超出太多
    
    # 声明进程池对象
    pool = multiprocessing.Pool(processes = 100)

    #文件读取，分配任务给进程
    fr=open(pname+".id",'r')

    # 向进程池中提交任务
    i=0
    for lineR in fr.readlines():
        i+=1
        if i>500:
            pass;
            #break; #测试用语句

        line=lineR.strip()
        pool.apply_async( worker,args=(line,) )
    fr.close() #关闭文件

    #分完任务，开始启动保存进程
    pOut = Process(target=worker_out)
    pOut.start()

    #等待读进程结束
    pool.close()
    pool.join()

    #主线程查看队列，决定是否关掉写循环
    while not q.empty():
        time.sleep(1)#每一秒检查一次队列是否为空

    pOut.terminate(); #终止死循环

print(time.time()-start,'s', '='*10, "End of main process", os.getpid())



2.检查出下载失败的id
# 检测下载个数和结果的差别。
fda1=[]
fda2=[]
#文件读取
def file2Arr(fname):
    fr1=open(fname,'r')

    i=0
    arr=[]
    for lineR in fr1.readlines():
        i+=1
        if i>10:
            pass;
            #break; #测试用语句

        line=lineR.strip()
        arr.append(line)
    fr1.close();
    return arr;

fda1=file2Arr("/home/wangjl/web/all_stock_data/20181210.id")
fda2=file2Arr("/home/wangjl/web/all_stock_data/file")

for id in fda1:
    if id not in fda2:
        print(id)



3.整合出来某一天的所有上市公司的信息
import pandas as pd

# 从文件获取某一天的信息
id="600001"

def getByID_Date(id,date):
    #使用pandas读取txt文件，默认分隔符是逗号。
    df=pd.read_csv("20181210/"+id+'.csv',sep=",", index_col="日期")
    
    #pandas 如果不存在某一行
    if date in df.index:
        return df.loc[[date]] #某一行
    else:
        return df[1:1]

#getByID_Date(id,"2018-12-07")

#合并两条信息
#t1=getByID_Date('000001',"2018-12-07")
#t2=getByID_Date('000002',"2018-12-07")
#t1.append(t2)

#读取code列表
ids=[]
f=open("/home/wangjl/web/all_stock_data/20181210.id",'r')
for code in f.readlines():
    ids.append( code.strip() )
f.close()
print( len(ids) )


#同过循环读取所有的code的第一行
today=""
i=0
for code in ids:
    i+=1
    if i%300==0:
        print(i, ' processing ', code )

    row=getByID_Date(code,"2018-12-07")
    if i==1:
        today=row
    else:
        today=today.append(row)
print("==end==")

pring( today )

data2=today[["股票代码","名称","总市值"]]
data2.to_csv("data2.csv")

import os
os.getcwd()
#'/home/wangjl/web/all_stock_data' 到R中用ggplot2画图。




https://m.weibo.cn/search?containerid=100103type%3D1%26q%3D%E4%B8%87%E7%A7%91&display=0&retcode=6102
https://m.weibo.cn/api/container/getIndex?type=all&queryVal=000063&featurecode=20000320&luicode=10000011&lfid=106003type%3D1&title=000063&containerid=100103type%3D1%26q%3D000063&page='+str(i)


https://m.weibo.cn/api/container/getIndex?type=all&queryVal=000063&featurecode=20000320&luicode=10000011&lfid=106003type%3D1&title=000063&containerid=100103type%3D1%26q%3D000063&page=1


var='带着微博去日本'
var=urllib.request.quote(var)
url='https://m.weibo.cn/api/container/getIndex?type=all&queryVal='+var+'&featurecode=20000320&luicode=10000011&lfid=106003type%3D1&title='+var+'&containerid=100103type%3D1%26q%3D'+var+'&page=1'





========================================
Python 爬虫基础Selenium库的使用【测试用的，对爬虫作用不大】
----------------------------------------
Selenium是一个用于测试网站的自动化测试工具，支持各种浏览器包括Chrome、Firefox、Safari等主流界面浏览器，同时也支持phantomJS无界面浏览器。

支持批量登陆，可以设置cookie等。




1.安装
$ pip install --user selenium


2.测试例子

打开百度页面并在输入框输入搜索内容（默认为firework）
注意：Selenium 2.53支持Firefox47版本及以下，记得去掉“浏览器更新”，要不报错
不要随便升级Firefox！！


要用chrome则需要在python根目录D:\Program Files\Python36 放上 chromedriver.exe
查chrome版本: chrome://version/
下载 http://npm.taobao.org/mirrors/chromedriver/86.0.4240.22/。
和win10最接近的是 chromedriver_win32.zip，解压获得exe文件



# 1. Selenium默认为Firefox。验证
from selenium import webdriver

driver = webdriver.Chrome()
#driver = webdriver.Firefox()
# 将控制的webdriver的Firefox赋值给driver；获得了浏览器对象才可以启动浏览器，打开网址，操作页面

driver.get("http://www.baidu.com")
# 获得浏览器对象后，通过get()方法，可以向浏览器发送网址

driver.find_element_by_id('kw').send_keys('hello')
# 这里通过 id = kw 定位到搜索框，并通过键盘方法send_keys向输入框里输入'hello'

driver.find_element_by_id('kw').submit()
driver.close()


ref:
https://blog.csdn.net/qq_35103303/article/details/105452426





========================================
Requests库: HTTP for Humans
----------------------------------------
http://docs.python-requests.org/en/master/

Requests is the only Non-GMO(非转基因) HTTP library for Python, safe for human consumption.

1.目的：抓取豆瓣读书短评 https://book.douban.com/subject/1084336/comments/

爬虫协议： https://www.douban.com/robots.txt

import requests
r = requests.get('https://book.douban.com/subject/1084336/comments/') #, auth=('user', 'pass')
r.status_code #200
r.headers['content-type'] #'text/html; charset=utf-8'
r.encoding #'utf-8' 编码，修改编码
r.text
#r.json() #只有r.headers['content-type']为json时才能用。否则报错。


========================================
|-- 实例: 爬取淘宝商品名字和价格到excel[requests, re, excel]
----------------------------------------

import requests
import re
import pandas as pd
header = {
    'authority': 's.taobao.com',
    'cache-control': 'max-age=0',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',
    'sec-fetch-dest': 'document',
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'sec-fetch-site': 'same-origin',
    'sec-fetch-mode': 'navigate',
    'sec-fetch-user': '?1',
    'referer': 'https://s.taobao.com/search?q=%E8%B6%B3%E7%90%83&imgfile=&js=1&stats_click=search_radio_all%3A1&initiative_id=staobaoz_20200405&ie=utf8&bcoffset=6&ntoffset=6&p4ppushleft=1%2C48&s=0',
    'accept-language': 'zh-CN,zh;q=0.9',
    'cookie': 'thw=cn; x=e%3D1%26p%3D*%26s%3D0%26c%3D0%26f%3D0%26g%3D0%26t%3D0%26__ll%3D-1%26_ato%3D0; hng=CN%7Czh-CN%7CCNY%7C156; ali_ab=180.172.240.249.1567693411690.7; enc=u%2BCzJ9S1Fo9kTwIcKrbYf1vN7AVRP1JYIOjjR%2BAIsrQzcToCpzrvzRlAAdCrMNXnZ%2FslPSIX%2Bdu40shsWSQ0RA%3D%3D; t=a07871129d64eabce9f220833fc51998; cookie2=16479d217ace4a0cfbd1a2b4d7364bd0; _tb_token_=35e35babb059e; alitrackid=www.taobao.com; lastalitrackid=www.taobao.com; _samesite_flag_=true; cna=hk/qFnPqmlYCAbfDVSBS/DDB; v=0; lgc=%5Cu5F20%5Cu96EA%5Cu83B919890605; dnk=%5Cu5F20%5Cu96EA%5Cu83B919890605; tracknick=%5Cu5F20%5Cu96EA%5Cu83B919890605; mt=ci=53_1; sgcookie=Ee9oz51Sk8fbJ%2BA8vMui%2F; unb=750307545; uc3=id2=VASqZcLWHtAZ&lg2=U%2BGCWk%2F75gdr5Q%3D%3D&vt3=F8dBxdAUzLH%2B9KBwfuw%3D&nk2=ttRVdDW4alPeideNVX0%3D; csg=89c059fd; cookie17=VASqZcLWHtAZ; skt=7051544796973d76; existShop=MTU4NjA4NTIzOA%3D%3D; uc4=nk4=0%40tALh8bx32hdeF8EvCtBqbWNyTm22Nw1P4A%3D%3D&id4=0%40Vh3K2m1I%2FxMckPvVFkw8gvH2fTA%3D; _cc_=VT5L2FSpdA%3D%3D; tg=0; _l_g_=Ug%3D%3D; sg=553; _nk_=%5Cu5F20%5Cu96EA%5Cu83B919890605; cookie1=BxuTs3kIwQ21WSsACre4vSHNYo7Th3JjGSHPLbERMAY%3D; tfstk=c2FABPA7lgjm_fNoHSBo1Tlu5bjhZbjtVEiM6NGERodPxmOOi7P396ANhVtxwaC..; uc1=cookie14=UoTUPOT6vg7A8A%3D%3D&lng=zh_CN&cookie16=UIHiLt3xCS3yM2h4eKHS9lpEOw%3D%3D&existShop=false&cookie21=URm48syIYn73&tag=8&cookie15=UIHiLt3xD8xYTw%3D%3D&pas=0; l=dBSnSc94QBIhXEi2BOCMqDCnsiQO9IRRgulLvDzBi_5Cq1Ts2gQOo1N9ve96cjWcGbLB42xcj_9tzUEg-yJjJ0YEae1VivH2Cef..; isg=BEFBuPhl-xmO8xckaTHAGMaZUI1bbrVgnT7RBqOWAMinimVc67oMMWKIbP7Mgk2Y; JSESSIONID=E729E7B4BC514D517ECB121824B2BB28',
}

def getHTML(url):
    try:
        r=requests.get(url,headers=header,timeout=30)
        r.raise_for_status()
        r.encoding='utf-8'
        return r.text
    except:
        return ''

def parserPage(nlist,plist,html):
    try:
        pricelist=re.findall(r'\"view_price\":\"\d+\.\d*\"',html)
        namelist=re.findall(r'"raw_title":".*?\"',html)
        for i in range(len(pricelist)):
            name=namelist[i].split(':')[-1]
            price = pricelist[i].split(':')[-1]
            
            
            nlist.append(name.strip('"'))
            plist.append(float( price.strip('"') ) ) 
    except:
        return ''

def printList(nlist,plist,xlsName='aaa2.xlsx',sheet_name='default'):
    '''存储到excel'''
    #df1 = pd.DataFrame(nlist,columns=['product'])
    #df2 = pd.DataFrame(plist,columns=['price'])
    #df3 = pd.concat([df1,df2],axis=1,ignore_index=False)
    #df3.to_excel('aaa.xlsx',sheet_name='aaa')
    
    df=pd.DataFrame(data={'name':nlist, 'price':plist})
    df.to_excel(xlsName, sheet_name=sheet_name)
    
def main():
    good = '足球'
    depth = 2
    start_url = 'https://s.taobao.com/search?q=' + good
    nlist=[]
    plist=[]
    for i in range(depth):
        print('Processing page ', i)
        try:
            url = start_url + '&s=' + str(44*i)
            html = getHTML(url)
            parserPage(nlist,plist,html)
        except:
            continue
    printList(nlist,plist, 'me.xlsx')
    
main()
print('==end==')





结果文件me.xlsx
	name	price
0	4号3号青少年足球 男子儿童真皮 小学生足球	57
1	正品李宁足球5号成人男孩4小学生3儿童耐磨非真皮青少年训练比赛	65
2	包邮战舰正品成人5号足球PU 训练用球3号4号耐磨小学生儿童足球	24.9
3	阿迪达斯官网 adidas FIN IST PRO 男子运动足球FH7343	1099




========================================
beautifulSoup库: 从HTML或XML文件中提取数据
----------------------------------------
https://www.crummy.com/software/BeautifulSoup/bs4/doc/

1.评论数据格式:评分、内容 view-source:https://book.douban.com/subject/1084336/comments/

<span class="comment-info">
	<a href="https://www.douban.com/people/rebekah/">眠去</a>
	<span class="user-stars allstar50 rating" title="力荐"></span>
	<span>2007-02-08</span>
</span>

<p class="comment-content">
	<span class="short"> 十几岁的时候渴慕着小王子，一天之间可以看四十四次日落。是在多久之后才明白，看四十四次日落的小王子，他有多么难过。</span>
</p>


2.代码解析 
from bs4 import BeautifulSoup
soup=BeautifulSoup(r.text,"lxml")

soup.title
#<title>
#    小王子 短评
#</title>

type(soup.title) #属性类型 bs4.element.Tag
soup.title.name #属性名字 'title'

tag=soup.p #拿到p标签 <p class="appintro-title">豆瓣</p>
soup.p.attrs #拿到p标签的属性 {'class': ['appintro-title']}
soup.p["class"] #键值对，可以直接用key拿到value ['appintro-title']

tag.string #'豆瓣' 拿到标签内的内容
type(tag.string) #bs4.element.NavigableString

最常用的方法是 soup.find_all("span"); 只需找到第一个则用 soup.find("span")
该方法还可以带上属性名
# https://www.cnblogs.com/zhaof/p/6930955.html

patterns=soup.find_all("p","comment-content")
for item in patterns:
    print(item.span.string)



总结：
推荐使用lxml解析库，必要时使用html.parser
标签选择筛选功能弱但是速度快
建议使用find()、find_all() 查询匹配单个结果或者多个结果
如果对CSS选择器熟悉建议使用select()
记住常用的获取属性和文本值的方法




3.获取评论的完整代码
import requests
from bs4 import BeautifulSoup
r = requests.get('https://book.douban.com/subject/1084336/comments/')
soup=BeautifulSoup(r.text,"lxml")
pattern=soup.find_all("p","comment-content")
i=1
for item in patterns:
    print(i,item.span.string)
    i+=1

#其实，小王子7万多评论，后面还有很多页
https://book.douban.com/subject/1084336/comments/hot?p=2




4.怎么获取评分？用正则表达式
import re 
pattern2=re.compile('<span class="user-stars allstar(.*?) rating"') 
p=re.findall(pattern2,r.text)
sum=0
for star in p:
	print(star)
	sum+=int(star)
print(sum) #710




========================================
|-- 实例: bs4 库访问淘宝、京东搜索页
----------------------------------------
pip install bs4;


1. re库 搜索淘宝
import re
import requests

header = {
    'authority': 's.taobao.com',
    'cache-control': 'max-age=0',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',
    'sec-fetch-dest': 'document',
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'sec-fetch-site': 'same-origin',
    'sec-fetch-mode': 'navigate',
    'sec-fetch-user': '?1',
    'referer': 'https://s.taobao.com/search?q=%E4%B9%A6%E5%8C%85&imgfile=&commend=all&ssid=s5-e&search_type=item&sourceId=tb.index&spm=a21bo.2017.201856-taobao-item.1&ie=utf8&initiative_id=tbindexz_20170306',
    'accept-language': 'zh-CN,zh;q=0.9',
    'cookie': 'thw=cn; x=e%3D1%26p%3D*%26s%3D0%26c%3D0%26f%3D0%26g%3D0%26t%3D0%26__ll%3D-1%26_ato%3D0; hng=CN%7Czh-CN%7CCNY%7C156; ali_ab=180.172.240.249.1567693411690.7; enc=u%2BCzJ9S1Fo9kTwIcKrbYf1vN7AVRP1JYIOjjR%2BAIsrQzcToCpzrvzRlAAdCrMNXnZ%2FslPSIX%2Bdu40shsWSQ0RA%3D%3D; t=a07871129d64eabce9f220833fc51998; _m_h5_tk=5b2e98ccefd57b63b1b3b41bd5850717_1585487542936; _m_h5_tk_enc=8809c3d93c7e6c421ed5e427eef60140; cookie2=16479d217ace4a0cfbd1a2b4d7364bd0; _tb_token_=35e35babb059e; alitrackid=www.taobao.com; lastalitrackid=www.taobao.com; _samesite_flag_=true; cna=hk/qFnPqmlYCAbfDVSBS/DDB; v=0; sgcookie=EBw3a2UCbvNidStS0rFep; unb=750307545; uc3=vt3=F8dBxdAW4wRetZrR4Uw%3D&id2=VASqZcLWHtAZ&nk2=ttRVdDW4alPeideNVX0%3D&lg2=WqG3DMC9VAQiUQ%3D%3D; csg=233624e1; lgc=%5Cu5F20%5Cu96EA%5Cu83B919890605; cookie17=VASqZcLWHtAZ; dnk=%5Cu5F20%5Cu96EA%5Cu83B919890605; skt=e7e6171124709afb; existShop=MTU4NTgyMjk3NA%3D%3D; uc4=id4=0%40Vh3K2m1I%2FxMckPvVFk9%2BFCFFbMk%3D&nk4=0%40tALh8bx32hdeF8EvCtBqbWNyTSL4w8AIjw%3D%3D; tracknick=%5Cu5F20%5Cu96EA%5Cu83B919890605; _cc_=VFC%2FuZ9ajQ%3D%3D; _l_g_=Ug%3D%3D; sg=553; _nk_=%5Cu5F20%5Cu96EA%5Cu83B919890605; cookie1=BxuTs3kIwQ21WSsACre4vSHNYo7Th3JjGSHPLbERMAY%3D; tfstk=ccxFBNMc_DnEcm6evijPd8YS6oWCa_JHElWf-E-WD_njX7bdgsc8wOj3lOW5Mcbh.; mt=ci=53_1; uc1=cookie16=U%2BGCWk%2F74Mx5tgzv3dWpnhjPaQ%3D%3D&cookie21=VFC%2FuZ9ainBZ&cookie15=WqG3DMC9VAQiUQ%3D%3D&existShop=false&pas=0&cookie14=UoTUP2oXN0oW%2BA%3D%3D; JSESSIONID=69410DFCB1877E913C50EFC7DB50A632; l=dBSnSc94QBIhX1WDBOCidDCHSfbOSIRxjulLvXoei_5Qx6L_fc_Oo_gtLFp6VjWftCYB42xcj_99-etkiKy06Pt-g3fPaxDc.; isg=BB4epCDSzMwZgBitgmDXsV34b7Rg3-JZNn8eT8inimFc677FMG8yaUSJ5_dnYNpx',
}

infoList=[]
count = 0
kw = '书包'
depth = 2 #爬取2页
start_url = 'https://s.taobao.com/search?q='+kw

#for循环每一页
for i in range(depth):
    try:
        url = start_url + '&s=' + str(44*i)
        #当i=0，结果：https://s.taobao.com/search?q=书包&s=0，就是第一页的搜索结果
        html = requests.get(url,headers=header).text
    except:
          print('Fail to Get Data')  

price_list = re.findall(r'\"view_price\":\"\d+\.\d*\"',html)
name_list = re.findall(r'\"raw_title\":\".*?\"',html)

for j in range(len(price_list)):
    price = price_list[j].split(':')[-1]
    name = name_list[j].split(':')[-1]
    infoList.append([name,price])

    for g in infoList:
        count = count+1  
        print("{:4}\t{:8}\t{:16}".format(count,g[0],g[1]))

#缺少标题





2. 访问京东，结果中搜索data-sku，就是目标条目

(1)主体结构
import re
import requests
from bs4 import BeautifulSoup

header = {
    'Connection': 'keep-alive',
    'Cache-Control': 'max-age=0',
    'Upgrade-Insecure-Requests': '1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',
    'Sec-Fetch-Dest': 'document',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Sec-Fetch-Site': 'same-site',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-User': '?1',
    'Referer': 'https://www.jd.com/?cu=true&utm_source=baidu-pinzhuan&utm_medium=cpc&utm_campaign=t_288551095_baidupinzhuan&utm_term=0f3d30c8dba7459bb52f2eb5eba8ac7d_0_e22e00773bdf412a8d377d31432d40e4',
    'Accept-Language': 'zh-CN,zh;q=0.9',
}

#获取每一页的url
depth = 2
for i in range(1,depth+1):
    try:
        url = 'https://search.jd.com/Search?keyword=烤箱&enc=utf-8&qrst=1&rt=1&stop=1&vt=2&wq=烤箱&stock=1&page='+str(i)+'&s='+str(1+(i-1)*30)+'&click=0&&scrolling=y'
        print(url)
        r = requests.get(url,headers=header)
        r.encoding = 'utf-8'
        soup = BeautifulSoup(r.text,'html.parser')
        print(soup.prettify())
    except:
        print('fail to get data')

#
# 可以接着对内容进行过滤，获得商品图、名字，价格list。
#https://www.cnblogs.com/cymwill/articles/7574479.html
patterns=soup.find_all("li", attrs={'data-sku': re.compile("\d*")} ) #获取li标签，要求带有data-sku属性，且属性值为数字
print('len:',len(patterns)) #30个

print( patterns[0].prettify() ) #显示1个



####### 输出示例，解析京东网站结构
<li class="gl-item" data-sku="100011674140">
 <div class="gl-i-wrap">
 
 
	<!--图片-->
  <div class="p-img">
   <a href="https://item.jd.com/100011674140.html" onclick="xxx" target="_blank" title="柏翠 (petrus )电烤箱家用小型 30L全自动多功能智能烘焙 上下独立控温 热风循环 PE3035（樱花粉）">
    <img class="err-product" data-img="1" height="220" source-data-lazy-img="//img10.360buyimg.com/n7/jfs/t1/107731/32/7599/181930/5e5dfbebE26f7e90b/67c1b3f0811d4aad.jpg" width="220"></img>
   </a>
   <div data-catid="759" data-lease="" data-presale="" data-venid="1000005210"></div>
  </div>
  
	<!--价格-->
  <div class="p-price">
   <strong class="J_100011674140" data-done="1"> <em>￥</em> <i>469.00</i> </strong>
  </div>
  
	<!--名字-->
  <div class="p-name p-name-type-2">
   <a href="https://item.jd.com/100011674140.html" onclick="xxx" target="_blank" title="柏翠 (petrus )电烤箱家用小型 30L全自动多功能智能烘焙 上下独立控温 热风循环 PE3035（樱花粉）">
    <em>柏翠 (petrus )电
     <font class="skcolor_ljg">烤箱</font>
     家用小型 30L全自动多功能智能烘焙 上下独立控温 热风循环 PE3035（樱花粉）
    </em>
    <i class="promo-words" id="J_AD_100011674140"></i>
   </a>
  </div>
  
  
  
  <div class="p-commit">
   <strong><a href="https://item.jd.com/100011674140.html" id="J_comment_100011674140" onclick="xxx" target="_blank"></a></strong>
  </div>
  
  <div class="p-shop" data-dongdong="" data-reputation="63" data-score="0" data-selfware="1" data-shopid="1000005210" data-verderid="1000005210"></div>


  <div class="p-icons" id="J_pro_100011674140">
   <i class="goods-icons J-picon-tips J-picon-fix" data-idx="1" data-tips="京东自营，品质保障">自营</i>
  </div>
  
  
  <div class="p-operate">
   <a class="p-o-btn contrast J_contrast" data-sku="100011674140" href="javascript:;" onclick="x"><i></i>对比</a>
   <a class="p-o-btn focus J_focus" data-sku="100011674140" href="javascript:;" onclick="x"><i></i>关注</a>
   <a class="p-o-btn addcart" data-limit="0" href="//cart.jd.com/gate.action?pid=100011674140&amp;pcount=1&amp;ptype=1" onclick="x" target="_blank"><i></i>加入购物车</a>
  </div>
  
  <span class="p-promo-flag">广告</span>
  <img source-data-lazy-advertisement="xxx"/>
  
 </div>
</li>





(2) 获取更多信息，比如产品的名字、图片、价格
# 拿到价格
price=patterns[0].find("div", class_="p-price").find("i").text
print(price)
# 469.00

#图片
picture=patterns[0].find("div", class_="p-img").find("img").get('source-data-lazy-img')
picture
#'//img10.360buyimg.com/n7/jfs/t1/107731/32/7599/181930/5e5dfbebE26f7e90b/67c1b3f0811d4aad.jpg'

#产品名字
pname=patterns[0].find("div", class_="p-name").find("em").text
pname
# '柏翠 (petrus )电烤箱家用小型 30L全自动多功能智能烘焙 上下独立控温 热风循环 PE3035（樱花粉）'



(3) 更精确筛选
find_all( name , attrs , recursive , string , **kwargs )：搜索当前节点的所有子节点，孙子节点。

1) 链接提取、属性内容提取：x.get(‘href’)
2) 结合正则表达式，匹配更精确的属性值
解析信息标签结构，查找所有a标签，且每个a标签中href中包含关键字“elsie”,然后存入空列表中；
linklst = []
for x in soup.find_all('a', attrs={"class" :re.compile('lacie')}):
	link = x.get('href')
	if link:
		linklst.append(link)
#
for x in linklst: #验证：循环打印出linklist列表中的链接
	print(x)

3) 解析信息标签结构，查询所有a标签，然后输出所有标签中的“字符串”内容；
for x in soup.find_all('a'):
	string = x.get_text()
	print(string)
#
4) 自定义过滤器
如果没有合适过滤器,那么还可以定义一个方法,方法只接受一个元素参数。通过一个方法来过滤一类标签属性的时候, 这个方法的参数是要被过滤的属性的值, 而不是这个标签.
import re
def filter2(href):
    return href and not re.compile('明星').search(href)
print(soup.find_all(href=filter2))

#实例: 查找属性class中包含name的标签
import re
def filter3(class2):
    return class2 and re.compile('name').search(class2)
rs=soup.find_all(class_=filter3)
print(len(rs), rs[0])




ref:
更精细的筛选器: https://blog.csdn.net/Lizejin961019/article/details/89321634




========================================
【难点】 怎么模拟真实用户访问
----------------------------------------

很多网站的反爬虫机制都设置了访问间隔时间，一个IP如果短时间内超过了指定的次数就会进入“冷却CD”，所以除了轮换IP和user_agent，可以设置访问的时间间间隔长一点，比如没抓取一个页面休眠一个随机时间。

 - 随机访问间隔
 - 轮换IP
 - 构造合理的HTTP请求头
 - 设置Cookie的学问
 - 可以考虑多进程（随机下载，之后再按照顺序合并）



1. 爬虫设置随机访问时间间隔

import time,random
for i in range(10): #随机时间段后[2s,12s]执行
    pause=2+10*random.random()
    print(str(i)+' '+str(pause))
    time.sleep(pause)

感觉极限做到3+10*rand就可以了，时间再长就太浪费时间了。
更精简的版本
import time,random
time.sleep(random.random()*3)




2.怎么随机替换 header头呢？

(1)Upgrade-Insecure-Requests：参数为1。该指令用于让浏览器自动升级请求从http到https,用于大量包含http资源的http网页直接升级到https而不会报错。简洁的来讲,就相当于在http和https之间起的一个过渡作用。就是浏览器告诉服务器，自己支持这种操作，我能读懂你服务器发过来的上面这条信息，并且在以后发请求的时候不用http而用https；
(2)User-Agent：有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，我们需要设置一个浏览器的User-Agent；
(3)Accept：浏览器可接受的MIME类型，可以根据实际情况进行设置；
(4)Accept-Encoding：浏览器能够进行解码的数据编码方式，比如gzip。Servlet能够向支持gzip的浏览器返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间；
(5)Accept-Language：浏览器所希望的语言种类，当服务器能够提供一种以上的语言版本时要用到；
(6)Cookie：这是最重要的请求头信息之一。中文名称为“小型文本文件”或“小甜饼“，指某些网站为了辨别用户身份而储存在用户本地终端（Client Side）上的数据（通常经过加密）。定义于RFC2109。是网景公司的前雇员卢·蒙特利在1993年3月的发明。


headers = {
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36',
  'Referer': 'https://github.com/'
}
requests.get(url, headers=headers)




3. 创建自己的代理IP池

https://www.xicidaili.com/nn/1
https://www.xicidaili.com/nn/2








========================================
### 附录 ###
----------------------------------------


========================================
[收集]爬虫好用的函数
----------------------------------------

#1.获取名单
import requests, time 

def getHTMLText(url): #获得所需的网页源代码
    try:
        user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'
        headers = {'User-Agent': user_agent}
        r = requests.get(url, headers=headers, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""
    
def getFileName():
    dirname = time.strftime('%Y%m%d',time.localtime(time.time()))
    dirname+='sh'
    return  dirname





-








========================================
模拟登陆
----------------------------------------

模拟登录拉钩网
https://blog.csdn.net/kuangshp128/article/details/83302574






========================================
爬虫代理IP（代理池）的使用方法
----------------------------------------

https://blog.csdn.net/weixin_42575020/article/details/88826472

第一种：selenium加上代理，selenium主要是实现自动化登录验证等操作
第二种：requests调用代理
第三种：urllib调用代理




========================================
知乎 API
----------------------------------------
1. 搜索引擎给出的

https://api.zhihu.com/topstory/follow

我关注的人: https://api.zhihu.com/people/81de8d50e9876895b0987f39ba0149c7



2. 
热搜词汇: https://www.zhihu.com/api/v4/search/top_search




========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------

