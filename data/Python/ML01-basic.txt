MachineLearning-01


TensorFlow https://www.tensorflow.org/tutorials/
Darcula主题

目标: 掌握回归、聚类、分类、降维等机器学习的基本算法，研究深度学习算法。



前置知识
	1.机器学习的扎实基础，熟悉深度学习。
	2.数学：熟练线性代数和概率论（很重要）。
	3.编程：Python、PyTorch 和 NumPy。
	3.很多课程中使用英语，因此学生应该熟悉技术英语。
#



机器学习算法博客
https://blog.csdn.net/v_JULY_v/article/list/2


https://github.com/julycoding/The-Art-Of-Programming-By-July



========================================
教学大纲: Introduction to Machine Learning Course
----------------------------------------
TIMELINE: Approx. 10 Weeks
SKILL LEVEL: Intermediate


1. Introduction to Machine Learning Course
Machine Learning is a first-class ticket to the most exciting careers in data analysis today. As data sources proliferate along with the computing power to process them, going straight to the data is one of the most straightforward ways to quickly gain insights and make predictions.

计算机和统计学结合预测。
Machine learning brings together computer science and statistics to harness that predictive power. It’s a must-have skill for all aspiring data analysts and data scientists, or anyone else who wants to wrestle all that raw data into refined trends and predictions.

This is a class that will teach you the end-to-end process of investigating data through a machine learning lens. It will teach you how to extract and identify useful features that best represent your data, a few of the most important machine learning algorithms, and how to evaluate the performance of your machine learning algorithms.


2. 大纲
LESSON 1: Welcome to Machine Learning
Learn what Machine Learning is and meet Sebastian Thrun(代课老师人名)!
Find out where Machine Learning is applied in Technology and Science.


LESSON 2: Naive Bayes
Use Naive Bayes with scikit learn in python.
Splitting data between training sets and testing sets with scikit learn.
Calculate the posterior probability and the prior probability of simple distributions.


LESSON 3: Support Vector Machines
Learn the simple intuition behind Support Vector Machines.
Implement an SVM classifier in SKLearn/scikit-learn.
Identify how to choose the right kernel for your SVM and learn about RBF and Linear Kernels.


LESSON 4: Decision Trees
Code your own decision tree in python.
Learn the formulas for entropy and information gain and how to calculate them.
Implement a mini project where you identify the authors in a body of emails using a decision tree in Python.


LESSON 5: Choose your own Algorithm
Decide how to pick the right Machine Learning Algorithm among K-Means, Adaboost, and Decision Trees.


LESSON 6: Datasets and Questions
Apply your Machine Learning knowledge by looking for patterns in the Enron Email Dataset.
You'll be investigating one of the biggest frauds in American history!


LESSON 7: Regressions
Understand how continuous supervised learning is different from discrete learning.
Code a Linear Regression in Python with scikit-learn.
Understand different error metrics such as SSE, and R Squared in the context of Linear Regressions.


LESSON 8: Outliers
Remove outliers to improve the quality of your linear regression predictions.
Apply your learning in a mini project where you remove the residuals on a real dataset and reimplement your regressor.
Apply your same understanding of outliers and residuals on the Enron Email Corpus.


LESSON 9: Clustering
Identify the difference between Unsupervised Learning and Supervised Learning.
Implement K-Means in Python and Scikit Learn to find the center of clusters.
Apply your knowledge on the Enron Finance Data to find clusters in a real dataset.


LESSON 10: Feature Scaling
Understand how to preprocess data with feature scaling to improve your algorithms.
Use a min mx scaler in sklearn.




ref: https://www.udacity.com/course/intro-to-machine-learning--ud120



========================================
我的代码与实例
----------------------------------------

1. https://github.com/DawnEve/ML_MachineLearning

进一步学习框架:
Keras; 
TensorFlow;
PyTorch;



2. 公众号： 
机器学习算法清单！附Python和R代码 <数据派THU  3月8日



3. 慕课网资源
序号	课程名称	网址
1	Python爬虫工程师养成计划（套餐923.1元）	https://coding.imooc.com/learningpath/route?pathId=23
2	spark从零开始（免费）	https://www.imooc.com/learn/814
3	大数据入门到spark信息处理（套餐1399元）	https://order.imooc.com/pay/confirm/goods_ids/6-883
4	初识机器学习-理论篇（免费）	https://www.imooc.com/learn/717
5	Python实现机器学习（免费）	https://www.imooc.com/learn/1174
6	推荐算法理论与实践（免费）	https://www.imooc.com/learn/990
7	神经网络简介（免费）	https://www.imooc.com/learn/930
8	人工智能学习路线5门课（Step3-5: 1159.4元）	https://coding.imooc.com/learningpath/route?pathId=28







========================================
|-- 数据集 dataset 及分割：训练集和测试集
----------------------------------------

1. UCI 数据集
https://archive.ics.uci.edu/ml/datasets.php
https://archive-beta.ics.uci.edu/




2. 分割的关键是：要选出来合适的整数行号。

(1).用python的random
import random
def getRandomIndex(n, x):
	# 索引范围为[0, n), 随机选x个不重复
    index = random.sample(range(n), x)
    return index

(2).用numpy.random.choice
import numpy as np
def getRandomIndex(n, x):
	# 索引范围为[0, n)，随机选x个不重复，注意replace=False才是不重复，replace=True则有可能重复
    index = np.random.choice(np.arange(n), size=x, replace=False)
    return index
#



3.已经获取到测试集的索引了，那么得将其余的索引单独做一个数组作为训练集的索引，做法如下

import numpy as np
# 先根据上面的函数获取test_index
test_index = np.array(getRandomIndex(n, x))
# 把test_index从总的index中减去就得到了train_index
train_index = np.delete(np.arange(n), test_index)




4. 写成一个函数
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 读取数据
iris=pd.read_csv('../iris_data/iris.csv', index_col=0) # 将第一列作为行名字
iris.head()

# 分割
def splitData(df, test_ratio):
    # 索引范围为[0, n), 随机选x个不重复
    n=df.shape[0]
    x=round(n*test_ratio)
    index = np.random.choice(np.arange(n), size=x, replace=False)
    #
    test_index = np.array(index)
    train_index = np.delete(np.arange(n), test_index)
    return df.iloc[train_index,],df.iloc[test_index,]
np.random.seed(1)
train_set, test_set=splitData(iris, 0.2)
print(train_set.shape)
print(test_set.shape)





ref:
https://blog.csdn.net/qq_32623363/article/details/104180152















========================================
参考资源: 知名机器学习与AI课程
----------------------------------------

本文依赖的
# 电子书
https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/model_optimization.html
Documentation on all topics that I learn on both Artificial intelligence and machine learning.
Topics Covered:
	Artificial Intelligence Concepts
	Search
	Decision Theory
	Reinforcement Learning
	Artificial Neural Networks
	Back-propagation
	Feature Extraction
	Deep Learning
	Convolutional Neural Networks
	Deep Reinforcement Learning
	Distributed Learning
	Python/Matlab deep learning library
#
进度
/linear_algebra.html



# 视频 【白板推导系列】【合集 1～23】
https://www.bilibili.com/video/BV1aE411o7qd?p=1
进度
高斯分布 3,4,


# 白话西瓜书 视频 
https://www.bilibili.com/video/BV17J411C7zZ?p=2



1. 流派
频率派 - 统计机器学习
贝叶斯派 - 概率图模型




2. 书
(1) 李航 统计学习方法
感知机
k聚类
朴素贝叶斯
决策树
逻辑回归

支持向量机SVM
提?
EM算法
隐马尔科夫
系?

(2) 周志华 西瓜书

(3) PRML《Pattern Recognition and Machine Learning》
book: https://www.microsoft.com/en-us/research/people/cmbishop/#prml-book
code: 
贝叶斯角度
回 
分
神
核
稀疏矩阵

图 
混合
近似算法
采样
连续采样
随机森林

(4) MLAPP 
百科全书式的，贝叶斯派

(5) ESL
频率派

(6) Deep Learning 圣经
中译版 - 张志华团队


(7)Foundations of Machine Learning(by MIT)
https://mitpress.ublish.com/book/foundations-of-machine-learning--2#purchase
开源版本: https://mitpress.ublish.com/ereader/7093/?preview=#page/86

全部课程: http://rob.schapire.net/courses.html







3. 视频
(1) 台大 林轩田 
基石，理论部分很精彩、通俗化 
	VC theory;
	正则化;
	线性模型

技法
	SVM
#
# 机器学习基石&技法
这两门课是台大林轩田老师开设的机器学习入门课。基石课注重理论，涵盖了 VC Dimension、Overfitting、Regularization、Validation 等非常基本的问题，；技法课则将 SVM、AdaBoost、Decision Tree、Random Forest、Deep Learning、RBF Network 等大量实际算法分成三类加以介绍，每个算法都讲的较深，有大量的数学推导，并且非常注重模型、算法之间的相互联系。课程幻灯片制作精美，讲课深入浅出。它的作业很具有挑战性，需要花费大量的时间。

课程视频和讲义（http://www.csie.ntu.edu.tw/~htlin/mooc/）
讨论区（https://www.csie.ntu.edu.tw/~htlin/course/ml15fall/）
参考用书（http://book.caltech.edu/bookforum/）
作业参考（http://blog.csdn.net/a1015553840/article/details/51085129#reply）



(2) 张志华
机器学习导论 (概率角度)
统计机器学习 (贝叶斯)

(3) NG: CS229 斯坦福课堂录像，有很多推导;

(4) 徐亦达 - 概率模型
深度很深，github上有note

(5) 台大 李宏毅
ML2017
MLDS 2018

https://www.bilibili.com/video/BV13x411v7US?p=1



(6) 【白板推导系列】【合集 1～23】
https://www.bilibili.com/video/BV1aE411o7qd?p=1

白板笔记 
https://www.yuque.com/books/share/f4031f65-70c1-4909-ba01-c47c31398466?#
https://github.com/zhulei227/ML_Notes








4.课程

微软的 AI 入门课: https://github.com/microsoft/AI-For-Beginners


fast.ai(http://fast.ai/) ：它针对程序员提供了两个很不错的关于深度学习的课程，以及一个关于可计算线性代数的课程。是开始编写神经网络代码的好地方，随着课程深度的延伸，当你学到更多理论的时候，你可以尽快用代码实现。

neuralnetworksanddeeplearning.com（http://neuralnetworksanddeeplearning.com/chap1.html）：一本关于基本知识的很好的在线书籍。关于神经网络背后的理论。作者以一种很好的方式解释了你需要知道的数学知识。它也提供并解释了一些不使用任何深度学习框架从零开始编写神经网络架构的代码。

Andrew Ng 的深度学习课程（https://www.coursera.org/specializations/deep-learning）：coursera 上的课程，也是有关学习神经网络的。以非常简单的神经网络例子开始，逐步到卷积神经网络以及更多。

3Blue1Brown（https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw）：YouTube 上也有一些能够帮助你理解神经网络和线性代数的很好的视频。它们展示了很棒的可视化形式，以及以非常直觉的方式去理解数学和神经网络。

Stanford CS231 课程（http://cs231n.stanford.edu/）：这是关于用于视觉识别的卷积神经网络的课堂，可以学到很多关于深度学习和卷积神经网络的具体内容。







5. 领域
Speech and natural language processing
Face Recognition
Image Classification, object detection
Car Driving
Playing complex games (Alpha Go)
Control strategies (Control engineering)





6. 分类
(1)有监督、无监督；
(2)有监督的实例 Supervised Learning
classification: 分类，if a tumour is benign or malignent

Regression: 回归，预测下个月的销量;

(3) 无监督的实例 Unsupervised Learning
Clustering: You ask the computer to separate similar data into clusters, this is essential in research and science.

High Dimension Visualisation: Use the computer to help us visualise high dimension data.

Generative Models: After a model captures the probability distribution of your input data, it will be able to generate more data. This can be very useful to make your classifier more robust.





7. 代码实例
(1) 100天机器学习代码 https://github.com/Avik-Jain/100-Days-Of-ML-Code






8. 大佬的博客

# https://karpathy.ai/
I am the Sr. Director of AI at Tesla, where I lead the neural networks / computer vision team of the Autopilot. 
特斯拉人工智能主管的博客


# https://karlsims.com/
Karl Sims is a digital media artist and visual effects software developer. 
数字艺术家。作品在全球广泛展览。

# https://allenai.org/team/orene/students
Oren Etzioni | Chief Executive Officer
Dr. Oren Etzioni is Chief Executive Officer at AI2. 
Allen 研究所主任。

# http://www.erichorvitz.com/
Eric Horvitz | Chief Scientific Officer, Microsoft
Pursuing principles of computational intelligence. 
微软研究主管。






========================================
|-- 扫盲: 名词、知识点
----------------------------------------
(1)argmax是一种函数，是对函数求参数(集合)的函数。当我们有另一个函数y=f(x)时，若有结果x0= argmax(f(x))，则表示当函数f(x)取x=x0的时候，得到f(x)取值范围的最大值；若有多个点使得f(x)取得相同的最大值，那么argmax(f(x))的结果就是一个点集。

换句话说，argmax(f(x))是使得 f(x)取得最大值所对应的变量点x(或x的集合)。arg即argument，此处意为“自变量”。


(2) 如何通俗易懂地解释「范数」？
https://zhuanlan.zhihu.com/p/26884695
https://blog.csdn.net/a493823882/article/details/80569888

我们知道距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。

l0范数，l1范数-正则项与稀疏解;


(3) 什么是正则化、正则项？
带有L1正则化的回归模型通常被称为Lasso Regression，带有L2正则化的回归模型通常被称为Ridge Regression。
https://zhuanlan.zhihu.com/p/35707975







========================================
统计的学派: 频率派Frequentist vs 贝叶斯派Bayesian
----------------------------------------
1. 概率论引入机器学习是很自然的事情

X: data 数据, X=(x1 x2 ... xn)^T, NxP维矩阵; x1是p维行向量 x1=(x11 x12 ... x1p);
theta: parameter 参数

x~p(x|theta) 是概率模型;


频率派: 
	认为theta是未知常量，虽未知但是不会变，X是随机变量r.v;
	最常用的方法是极大似然估计, Theta_MLE=arg max(logP(X|theta))
		P(X|theta)=连乘(i=1,N, P(xi|theta)); 两边同时取log，得 log(P(X|theta))=累加(i=1,N, logP(xi|theta));

贝叶斯派:
	认为 theta也是 r.v, theta~P(theta) 是先验知识;
	P(theta|X)=P(X|theta)*P(theta)/P(X), 其中 P(theta|X)叫后验概率;
		P(X|theta)是最大似然, P(theta)是先验知识, 
		分母是 积分(对theta, P(X|theta)*P(theta) dtheta);
		
		p(theta) 叫先验概率 Prior probability是事件发生之前我们对theta事件的判断;
		P(theta|X) 叫后验概率 Posterior probability 是事件X发生后我们对theta事件的重新估算;
		系数 P(X|theta)/P(X) 称为"可能性函数"（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。
		
#
	MAP(maximum a posteriori probability) 极大后验概率; 就是找到概率最大的点作为代替或估计; 
		也就是众数，P(X)和theta实际上没有关系，就是一个积分常量; P(theta|X)=~P(X|theta)*P(theta)
		Theta_MAP=arg max(P(theta|X))=arg max(P(X|theta)*P(theta));
		这其实不算太正统的贝叶斯
	贝叶斯估计, 就是实打实的求出来分母的那个积分，求出来后验概率P(theta|X)=P(X|theta)*P(theta)/P(X)
#
	贝叶斯预测BAYESIAN INFERENCE(贝叶斯推断): X, xN, 求P(xN | X)，中间引入已知条件theta作为已知数据X和新数据xN之间的桥梁，
		P(xN | X)=积分(p(xN,theta|X) dtheta)=积分( p(xN|theta)*p(theta|X) dtheta )
		可见，为了做预测，必须先求出后验概率p(theta|X)
#


2. 贝叶斯估计和极大似然估计在思想上有很大不同，代表着统计学中贝叶斯学派和频率学派对统计的不同认识。

	贝叶斯 引出 概率图模型，因为要求出所有已知条件下该事件发生的条件概率;
		本质就是求积分
		数值积分求不出来，可以使用蒙特卡罗估计MCMC(Monte Carlo Method)
		
		
	频率 引出 统计机器学习 
		本质是优化，是解loss function;
		1)先设计模型，概率模型、
		2)然后导出loss function
		3)梯度下降法等优化
#





############//补充

3.贝叶斯方法简介
(1)首先贝叶斯定理的基本形式为
	后验=似然度 x 先验/证据
	p(x|y)=p(y|x)p(x)/p(y)
#
推导: x和y事件同时发生的概率
p(xy)=p(y|x)p(x)
p(xy)=p(x|y)p(y)
也就是 p(y|x)p(x)=p(x|y)p(y)


全概率公式 
事件B发生的所有可能结果B1，B2，…，Bn，事件A发生的概率P(A)，则 p(A)=p(A,o)=累加(i=1,n, P(A,oi) )




(2)频率学派和贝叶斯学派的联系和区别：

频率学派不假设任何的先验知识，不参照过去的经验，只按照当前已有的数据进行概率推断。
而贝叶斯学派会假设先验知识的存在（猜测大象的重量），然后再用采样逐渐修改先验知识并逼近真实知识。

但实际上，在数据量趋近无穷时，频率学派和贝叶斯学派得到的结果是一样的，也就是说频率方法是贝叶斯方法的极限。



频率学派和贝叶斯学派的概率定义，频率学派认为模型是一成不变的，贝叶斯学派认为模型是随着数据的更新而不断更新，频率学派和贝叶斯学派都可以使用最大似然函数来估计模型。







例1: 抛硬币，已观测数据集为5次向上，求正面向上的概率w
1)频率派:
似然函数: P(D|w)=w^5;
最大似然函数 P(D|w)max=1;
所以 w^5=1; w=?;

硬币正面向上的概率为w，模型明显存在问题，称为过拟合

2) 贝叶斯派:
假设硬币正面向上的先验概率为p(w),根据贝叶斯定理得:
p(w|D)=p(D|w)p(w)/p(D)
最大化后验概率为 p(w|D)
w=p(w|D)max，w是正面向上的概率;
??




例2: Red盒子: 6橘子+2苹果; blue盒子: 1橘子+3苹果;
选择Red盒子的概率0.4，选择Blue盒子的概率是0.6，随机从一个盒子拿出一个水果
(1)求水果为橘子的概率;
(2)当水果为橘子时，求随机选择的是Red盒子的概率;
解:
(1)记作F为水果，F=o是橘子，F=a是苹果;
记作B是篮子，B=r是Red，B=b是Blue;
P(F=o)=P(F=o, B=r)+P(F=o, B=b) #全概率公式
=P(F=o|B=r)P(B=r) + P(F=o|B=b)P(B=b) #贝叶斯公式
=6/8*0.4+1/4*0.6=0.45;

(2)
P(B=r|F=o)=P(F=o|B=r)P(B=r)/p(F=o) #贝叶斯公式，带入上一问求到的P(F=o)
=6/8*0.4/0.45=2/3

由(2)可知，选择红色盒子概率为0.4，该概率为先验概率；
当观测数据为橘子时，选择红色盒子的概率变成0.67，该概率为后验概率。
再次证明了贝叶斯估计模型的概率是随着观测数据的变化而变化的。




例3:假设一个常规的检测结果的敏感度与可靠度均为99%，即病毒携带者每次检测呈阳性（+）的概率为99%。而非病毒携带者每次检测呈阴性（-）的概率为99%。
从检测结果的概率来看，检测结果是比较准确的，但是贝叶斯定理却可以揭示一个潜在的问题。
假设某小镇对全体住户进行病毒检测，已知0.5%的住户携带病毒。请问每位检测结果呈阳性的住户携带病毒的概率有多高？
解: 记 V为病毒携带事件{yes,no}; R为检测结果{+,-}

(1)P(R=+)=P(R=+, V=yes)+P(R=+, V=no) #全概率公式 
=P(+|yes)P(yes) + P(+|no)P(no) #贝叶斯公式
=0.99*0.5% + (1-0.99)*(1-0.5%)
=0.0149;

(2) P(V=yes|R=+)=P(R=+|V=yes)P(V=yes)/P(R=+)=0.99*0.5%/0.0149=0.332

(3)同时，我们可以计算一下假如一个人携带病毒，但他误检测成阴性的概率
P(yes|-)=P(-|yes)P(yes)/P(-)=(1-0.99)*0.5%/(1-0.0149)=0.00005075

可见，一个人带病毒但被误检测为阴性的概率只有0.005%，也就是说一个人如果检测为阴性，则基本可以判定他没有携带病毒。
但是一个人如果监测为阳性，则只有33%的概率确定他携带病毒。

很多医学监测当中的案例很相似，假阳性比假阴性更值得我们关注！









4.再次理解区别
1) 频率学派
高中数学对概率的定义：在大量重复进行同一实验事件A发生的频率总是接近某一个常数，并在它附近进行摆动，这时将这个常数叫事件A的概率，记作P(A)。

这是古典频率学派对概率的定义，定义包含了二个要点：
i)事件A发生的概率是常数。
ii)事件A发生的概率是重复多次进行同一实验得到的。

频率学派的局限性：
频率学派评估可重复实验事件发生的概率具有一定的现实意义。

但是假如评估本世纪末北极圈的冰川消失的概率，按照频率学派的思想，首先需要创造无数个平行世界，然后计算北极圈冰川消失的平行世界的频率，记该频率为冰川消失的概率。
目前，创造无数个平行世界的技术还不成熟，因此频率学派在评估不可重复实验事件发生的概率具有很大的限制性。


2) 贝叶斯学派
贝叶斯学派对概率的定义：贝叶斯学派评估事件A发生的概率带有主观性，且事件A发生的概率是当前观测数据集D下的概率，即条件概率P(A|D)，当观测数据集更新为D1时，则事件A发生的概率为P(A|D1)，不同的数据集预测A事件发生的概率不同。贝叶斯学派评估事件A发生的概率会引用先验概率和后验概率两个概念，贝叶斯定理是搭建先验概率和后验概率的桥梁。

定义包含了三个要点：
（I）、事件A发生的概率是变化的，并非常数。
（II）、事件A发生的概率是特定数据集下的条件概率。
（III）、事件A发生的概率是后验概率，且事件A发生的先验概率已给定。
贝叶斯学派的难点在于如何设置合理反映事件A发生的先验概率，不同的先验概率得到的结果不一样。






5.经典著作《人工智能：现代方法》的作者之一 Peter Norvig 曾经写过一篇介绍如何写一个拼写检查/纠正器的文章，详情戳这里。
http://norvig.com/spell-correct.html


贝叶斯推断可用于拼写纠正，实际上就是计算 P(我们猜测用户要输入的单词|用户实际输入的单词)

用T表示我们猜测用户输入的单词，用S表示用户实际输入的单词，那么就是求P(t|S) = P(S|t)P(t)/P(S) 的大小。

对于同一个单词，P(S)的概率是一样的，那么就等价于P(t|S)∝ P(S|t)×P(t)。  ∝是正比于，不是无穷大
那么要是的P(t|S)最大，就是使得P(S|t)×P(t)最大。

P(S|t)名义上是指我们猜测的单词t是用户真正想输入单词的概率，不同的单词概率不同，这就涉及到最大似然估计。例如用户输入的单词是thriw,这时throw跟thraw都有可能，但是你会想到，o跟i在键盘上很接近，用户可能要输的单词是throw的可能性比thraw的可能性大得多，根据最大似然估计找出最可能的单词。但是，有时候光有最大似然并不能完美的解决问题，我们还需要利用先验概率P(t)。

P(t)使我们猜测的单词出现的概率，这些单词t1、t2、t3....理论上有无穷种，但它是一种先验概率，对于单词来说，可能有点抽象。这里举一个分词的例子：

The girl saw the boy with a telescope.
如果仅用最大似然估计方法的话，可能会给出两种结果：1 The girl saw | the boy with a telescope    2.The girl saw the boy | with a telescope

但是根据我们的常识，一个女孩看着一个拿着望远镜的男孩？拿着望远镜有点莫名其妙，与“看”这个动作联系起来，那么最合适的解释恐怕是女孩拿着望远镜看那个男孩。那么得出这个结论，就是用到我们的先验知识，也就是P(t)。







ref:
浅谈频率学派和贝叶斯学派 https://blog.csdn.net/algorithmPro/article/details/83868827
深度学习贝叶斯，这是一份密集的6天速成课程（视频与PPT） https://baijiahao.baidu.com/s?id=1610925040333198359&wfr=spider&for=pc
拼写检查代码 http://norvig.com/spell-correct.html


//todo https://www.bilibili.com/video/BV1aE411o7qd?p=3







========================================
数学基础 概率 - 高斯分布(极大似然估计、有偏无偏、从概率密度观察)
----------------------------------------
Linear Gaussian Model;

1.
Data=(x1 x2 ... xn)^T, 每个xi是p维行向量，则data为 n行xp列 矩阵.

xi 独立同分布iid于 N(mu,sigma^2)
theta=(mu, sigma^2)
theta_MLE=arg max(theta, P(X|theat));


为了简便，简化，用p=1维的来证明
令p=1， theta=(mu, sigma^2)
p(x)=1/[sqrt(2pi)*sigma] * exp^(-(x-mu)^2/(2*sigma^2))

log(P(X|theta))=log 连乘(i=1, N, log(P(xi|theta)) );
然后求mu_MLE, 求偏导数，(过程见草稿纸)
(1)得到mu_MLE=1/N *累加(i=1,N, xi)，无偏估计;

同样方法，得到
(2)sigma^2_MLE=1/N *累加(i=1,n, (xi-mu)^2 ); 有偏估计
因为其数学期望不等于本身。



2. 估计量的数学期望是否等于真实值，等于则为无偏估计，否则为有偏估计;

方差的无偏估计量是 1/(N-1) *累加(i=1,n, (xi-mu)^2 ); 无偏的



3. 





========================================
1.SVM, Support Vector Machine
----------------------------------------
1.Understanding Support Vector Machine algorithm from examples (along with code)
https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/

中文翻译参考 公众号：关于支持向量机相关知识汇集，by 无人机  2016-08-30



(1) 拉格朗日乘子法 (求最优化问题)
拉格朗日乘子（Lagrange multiplier）
https://blog.csdn.net/zhengxqq27/article/details/88975004
https://blog.csdn.net/jinzhichaoshuiping/article/details/71731793
https://www.cnblogs.com/ooon/p/5721119.html

1) 无约束条件求最值
就是求导=0，在极值点中找最值。

2) 带等式约束的最优化问题
例 求最小值 f(x,y)=x^2+y^2, 约束条件 gi(x,y)=x+y-1=0

画出f(x,y)=x^2+y^2等高线，就是围绕着原点的同心圆。
- 等高线上函数取值相同；
- 距离圆心越近，函数值越小；

我们只关心g(x,y）= 0 的情况，所以我们只画出x+y-1 = 0的线( 直线x+y-1 = 0也叫作可行域(feasible set) )

--> 那么我们究竟要如何求解上述问题的最优解呢？仔细观察上图，你能不能发现，在约束条件g(x,y) = 0 的条件下，f(x,y)取得最小值的情况是函数 f(x,y) 和函数 g(x,y) 的梯度平行！

也就是: 当函数 f(x,y) 和函数 g(x,y) 的梯度平行时，函数 f(x,y)取得最小值。
数学表述，就是 grad(f(x,y))=Lambda* grad(gi(x,y))
上面公式中的λ叫做拉格朗日乘子 ，代表着上述式子取得最优解时并不一定要两个函数的梯度相同，只要平行即可。
上式等价于 grad(f(x,y))-Lambda* grad(gi(x,y))=0


定义拉格朗日函数（通过拉格朗日函数将约束条件融合到目标函数里去，从而只用一个函数表达式便能清楚的表达出我们的问题）：
令函数 L(x,y, Lambda)=f(x,y) - Lambda * g(x,y)
其梯度为 grad(L(x,y, Lambda))=grad(f(x,y))-Lambda* grad(gi(x,y))

函数L(x,y,λ)具有以下性质：
- 当x,y均相同时，函数L(x,y,λ)的值等于函数f(x,y)的值（因为减掉的λg(x,y)的值始终为0）；   //看不懂这一步怎么来的？看懂了，是已知限制条件g(x,y)=0
- 函数L(x,y,λ)的导函数为0的点，即为函数f(x,y)的极值点(根据以上分析可得)；

grad(L(x,y,λ))=0, 就是解线性方程组:
dL/dx=0
dL/dy=0
dL/dλ=0

2x-λ=0
2y-λ=0
-x-y+1=0

也就是x=0.5, y=0.5, λ=1
结论: 在g(x,y)=0约束条件下，f(x,y)的最小值在x=0.5,y=0.5处得到。

所以，求解带等式约束条件的最优解问题，可通过构造对应的拉格朗日函数求解。



3) 带等式约束与不等式约束的最优化问题
求f(x)最小值，其中约束条件： 
hi(x)=0, i=1,2,...,m;
gj(x)<=0, j=1,2,...,n;

对每条约束条件添加拉格朗日乘子，上式写成拉格朗日函数为:
L(x, a, b)=f(x) + 求和(i=1,m, ai*hi(x)) + 求和(j=1,n, bj*gj(x));
ai>=0

由限制条件可得:
f(x)=max(a,b,  L(x,a,b)) > L(x,a,b)   //todo 不懂怎么来的
所以 min x f(x)=min x max a,b L(x,a,b)

由（2），我们仍然可以通过令其各变量的偏导数为0来求解最优解，但是这样或许并不是一个很好的解决方案，原因如下：
     i) 参数α和β总共 m+n 个，如果全部求偏导工作量太大，不现实；
     ii)此问题可能根本没有最优解；
针对上述情况，我们对问题换一种思路思考，利用对偶的思想，将原问题转化为其对偶问题进行求解.



4)转化为对偶问题求解
拉格朗日对偶性（Lagrange Duality）
对偶问题性质：
- 1.对偶问题的对偶是原问题；
- 2.无论原始问题是否是凸的，对偶问题都是凸优化问题；
- 3.对偶问题可以给出原始问题一个下界；
- 4.当满足一定条件时，原始问题与对偶问题的解是完全等价的；
 

所以利用对偶性可以将原始的非凸优化问题转化为凸优化问题求解。 
原始问题的对偶问题为：

min x max a,b L(x,a,b)
D(a,b)=min x L(x,a,b)










2. SVM 的优化问题推导
https://blog.csdn.net/zwl1584671413/article/details/78932601
支持向量机通俗导论（理解SVM的三层境界） https://blog.csdn.net/v_july_v/article/details/7624837











========================================
2. logistics 回归 ( LR )
----------------------------------------

1. 【典藏】Logistic 回归：从入门到进阶
http://www.360doc.com/content/15/1024/07/22609018_507952382.shtml

【独家】一文读懂回归分析
http://www.360doc.com/content/17/0420/20/99071_647188582.shtml






2. py 版的手写LR代码

https://github.com/DawnEve/ML_MachineLearning/tree/master/logistic








========================================
Self Organizing Maps (SOM): 一种基于神经网络的聚类算法
----------------------------------------
自组织映射神经网络， 即Self Organizing Maps (SOM)， 可以对数据进行无监督学习聚类。它的思想很简单，本质上是一种只有输入层--隐藏层的神经网络。隐藏层中的一个节点代表一个需要聚成的类。训练时采用“竞争学习”的方式，每个输入的样例在隐藏层中找到一个和它最匹配的节点，称为它的激活节点，也叫“winning neuron”。 紧接着用随机梯度下降法更新激活节点的参数。同时，和激活节点临近的点也根据它们距离激活节点的远近而适当地更新参数。

所以，SOM的一个特点是，隐藏层的节点是有拓扑关系的。这个拓扑关系需要我们确定，如果想要一维的模型，那么隐藏节点依次连成一条线；如果想要二维的拓扑关系，那么就行成一个平面，如下图所示（也叫Kohonen Network）：

...
既然隐藏层是有拓扑关系的，所以我们也可以说，SOM可以把任意维度的输入离散化到一维或者二维(更高维度的不常见)的离散空间上。 Computation layer里面的节点与Input layer的节点是全连接的。

拓扑关系确定后，开始计算过程，大体分成几个部分：
1） 初始化：每个节点随机初始化自己的参数。每个节点的参数个数与Input的维度相同。
2）对于每一个输入数据，找到与它最相配的节点。假设输入时D维的， 即 X={x_i, i=1,...,D}，那么判别函数可以为欧几里得距离：
3)找到激活节点I(x)之后，我们也希望更新和它临近的节点。令S_ij表示节点i和j之间的距离，对于I(x)临近的节点，分配给它们一个更新权重：
简单地说，临近的节点根据距离的远近，更新程度要打折扣。
4）接着就是更新节点的参数了。按照梯度下降法更新：

迭代，直到收敛。


## 与K-Means的比较
同样是无监督的聚类方法，SOM与K-Means有什么不同呢？
（1）K-Means需要事先定下类的个数，也就是K的值。 SOM则不用，隐藏层中的某些节点可以没有任何输入数据属于它。所以，K-Means受初始化的影响要比较大。
（2）K-means为每个输入数据找到一个最相似的类后，只更新这个类的参数。SOM则会更新临近的节点。所以K-mean受noise data的影响比较大，SOM的准确性可能会比k-means低（因为也更新了临近节点）。
（3） SOM的可视化比较好。优雅的拓扑关系图 。


参考文献：http://www.cs.bham.ac.uk/~jxb/NN/l16.pdf
https://www.cnblogs.com/sylvanas2012/p/5117056.html








========================================
MCMC(一)蒙特卡罗方法及其收敛性判断: Markov Chain Monte Carlo (MCMC) simulations
----------------------------------------
MCMC(一)蒙特卡罗方法 https://www.cnblogs.com/pinard/p/6625739.html
MCMC(二)马尔科夫链
MCMC(三)MCMC采样和M-H采样
MCMC(四)Gibbs采样


1. MCMC概述
从名字我们可以看出，MCMC由两个MC组成，即蒙特卡罗方法（Monte Carlo Simulation，简称MC）和马尔科夫链（Markov Chain ，也简称MC）。要弄懂MCMC的原理我们首先得搞清楚蒙特卡罗方法和马尔科夫链的原理。我们将用三篇来完整学习MCMC。在本篇，我们关注于蒙特卡罗方法。


2. 蒙特卡罗方法引入
蒙特卡罗原来是一个赌场的名称，用它作为名字大概是因为蒙特卡罗方法是一种随机模拟的方法，这很像赌博场里面的扔骰子的过程。最早的蒙特卡罗方法都是为了求解一些不太好求解的求和或者积分问题。比如积分

theta=积分(a,b){f(x)dx}

如果我们很难求解出f(x)的原函数，那么这个积分比较难求解。当然我们可以通过蒙特卡罗方法来模拟求解近似值。如何模拟呢？假设我们函数图像如下图:

图略：x轴a到b，y轴就是曲线的值作为顶部不规则方块的面积。

则一个简单的近似求解方法是在[a,b]之间随机的采样一个点。比如x0,然后用f(x0)代表在[a,b]区间上所有的f(x)的值。那么上面的定积分的近似求解为:
(b-a)*f(x0)

虽然上面的方法可以一定程度上求解出近似的解，但是它隐含了一个假定，即x在[a,b]之间是均匀分布的，而绝大部分情况，x在[a,b]之间不是均匀分布的。如果我们用上面的方法，则模拟求出的结果很可能和真实值相差甚远。　

怎么解决这个问题呢？ 如果我们可以得到x在[a,b]的概率分布函数p(x)，那么我们的定积分求和可以这样进行：
theta=积分(a,b){f(x) dx}=积分(a,b){f(x)/p(x) * p(x) dx} ≈ 1/n *( 累加(i=0, n-1){f(xi)/p(xi)} )

上式最右边的这个形式就是蒙特卡罗方法的一般形式。当然这里是连续函数形式的蒙特卡罗方法，但是在离散时一样成立。

可以看出，最上面我们假设x在[a,b]之间是均匀分布的时候，p(xi)=1/(b−a)，带入我们有概率分布的蒙特卡罗积分的上式，可以得到：
1/n *( 累加(i=0, n-1){f(xi)/ (1/(b-a)) } ) = (b-a)/n * 累加(i=0,n-1){f(xi)}

也就是说，我们最上面的均匀分布也可以作为一般概率分布函数p(x)在均匀分布时候的特例。那么我们现在的问题转到了如何求出x的分布p(x)对应的若干个样本上来。


//todo




3. 概率分布采样


4. 接受-拒绝采样


5. 蒙特卡罗方法小结






refer:


========================================
MCMC(二)马尔科夫链
----------------------------------------
MCMC(一)蒙特卡罗方法
MCMC(二)马尔科夫链 https://www.cnblogs.com/pinard/p/6632399.html
MCMC(三)MCMC采样和M-H采样
MCMC(四)Gibbs采样


1.
马尔科夫链定义本身比较简单，它假设某一时刻状态转移的概率只依赖于它的前一个状态。举个形象的比喻，假如每天的天气是一个状态的话，那个今天是不是晴天只依赖于昨天的天气，而和前天的天气没有任何关系。

当然这么说可能有些武断，但是这样做可以大大简化模型的复杂度，因此马尔科夫链在很多时间序列模型中得到广泛的应用，比如循环神经网络RNN，隐式马尔科夫模型HMM等，当然MCMC也需要它。


这个马尔科夫链是表示股市模型的，共有三种状态：牛市（Bull market）, 熊市（Bear market）和横盘（Stagnant market）。每一个状态都以一定的概率转化到下一个状态。比如，牛市以0.025的概率转化到横盘的状态。这个状态概率转化图可以以矩阵的形式表示。

如果我们定义矩阵阵P某一位置P(i,j)的值为P(j|i),即从状态i转化到状态j的概率，并定义牛市为状态0， 熊市为状态1, 横盘为状态2. 这样我们得到了马尔科夫链模型的状态转移矩阵为：
P={0.90,0.15,0.25; 
   0.075,0.8,0.25; 
   0.025,0.05,0.5} 
   //竖列1;竖列2;竖列3;
#


(2) 什么是转移概率矩阵(Transition Probability Matrix)
　　转移概率矩阵：矩阵各元素都是非负的，并且各行元素之和等于1，各元素用概率表示，在一定条件下是互相转移的，故称为转移概率矩阵。
	P(k)表示k步转移概率矩阵。列表示Xm的状态，行表示X(m+1)的状态。
#




2. 马尔可夫链最终会趋于稳定，而且与初始条件无关。
初始状态为a，转移概率矩阵为P，假设最终矩阵为 x，则可以表示为 
	aT.P^n=xT
	aT.P^n.P=xT
	所以 xT.P=xT
	两边求转置 PT.x=1.x
	根据定义，x是矩阵PT的 特征值为1 对应的特征向量。
#



同时，对于一个确定的状态转移矩阵P，它的n次幂Pn在当n大于一定的值的时候也可以发现是确定的，我们还是以上面的例子为例，计算代码如下：
#
matrix = np.matrix([[0.9,0.075,0.025],[0.15,0.8,0.05],[0.25,0.25,0.5]], dtype=float)
for i in range(10):
    matrix = matrix*matrix
    print( "Current round:" , i+1 )
    print( matrix )
#
不管我们的初始状态是什么样子的，只要状态转移矩阵不发生变化，当n→∞n→∞时，最终状态始终会收敛到一个固定值。



也就是说我们的马尔科夫链模型的状态转移矩阵收敛到的稳定概率分布与我们的初始状态概率分布无关。
这是一个非常好的性质，也就是说，如果我们得到了这个稳定概率分布对应的马尔科夫链模型的状态转移矩阵，则我们可以用任意的概率分布样本开始，带入马尔科夫链模型的状态转移矩阵，这样经过一些序列的转换，最终就可以得到符合对应稳定概率分布的样本。



(2) 使用R验证如下
P=matrix(c(0.82,0.15,0.03,  0.18, 0.67,0.15, 0.08,0.35,0.57 ), nrow=3, byrow = T);P
#     [,1] [,2] [,3]
#[1,] 0.82 0.15 0.03
#[2,] 0.18 0.67 0.15
#[3,] 0.08 0.35 0.57

A=matrix( c(0.1,0.9,0), byrow = T, nrow=1 );A 
#0.4542683 0.3810976 0.1646341


# 经过100次迭代后的矩阵A已经稳定
for(i in seq(1,100)){
  A=A %*% P
  print(paste(i, A[1,1],A[1,2],A[1,3]))
}
# [1] "100 0.454268292682926 0.381097560975609 0.164634146341463"

eigen( t(P) )
#eigen() decomposition
#$values
#[1] 1.0000000 0.6852417 0.3747583
#
#$vectors
#[,1]       [,2]       [,3]
#[1,] -0.7381848 -0.8152669  0.2162928
#[2,] -0.6192825  0.4464259 -0.7899918
#[3,] -0.2675301  0.3688410  0.5736989

-0.7381848/0.454268292682926
#[1] -1.624997
eigen( t(P) )$vectors[,1]/(-1.624997)
#[1] 0.4542684 0.3810977 0.1646342 #恰好是PT在特征值为1对应的特征向量。
#

-0.5773503/0.454268292682926
#[1] -1.270946
eigen(P)$vectors[,1]/(-1.270946) #这个不是




(3) 马尔可夫链 细致平稳条件(Detailed Balance Condition)
首先，马尔科夫链要能收敛，需要满足以下条件：

1).可能的状态数是有限的。
2).状态间的转移概率需要固定不变。
3).从任意状态能够转变到任意状态。
4).不能是简单的循环，例如全是从x到y再从y到x。

以上是马尔可夫链收敛的必要条件。






6.应用
马尔可夫链在实际中有非常广泛的应用。例如奠定互联网基础的PageRank算法，就是由马尔可夫链定义的。如果N是已知网页的数量，一个页面i有ki个链接到这个页面，那么它到链接页面的转换概率为 α/ki + (1−α)/N，到未链接页面的概率为(1−α)/N，参数α的取值大约是0.85。

另外像语音识别中的HMM隐马尔可夫模型，也在实际中使用非常多，并且在DNN问世之前一直都是语音识别领域中最主流的方法。








ref:
1.数学建模常用模型23：马尔可夫预测方法
https://blog.csdn.net/qq_41686130/article/details/81906527

2.矩阵的极限与马尔科夫链

https://blog.csdn.net/baimafujinji/article/details/6473006



3.小白都能看懂的马尔可夫链详解
https://blog.csdn.net/bitcarmanlee/article/details/82819860








========================================
t-SNE原理与推导
----------------------------------------
t-SNE(t-distributed stochastic neighbor embedding)是用于降维的一种机器学习算法，由 Laurens van der Maaten 和 Geoffrey Hinton在08年提出。t-SNE 作为一种非线性降维算法，常用于流行学习(manifold learning)的降维过程中并与LLE进行类比，非常适用于高维数据降维到2维或者3维，便于进行可视化。

t-SNE是由SNE(Stochastic Neighbor Embedding, SNE; Hinton and Roweis, 2002)发展而来。首先介绍SNE的基本原理，之后再扩展到t-SNE。最后是t-SNE的实现以及一些优化。




>> 详见 R/机器学习: PCA及t-SNE;






ref:
https://blog.csdn.net/scott198510/article/details/76099700
https://yifdu.github.io/2018/11/12/t-SNE/
https://www.biaodianfu.com/t-sne.html







========================================
集成学习 Ensemble Algorithms
----------------------------------------

关于集成学习（aggregation），林轩田老师在技法课中设为lecture 7 到lecture 11，大概五个小时，可以全面的了解集成算法的相关细节，以及背后的数学原理，可以更好的在实践中运用集成算法。
https://www.csie.ntu.edu.tw/~htlin/mooc/



1.
集成学习是一种机器学习框架，其主要思想就是将多个基础模型组合起来，提高整体模型的泛化能力。集成学习的思想背后有比较成熟的数学理论作支撑，也即Valiant和Kearns提出的PAC (Probably approximately correct) 学习框架下的强可学习和弱可学习理论。该理论指出：在PAC 的学习框架中，一个概念如果存在一个多项式的学习方法能够学习它，并且如果预测正确率很高，那么就称这个概念是强可学习的；如果正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。随后，Schapire证明了强可学习和若可学习是等价的，也就是说弱学习模型是可以通过组合提升为强学习模型的，由此便形成了后来的集成学习的思想。


集成学习的思想其实是比较自然的，俗话说的“三个臭皮匠，顶个诸葛亮”，就是一种典型的集成学习的思想。那么集成学习的框架下具体包含哪些算法呢？
根据南京大学周志华老师2009年发表的一篇关于集成学习的综述，集成学习的框架主要有三种：boosting，bagging以及stacking，
- 其中boosting包含有Adaboost 和GBDT等，
- bagging的典型代表是Random Forest，
- stacking则是多种基础模型的结合，
这三种方法思想大同小异，但是模型训练的过程不同。


(1)装袋法Bagging：基于数据随机重抽样的分类器构建方法。从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。

(2)提升法Boosting：训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化，每次都是提高前一次分错了的数据集的权值，最后对所有基模型预测的结果进行线性组合产生最终的预测结果。

(3)Stacking：将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测：

Stacking算法分为2层，第一层是用不同的算法形成T个弱分类器，同时产生一个与原数据集大小相同的新数据集，利用这个新数据集和一个新算法构成第二层的分类器。


理论上，Boosting 可以生成任意精确的分类器，而基础学习器则可以任意弱，只需要比瞎猜好一点就OK~
Bagging 减小方差（variance ），而Boosting减小偏差（bias），关于具体的细节 https://www.zhihu.com/question/26760839




2.
严格意义上来说，这不算是一种机器学习算法，而更像是一种优化手段或者策略，它通常是结合多个简单的弱机器学习算法，去做更可靠的决策。有人把它称为机器学习中的“屠龙刀”，非常万能且有效，集成模型是一种能在各种的机器学习任务上提高准确率的强有力技术，集成算法往往是很多数据竞赛关键的一步，能够很好地提升算法的性能。哲学思想为“三个臭皮匠赛过诸葛亮”。拿分类问题举个例，直观的理解，就是单个分类器的分类是可能出错，不可靠的，但是如果多个分类器投票，那可靠度就会高很多。

集成方法是由多个较弱的模型集成模型组，一般的弱分类器可以是DT, SVM, NN, KNN等构成。其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。该算法主要的问题是要找出哪些较弱的模型可以结合起来，以及如何结合的方法。这是一个非常强大的技术集，因此广受欢迎。



常用的模型融合增强方法
	Bagging (Bootstrapped Aggregation)
	Random Forest
	Boosting
	AdaBoost （Adaptive Boosting）
	Gradient Boosting Machines (GBM)梯度推进机
	Gradient Boosted Regression Trees (GBRT)梯度提升回归树
	Stacked Generalization (blending)层叠泛化
#


集成学习的关键有两点：
	如何构建具有差异性的分类器；
	如何多这些分类器的结果进行整合。
#

基学习器之间的性能要有较大的差别，否则集成效果会很不好，也就是要保证多样性（Diversity），基学习器可以是DT, SVM, NN, KNN等，也可以是训练过程不同的同一种模型，例如不同的参数，不同的训练集，或者不同的特征选择等。


(2)关于基础分类器结果整合的主要方式
对于回归预测（数值预测）
	简单平均（Simple Average），就是取各个分类器结果的平均值。 
	加权平均（Weighted Average）。
对于分类（类别预测）
	简单投票（Majority Voting）：就是每个分类器的权重大小一样，少数服从多数，类别得票数超过一半的作为分类结果
	加权投票（Weighted Majority Voting）：每个分类器权重不一。
	概率投票（Soft vote）：有的分类器的输出是有概率信息的，因此可用概率投票。
#


(3)基于Bootstrap 的Bagging 算法
Bootstrap是一种有放回的重复抽样方法，抽样策略就是简单的随机抽样。

Bagging（Bootstrapped Aggregation的简称）对于给定数据处理任务，采用不同的模型、参数、特征训练出多个模型，最后采用投票或者加权平均的方式输出最终结果。基学习器可以是相同的模型，也可以是不同的，一般使用的是同一种基学习器，最常用的是DT决策树。


(4)基于Bagging的Random Forest
随机森林（Random Forest）是许多决策树的平均，每个决策树都用通过Bootstrap的方式获得随机样本训练。森林中的每个独立的树都比一个完整的决策树弱，但是通过将它们结合，可以通过多样性获得更高的整体表现。

随机森林是当今机器学习中非常流行的算法。它是一种“集群智慧”，非常容易训练（或构建），且它往往表现良好。

随机森林具有很多的优点：
	所有的数据都能够有效利用，而且不用人为的分出一部分数据来做cross-validation；
	随机森林可以实现很高的精确度，但是只有很少的参数，而且对于分类和回归都适用；
	不用担心过拟合的问题；
	不需要事先做特征选择，每次只用随机的选取几个特征来训练树。
它的缺点是：
	相比于其他算法，其输出预测可能较慢。
#









ref:
1. 公众号 数据派THU 2017-08-07





========================================
|-- boosting 之 adaBoost
----------------------------------------

本文主要介绍boosting学习框架中的Adaboost。
https://www.cs.princeton.edu/~schapire/papers/explaining-adaboost.pdf


1.
Boosting是一种广泛应用的集成学习框架，该框架一般的训练过程是依次训练基础模型，并在训练过程中对训练集不断地进行调整，也即当前训练所用的训练集由前一次训练的训练集根据某些策略调整得到，最后将所有基础模型组合起来即为最终得到的模型。Boosting学习框架中最具代表性的算法就是Adaboost，因此，本文将通过Adaboost来深入学习boosting思想及其具体实现。


大家都知道Adaboost算法是一种应用非常广泛也非常有效的机器学习方法，也被评为数据挖掘领域十大经典算法之一。
那么什么是Adaboost算法，一句话描述就是：在当前基础模型训练时，提高训练集中前一个基础模型误分类样本的权重，使得前面被误分类的样本获得更多的关注，从而提升当前基础模型。
在机器学习中，Adaboost 是一种有监督的分类（classification）学习算法。

...
权重和为1.
从基础模型的权重系数的计算式上可以看出，分类错误率越大的模型系数值越小（注意这里的系数值可以是负数）。

最后一步中基础模型的线性组合不需要再进行训练，这一点与stacking集成学习框架有所区别。


对于boosting的方法值得注意的一点是，训练的过程改变的是样本的权重，并没有改变样本本身，权重的作用一般体现在分类误差的计算中，当然也有人利用权重分布来对样本进行抽样，这种方法也是可行的，但并不是boosting的核心方式。

Adaboost的最终结果为每个弱学习器加权的结果。

相对随机森林的Bootstrap Sampling重采样技术，可以看出Adaboost的权重调整是有目的性，基于上一个学习器的经验，这也导致Adaboost在基学习器层是串行的。



(2)
AdaBoost的核心思想在于样本权重的更新和弱分类器权值的生成，
- 样本权重的更新保证了前面的弱分类器重点处理普遍情况，
- 后续的分类器重点处理疑难杂症。
最终，弱分类器加权组合保证了前面的弱分类器会有更大的权重，这其实有先抓总体，再抓特例的分而治之思想。





(5)
AdaBoost 优点：
	很容易实施
	几乎没有参数需要调整
	不用担心过拟合
缺点：
	公式中的α是局部最优解，不能保证是最优解
	对噪声很敏感
#


目前已经有很多非常成熟的开源packages可以直接应用（例如sklearn的AdaboostClassifier）
scikit-learn 库中，包含 AdaBoostRegression（回归）和 AdaBoostClassifier（分类）两种。






2.adaBoost 伪代码:

输入: 训练集 D=(x1,y1), ..., (xm,ym); xi属于R^m, yi属于{-1,+1}
	基础学习模型: L
	训练轮数: T
#
训练:
D1=(w11, w12, ..., w1m), w1i=1/m, i=1,2,...,m;  # 初始化样本权重，m表示样本总数
for t=1,2,...,T do:                            # 表示T轮训练过程
	ht=L(D, Dt)
	et=求和(i=1,m, wti*I*(ht(xi) != yi));      # 计算训练误差
	alphat=1/2*ln( (1-et)/et )                 # 计算当前基础模型的组合系数，也就是 基学习器的权重: 分类器的分类错误率越高，相应的权重就越大。该分类器只有纠正错误才能降低整体loss;
	Zt=求和(i=1,m, wti*exp(-alphat*yt*ht(xi)))
	for i=1,2,...,m do                         # 更新样本权重
		w(t+1,i) = w(ti)/Zt*exp(-alphat*yi*ht(xi))
	end for 
	D(t+1)=(w(t+1,1), w(t+1,2),...,w(t+1,m))
	save ht,alphat
end for
输出 H(x)=sign(求和(t=1,T, alphat*ht(x)))



Adaboost是通过改变样本的数据分布来实现的，AdaBoost 会判断每次训练的样本是否正确分类，对于正确分类的样本，降低它的权重，对于被错误分类的样本，增加它的权重。再基于上一次得到的分类准确率，来确定这次训练样本中每个样本的权重。然后将修改过权重的新数据集传递给下一层的分类器进行训练。这样做的好处就是，通过每一轮训练样本的动态权重，可以让训练的焦点集中到难分类的样本上，最终得到的弱分类器的组合更容易得到更高的分类准确率。











ref:
1. 公众号: 数据派THU 2017-08-14
2. 机器学习实战 | Adaboost: https://mp.weixin.qq.com/s?__biz=MzUzMTEwODk0Ng==&mid=2247488350&idx=1&sn=ffa3d2b18d5d6cfaecdfd48368a69dbe







========================================
常用的距离: 距离度量的多种方法
----------------------------------------
距离三要素: 1)非负性；2)对称性；3)三角不等式。（PS：有些距离并不能同时满足）




# 版本号:
$ pip3 list | grep -i umap
umap-learn                        0.5.2


# 目前支持的距离
$ python3
>>> import umap
>>> help(umap.UMAP)
* euclidean
* manhattan
* chebyshev
* minkowski
* canberra
* braycurtis
* mahalanobis
* wminkowski
* seuclidean
* cosine
* correlation
* haversine
* hamming
* jaccard
* dice
* russelrao
* kulsinski
* ll_dirichlet
* hellinger
* rogerstanimoto
* sokalmichener
* sokalsneath
* yule




机器学习领域 几种距离度量方法
https://blog.csdn.net/Zhang_Pro/article/details/107097892
https://blog.csdn.net/yeler082/article/details/85803155
https://blog.csdn.net/weixin_33743248/article/details/91946456
https://blog.csdn.net/yeler082/article/details/85801823

1.欧氏距离(Euclidean Distance): 坐标差的平方和，再开方;
2.曼哈顿距离(Manhattan Distance): 驾驶距离。对二维的点 d12=|x1-x2| + |y1-y2|
3.切比雪夫距离 (Chebyshev Distance): 
	国际象棋中，国王可以直行、横行、斜行，所以国王走一步可以移动到相邻8个方格中的任意一个。国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？这个距离就叫切比雪夫距离。
	对二维平面: d12=max(|x1-x2|, |y1-y2|)

4.闵可夫斯基距离(Minkowski Distance):
	闵氏距离不是一种距离，而是一组距离的定义，是对多个距离度量公式的概括性的表述。
	d12=power( 求和(k=1,n, |x1k-x2k|^p),  1/p)
	当p=1时，就是曼哈顿距离；
	当p=2时，就是欧氏距离；
	当p-> 无穷大时，就是切比雪夫距离；
5.标准化欧氏距离 (Standardized Euclidean Distance): 
	d12=sqrt(求和(k=1,n, ( (x1k-x2k)/sk )^2 ))
	如果将方差的倒数看成一个权重，也可称之为加权欧氏距离(Weighted Euclidean distance)。
#
6.马氏距离(Mahalanobis Distance): 马氏距离是基于样本分布的一种距离。
	度量点到分布的距离:The Mahalanobis distance is a measure of the distance between a point P and a distribution D, introduced by P. C. Mahalanobis in 1936.


7.余弦距离(Cosine Distance)
	几何中，夹角余弦可用来衡量两个向量方向的差异；机器学习中，借用这一概念来衡量样本向量之间的差异。
8.汉明距离(Hamming Distance)
	两个等长字符串s1与s2的汉明距离为：将其中一个变为另外一个所需要作的最小字符替换次数。
		The Hamming distance between "1011101" and "1001001" is 2. 
		The Hamming distance between "2143896" and "2233796" is 3. 
		The Hamming distance between "toned" and "roses" is 3.
9.杰卡德距离(Jaccard Distance)
	杰卡德相似系数(Jaccard similarity coefficient)：两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示;
	杰卡德距离(Jaccard Distance)：d(A,B)=1-J(A,B)=1-|A交B|/|A并B|

10.相关距离(Correlation distance)
	相关系数：是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）.
	相关距离 D(x,y)=1-row(x,y)

11.信息熵(Information Entropy):
	以上的距离度量方法度量的皆为两个样本（向量）之间的距离，而信息熵描述的是整个系统内部样本之间的一个距离，或者称之为系统内样本分布的集中程度（一致程度）、分散程度、混乱程度（不一致程度）。
	
	Entropy(X)=求和( -pi*log2(pi))
	
	系统内样本分布越分散(或者说分布越平均)，信息熵就越大。
	分布越有序（或者说分布越集中），信息熵就越小。
	当X只有一个分类时，信息熵取最小值0。
	
		分成10份: -0.1*log2(0.1) *10 #[1] 3.321928
		0.5 0.5: > p=0.5; -(p*log2(p)+(1-p)*log2(1-p)) #[1] 1
		0.3 0.7: > p=0.3; -(p*log2(p)+(1-p)*log2(1-p)) #[1] 0.8812909
		0.1 0.9: > p=0.1; -(p*log2(p)+(1-p)*log2(1-p)) #[1] 0.4689956
#
12.巴氏距离（Bhattacharyya Distance）:
	Bhattacharyya距离测量两个离散或连续概率分布的相似性。
	D(p,q)=-ln(BC(p,q)), 其中BC(p,q)是Bhattacharyya系数: 
		离散型分布: BC(p,q)=求和(x属于X, sqrt(p(x)*q(x)) )
		连续型分布: BC(p,q)=积分( sqrt(p(x)*q(x)) dx )
	Hellinger距离: sqrt(1-BC)
#

import numpy as np
 
p = np.asarray([0.65, 0.25, 0.07, 0.03])
q = np.array([0.6, 0.25, 0.1, 0.05])
 
BC = np.sum(np.sqrt(p * q))
 
# Hellinger距离：
h = np.sqrt(1 - BC)
 
# 巴氏距离：
b = -np.log(BC)
print(b)




13.卡方距离(Chi-square measure): 多用于直方图比较.

d=(x-y)^2/y


import numpy as np
from scipy.stats import chisquare
 
list_observe = np.array([30, 14, 34, 45, 57, 20])
list_expect = np.array([20, 20, 30, 40, 60, 30])
 
# 方法一:根据公式求解（最后根据c1的值去查表判断）
c1 = np.sum( np.square(list_observe - list_expect) / list_expect )
 
# 方法二：使用scipy库来求解
c2, p = chisquare(f_obs=list_observe, f_exp=list_expect)
print(c1, c2)
'''
返回NAN，无穷小
'''
if p > 0.05 or p == "nan":
    print("H0 win,there is no difference")
else:
    print("H1 win,there is difference")
# 11.441666666666666 11.441666666666666
# H1 win,there is difference


14.KL 距离(Kullback–Leibler divergence): 度量两个分布之间的距离
	KL距离不具有对称性，即P到Q的距离，不一定等于Q到P的距离
	http://en.wikipedia.org/wiki/Kullback–Leibler_divergence
	DKL(p||q)=∑(i属于X, p(i)*ln(p(i)/q(i)) ), 其中p和q是两个分布，KL散度越大说明分布的相似度越低。
#






========================================
|-- jaccard距离
----------------------------------------
1.jaccard index又称为jaccard similarity coefficient用于比较有限样本集之间的相似性和差异性
定义：给定两个集合A,B jaccard 系数定义为A与B交集的大小与并集大小的比值
	J(A,B)= A和B交集的大小 / A和B并集的大小; 
#
性质: 
- J(A,B)取值范围[0,1]
- jaccard值越大说明相似度越高
- 当A和B都为空时，jaccard(A,B)=1；

jaccard相似度的缺点是只适用于二元数据的集合。

例: 已知有序集合A,B，每个集合都含有n个二元的属性，即每个属性都是0或1，
M11表示A和B对应位都是1的属性的数量
M10表示A中为1，B中对应位为0的总数量
M01表示A中为0，B中对应位为1的总数量
M00表示对应位都为0的总数量
M11+M10+M01+M00=n

   A0  A1  # A糖尿病
B0 M00 M10
B1 M01 M11
# B心脑血管疾病

注: 我们只关注有病的情况，所以分子和分母中没有M00。
如果我们计算A和B的J距离，则不需要M00部分，因为它不属于任何一个疾病。
糖尿病A的包括 M10+M11, 心脑血管B的包括 M01+M11
交集是 M11, 并集是 M10+M01+M11;
jaccard相似度 J(A,B)=M11/(M10+M01+M11)
jaccard距离 Dj(A,B)=1-J(A,B)=(M10+M01)/(M10+M01+M11)







2. jaccard距离
与Jaccard 系数相关的指标叫做Jaccard 距离，用于描述集合之间的不相似度。Jaccard 距离越大，样本相似度越低。公式定义如下：
	Dj(A,b)=1-J(A,B)
#





========================================
|-- 卡方距离 Chi-Square distance: 机器视觉用于衡量图像的相近程度
----------------------------------------
Comparing two histograms using Chi-Square distance

1. 提问版
https://stats.stackexchange.com/questions/184101/comparing-two-histograms-using-chi-square-distance

I want to compare two images of faces. I calculated their LBP-histograms. So now I need to compare these two histograms and get something that will tell how much these histograms are equal (0 - 100%).

There are many ways of solving this task, but authors of LBP method emphasize (Face Description with Local Binary Patterns: Application to Face Recognition. 2004) that Chi-Square distance perfoms better than Histogram intersection and Log-likelihood statistic.

(1)Authors also show a formula of Chi-Square distance:

∑(i=1,n,  (xi−yi)^2/(xi+yi) )
Where n is a number of bins, xi is a value of first bin, yi is a value of second bin.

(2)In some researches (for example The Quadratic-Chi Histogram Distance Family) I saw that the formula of Chi-Square distance is:

0.5*∑(i=1,n,  (xi−yi)^2/(xi+yi) )
这个就是加了个0.5的公式1.


(3)And there http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm I see that formula of Chi-Square distance is:

∑(i=1,n,  (xi−yi)^2/yi )

这个就是卡方的定义了


疑问:
I stuck with it. I have several questions:
我该用哪个？怎么公式都不一样？





2. [公式3] OpenCV 提供的方法: Chi-Square ( CV_COMP_CHISQR )
https://docs.opencv.org/2.4/doc/tutorials/imgproc/histograms/histogram_comparison/histogram_comparison.html

To compare two histograms ( H1 and H2 ), first we have to choose a metric (d(H1, H2)) to express how well both histograms match.

d(H1,H2) =  求和(i, ( H1(i)-H2(i) )^2 / H1(i) )

结果如下: 越相近的，相关系数越大，卡方距离越小。
Method	Base - Base	Base - Half	Base - Test 1	Base - Test 2
Correlation	1.000000	0.930766	0.182073	0.120447
Chi-square	0.000000	4.940466	21.184536	49.273437




3. [公式2]
https://www.geeksforgeeks.org/chi-square-distance-in-python/

# importing numpy library 
import numpy as np 
  
# Function to calculate Chi-distance 
def chi2_distance(A, B): 
  
    # compute the chi-squared distance using above formula 
    chi = 0.5 * np.sum([((a - b) ** 2) / (a + b)  
                      for (a, b) in zip(A, B)]) 
  
    return chi 
  
# main function 
if __name__== "__main__": 
    a = [1, 2, 13, 5, 45, 23] 
    b = [67, 90, 18, 79, 24, 98] 
  
    result = chi2_distance(a, b) 
    print("The Chi-square distance is :", result) 
#The Chi-square distance is : 133.55428601494035



4. [公式2]
https://www.researchgate.net/post/What_is_chi-squared_distance_I_need_help_with_the_source_code

However, unlike the test statistic, d(x,y) is symmetric wrt. x and y, which is often useful in practice, e.g., when you want to construct a kernel out of the histogram distances.
直方图距离，如果是对称的，还可以用作核函数。


Similarity measures can be compared in terms of simplicity, speed, dimnesionality, immunity to  outliers and noise, etc. 
https://www.researchgate.net/publication/264995324_Dimensionality_Invariant_Similarity_Measure
This paper presents a new similarity measure to be used for general tasks including supervised learning, which is represented by the K-nearest neighbor classifier (KNN).
 

========================================
|-- 微生物多样性分析之 布雷柯蒂斯(Bray-Curtis) 距离
----------------------------------------
1. Bray-Curtis

Bray-Curtis距离是以该统计指标的提出者J. Roger Bray和John T. Curtis的名字命名的，主要基于OTUs的计数统计，比较两个群落微生物的组成差异。与unifrac距离，包含的信息完全不一样；相比于jaccard距离，Bray-Curtis则包含了OTUs丰度信息。

所有聚类分析都是根据样本之间的距离来计算的，bray-curtis距离是反应两个群落差异性方面最常用的指标，大部分生态学家喜欢用这个指标表征两个群落的差异性。

布雷柯蒂斯(Bray-Curtis)距离
布雷柯蒂斯距离经常用于生态学和环境科学中定义的距离，计算坐标之间的距离。距离通常取值在[ 0 , 1 ] [0,1][0,1]之间。它也可以用于计算样本之间的差异性。




2. 运算
(1) 数学原理可以参见这个pdf

i) 说法1：分子是极差的绝对值的和
http://www.econ.upf.edu/~michael/stanford/maeb5.pdf
另外如果对R软件熟悉的话，可以参考R中的 vegan 和 ecodist 这两个包，这两个包都涉及到了生态距离的计算。

	a b c d e sum
s29 11 0 7 8 0 26
s30 24 37 5 18 1 85

2样本的差异程度=|11-24| + |0-37| + |7-5| + |8-18| + |0-1| = 63/111 = 0.568 
这个算法考虑了样本量的差异，所以使用原始count，而不是百分比计算。
- 乘以100换算成 [0, 100]
- 完全一样了 0；每一列都有一个样本为0，则为100；

相似度，可以认为 100- 0.568*100 = 43.2




ii)举例说法2:  分子是取最小值的和  ----> 我认为可能不对。
群落A和群落B的OTU统计如下表
community	OTU1	OTU2	OTU3	OTU4	OTU5  sum
A	10	8	4	1	1  24
B	7	3	8	4	0  22

min(S_(A,i)S_(B,i)) = 7+3+4+1+0 = 15

sum(S_(A,i)) = 10+8+4+1+1 = 24
sum(S_(B,i)) = 7+3+4+8+4+0 = 22

D = 1 - 2*15/(24+22) = 0.3478







(2) 后续计算
测序数据分析之OTU https://blog.csdn.net/qq_42458954/article/details/84935625

简言之，基本思路为：质控-挑选代表序列（OTU/ESV）-物种注释-生成Feature表-多样性分析（整体）-差异比较（局部）-机器学习









3. 名词解释: OTU

运算的分类单位 operational taxonomic unit 缩写OTU，指在数量分类学方面作为对象的分类单位之总称，有种、变种、个体等。在用群析的时候，根据相似系数值和由任意标准去归纳整理有可能的，因为也有与历来分类单位的等级（rank）不一致的情况，所以使用了这个术语。

OTU，是指系统发生分析中的一个外部节点，是一个假定的分类单元；常见于动植物种类系统发育分析。例如 6 clone/2 OTUs 是指要进行系统分类分析的个体，即每2个待分析的个体包含6个克隆。

在微生物的免培养分析中经常用到，通过提取样品的总基因组DNA，利用16S rRNA或ITS的通用引物进行PCR扩增，通过测序以后就可以分析样品中的微生物多样性，那怎么区分这些不同的序列呢，这个时候就需要引入operational taxonomic units，一般情况下，如果序列之间，比如不同的 16S rRNA序列的相似性大于98%就可以把它定义为一个OTU，每个OTU对应于一个不同的16S rRNA序列，也就是每个OTU对应于一个不同的细菌（微生物）种。

通过OTU分析，就可以知道样品中的微生物多样性和不同微生物的丰度。同时通过OTU的稀有度分析，还可以看出你的测序量是否足够反应样品中的大部分微生物种，如果稀有度曲线趋于平稳就说明你的分析结果已经包括了样品中的大部分微生物种，如果稀有度还在向上，就说明你的测序量不够，你的结果没有包括样品中的大部分微生物种。






ref:
这2个认为取最小值的和
https://www.omicsclass.com/article/115
http://www.dengfeilong.com/weishengwu/130.html

还有一个认为取极差的绝对值的和
http://www.econ.upf.edu/~michael/stanford/maeb5.pdf





========================================
|-- 汉明距离（Hamming Distance）
----------------------------------------
1. 定义
汉明距离以美国数学家理查德·卫斯里·汉明的名字命名，表示两个相同长度的字符串在相同位置上不同字符的个数。用d(x,y)来表示x和y两个字符串的汉明距离。汉明距离可以用来计算两个文本之间的相似度，根据不同字符的个数来判断两个文本是否相似。

把两个字符进行异或运算，如果字符a和字符b相同，则a^b=0，我们只需要统计异或结果不为0的个数。

d(10010,10000)=1
d(abcbc,abdab)=3


(1)用R实现计算距离 
https://rdrr.io/cran/e1071/man/hamming.distance.html

library(e1071)
hamming.distance("10010", "10000") #1
hamming.distance("abcbc", "abdab") #1 
# 小结：说明上面是按照字符串为原子的，如果想按字母为原子呢？


# 把字符串分隔开
split2=function(x){
  strsplit(x, "")[[1]]
}
split2("abcd") # [1] "a" "b" "c" "d"

hamming.distance( split2("abcbc"), split2("abdab") ) #3 

# 包装为函数
hm.distance=function(str1, str2){
  hamming.distance( split2(str1), split2(str2) )
}
# 测试
hm.distance("abcbc", "abdab") #3


(2) 计算CDR3的序列的汉明距离
# 完全相同，则距离0，有n个字母不同，则距离x；如果长度不同，无法比较，距离设置为-1

cdr3.arr=c("CSATGGVGLDTQYF", "CSATGGVILDTQYF", "CSATGGVVLDTQYF", "CSATGGVVLDTQYF", "CSATGGVVLDTQFF", "CRATGGVVLDTQYF", "CGATGGVVLDTQYF", "CSATGGVVLGTQYF", "CSATGGVVLDTQFF", "CRATGGVVLDTQYF", "CSATGGVVLDTQYF", "CSATGGVVLDTQYF", "CGATGGVVLDTQYF", "CSATGGVVLETQYF", "CSGSENTGELFF", "CASSRNTGELFF", "CASSANTGELFF", "CVTSDNTGELFF", "CSASDNTGELFF", "CATSENTGELFF", "CASSHNTGELFF", "CASSTNTGELFF", "CASSTNTGELFF", "CASSTNTGELFF", "CASSENTGELFF", "CASSLNTGELFF", "CASSLNTGELFF", "CASSLNTGELFF", "CASSLNTGELFF", "CASSLNTGELFF", "CASSLNTGELFF", "CASSLNTGELFF", "CASSSNTGELFF", "CSASENTGELFF", "CASSPNTGELFF", "CASSPNTGELFF", "CASSPNTGELFF", "CSASGNTGELFF", "CSASGNTGELFF", "CSASGNTGELFF", "CASSKNTGELFF", "CSASSNTGELFF", "CASSSKGLQVAGEETQYF", "CASRKGLQGPRQVQFF", "CASTKGLQETQYF", "CASSLKGLQQGNIQYF", "CAISESKGLQAQHF", "CASSKGLQETQYF", "CASSFKGLQETQYF", "CASIPALGGEQYF", "CASIPALAVSSYNEQFF", "CASIPALGGNYEQYF", "CASIPALGLAGGPSEQFF")
cdr3.arr=unique(cdr3.arr)
len=length(cdr3.arr)
len #35

# empty mattrix
dat=matrix(-1, nrow = len, ncol = len)
# upper triangle
for(i in 1:len){
  str1=cdr3.arr[i]
  for(j in i:len){
    str2=cdr3.arr[j]
    if(nchar(str1) == nchar(str2) )
      dat[i, j]=hm.distance(str1, str2)
  }
}
# lower triangle
for(i in 1:len){
  str1=cdr3.arr[i]
  for(j in 1:(i-1) ){
      dat[i, j]=dat[j, i]
  }
}
rownames(dat)=cdr3.arr
colnames(dat)=cdr3.arr
View(dat)


(3) 如何画热图
汉明相似度: 距离越小的相似度越大。

自定义吧， 1- x/max(x)

Dist = 1- dat/max(dat)
heatmap(Dist)






ref:
https://blog.csdn.net/u012948161/article/details/122556403






========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


========================================
----------------------------------------


