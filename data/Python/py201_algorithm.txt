数据结构与算法

这里主要是python版本的
还有一个c版本的: C/数据结构与算法




推荐: 《数据结构与算法之美》的学习笔记和python代码实现
https://github.com/xiao-xiaoming/DataStructure-BeautyOfAlgorithm


算法图解(py2.7 -> 我改为了 py3.7): 
	www.manning.com/books/grokking-algorithms
	https://github.com/egonschiele/grokking_algorithms
	https://blog.51cto.com/u_15334850/3502082 可以考虑其他语言实现

  * 1 算法简介: 二分查找，大O表示法
  * 2 选择排序
  * 3 递归
  * 4 快速排序
  * 5 散列表
  * 6 广度优先搜索: 图简介，最短路径，队列
  * 7 迪克斯特拉算法
  * 8 贪婪算法
  * 9 动态规划 
  * 10 K最近邻算法
  * 11 接下来如何做: 10种算法
	树
	反向索引
	傅里叶变换
	并行算法
	MapReduce
	布隆过滤器和 HyperLogLog 
	SHA 算法 
	据不敏感的散列算法
	Diffie-Hellman 密钥交换
	线性规划



https://www.khanacademy.org/  更多算法。

数据结构 可视化 https://www.cs.usfca.edu/~galles/visualization/Algorithms.html





========================================
常用的算法
----------------------------------------

1. 数据结构
数组
链表
栈
跳表
图
Trie树



2. 排序
二分查找
搜索
哈希算法
贪心算法
分治算法
回溯算法
动态规划
字符串匹配算法

递归





========================================
二分查找，递归，大O表示法
----------------------------------------
1. 二分查找
必须是有序元素列表，如果要查找的元素存在，则返回其下标序号，如果不存在，则返回 null.

使用二分查找，每次排除一半数字。
所以1-100的数字，二分查找总能够在_步以内找到答案？ 答案是7步。

对于包含n个元素的列表，
	简单查找最多需要 n 步；大O表示就是 O(n) ，称作线性时间。
	而二分查找最多需要 log2(n) 步；大O表示就是 O(log n)，称作对数时间。

大O表示法的log是指的 log2.



(2) py算法
随机生成一个1-100的数字，用户猜测后给出大了、小了、对三种回复。正确则回复下标，错误则回复-1.

函数接收一列排序后的数字，和一个数字。

# 二分法查找
import math

def binary_search(sorted_arr, num, debug=False):
    low=0
    high=len(sorted_arr)-1
    mid= math.floor( (low+high)/2 )
    #
    while low<=high:
        mid= math.floor( (low+high)/2 )
        guess=sorted_arr[mid]
        if debug:
            print(low, mid, high)
        if guess==num:
            return mid;
        elif guess>num:
            high=mid-1
        else:
            low=mid+1
    return -1

arr1=[1,3,5,6,7]
print(binary_search(arr1, 50))
print(binary_search(arr1, 0))
print(binary_search(arr1, 7))
print(binary_search(arr1, 6, 0))
#print(binary_search(arr1, 6, True))






2. 大O表示法 算法的时间增速

(1)
大O表示法让你能够比较操作数，它指出了算法运行时间的增速。

大O 表示法指出了最糟情况下的运行时间

   算法的速度指的并非时间，而是操作数的增速。
   谈论算法的速度时，我们说的是随着输入的增加，其运行时间将以什么样的速度增加。
   算法的运行时间用大O表示法表示。
   O(log n)比O(n)快，当需要搜索的元素越多时，前者比后者快得越多。



(2) 一些常见的大O 运行时间
下面按从快到慢的顺序列出了你经常会遇到的5种大O运行时间。
  * O(log n)，也叫对数时间，这样的算法包括二分查找。
  * O(n)，也叫线性时间，这样的算法包括简单查找。
  * O(n * log n)，这样的算法包括第4章将介绍的快速排序——一种速度较快的排序算法。
  * O(n2)，这样的算法包括第2章将介绍的选择排序——一种速度较慢的排序算法。
  * O(n!)，这样的算法包括接下来将介绍的旅行商问题的解决方案——一种非常慢的算法。





3. 旅行商 问题 O(n!)

某人要去5个城市，同时要确保旅程最短。
需要穷举各种前往的顺序，计算每次的总路程，从而挑选最短路程的方案。
5个城市，需要穷举 5! =120 个方案。
6个城市，6!=720 个方案。
7个城市，7!=5040 个方案。






小结
  * 二分查找的速度比简单查找快得多。
  * O(log n)比O(n)快。需要搜索的元素越多，前者比后者就快得越多。
  * 算法运行时间并不以秒为单位。
  * 算法运行时间是从其增速的角度度量的。
  * 算法运行时间用大O表示法表示。



========================================
选择排序：数组和链表
----------------------------------------

排序后才能进行二分查找。
选择排序时下一节的 快速排序 的基石，而 快速排序 又是一种重要的算法。


1. 内存

内存可以看做很多抽屉，你有钥匙(地址)，就可以在对应的抽屉放东西。

fe0ffeeb是一个内存单元的地址。

存一个时，请求一个地址。
存多个时，可以使用 数组和链表。



2. 数组和链表

数组占用连续的空间。
	知道第一个元素的地址，可以算出来任何一个元素的地址。这就是说的数组支持随机访问。

链表占用空间不要求必须连续。
	链表必须同时保存数据和下一个元素的地址。就像一个寻宝游戏。
	链表不能直接读取最后一个元素，只能一个一个找到最后一个的地址，然后读取其中的元素。
	如果需要跳跃，链表的效率很低。

--     数组 | 链表
读取  O(1)  |  O(n)
插入  O(n)  |  O(1)
删除  O(n)  |  O(1)

其中 O(n) = 线性时间，O(1) = 常量时间。
我对链表删除时间 O(1) 持怀疑态度，他还需要查找地址的时间呢，既然是最坏的情况。 //todo


前面解释了读取。这里说一下插入新值。
插入一个元素时，数组需要把后面所有元素向后移动，而链表只需要修改前面一个元素指向的地址。
如果连续的空间不够，数组还需要把所有元素复制到其他地方。

删除元素时，链表同样比数组快。





3. 选择排序 
(1) 过程
有一些歌曲和其播放次数的数据，想按照播放次数对歌曲排序。
先挑选播放最多的，抽出来。 O(n)
然后再挑选最多的，抽出来。 O(n-1)
...
也就是需要执行 n次。
1+2+...+n=n(n+1)/2, 只记录幂次最高的，并忽略常数项系数，即O(n^2)，或者 O(n × n)。


选择排序是一种灵巧的算法，但其速度不是很快。
快速排序是一种更快的排序算法，其运行时间为O(n log n)，这将在下一章介绍。


数组的 pop 元素能按下标移除数组中的该元素，并返回该值。
arr=[20,1,6,100,5]
rs1=arr.pop(1)
print(rs1)
print(arr)
输出:
1
[20, 6, 100, 5]


参考实现
# 选择排序

arr=[20,1,6,100,5]

# 返回最小元素的下标
def findSmallest(arr):
    small=arr[0]
    index=0
    for i in range( len(arr)):
        if small> arr[i]:
            small=arr[i]
            index=i;
    return index

# use this function in sorting
def searchSort(arr):
    arr2=[]
    for i in range( len(arr) ):
        print(i, arr2)
        smallest=findSmallest(arr)
        arr2.append(arr.pop(smallest))
    return arr2;

print(arr)
arr2=searchSort(arr)
print(arr2)






小结
  * 计算机内存犹如一大堆抽屉。
  * 需要存储多个元素时，可使用数组或链表。
  * 数组的元素都在一起。
  * 链表的元素是分开的，其中每个元素都存储了下一个元素的地址。
  * 数组的读取速度很快。
  * 链表的插入和删除速度很快。
  * 在同一个数组中，所有元素的类型都必须相同（都为int、double等）。






排序算法: https://www.runoob.com/w3cnote_genre/algorithm









========================================
递归
----------------------------------------

1. 递归不能死循环
要有结束条件。



2. 调用栈 call stack

栈，只有两种操作: 压入(插入到顶端)和弹出(从顶端读取并删除)。

这个存储多个函数的变量，被称为 调用栈。


(1) 递归实现
def fact(x):
    if x<=1:
        return 1;
    else:
        if debug:
            print(x)
        return x*fact(x-1)

debug=True
x=fact(5)
print(x)

# 输出
5
4
3
2
120

注意，每个fact调用都有自己的x变量。在一个函数调用中不能访问另一个的x变量。


(2) 递归很长占用大量内存怎么办？
两种方案可以选择，改写为
	循环
	尾递归 //todo


小结
  * 递归指的是调用自己的函数。
  * 每个递归函数都有两个条件：基线条件和递归条件。
  * 栈有两种操作：压入和弹出。
  * 所有函数调用都进入调用栈。
  * 调用栈可能很长，这将占用大量的内存。




========================================
快速排序：使用 分而治之 (divide and conquer，D&C) 的策略
----------------------------------------

1. 分而治之 是 一种著名的递归式问题解决方法。

面对新问题时，你不再束手无策，而是自问：“使用分而治之能解决吗？”
D&C并非可用于解决问题的算法，而是一种解决问题的思路。


使用三个问题引入


(1) 问题: 把大小为 168*64 的地分成均匀的方块，且分出的方块尽可能的大。

- 分成 168*64 是方块，但是每个方块太小了。
- 一分为二，不是方块不行。
- 每块大小不同也不行。

使用D&C解决问题的过程包括两个步骤。
1) 找出基线条件，这种条件必须尽可能简单。
2) 不断将问题分解（或者说缩小规模），直到符合基线条件。

欧几里得算法：“适用于这小块地的最大方块，也是适用于整块地的最大方块”

伪算法
- 基线条件是分成 n*n 的方块，初步方案是先分成尽可能大的方块；
- 先分成 (64*2)*64 的两块，剩下 40*64
- 余下的再分成 40*40 的一块，还剩下 40*24
- 余下的再分成 24*24 的一块，还剩下 16*24
- 余下的再分成 16*16 的一块，还剩下 16*8
- 正好分成 (8*2)*8 的两块。

返回验证， n=8
2n * n
16*16 = 2n*2n;
24*24 = 3n*3n;
40*40 = 5n*5n;
(64*2)*64=(8n*2)*8n
都合适，那么这个就是最大的方块了: 8*8。
方块总数: 21n * 8n


实现：//todo





(2) 问题: 把几个数字相加，并求和。

# 分而治之: 数组求和，递归法;

def sumArr(arr):
    sum=0;
    for i in arr:
        sum+=i 
    return sum 

print(sumArr([2,4,6]))


# 方法二，使用递归，数组长度为1或0时返回，否则拿出一个值加上其余 求和数组
def sumArr2(arr):
    if len(arr)==0:
        return 0
    else:
        return arr[0] + sumArr2(arr[1:])

print(sumArr2([2,4,6]))


如果你喜欢递归或想学习一门新语言，可以研究一下Haskell。

二分查找也是一种分而治之的算法，请找出二分查找算法的基线条件和递归条件。



(3) 快速排序

快速排序是一种常用的排序算法，比选择排序快得多。
例如，C语言标准库中的函数qsort实现的就是快速排序。
快速排序也使用了D&C。

归纳证明: 像数学归纳法，证明一个，然后证明可以化简，然后就可以了。
	我能站到第一个梯子横板上，我能从一个横板爬到上一个横板，然后我就能爬到梯子最顶端。



算法实现：
# 快速排序，使用递归，是一种分而治之的策略。

def quickSort(arr):
    if len(arr)<2:
        return arr;
    else:
        pivot=arr[0]; #第一个数做参考，其余部分，分割成比它小的，比它大的
        less = [i for i in arr[1:] if i<=pivot]
        greater =[i for i in arr[1:] if i>pivot]
        
        # 然后分别排序，拍好后放出来
        return quickSort(less)+[pivot] + quickSort(greater)

print(quickSort([10,2,30,-100,5]))
# [-100, 2, 5, 10, 30]





(4) 合并排序（merge sort）

还有一种名为合并排序（merge sort）的排序算法，其运行时间为O(n log n)，比选择排序快得多！
快速排序的情况比较棘手，在最糟情况下，其运行时间为O(n^2)。

与选择排序一样慢！但这是最糟情况。在平均情况下，快速排序的运行时间为O(n log n)。你可能会有如下疑问。
- 这里说的最糟情况和平均情况是什么意思呢？
- 若快速排序在平均情况下的运行时间为O(n log n)，而合并排序的运行时间总是O(n log n)，为何不使用合并排序？它不是更快吗？



(5) 最佳情况/最糟情况/平均情况
二分查找，如果是从低到高的1-8。
每次使用第一个元素作为参考，则需要8次，每次第一个数组都是空数组。
如果每次都使用中间值，则只需要 log2(8)次。

但是两种情况，每次都需要和参考值比较n次。


在这个示例中，层数为O(log n)（用技术术语说，调用栈的高度为O(log n)），而每层需要的时间为O(n)。
因此整个算法需要的时间为O(n) * O(log n) = O(n log n)。这就是最佳情况。

在最糟情况下，有O(n)层，因此该算法的运行时间为O(n) * O(n) = O(n2)。

知道吗？这里要告诉你的是，最佳情况也是平均情况。
只要你每次都随机地选择一个数组元素作为基准值，快速排序的平均运行时间就将为O(n logn)。
快速排序是最快的排序算法之一，也是D&C典范。






小结
  * D&C将问题逐步分解。使用D&C处理列表时，基线条件很可能是空数组或只包含一个元素的数组。
  * 实现快速排序时，请随机地选择用作基准值的元素。快速排序的平均运行时间为O(n log n)。
  * 大O表示法中的常量有时候事关重大，这就是快速排序比合并排序快的原因所在。
  * 比较简单查找和二分查找时，常量几乎无关紧要，因为列表很长时，O(log n)的速度比O(n)快得多。









========================================
散列表(hash table)：实现、冲突、散列函数
----------------------------------------

学习散列表的内部机制：实现、冲突和散列函数。这将帮助你理解如何分析散列表的性能。

缓存是一种常用的加速方式，所有大型网站都使用缓存，而缓存的数据则存储在散列表中！

1.散列表适合用于：
  * 模拟映射关系；
  * 防止重复；
  * 缓存/记住数据，以免服务器再通过处理来生成它们。


2. 冲突

哈希函数冲突怎么办？比如 返回首字母，返回字符串长度等这些hash函数就有这些问题。
最简单的办法，如果两个键映射到了同一个位置，就在这个位置存储一个链表。

如果需要保存的数据都在某一个位置后面的链表中，则hash结构和链表一样慢。

经验教训：
	hash 函数很重要。
	冲突时同一位置的链表不能太长！



3. 性能

(1) 填装因子 

填装因子度量的是散列表中有多少位置是非空的 = 填充的条目/位置总数。

比如: _ 1 _ 的填装因子是 1/3.
如果只有 50个位置，但是塞了100个条目呢？填装因子是 100/50=2.


(2) 良好的散列函数

良好的散列函数让数组中的值呈均匀分布。
糟糕的散列函数让值扎堆，导致大量的冲突。

如果你好奇，可研究一下SHA函数。


散列函数的结果必须是均匀分布的，这很重要。它们的映射范围必须尽可能大。
最糟糕的散列函数莫过于将所有输入都映射到散列表的同一个位置。




4. 小结 
你几乎根本不用自己去实现散列表，因为你使用的编程语言提供了散列表实现。你可使用 Python提供的散列表，并假定能够获得平均情况下的性能：常量时间。

散列表是一种功能强大的数据结构，其操作速度快，还能让你以不同的方式建立数据模型。

你可能很快会发现自己经常在使用它。
  * 你可以结合散列函数和数组来创建散列表。
  * 冲突很糟糕，你应使用可以最大限度减少冲突的散列函数。
  * 散列表的查找、插入和删除速度都非常快。
  * 散列表适合用于模拟映射关系。
  * 一旦填装因子超过0.7，就该调整散列表的长度。
  * 散列表可用于缓存数据（例如，在Web服务器上）。
  * 散列表非常适合用于防止重复。










========================================
广度优先搜索 (breadth-first search, BFS)
----------------------------------------

1. 广度优先搜索让你能够找出两样东西之间的最短距离，不过最短距离的含义有很多！
使用广度优先搜索可以：
  * 编写国际跳棋AI，计算最少走多少步就可获胜；
  * 编写拼写检查器，计算最少编辑多少个地方就可将错拼的单词改成正确的单词，如将READED改为READER需要编辑一个地方；
  * 根据你的人际关系网络找到关系最近的医生


在我所知道的算法中，图算法应该是最有用的。




2. 查找A到B的最少换乘方案

- 先找一步能到的地方
- 如果无法到达B，则继续找2步能到的点；
- 如果还无法到B，则继续找3步能到的点；
...

这种问题被称为最短路径问题（shorterst-path problem）。


(1)  你经常要找出最短路径

这可能是前往朋友家的最短路径，也可能是国际象棋中把对方将死的最少步数。
解决最短路径问题的算法被称为广度优先搜索。

(2) 要确定如何从双子峰前往金门大桥，需要两个步骤。
- 使用图来建立问题模型。
- 使用广度优先搜索解决问题。



3. 什么是图
(1)图是节点和边组成。
边可以有方向 和 权重。

(2)
有方向：有向图 directed graph，有箭头。
无方向的：无向图 undirected graph，没有箭头。

(3)
有权重：权重图。
带权重的图称为加权图（weighted graph），不带权重的图称为非加权图（unweighted graph）。


(4)
一个节点可能与众多节点直接相连，这些节点被称为邻居。







4. 广度优先搜索

(1) 解决的问题
第一类问题：从节点A出发，有前往节点B的路径吗？
第二类问题：从节点A出发，前往节点B的哪条路径最短？


第一个问题：我是种芒果的，我的朋友有芒果经销商吗？
从社交网络开始，比如Facebook为例:
先从朋友查找，如果有结束。
再遍历朋友，逐个加朋友的朋友，是经销商吗？是则结束。
...


再说一次，广度优先搜索可回答两类问题。
- 第一类问题：从节点A出发，有前往节点B的路径吗？（在你的人际关系网中，有芒果销售商吗？）
- 第二类问题：从节点A出发，前往节点B的哪条路径最短？（哪个芒果销售商与你的关系最近？）


(2) 一度关系在二度关系之前加入查找名单。

你按顺序依次检查名单中的每个人，看看他是否是芒果销售商。这将先在一度关系中查找，再在二度关系中查找，因此找到的是关系最近的芒果销售商。

广度优先搜索不仅查找从A到B的路径，而且找到的是最短的路径。


你需要按添加顺序进行检查。有一个可实现这种目的的数据结构，那就是队列（queue）。




5. 队列
队列只支持两种操作：入队和出队。

队列是一种先进先出（First In First Out，FIFO）的数据结构，
而栈是一种后进先出（Last In First Out，LIFO）的数据结构。




6. 实现图
图就是点之间的对应关系。A->B 
正好，散列表可以实现键值对映射： key -> value.

(1)
比如，表示你和你的直接邻居：
graph = {}
graph["you"] = ["alice", "bob", "claire"]

注意，“你”被映射到了一个数组，因此graph["you"]是一个数组，其中包含了“你”的所有邻居。



(2) 更复杂的图的Python代码如下

graph = {}
graph["you"] = ["alice", "bob", "claire"]
graph["bob"] = ["anuj", "peggy"]
graph["alice"] = ["peggy"]
graph["claire"] = ["thom", "jonny"]
graph["anuj"] = []
graph["peggy"] = []
graph["thom"] = []
graph["jonny"] = []

散列表是无序的，因此添加键—值对的顺序无关紧要。





7. 实现广度优先算法

(1) 伪代码
创建队列，保存直接邻居
从对首遍历：
	是要找的，返回，结束查找。
	不是要找的
		弹出这个人
		并把这个人的邻居添加到队伍末尾。
如果队伍为空，则说明这个人的直接和间接邻居都没有要找的目标。


(2) py实现: 简陋实现
# 广度优先算法:
# 维护一个队列，先加入一度节点；
# 遍历，如果找到了就停止；没找到，就弹出，并把其好友添加到队列末尾。
# 直到返回，或者找不到。

# 创建图
graph = {}
graph["you"] = ["alice", "bob", "claire"]
graph["bob"] = ["anuj", "peggy"]
graph["alice"] = ["peggy"]
graph["claire"] = ["thom", "jonny"]
graph["anuj"] = []
graph["peggy"] = []
graph["thom"] = []
graph["jonny"] = []


# 在Python中，可使用函数deque来创建一个双端队列。
from collections import deque
search_queue = deque()
search_queue += graph["you"]

# 判断这个人是不是芒果经销商，最后一个字母是m 的停止
def person_is_seller(name):
    return name[-1] == 'm'

# 开始搜索
def bfs(search_queue):
    i=0
    while search_queue:
        person = search_queue.popleft() #左侧弹出一个元素
        
        i+=1
        print(f"[{i}] Cur:", person);
        
        if person_is_seller(person):
            print(person + " is a mango seller!")
            return True
        else:
            search_queue += graph[person]
    return False

bfs(search_queue);
print("==End==")


这个粗糙的实现有个问题，就是有人(peggy) 被查询了2次。
首先同一个元素检查2次是在浪费时间，其次这有可能陷入死循环，如果两个人互为好友。
应该使用一个表格，记录检查过的元素。



(3) 广度优先算法 标准实现
# 维护一个队列，先加入一度节点；
# 遍历，如果找到了就停止；没找到，就弹出，并把其好友添加到队列末尾。
# 直到返回，或者找不到。
from re import search

#
# v2: 为了避免死循环，要记录搜寻过的元素，并跳过它。
# 另一个更新，是把函数封装的更好：传入图和要查找的起始点。

# 创建图
graph = {}
graph["you"] = ["alice", "bob", "claire"]
graph["bob"] = ["anuj", "peggy"]
graph["alice"] = ["peggy"]
graph["claire"] = ["thom", "jonny"]
graph["anuj"] = []
graph["peggy"] = []
graph["thom"] = []
graph["jonny"] = []

# 在Python中，可使用函数deque来创建一个双端队列。
from collections import deque

# 判断这个人是不是芒果经销商，最后一个字母是m 的停止
def person_is_seller(name):
    return name[-1] == 'm'

# 开始搜索
def bfs2(graph, name="you", debug=True):
    search_queue = deque()
    search_queue += graph[name]

    searched=[]
    i=0
    while search_queue:
        i+=1
        person = search_queue.popleft() #左侧弹出一个元素
        if person in searched:
            print(f"[{i}]==>Jump:", person)
            continue;
        else:
            searched.append(person);
        
        if debug:
            print(f"[{i}] Cur:", person);
            print("\t>>>queue:", search_queue);
        
        if person_is_seller(person):
            print(person + " is a mango seller!")
            return True
        else:
            search_queue += graph[person]
    return False

bfs2(graph, "you");
print("==End==")





(4) 算法的时间复杂性

检查每条边 O(边数)
使用队列，检查每个人，将一个人添加到队尾的时间是固定的 O(1)，则总时间为 O(人数).
所以，广义优先搜索的运行时间为 O(人数 + 边数)，通常写作 O(V+E)
V 顶点 Vertice
E 边数 Edge



(5) 拓扑排序
从某种程度上说，这种列表是有序的。如果任务A依赖于任务B，在列表中任务A就必须在任
务B后面。这被称为拓扑排序，使用它可根据图创建一个有序列表


家谱结构的方向不能颠倒，这种图被称为树。树是一种特殊的图，其中没有往后指的边






8. 小结
  * 广度优先搜索指出是否有从A到B的路径。
  * 如果有，广度优先搜索将找出最短路径。
  * 面临类似于寻找最短路径的问题时，可尝试使用图来建立模型，再使用广度优先搜索来解决问题。
  * 有向图中的边为箭头，箭头的方向指定了关系的方向，例如，rama→adit表示rama欠adit钱。
  * 无向图中的边不带箭头，其中的关系是双向的，例如，ross - rachel表示“ross与rachel约会，而rachel也与ross约会”。
  * 队列是先进先出（FIFO）的。
  * 栈是后进先出（LIFO）的。
  * 你需要按加入顺序检查搜索列表中的人，否则找到的就不是最短路径，因此搜索列表必须是队列。
  * 对于检查过的人，务必不要再去检查，否则可能导致无限循环。









========================================
狄克斯特拉算法（Dijkstra’s algorithm）：加权图的最短距离
----------------------------------------

1. 狄克斯特拉算法包含4个步骤。
(1) 找出“最便宜”的节点，即可在最短时间内到达的节点。
(2) 更新该节点的邻居的开销，其含义将稍后介绍。
(3) 重复这个过程，直到对图中的每个节点都这样做了。
(4) 计算最终路径。



2.使用范围
狄克斯特拉算法只适用于有向无环图（directed acyclic graph，DAG）。

这就是狄克斯特拉算法背后的关键理念：找出图中最便宜的节点，并确保没有到该节点的更便宜的路径！



3. 负权边

如果有负权边，就不能使用狄克斯特拉算法。


为狄克斯特拉算法这样假设：对于处理过的海报节点，没有前往该节点的更短路径。
这种假设仅在没有负权边时才成立。


在包含负权边的图中，要找出最短路径，可使用另一种算法——贝尔曼-福德算法（Bellman-Ford algorithm）。



4. 实现

需要维护三个散列表： Graph, costs, parents.

随着算法的进行，不断更新 costs 和 parents 表。

(1) 图 
这里有权重，你需要保存邻居节点和权重。

graph={}
graph["start"] = {}
graph["start"]["a"] = 6
graph["start"]["b"] = 2

获取起点的邻居：
print( graph["start"].keys()  ) #["a", "b"]

获取起点到邻居的权重：
print( graph["start"]["a"] ) #6
print( graph["start"]["b"] ) #2

下面来添加其他节点及其邻居。
graph["a"] = {}
graph["a"]["fin"] = 1
graph["b"] = {}
graph["b"]["a"] = 3
graph["b"]["fin"] = 5
graph["fin"] = {}  #终点没有任何邻居



(2) 需要用一个散列表 (costs) 来存储每个节点的开销。
节点的开销指的是从起点出发前往该节点需要多长时间。

对于还不知道的开销，你将其设置为无穷大。
python 的无穷大: infinity = float("inf")


创建开销表的代码如下：
infinity = float("inf")
costs = {}
costs["a"] = 6
costs["b"] = 2
costs["fin"] = infinity


(3) 还需要一个存储父节点的散列表( parents )

parents = {}
parents["a"] = "start"
parents["b"] = "start"
parents["fin"] = None 


(4) 最后，你需要一个数组，用于记录处理过的节点，因为对于同一个节点，你不用处理多次。
processed = [] 


(5) 伪代码 

只要还有要处理的节点：
	获取离起点最近的节点
	更新其邻居的开销
	如果有邻居的开销被更新，同时更新其父节点
	该节点标记为处理过。


(6) python 实现

node = find_lowest_cost_node(costs) #从未处理的节点中找到开销最小的节点
while node is not None:  #在所有节点都被处理过后结束
	cost = costs[node]   #到该节点的权重
	neighbors = graph[node] #该节点的邻居
	for n in neighbors.keys():  #遍历该节点的邻居
		new_cost = cost + neighbors[n] #到给节点的邻居的权重=老权重，+连线权重
		if costs[n] > new_cost:  #如果经过当前节点前往邻居更近，则
			costs[n] = new_cost     #更新该邻居节点的开销
			parents[n] = node       # 修改该邻居节点的父节点为当前节点
	processed.append(node)   #将当前节点标记为处理过
	node = find_lowest_cost_node(costs) #找到下一个要处理的节点，并循环


目前还没有实现 函数find_lowest_cost_node()。
我们再次回顾过程。

在纸上画吧。
起点到终点，加上中间的AB共4个点。
start:{A:6, B:2}
A:{stop:1,}
B:{A:3, stop:5}

起始 costs 表，起点到各个点的距离
A:6
B:2
fin:无穷大
costs: {'a': 6, 'b': 2, 'fin': inf}

parents: {'a': 'start', 'b': 'start', 'fin': None}

开始costs最小的是B点。
到B的cost=2，邻居是 {A:3, final:5}
	n=A 时
		new_cost= B的开销 + 经过点B到A的权重=2+3=5
		起点到A的开销是 6 > 经过B再到A的距离 5: 
			进入if，更新A的开销和父节点
			parents: {'a': 'b', 'b': 'start', 'fin': None}
			costs: {'a': 5, 'b': 2, 'fin': inf}

	n=final 时
		new_cost = B的开销 + 经过B到final的权重=2+5=7
		起点到final的开销 初始为无穷大 > 经过B再到final的距离 7：
			进入if，更新 final 的开销和父节点
			parents: {'a': 'b', 'b': 'start', 'fin': "b"}
			costs: {'a': 5, 'b': 2, 'fin': 7}

从未处理节点中，找开销最小的点。这时是 A 
到A点的 cost=5，邻居是 {final:1}
	n=final时
		new_cost = A的开销 + 结果A到final的开销=5+1=6
		起点到final的开销 7 > 经过A到final的距离6:
			进入if，更新final的开销和父节点
			parents: {'a': 'b', 'b': 'start', 'fin': "a"}
			costs: {'a': 5, 'b': 2, 'fin': 6}


(7) 实现函数 find_lowest_cost_node()

def find_lowest_cost_node(costs):
	lowest_cost = float("inf")
	lowest_cost_node = None
	for node in costs: #遍历costs表的节点：遍历所有节点
		cost = costs[node] #该节点的开销
		if cost < lowest_cost and node not in processed: #选出未处理过的最小开销节点
			lowest_cost = cost
			lowest_cost_node = node
	return lowest_cost_node





5. 完整实现

#《算法图解》之狄克斯特拉算法
#https://blog.51cto.com/u_15334850/3502082

# the graph
graph = {}
graph["start"] = {}
graph["start"]["a"] = 6
graph["start"]["b"] = 2

graph["a"] = {}
graph["a"]["fin"] = 1

graph["b"] = {}
graph["b"]["a"] = 3
graph["b"]["fin"] = 5

graph["fin"] = {}
print(">>graph:", graph)


# the costs table
infinity = float("inf")
costs = {}
costs["a"] = 6
costs["b"] = 2
costs["fin"] = infinity
print(">>costs:", costs)


# the parents table
parents = {}
parents["a"] = "start"
parents["b"] = "start"
parents["fin"] = None
print(">>parents:", parents)

processed = []

def find_lowest_cost_node(costs):
    lowest_cost = float("inf")
    lowest_cost_node = None
    # Go through each node.
    for node in costs:
        cost = costs[node]
        # If it's the lowest cost so far and hasn't been processed yet...
        if cost < lowest_cost and node not in processed:
            # ... set it as the new lowest-cost node.
            lowest_cost = cost
            lowest_cost_node = node
    return lowest_cost_node


# Find the lowest-cost node that you haven't processed yet.
node = find_lowest_cost_node(costs)

# If you've processed all the nodes, this while loop is done.
while node is not None:
    print(">>node:", node)
    
    cost = costs[node]
    
    # Go through all the neighbors of this node.
    neighbors = graph[node]
    print(">>", node, " 's neighbors:", neighbors);
    
    for n in neighbors.keys():
        print("\t>>parents:", parents)
        print("\t>>costs:", costs)
        
        new_cost = cost + neighbors[n]
        print("\t>>n=", n, ", cost[n]:", costs[n], ", new_cost:", new_cost);
        # If it's cheaper to get to this neighbor by going through this node...
        if costs[n] > new_cost:
            print("\t>> update ...")
            # ... update the cost for this node.
            costs[n] = new_cost
            # This node becomes the new parent for this neighbor.
            parents[n] = node
    # Mark the node as processed.
    processed.append(node)
    # Find the next node to process, and loop.
    node = find_lowest_cost_node(costs)

print("\nCost from the start to each node:")
print(">costs: ", costs) #dynamic
print(">parents: ", parents) #dynamic
print(">graph: ", graph) #no change during process.


# find path from parents
# {'a': 'b', 'b': 'start', 'fin': 'a'}
x="fin"
chain=[x]
while parents.get(x) is not None:
    x=parents.get(x)
    chain.append(x);
print("->".join( chain[::-1] )) # start->b->a->fin




6.小结
  * 广度优先搜索用于在非加权图中查找最短路径。
  * 狄克斯特拉算法用于在加权图中查找最短路径。
  * 仅当权重为正时狄克斯特拉算法才管用。
  * 如果图中包含负权边，请使用贝尔曼-福德算法。



ref:
https://www.geeksforgeeks.org/dijkstras-shortest-path-algorithm-greedy-algo-7/






========================================
贪婪算法 - NP不完全问题的近似求解: (背包问题)、旅行商问题、集合覆盖问题
----------------------------------------

目标 
  * 学习如何处理不可能完成的任务：没有快速算法的问题（NP完全问题）。
  * 学习识别NP完全问题，以免浪费时间去寻找解决它们的快速算法。
  * 学习近似算法，使用它们可快速找到NP完全问题的近似解。
  * 学习贪婪策略——一种非常简单的问题解决策略。



1. 排课问题: 一个教室极可能多的排课

选最先结束的课；
然后上一节课结束后开始的课中，最先结束的课；
...

每一步都是最优的，通过局部最优获取全局最优，或近似最优。




2. 背包问题

有一个40磅的背包，有三件物品

物品   /价值    /重量(磅)
音响   $3000    30p
笔记本 $2000    20p
吉他   $1500    15p

使用贪婪策略：
先找到价值最大的商品，音响 3k，重量 30p，背包还剩10p，装不下其他的了。
如果它装另外连个物品，共 35p，价值 3.5k，比上一个更多。
说明这种情况下，贪婪算法失败了。
但也算接近最优解了。


从这个示例你得到了如下启示：在有些情况下，完美是优秀的敌人。
有时候，你只需找到一个能够大致解决问题的算法，此时贪婪算法正好可派上用场，因为它们实现起来很容易，得到的结果又与正确结果相当接近。









3. 集合覆盖问题

你办了一个节目，想让50个洲的听众都能收听。
每个电台覆盖的洲不同，两个电台覆盖的州可能重复。
每个电台播放都收取费用，怎么选用尽可能少的电台，覆盖全部州。


(1) 如果使用完整遍历，则需要遍历各种组合：
先看每个电台C(n, 1)，有没有能覆盖全部州的；
然后看2个电台的情况，各种两两组合C(n,2)，看是否覆盖全部州；
然后三个电台的情况，各种C(n, 3)，看有没有覆盖全部州的；
...
需要的总时间是：C(n,1)+C(n,2)+...+C(n,n)=2^n;
这个时间增速太快了，指数级的。
时间复杂度就是 O(2^n);



(2) 使用贪婪策略：
先找到覆盖最多州的电台；
然后在没覆盖的州中，选择覆盖最多的电台；
...
直到所有州都覆盖，或者电台用完。


这是一种近似算法（approximation algorithm）。在获得精确解需要的时间太长时，可使用近
似算法。判断近似算法优劣的标准如下：
	速度有多快；
	得到的近似解与最优解的接近程度


参考实现:

# 贪婪算法： 集合覆盖度问题
# 每个电台覆盖几个州，2个电台覆盖的州可能有重叠。求覆盖全部州需要至少哪几个电台？

# 传入所有的州，转为一个集合
states_needed=set(["mt", 'wa', 'or', 'id','nv','ut','ca','az'])

# 电台，及覆盖的州
stations={}
stations["kone"]=set(['id','nv','ut'])
stations['ktwo']=set(['wa','id','mt'])
stations['kthreee']=set(['or','nv','ca'])
stations['kfour']=set(['nv','ut'])
stations['kfive']=set(['ca','az'])

# 最终选用的电台
final_stations=set();

i=0;
while len(states_needed)>0:
    i+=1;
    if i>1000:
        break; #防止死循环
    
    # 一轮挑选一个覆盖(未覆盖区域)最广泛的电台
    best_station = None;
    # 覆盖的州
    states_covered=set();
    # 遍历电台
    for station, states_for_station in stations.items():
        # 求交集：未覆盖的州，该电台覆盖的州
        covered= states_needed & states_for_station;
        # 记录覆盖最多未覆盖州的电台
        if len(covered) > len(states_covered):
            best_station = station;
            states_covered = covered;
    # for 结束，就找到了覆盖最多的(未覆盖)州的电台
    # 则从未覆盖州中去掉这几个州
    states_needed -= states_covered;
    # 最终选用的电台增加一个
    final_stations.add(best_station)
    print(i, best_station)

# 集合无法保证顺序，添加顺序和打印集合的输出不一致。
print(final_stations)




(3) 下面各种算法是否是贪婪算法。
快速排序。
广度优先搜索。
狄克斯特拉算法。







4. 旅行商问题

(1) 遍历角度看
旅行商问题假设：
	A->B和B->A是不同的路线，因为有单行道，有绕行，路程可能不同。
	从哪里开始不确定，只要能不重复的走遍5个城市即可。

所以可以看做一个排列组合问题：
	先选中一个作为入口，5种情况
	再从其余的城市选一个，4种情况；
	...
	总共是 5! 种方案。

时间复杂度就是 O(n!);
阶乘级别的，增速也很快。



(2) 贪婪算法近似解答
随机选择一个城市作为入口，然后选择其余城市中距离最近的，
	然后选择其余城市中距离最近的，...。

//todo








5. NP 完全问题

旅行商问题和集合覆盖问题有一些共同之处：你需要计算所有的解，并从中选出最小/最短的那个。这两个问题都属于NP完全问题。

NP完全问题的简单定义是，以难解著称的问题，如旅行商问题和集合覆盖问题。
很多非常聪明的人都认为，根本不可能编写出可快速解决这些问题的算法。


很难判断出要解决的问题是否属于NP完全问题。比如，
	上一节说到的，从A到B的最短路径，就是可解的。
	找经过几个点的最短路径，就是旅行商问题，NP完全问题。


没办法判断问题是不是NP完全问题，但还是有一些蛛丝马迹可循的。
	元素较少时算法的运行速度非常快，但随着元素数量的增加，速度会变得非常慢。
	涉及“所有组合”的问题通常是NP完全问题。
	不能将问题分成小问题，必须考虑各种可能的情况。这可能是NP完全问题。
	如果问题涉及序列（如旅行商问题中的城市序列）且难以解决，它可能就是NP完全问题。
	如果问题涉及集合（如广播台集合）且难以解决，它可能就是NP完全问题。
	如果问题可转换为集合覆盖问题或旅行商问题，那它肯定是NP完全问题。







========================================
动态规划 Dynamic Programming (基础中最难的部分)
----------------------------------------
目标:
* 学习动态规划，这是一种解决棘手问题的方法，它将问题分成小问题，并先着手解决这些小问题。
* 学习如何设计问题的动态规划解决方案。


这是一类很难的算法，需要结合例子反复理解、实践: https://www.geeksforgeeks.org/dynamic-programming/




1. 背包问题 

数字细节和上文的略有调整。
有一个能装4磅的背包，另有三件物品，怎么才能装价值尽可能大的物品？

物品   /价值    /重量(磅)
音响   $3000    4p
笔记本 $2000    3p
吉他   $1500    1p


(1) 遍历算法
遍历各种组合，判断能否装下，能装下的再计算总金额。

0
音响 3000   4p
笔记本 2000 3p
吉他 1500 1p
音响+笔记本 7p 装不下
音响+吉他  5p 装不下
笔记本+吉他 3500 4p   ### max
音响+笔记本+吉他 装不下

共 C(3,1)+C(3,2)+C(3,3)=2^3=8种组合。
如果n件物品，就需要 2^n 个组合，时间复杂度 O(2^n)，太慢。

商品多到一定程度该策略就不可行了，太慢。比如快递车需要运输哪些物品，保证高价值的商品先送到。
如何找到最优解呢？



(2) 动态规划算法
对于4磅的大背包，可以考虑2个小背包的情况: 1+3

动态规划是一个难以理解的概念，我们可以从示例开始。

填充一个 3行x4列的 表格。3行分别是三个物品，4列分别是1-4磅。

		1	2	3	4
吉他 
音响 
笔记本

1) 吉他行: 吉他 1500 1p
这时只有一个吉他可以填充，包的大小是变化的。

如果包只有1p，则可以填充吉他，最大价值是 1500.
如果包只有2p，则可以填充吉他，最大价值是 1500.
如果包只有3p，则可以填充吉他，最大价值是 1500.
如果包只有4p，则可以填充吉他，最大价值是 1500.

		1	2	  3	   4
吉他  1.5   1.5   1.5  1.5
音响 
笔记本

这行表示的是当前的最大价值。


2) 音响行: 音响 3000   4p
目前你可以塞 吉他和音响了。

如果包只有1p，它之前有一个1p的吉他，剩余空间塞不下了，最大价值还是 1500.
如果包只有2p 或 3p，它之前有一个1p的吉他，剩余空间塞不下了，最大价值还是 1500.
如果包只有4p，它之前有一个1p的吉他，剩余空间塞不下了，最大价值还是 1500，
	或者它还可以掏出来吉他，只装一个4p的音响，最大价值是 3000，这才是最大价值。

		1	2	  3	   4
吉他  1.5   1.5   1.5  1.5
音响  1.5   1.5   1.5  3
笔记本

我们看到，价值最大化，是考虑以下两种情况的最大者:
	只装当前物品的价值;
	之前的最大价值和剩余空间能装东西的最大价值;


3) 笔记本电脑行: 笔记本 $2000    3p
这时有三个物品可以塞进包里了。
音响   $3000    4p
笔记本 $2000    3p
吉他   $1500    1p

如果包只有1p，它之前有一个1p的吉他，剩余空间塞不下了，最大价值还是 1500.
如果包只有2p，它之前有一个1p的吉他，剩余空间1p塞不下了，最大价值还是 1500.
如果包只有3p，它之前有一个1p的吉他，剩余空间2p塞不下了，最大价值还是 1500.
	或者 只塞一个 笔记本电脑 3p，最大价值是 2000
如果包只有4p，它之前有一个4p的音响，剩余空间0p塞不下了，最大价值 3000.
	或者 只塞一个笔记本电脑 3p，还余下的1p(在装入笔记本电脑前的一行中，1p对应的格子是: 1500的吉他)，最大价值是 3500

		1	2	  3	   4
吉他  1.5   1.5   1.5  1.5
音响  1.5   1.5   1.5  3
笔记本 1.5  1.5   2    3.5



4) 公式
==> 总结一下我们计算每个单元格时使用的公式: i 表示行，j表示列。
cell[i][j] = max(
	上一个单元格的价值( cell[i-1][j] ),
	当前商品的价值 + 剩余空间的价值( cell[i-1][j-当前商品的重量] )
)
第一行上面都补充0，第一列前面都补充0.

像是黑魔法一样，我们得到了4磅包能装的最大价值是: 3500(笔记本电脑3p + 吉他1p)




5) 再增加一个商品: iPhone手机 $2000 1p
现在有4个物品了：
音响   $3000    4p
笔记本 $2000    3p
吉他   $1500    1p
手机   $2000    1p

我们重新计算表格:
如果包只有1p，它之前有一个1p的吉他，剩余空间塞不下了，最大价值 1500.
	或者 只装 1p的手机，2000
如果包只有2p，它之前有一个1p的吉他，剩余空间1p 能装下这个1p的手机，最大价值 1500+2000=3500.
	或者 只塞 1p 的手机，剩余空间1p在手机之前的最大价值是 1500的吉他，最大价值 2000+1500=3500.
如果包只有3p，它之前有一个3p的笔记本电脑，剩余空间0，最大价值 2000.
	或者 只塞 1p 的手机，剩余空间2p在手机之前的最大价值是 1p的吉他，最大价值是 2000+1500=3500
如果包只有4p，它之前有一个1p的吉他+3p的笔记本电脑，最大价值 1500+2000=3500.
	或者 只塞 1p 的手机，剩余的空间3p空间，在手机之前的最大价值是 3p的笔记本电脑，最大价值是 2000+2000=4000

		1	2	  3	   4
吉他  1.5   1.5   1.5  1.5
音响  1.5   1.5   1.5  3
笔记本1.5   1.5   2    3.5
手机   2    3.5   3.5  4    

从左上角开始往下走，最大值不可能降低。因为每次迭代，都是保留当前的最大值，最大价值要和过去比，不可能比过去低。



6) 假设你还可偷另外一件商品——MP3播放器，它重1磅，价值1000美元。你要偷吗？

// todo 



7) 如果填充的顺序发生变化，会影响最后的答案吗？
比如改为：音响、笔记本电脑、吉他。

音响   $3000    4p
笔记本 $2000    3p
吉他   $1500    1p

答案是没有变化。


8) 可以逐列填充吗？
本问题可以，但是其他问题不一定。


9) 增加一件更小的商品将如何呢
假设你还可以偷一条项链，它重0.5磅，价值1000美元。

由于项链的加入，你需要考虑的粒度更细，因此必须调整网格。
列变成 0.5, 1,1.5,2,2.5,3,3.5,4



10) 可以偷走商品的一部分吗？
比如大豆、玉米，偷走一部分。
动态规划无法处理这种情况。使用动态规划时，要么考虑拿走整件商品，要么考虑不拿，而没法判断该不该拿走商品的一部分。


但使用贪婪算法可轻松地处理这种情况！首先，尽可能多地拿价值最高的商品；如果拿光了，再尽可能多地拿价值次高的商品，以此类推。


11) 计算最终的解时会涉及两个以上的子背包吗

不会，因为算法就是2个小背包取最大值，大背包最多包含2个子背包。
但是，子背包可能再包含最多2个子背包。以此类推。



12) 最优解可能导致背包没装满吗
完全可能。假设你还可以偷一颗钻石。

比如有颗钻石非常大，重达3.5磅，价值100万美元，比其他商品都值钱得多。
剩余0.5磅装不下其他的东西了。




(3) 算法实现 粗糙版

# 背包问题的解，这个是粗糙解，还有很大优化空间。

# 一个背包最多能装4磅货物，则它能装的最大价值是多少？
#G吉他   $1500    1p
#S音响   $3000    4p
#N笔记本 $2000    3p

# 限制因素: 物品重量
weight=[1, 4, 3]
# 优化指标: 价值 max
price=[1500, 3000, 2000]

# 增加一个物品 p手机  $2000 1P
weight.append(1); price.append(2000)

# 重量最大值，粒度默认 1
weightMax=len(weight)+1;

# 创建表格，并初始化，边缘行列（第0行，第0列）也是0.
cell=[];
for i in range(0, len(price)+1 ):
    cell.append([]);
    for j in range(0,weightMax,1): #各种小背包的重量 粒度
        # print("item=",i, ", weight=", j)
        cell[i].append(0)
print("init:", cell)

# 装不装该物品的价值，这次遍历不含第0行，第0列。
print("分解为若干小问题: ");
for i in range(1, len(price)+1 ): #遍历物品
    for j in range(1,weightMax,1): #遍历背包的可能最大重量
        # 1.每个物品都有2个状态，装或者不装进去。返回两者的最大值。         
        # 2.如果该物品超重，直接返回不装它的状态
        # (1)不装它的价值，就是上一个单元格的价值
        val1=cell[i-1][j]
        # (2)装它的价值，就是 当前商品的价值 + 剩余空间的价值 cell[i-1][j-当前商品的重量]
        cW=weight[i-1] #当前物品的重量        
        val2=0
        if j- cW >= 0: #当前背包的最大容量 - 当前物品的重量不能是负数
            val2 = price[i-1] + cell[i-1][j-cW]
        
        cell[i][j]=max(val1, val2)
        print(f">> [{i},{j}]",cW,val1, val2, cell[i][j])
print("result:", cell)

# 打印结果, 不含第0行，第0列。
print("\n>> Row: item; Column: weight. Max: last cell.")
for i in range(1, len(price)+1 ):
    print(cell[i][1:])


输出:
>> Row: item; Column: weight. Max: last cell.
[1500, 1500, 1500, 1500]
[1500, 1500, 1500, 3000]
[1500, 1500, 2000, 3500]
[2000, 3500, 3500, 4000]














2. 旅游行程最优化

(1) 你要去度假，假期2天，你想去尽可能多的地方。你的清单如下:

景点	时间(天)	评分
A	    0.5		7
B	    0.5		6
C	    1		9
D	    2		9
E	    0.5		8

这也是一个背包问题:
打分表示想去看的程度: 对应背包的价值
约束条件是时间：对应背包的容量。

填表，记录每个景点对应的最大打分。
	0.5	1	1.5	2
A 
B 
C 
D 
E 



(2) 处理相互依赖的情况

如果取了E(巴黎)和F、G、H(巴黎的景点)在同一个方向，单独去F、G、H都需要1天，但是去了E再去F、G、H则总共只需要0.5天。
如何处理这种带有依赖的情况？

没办法建模。动态规划功能强大，它能够解决子问题并使用这些答案来解决大问题。
但仅当每个子问题都是离散的，即不依赖于其他子问题时，动态规划才管用。
这意味着使用动态规划算法解决不了去巴黎玩的问题。











3. 最长公共子串

(1) 启示
* 动态规划可帮助你在给定约束条件下找到最优解。在背包问题中，你必须在背包容量给定的情况下，偷到价值最高的商品。
* 在问题可分解为彼此独立且离散的子问题时，就可使用动态规划来解决。


(2) 要设计出动态规划解决方案可能很难
下面是一些tips

* 每种动态规划解决方案都涉及网格。
* 单元格中的值通常就是你要优化的值。在前面的背包问题中，单元格的值为商品的价值。
* 每个单元格都是一个子问题，因此你应考虑如何将问题分成子问题，这有助于你找出网格的坐标轴。


(3) 词典单词匹配问题
你是词典网站管理员，用户输入 fish 时错误的输入了 hish。
你的字典里没有这几个词，但是有类似的单词，他到底想查 hash 还是 vista 呢？
你怎么推测他最可能想查哪个单词呢？


先思考该问题怎么使用动态规划解决？
没思路也没关系，勤思考总是好的。



==> 绘制表格
单元格中的值是什么？
如何将这个问题划分为子问题？
网格的坐标轴是什么？

我们尝试找出 fish 和 hish 的最长公共子串。以及 hish 和 vista 的最长公共子串。 
单元格内通常是要优化的值，那就是最长公共子串的长度了。

如何划分为子问题？比较子串，而不是整个单词。那么坐标很可能是这2个单词了。

	H I S H 
F 
I 
S 
H 

这确实很难懂。耐心看，反复琢磨。
如果你一眼就理解了，你应该相信自己是天才。


==> 填充网格
我们目测知道 fish 和 hish 的最大子串是 ish，然后看怎么设计流程。
还是好难，找不到简单方法，给点提示，下面是表格的一部分:
	H I S H 
F   0 0
I 
S       2 0
H         3

每个单元格都是一个子问题的值。
为何 cell[3,3]=2, cell[3,4]=0 呢?
请尝试找出公式，然后接着读。
找不出来...也可以接着读。


完整表格
  H I S H 
F 0 0 0 0
I 0 1 0 0 
S 0 0 2 0
H 0 0 0 3

公式就是：
* 如果单元格x和y对应的字母不同，则cell[i,j]=0
* 如果单元格x和y对应的字母相同，则对应单元格的值为其左上角单元格的值+1

写成伪代码
if word_a[i] == word_b[j]:
	cell[i][j] = cell[i-1][j-1] + 1 #字母相同
else:
	cell[i][j] = 0 #字母不同



==> 单词hish和vista的最长公共子串时的表格
  V I S T A
H 0 0 0 0 0
I 0 1 0 0 0
S 0 0 2 0 0
H 0 0 0 0 0

最大值就是问题的答案，是2个，目测是 is

注意：最后一个单元格不是问题的答案！
对于背包问题，最终答案都在最后一个单元格中。
但是对于最大子串问题，答案为网格中的最大数字，可能不在最后一个格子内。


我们认为最长子串最长的是最佳答案， fish。





4. 最长公共子序列

用户不小心输入了 fosh，他原本想输入的是 fish 还是 fort 呢？
使用最长公共子串比较，都是2: sh 和 fo。
但是我们知道 fosh 与 fish 更像，这里比较的是公共 子序列，不是 子串。
子序列可以不连续 f*sh.

公共 子序列 规则:
* 如果2个字母不同，就选择上方和左方邻居中较大的那个。(与计算 子串时不同)
* 如果两个字母相同，就选择其左上角的值+1。 (与计算 子串时相同)

伪代码:
if word_a[i] == word_b[j]:
	cell[i][j] = cell[i-1][j-1] + 1 #字母相同
else:
	cell[i][j] = max(cell[i-1][j], cell[i][j-1]) #字母不同时


本章到这里就结束了！它绝对是本书最难理解的一章。





5. 动态规划都有哪些实际应用呢？

* 生物学家根据最长公共序列来确定DNA链的相似性，进而判断度两种动物或疾病有多相似。最长公共序列还被用来寻找多发性硬化症治疗方案。

* 你使用过诸如git diff等命令吗？它们指出两个文件的差异，也是使用动态规划实现的。

* 前面讨论了字符串的相似程度。编辑距离（levenshtein distance）指出了两个字符串的相似程度，也是使用动态规划计算得到的。编辑距离算法的用途很多，从拼写检查到判断用户上传的资料是否是盗版，都在其中。

* 你使用过诸如Microsoft Word等具有断字功能的应用程序吗？它们如何确定在什么地方断字以确保行长一致呢？使用动态规划！




6. 小结
	需要在给定约束条件下优化某种指标时，动态规划很有用。
	问题可分解为离散子问题时，可使用动态规划来解决。
	每种动态规划解决方案都涉及网格。
	单元格中的值通常就是你要优化的值。
	每个单元格都是一个子问题，因此你需要考虑如何将问题分解为子问题。
	没有放之四海皆准的计算动态规划解决方案的公式。




更多例子: https://www.geeksforgeeks.org/dynamic-programming/




========================================
K最近邻算法 k-nearest neighbours(KNN)
----------------------------------------
本章内容
	学习使用K最近邻算法创建分类系统。
	学习特征抽取。
	学习回归，即预测数值，如明天的股价或用户对某部电影的喜欢程度。
	学习K最近邻算法的应用案例和局限性。


1. 基本概念

(1) 朴素解释
对于一堆标记好的数据(橘子和橙子)，有一个点未知分类(神秘水果)，我们就求距离该点最近的K个点，看其中哪个分类多，就使用哪个点的结果。
关键点: 距离的计算方法，K的选择。

(2) 推荐系统
给用户推荐可能喜欢的商品(或电影)，看与他最近的用户喜欢什么，就给他推荐什么。


(3) 特征提取
为了计算距离，需要提取特征。

比如水果的颜色和个头，然后绘图。
用户注册时对各类电影的喜好程度打分。
	问题：如果有用户特别挑剔，对最喜好的电影也不打10分，只给5分怎么办？要标准化。否则无法和其他用户的打分做比较。
	如果有几个大V，他们的打分权重更高，怎么体现？


(4) 回归
怎么预测某个用户对某个电影的打分？
选出距离他最近的k个用户，取他们打分的平均值，作为预测值。

这就是 KNN 回归。



(5) 余弦相似度 cosine similarity

假设有两位品味类似的用户，但其中一位打分时更保守。
他们都很喜欢Manmohan Desai的电影Amar Akbar Anthony，但Paul给了5星，而Rowan只给4星。
如果你使用距离公式，这两位用户可能不是邻居，虽然他们的品味非常接近。

余弦相似度不计算两个矢量的距离，而比较它们的角度，因此更适合处理前面所说的情况。
如果你要使用KNN，就一定要研究研究它！


cos<a,b>=a.b / (|a|.|b|)



(6) 挑选合适的特征




2. 机器学习简介
(1) KNN 是入门机器学习的好例子。

(2) OCR
光学字符识别 optical character recognition, 根据照片识别文字。
需要训练。

(3) 垃圾邮件过滤器
朴素贝叶斯分类器 naive bayes classifier.

(4) 预测股票市场
很难，几乎不可能完成的任务。




3. 小结
但愿通过阅读本章，你对KNN和机器学习的各种用途能有大致的认识！
机器学习是个很有趣的领域，只要下定决心，你就能很深入地了解它。

	KNN用于分类和回归，需要考虑最近的邻居。
	分类就是编组。
	回归就是预测结果（如数字）。
	特征抽取意味着将物品（如水果或用户）转换为一系列可比较的数字。
	能否挑选合适的特征事关KNN算法的成败。






========================================
接下来如何做
----------------------------------------
概述本书未介绍的10种算法以及它们很有用的原因。
如何根据兴趣选择接下来要阅读的内容。


1.树

二叉查找树（binary search tree）的数据结构。
对于每个节点，左子节点的值都比它小，而右子节点的值都比它大。

查找二叉树的速度几乎和二分查找一样。在二叉树查找树的节点时，平均运行时间为 O(log n)，但最糟糕的情况下需要时间为 O(n)。
而有序数组查找时，即使在最糟糕的情况下所需时间也只有 o(log n)。
因此你可能认为有序数组比二叉查找树更佳。
然而，二叉查找树的插入和删除操作的速度要快得多。

二叉树查找也有缺点，不能直接索引，比如“给我第五个元素”。
平衡二叉树，平均访问时间 o(log n)。
常常是不平衡的，就是一边元素比较多，另一边比较少。

也有一些处于平衡状态的特殊二叉查找树，如红黑树。

那在什么情况下使用二叉查找树呢？B树是一种特殊的二叉树，数据库常用它来存储数据。
如果你对数据库或高级数据结构感兴趣，请研究如下数据结构：B树，红黑树，堆，伸展树。




2. 反向索引

散列表的键为单词，值为包含指定单词的页面。
hi: A,B
there:A,C 
we:C

网页找包含 hi的网页是 A和B，包含 there 的网页是A和C。
用户搜索there，则返回 A和C页面的地址。

这种数据结构被称为反向索引（inverted index），常用于创建搜索引擎。如果你对搜索感兴趣，从反向索引着手研究是不错的选择




3. 傅里叶变换
绝妙、优雅且应用广泛的算法少之又少，傅里叶变换算是一个。

https://betterexplained.com/
Better Explained是一个杰出的网站，致力于以通俗易懂的语言阐释数学，它就傅里叶变换做了一个绝佳的比喻：
给它一杯冰沙，它能告诉你其中包含哪些成分①。
换言之，给定一首歌曲，傅里叶变换能够将其中的各种频率分离出来。

傅里叶变换非常适合用于处理信号，可使用它来压缩音乐。

首先需要将音频文件分解为音符。傅里叶变换能够准确地指出各个音符对整个歌曲的贡献，让你能够将不重要的音符删除。这就是MP3格式的工作原理！
数字信号并非只有音乐一种类型。JPG也是一种压缩格式，也采用了刚才说的工作原理。
傅里叶变换还被用来地震预测和DNA分析。

使用傅里叶变换可创建类似于Shazam这样的音乐识别软件。傅里叶变换的用途极其广泛，你遇到它的可能性极高！




4. 并行算法

算法设计是由极限的，然后就靠拼硬件了。多核会比单核有较大提升。


并行对效率的提升不是线性的，因为：
	并行性管理开销。每个核处理部分功能，最后的合并也需要时间。
	负载均衡。如果分配的不均匀，一个核1s运行完了，另一个核60s还没结束。怎么均匀分配工作呢？




5. MapReduce 分布式算法(多台计算机)

数据表超过10亿行后，MySQL会很吃力。
可以通过 hadoop 使用 MapReduce 执行。

分布式算法非常适合用于在短时间内完成海量工作，其中的MapReduce基于两个简单的理念：映射（map）函数和归并（reduce）函数。

(1) 映射函数
映射函数很简单，它接受一个数组，并对其中的每个元素执行同样的处理。
例如，下面的映射函数将数组的每个元素翻倍。
>>> arr1 = [1, 2, 3, 4, 5]
>>> arr2 = map(lambda x: 2 * x, arr1)
[2, 4, 6, 8, 10] 


arr2包含[2, 4, 6, 8, 10]：将数组arr1的每个元素都翻倍！将元素翻倍的速度非常快，
但如果要执行的操作需要更长的时间呢？请看下面的伪代码。
>>> arr1 = # A list of URLs
>>> arr2 = map(download_page, arr1)

在这个示例中，你有一个URL清单，需要下载每个URL指向的页面并将这些内容存储在数组arr2中。
对于每个URL，处理起来都可能需要几秒钟。如果总共有1000个URL，可能耗时几小时！

如果有100台计算机，而map能够自动将工作分配给这些计算机去完成就好了。
这样就可同时下载100个页面，下载速度将快得多！这就是MapReduce中“映射”部分基于的理念。


(2) 归并函数
归并函数可能令人迷惑，其理念是将很多项归并为一项。
	映射是将一个数组转换为另一个数组。
	而归并是将一个数组转换为一个元素。

下面是一个示例。
>>> arr1 = [1, 2, 3, 4, 5]
>>> reduce(lambda x,y: x+y, arr1)
15

在这个示例中，你将数组中的所有元素相加：1 + 2 + 3 + 4 + 5 = 15！

MapReduce使用这两个简单概念在多台计算机上执行数据查询。
数据集很大，包含数十亿行时，使用MapReduce只需几分钟就可获得查询结果，而传统数据库可能要耗费数小时。





6. 布隆过滤器和 HyperLogLog

假设你管理着网站Reddit。每当有人发布链接时，你都要检查它以前是否发布过，因为之前未发布过的故事更有价值。

又假设你在Google负责搜集网页，但只想搜集新出现的网页，因此需要判断网页是否搜集过。

在假设你管理着提供网址缩短服务的bit.ly，要避免将用户重定向到恶意网站。你有一个清单，其中记录了恶意网站的URL。你需要确定要将用户重定向到的URL是否在这个清单中。

这些都是同一种类型的问题，涉及庞大的集合。

使用散列表即可。
只是Google需要建立数万亿个网页的索引，因此这个散列表非常大，需要占用大量的存储空间。
Reddit和bit.ly也面临着这样的问题。面临海量数据，你需要创造性的解决方案！


(1) 布隆过滤器
布隆过滤器是一种概率型数据结构。
使用散列表时，答案绝对可靠，而使用布隆过滤器时，答案却是很可能是正确的。
适用于不要求结果那么准确的情况。

(2) HyperLogLog
HyperLogLog是一种类似于布隆过滤器的算法。
如果Google要计算用户执行的不同搜索的数量，或者Amazon要计算当天用户浏览的不同商品的数量，要回答这些问题，需要耗用大量的空间！对





7. SHA 算法

(1) 比较文件
另一种散列函数是安全散列算法（secure hash algorithm，SHA）函数。给定一个字符串，SHA返回其散列值。

(2) 检查密码
数据库不保存明文密码，而是sha散列值。即使数据库泄密被拷贝走，用户也几乎没有损失。

你可将密码转换为散列值，但反过来不行。
这意味着计算攻击者窃取了Gmail的SHA散列值，也无法据此推断出原始密码！

SHA实际上是一系列算法：SHA-0、SHA-1、SHA-2和SHA-3。
本书编写期间，SHA-0和SHA-1已被发现存在一些缺陷。如果你要使用SHA算法来计算密码的散列值，请使用SHA-2或SHA-3。
当前，最安全的密码散列函数是bcrypt，但没有任何东西是万无一失的。


(3) 局部敏感的散列算法

有时候，你希望结果相反，即希望散列函数是局部敏感的。在这种情况下，可使用Simhash。
如果你对字符串做细微的修改，Simhash生成的散列值也只存在细微的差别。
这让你能够通过比较散列值来判断两个字符串的相似程度，这很有用！
	论文查重
	搜索引擎是否已经搜录某个网页等。

需要检查两项内容的相似程度时，Simhash很有用。






9. Diffie-Hellman 密钥交换
这里有必要提一提Diffie-Hellman算法，它以优雅的方式解决了一个古老的问题：如何对消息进行加密，以便只有收件人才能看懂呢？

Diffie-Hellman算法解决了如下两个问题。
	双方无需知道加密算法。他们不必会面协商要使用的加密算法。
	要破解加密的消息比登天还难。

Diffie-Hellman使用两个密钥：公钥和私钥。顾名思义，公钥就是公开的，可将其发布到网站上，通过电子邮件发送给朋友，或使用其他任何方式来发布。你不必将它藏着掖着。有人要向你
发送消息时，他使用公钥对其进行加密。加密后的消息只有使用私钥才能解密。只要只有你知道私钥，就只有你才能解密消息！

Diffie-Hellman算法及其替代者RSA依然被广泛使用。如果你对加密感兴趣，先着手研究Diffie-Hellman算法是不错的选择：它既优雅又不难理解。







10. 线性规划
最好的东西留到最后介绍。线性规划是我知道的最酷的算法之一。

所有的图算法都可使用线性规划来实现。线性规划是一个宽泛得多的框架，图问题只是其中的一个子集。但愿你听到这一点后心潮澎湃！

线性规划使用Simplex算法，这个算法很复杂，因此本书没有介绍。如果你对最优化感兴趣，就研究研究线性规划吧！






========================================
变点检测总结-Changepoint Detection
----------------------------------------
1. 文献中提到
https://www.thno.org/v10p10531.htm

- 变更点模型： bcp R包
- 变点检测总结-Changepoint Detection： https://zhuanlan.zhihu.com/p/335322056


The probes were partitioned into common and extended groups using `Bayesian analysis of the change point (BCP)`, which is implemented by R package 'bcp' (version 4.0.0)[22].

> 22. Erdman C, Emerson JW. bcp: An R package for performing a Bayesian analysis of change point problems. J Stat Softw. 2007;23:1-13










========================================
----------------------------------------






========================================
----------------------------------------





========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------




========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



