sklearn 笔记



参考书: 《深入浅出 Python机器学习》
第一章: 机器学习基本概念
第二章：Python环境配置
3-8章: 常见ML算法原理与实现
9-10: 数据预处理、聚类，特征工程
11-12: 模型优化，聚合算法
13: 文本数据处理
14: 爬虫
15: 展望




========================================
sklearn 简介，安装包 scikit-learn
----------------------------------------
1. 网站
https://scikit-learn.org/stable/
https://github.com/scikit-learn/scikit-learn

scikit-learn: machine learning in Python


Simple and efficient tools for predictive data analysis
Accessible to everybody, and reusable in various contexts
Built on NumPy, SciPy, and matplotlib
Open source, commercially usable - BSD license


六大类基本功能：
分类
回归
聚类
数据降维
模型选择
数据预处理





2. 十分钟上手sklearn：特征提取，常用模型，

1.PCA算法：主成分分析
2.LDA算法：线性评价分析
3.线性回归
4.逻辑回归
5.朴素贝叶斯
6.决策树
7.SVM
8.神经网络
9.KNN算法


(1) 安装

$ pip3 install scikit-learn -i https://pypi.douban.com/simple/

sklearn 与 scikit-learn 是同一个东西嘛？前置是后者的缩写。安装 假包sklearn 时会默认安装其 scikit-learn。


pip install -U scikit-learn
pip install --user scikit-learn

pip list 
## scikit-learn                      0.23.2
## sklearn                           0.0

文档
https://scikit-learn.org/stable/


(2) 内置/玩具 数据集简介

UCI 数据集: 
	https://archive.ics.uci.edu/ml/index.php
	新版 https://archive-beta.ics.uci.edu/
	http://archive.ics.uci.edu/ml/machine-learning-databases/




查看数据集列表
import sklearn.datasets
dir(sklearn.datasets)

'get_data_home',
'load_boston',
'load_breast_cancer',
'load_diabetes',
'load_digits',
'load_files',
'load_iris',
'load_linnerud',
'load_sample_image',
'load_sample_images',
'load_svmlight_file',
'load_svmlight_files',
'load_wine',


某个数据集的详情
from sklearn.datasets import load_boston
load_boston(return_X_y=False)
print(load_boston().DESCR)


载入某个数据集
X, y = load_boston(return_X_y=True)
print(X.shape) #(506, 13)


波士顿房价数据集
# 载入数据
from sklearn.datasets import load_boston
X,y=load_boston().data, load_boston().target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)






(3) 实例

X = [[0], [1], [2], [3]] # 已知数据点
y = [0, 0, 1, 1] # 已知点的标签

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X, y)
# KNeighborsClassifier(...)
print(neigh.predict([[1.1]])) #[0]

print(neigh.predict_proba([[0.9]]))
#[[0.66666667 0.33333333]]


如果导入包报错: ImportError: cannot import name 'MultiOutputMixin' from 'sklearn.base'
则需要重启一下python session，就正常了。





========================================
torch, tensorflow, keras, sklearn, 的侧重点、使用习惯与发展趋势
----------------------------------------
业界偏爱tf，学界偏爱pytorch，后者的内存管理为人诟病，真到实际部署还得tf
	只要能用到gpu的我都会用pytorch
	TF做部署还是挺方便的
	TF有种一子落错，满盘皆输的趋势，这种趋势已经比较难改变了
keras现在又分离出来了，都在瞎折腾，还是pytorch稳定

tf与其说是一个框架不如说是dsl了[飙泪笑]学习曲线和python本身完全分离
小模型用keras，大模型用torch，总归不会错[机智]

学pytorch吧，tensorflow学习成本太高。
tensorflow难用是公认的，PyTorch的好用也是公认的，学习PyTorch可以让你尽情施展算法能力。
Tensorflow2 不难了。我大概花了2个月学习并整理了一份TF2的教程。《30天吃掉那只TensorFlow2.0》，有需要的小伙伴可以参考一下哦。
	https://github.com/lyhue1991/eat_tensorflow2_in_30_days



(2)
比如说盖木头房子。想盖什么房子要先选木料，然后加工成需要的形状，最后组合钉装成想要的房子形状。

tensorflow 好比是木头，Keras 好比是拿 tensorflow 做好的木板。如果你盖的房子简单，形状大众，Keras 调用起来会很方便。但如果想设计特殊的房子，那就要从木料开始。

初学和入门的话建议用 keras。但是想要深入或者做自己的APP的话建议用 tensorflow。


(3)
新手且不打算对内部原理进行深入了解：推荐keras快速搭建、训练、测试。

新手且打算研究内部的结构，并且想要自己创造性的生成一些新的结构：keras先拿来跑一下baseline，知道一下机器学习基本流程和大概样子，然后去学tensorflow。（虽然不知道为什么大家都说tensorflow很难学然后选择放弃，但tensorflow的学习资源也是相比其他框架多得多的，所以有那么多资料和攻略，就更不怕难和麻烦了，看一下资料，读一下代码然后自己去实现一下，对自己的能力是很大的提高，而且现在tensorflow的API也很成熟了，各种类型的网络都有现成函数可以调用，所以不用害怕）习惯之后，你会发现其实只要你思路清晰，无非就是以下几个问题：
程序的规范（dtype的匹配，张量shape的匹配）
数据结构、数据接口
并行的处理
其他等等

如果后面有一些终端部署需求的，tensorflow也是再合适不过的，tensorflow.js以及相关的方案都是很棒的。








========================================
chapter 3: KNN 最近邻算法: K-Nearest Neighbor
----------------------------------------

1. 原理

计算新点p0到已知分类的点pn的距离，最近的k个点中哪一类最多，那么新点p0就属于哪一类。

KNN也可用于回归：取最近的几个点的y坐标的平均值作为该点的预测值。


2. 优缺点 

KNN的优点: 简单，容易理解。
KNN的缺点：
	需要对数据认真的预处理
	对规模超大的数据集拟合的时间较长
	对高维数据集拟合欠佳
	对于稀疏数据集束手无策




========================================
|-- KNN 二分类
----------------------------------------
2. 最简单实例: 已知二分类，预测一个新点的分类


(1) 训练集：生成已知标签的数据集

# 导入数据生成器
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

data=make_blobs(n_samples=200, centers=2, random_state=8)
X,y=data

plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.spring, edgecolor='k')
plt.show()

# 图略：上下两部分散点，上面紫色，下面黄色。


(2) 画出分类器
import numpy as np

# 导入KNN分类器
from sklearn.neighbors import KNeighborsClassifier

clf=KNeighborsClassifier()
clf.fit(X, y)

# 画网格背景
x_min, x_max = X[:, 0].min() -1, X[:,0].max()+1
y_min, y_max = X[:, 1].min() -1, X[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
# 对区域的每一点进行预测，作为颜色值
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)

# 画散点图
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel1, shading='auto') # 带颜色的网格, 第三个参数是颜色
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.spring, edgecolor='k') #散点图
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: KNN")
plt.show()

图略:
# 创建分类模型，由上部粉色区域和下部灰色区域组成。
# 如果有新数据，落到哪里就是哪个分类了。



(3) 画出新数据点 6.75, 4.82
# 在分类模型上画出该点：在 plt.show() 前加入这句: plt.scatter(6.75, 4.82, marker="*", c="red", s=200)

# 画网格背景
x_min, x_max = X[:, 0].min() -1, X[:,0].max()+1
y_min, y_max = X[:, 1].min() -1, X[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
# 对区域的每一点进行预测，作为颜色值
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)

# 画散点图
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel1, shading='auto') # 带颜色的网格, 第三个参数是颜色
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.spring, edgecolor='k') #散点图
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: KNN")
plt.scatter(6.75, 4.82, marker="*", c="red", s=200) #画出新的数据点
plt.show()

图略：新点是一个红色五角星。
# 从图中看，新点落在下方灰色区域中。



(4) 带入模型验证一次。
clf.predict([ [ 6.75, 4.82] ]) #确实归为第1类
# array([1])


print(clf.predict([ [6, 10] ])) #上方点 0
print(clf.predict([ [6, -0.29] ])) #下方点 1



========================================
|-- KNN 多分类
----------------------------------------

(1) 生成已知标签的数据集：500个点，分为5类
# 导入数据生成器
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# 修改 make_blobs 的 center 参数，分类数提高到5个
# 修改 n_samples 参数，把样本量也增加到 500个
data2=make_blobs(n_samples=500, centers=5, random_state=8)
X2,y2=data2

# 画散点图
plt.scatter(X2[:,0], X2[:,1], c=y2, cmap=plt.cm.spring, edgecolor='k')
plt.show()

#图略：5类中的2类有重叠部分。重叠部分一般不好区分，是分类错误最多的区域，。



(2) 使用KNN建模
import numpy as np

# 导入KNN分类器
from sklearn.neighbors import KNeighborsClassifier

clf=KNeighborsClassifier()
clf.fit(X2, y2)


# 画图
x_min, x_max = X2[:, 0].min() -1, X2[:,0].max()+1
y_min, y_max = X2[:, 1].min() -1, X2[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel1, shading='auto')

plt.scatter(X2[:,0], X2[:,1], c=y2, cmap=plt.cm.spring, edgecolor='k')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: KNN")
plt.show()

# 建立5个分区，大部分是正确分类的，重合区域、边界附近有少部分点是错误分类的。
#耗时比上一次多了很多。


(3) 输出在训练集中的正确率
clf.score(X2, y2) #0.956







========================================
|-- KNN 用于回归分析
----------------------------------------
KNN回归的原理: 对x轴进行遍历，取距离最近的几个点的y坐标的平均值作为预测值。

(1) 导入 make_regression 回归数据生成器
from sklearn.datasets import make_regression
# 生成特征数量为1，噪音为50的数据集
X,y=make_regression(n_features=1, n_informative=1, noise=50, random_state=8)

# 散点图
import matplotlib.pyplot as plt
plt.scatter(X,y, c="orange", edgecolor="k")
plt.show()

#图略： x范围+-3， y范围+-250，倾斜45度角的散点



(2) 建立KNN回归模型
# 导入用于回归分析的KNN模型
from sklearn.neighbors import KNeighborsRegressor
reg=KNeighborsRegressor()
# 用KNN模型拟合数据
reg.fit(X,y)

# 可视化
import numpy as np
z=np.linspace(-3,3, 200).reshape(-1,1)
plt.scatter(X,y,c="orange", edgecolor="k")
plt.plot(z, reg.predict(z), c="k", linewidth=3)
plt.title("KNN Regressor")
plt.show()
# 图略: 黑色表示KNN回归生成的模型。直观看，效果不好，大量的数据点没有被模型覆盖。


# 给模型评分
reg.score(X,y) #0.772


(3) 怎么提高模型打分？

# 调整 n_neighbors，默认5，我们减少该值
reg2=KNeighborsRegressor(n_neighbors=2)
reg2.fit(X,y)

# 再次可视化
import numpy as np
z=np.linspace(-3,3, 200).reshape(-1,1)
plt.scatter(X,y,c="orange", edgecolor="k")
plt.plot(z, reg2.predict(z), c="k", linewidth=3)
plt.title("KNN Regressor: n_neighbors=2")
plt.show()
# 图略：黑色曲线覆盖了更多的点，也就是说，模型变复杂了。


# 再次给模型评分
reg2.score(X,y) #0.858
# 打分确实提高了，0.77->0.86








========================================
|-- KNN 分类真实案例：酒的分级
----------------------------------------
假设我们对酒的品质一无所知，现在已知一个酒的各项参数，让给出分级，怎么做？

(1) 载入酒的数据
from sklearn.datasets import load_wine
wine_dataset=load_wine()
print(type(wine_dataset)) #这是一个很复杂的格式 <class 'sklearn.utils.Bunch'>


# 尝试了解该数据集
wine_dataset
# 略

wine_dataset.keys() #dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])
# 数据 data，分类目标 target, 分类名字 target_names，数据描述 DESCR，特征变量的名字 feature_names

wine_dataset.data.shape #(178, 13)  178行 样本，13列 特征变量

print(wine_dataset.DESCR) # 略
#可见共3个分类，class_0-2。
# 13个变量分别是: 酒精含量、苹果酸、色彩表合度等。




(2) 分拆成训练集和测试集
# train_test_split 函数，默认随机分组，75%的归为训练集，25%归为测试集。

# 一般使用X表示数据特征，y表示对应的标签。因为X是二维的数组，也称为矩阵，y是一维数组，也叫向量。

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split

# 拆分数据。random_state 随机数种子。当设置为0或者缺省时，每次生成的随机数都不同。
X_train, X_test, y_train, y_test=train_test_split(
    wine_dataset["data"], wine_dataset["target"], random_state=8
)
# 检查数据 行列 数
print("X_train", X_train.shape)
print("X_test", X_test.shape)
print("y_train", y_train.shape)
print("y_test", y_test.shape)

输出 
X_train (133, 13)
X_test (45, 13)
y_train (133,)
y_test (45,)


(3) 建模
# 导入 KNN 分类模型
from sklearn.neighbors import KNeighborsClassifier
# 指定模型的 n_neighbors 参数为1
knn=KNeighborsClassifier(n_neighbors=1) #最近的k=1个已知点

# fit: 用模型对数据进行拟合
knn.fit(X_train, y_train)
# KNeighborsClassifier(n_neighbors=1)

# 使用测试集检验模型
print(knn.score(X_train, y_train)) #1.0 对训练集全对
knn.score(X_test, y_test) #0.7111 测试集 71% 正确率


(4) 使用模型对新数据进行预测
import numpy as np
X_new=np.array([ [ 13.2, 2.77, 2.51, 18.5,96.6,1.04,2.55,0.57,1.47,6.2,1.05,3.33,820] ] )
print(X_new.shape) #(1, 13)

#预测
prediction=knn.predict(X_new)
print(prediction, wine_dataset["target_names"][prediction] )

输出：
(1, 13)
[2] ['class_2']



小结：测试集上的正确率确实有点低。尝试看看怎么优化。
不过，作为入门级的方法，主要用于理解整体建模套路。






========================================
chapter 4: glm 广义线性模型: 适合于高维数据
----------------------------------------

1. 线性模型是一类广泛应用于机器学习领域的预测模型。
使用输入数据集的特征的线性函数进行建模，并对结果进行预测。

(2). 概要
线性模型的基本概念
线性回归模型
岭回归模型
套索回归模型 Lasso
二元分类器中的 逻辑回归
和线性SVC模型



2. 线性模型的概念

公式 y_hat = w*x + b

对于只有一个特征变量的数据集，w是直线的斜率，b是截距。
如果特征值增加，每个w值就对应每个特征直线的斜率。

另一种角度看，模型的预测可以看做输入特征的加权和，而w代表每个特征的权重，当然w也可以为负数。


(1) 画一条直线

import numpy as np
import matplotlib.pyplot as plt

# 令x为-5到5之间，元素数为100的等差数列
x=np.linspace(-5,5,100)
# 输入直线方程
y=0.5*x + 3

plt.plot(x,y, c="orange")
plt.title("Straight line")
plt.show()


线性模型，就是通过训练数据确定自身斜率和截距。



(2) 通过2点确定一条直线

# 2点(1,3) (4,5) 确定一条直线

# 导入线性回归模型
from sklearn.linear_model import LinearRegression

# 输入2个点的横坐标
X=[[1], [4]]
# 输入2个点的纵坐标
y=[3,5]

# 用线性模型拟合这2个点
lr=LinearRegression().fit(X, y)
# 画出2个点和直线
z=np.linspace(0,5, 20)
plt.scatter(X,y,s=80) #画2个点
plt.plot(z, lr.predict(z.reshape(-1,1)), c='k')

plt.title("Straight line")
plt.show()

# 输出该直线的方程
w=lr.coef_[0]
b=lr.intercept_
print( "y = {:.3f}".format(w), "x", " + {:.3f}".format(b)   )




(3) 如果是3个点呢？
# 2点(1,3) (4,5) (3,3)确定一条直线
from sklearn.linear_model import LinearRegression

X=[[1], [4], [3]]
y=[3, 5, 3]

# 拟合
lr=LinearRegression().fit(X, y)

# 画图
z=np.linspace(0,5, 20)
plt.scatter(X,y,s=80) #画2个点
plt.plot(z, lr.predict(z.reshape(-1,1)), c='k')
plt.title("Straight line")
plt.show()

# 输出该直线的方程
w=lr.coef_[0]
b=lr.intercept_
print( "y = {:.3f}".format(w), "x", " + {:.3f}".format(b)   )


直线没有穿过任何一点，而是位于和3个点的距离和最小的位置。



(4) 生成更多点，做线性拟合

from sklearn.datasets import make_regression
#生成用于回归分析的数据
X,y=make_regression(n_samples=50, n_features=1, n_informative=1, noise=50, random_state=1)

# 线性拟合
reg=LinearRegression()
reg.fit(X,y)
# 生成等差数列z作为横轴，画线性模型的图形
z=np.linspace(-3,3, 200).reshape(-1,1)
plt.scatter(X, y, c='b', s=60)
plt.plot(z, reg.predict(z), c='k') #预测每个x对应的y
plt.title("Linear regression")
plt.show()

# 输出该直线的方程
w=reg.coef_[0]
b=reg.intercept_
print( "y = {:.3f}".format(w), "x", " + {:.3f}".format(b)   )


注意： sklearn 把下划线作为训练数据集的结尾，比如 coef_, intercept_, 以便和用户自定义参数区分开。

特征数只有1，用一条直线进行预测分析。
特征变量为2时，是一个平面。
更多，是一个超平面。

如果训练数据集的特征变量大于数据点的数量的时候，线性模型可以对训练数据做出近乎完美的预测。







3. 最基本的线性模型 - 线性回归

线性回归，也称为 普通最小二乘法(OLS)。
线性回归没有可供用户调节的参数。

实例: 使用 make_regression 函数 生成一个样本数量为100，特征数为2的数据集，
使用 train_test_split 函数分割训练集和验证集，
用线性回归模型计算出w值和b值。

(1) 无噪音模拟数据

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
X,y=make_regression(n_samples=100, n_features=2, n_informative=2, random_state=38)
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)
lr=LinearRegression().fit(X_train, y_train)

# 打印出模型
print("coef:", lr.coef_[:])
print("intercept:", lr.intercept_)
# y=w1*x1 +w2*x2 + b

# 打分
print("training set:", lr.score(X_train, y_train))
print("tesing set:", lr.score(X_test, y_test))

# 完全对的原因，是因为没有添加noise！真实世界的数据，噪音是很大的。


(2) 载入真实数据 - 糖尿病数据

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# 载入数据
from sklearn.datasets import load_diabetes
X,y=load_diabetes().data, load_diabetes().target

X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)
lr=LinearRegression().fit(X_train, y_train)

# 打印出模型
print("coef:", lr.coef_[:])
print("intercept:", lr.intercept_)
# y=w1*x1 +w2*x2 + b

# 打分
print()
print("training set:", lr.score(X_train, y_train))
print("tesing set:", lr.score(X_test, y_test))

# 分别是0.53 和0.46，打分降低了很多！

线性回归很容易过拟合。
在训练集和测试集之间打分差异过大，是过拟合的一个明确信号。

我们怎么控制模型的复杂度呢？岭回归是 标准线性回归的一个常用的替代模型。







========================================
|-- 使用L2正则化的线性模型 - 岭回归
----------------------------------------

保留全部特征变量，只是降低特征变量的系数来避免过拟合的方法，称为L2正则化。

(1) 糖尿病模型，岭回归

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split

# 载入数据
from sklearn.datasets import load_diabetes
X,y=load_diabetes().data, load_diabetes().target
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)

# 导入岭回归
from sklearn.linear_model import Ridge

# 使用岭回归对数据进行拟合
ridge=Ridge().fit(X_train, y_train)

# 打印出模型
print("coef:", ridge.coef_[:])
print("intercept:", ridge.intercept_)
# y=w1*x1 +w2*x2 + b

# 打分
print()
print("training set:", ridge.score(X_train, y_train))
print("tesing set:", ridge.score(X_test, y_test))

# 分别是 0.43 和 0.43，打分接近


可以说，复杂度越低的模型，在训练集上表现越差，但是其泛化能力会更好。

如果在意泛化能力，则应该选择岭回归，而不是线性回归模型



(2) 岭回归的参数调节

岭回归是在模型的简单性（使系数趋近于零）和它在训练集上的性能之间取的平衡的一种模型。
用户可以使用alpha参数控制模型更加简单，还是在训练集上性能更高。

上例中使用的默认alpha=1.

注意: alpha的取值并没有一定的规定。取决于特定的数据集。
增加alpha 会降低特征变量的系数，使其趋于零，从而降低在训练集的性能，但更有助于泛化。

1) 本例子使用 alpha =10.
from sklearn.linear_model import Ridge

# 使用岭回归对数据进行拟合，设置 alpha=10
ridge=Ridge(alpha=10).fit(X_train, y_train)

# 打印出模型
print("coef:", ridge.coef_[:])
print("intercept:", ridge.intercept_)
# y=w1*x1 +w2*x2 + b

# 打分
print()
print("training set:", ridge.score(X_train, y_train))
print("tesing set:", ridge.score(X_test, y_test))

# 分别是 0.15 和 0.16，测试集打分超过训练集了
# 也就是说，如果模型过拟合，可以通过提高 alpha 值来降低过拟合现象。


2) 降低 alpha 值会让系数的限制变得不那么严格。当alpha很小时，限制可以忽略不计，非常接近线性回归。
from sklearn.linear_model import Ridge

# 使用岭回归对数据进行拟合，设置 alpha=0.1
ridge=Ridge(alpha=0.1).fit(X_train, y_train)

# 打印出模型
print("coef:", ridge.coef_[:])
print("intercept:", ridge.intercept_)
# y=w1*x1 +w2*x2 + b

# 打分
print()
print("training set:", ridge.score(X_train, y_train))
print("tesing set:", ridge.score(X_test, y_test))

# 分别是 0.52 和 0.47
# 相比线性模型，alpha很小时，训练集打分略降低，而测试集打分略提高。




(3) alpha值对模型的影响

画图展示 不同 alpha 值对应的模型的 coef_ 属性。
较高的 alpha 值表示模型的限制更加严格。
所以我们认为，alpha值越高，coef_属性的数值会更小，反之 coef_ 属性的值更大。



import matplotlib.pyplot as plt

# alpha=0.1 时的模型系数
plt.plot(Ridge(alpha=0.1).fit(X_train, y_train).coef_, 'o', label="Ridge alpha=0.1")

# alpha=1 时的模型系数
plt.plot(Ridge(alpha=1).fit(X_train, y_train).coef_, 's', label="Ridge alpha=1")

# alpha=10 时的模型系数
plt.plot(Ridge(alpha=10).fit(X_train, y_train).coef_, '^', label="Ridge alpha=10")

# 绘制线性回归的系数作为对比
from sklearn.linear_model import LinearRegression
lr=LinearRegression().fit(X_train, y_train)
plt.plot(lr.coef_, "o", label="linear regression")
#
plt.xlabel("coefficient index")
plt.ylabel("coefficent magnitude")
plt.hlines(0,0,len(lr.coef_)) #水平直线，过原点
plt.legend()
plt.show()



横轴代表 coef_ 属性：
x=0 显示第一个特征变量的系数，
x=1 显示的是第二个特征变量的系数，
以此类推，直到 x=10时。
纵轴显示特征变量的系数数量级。

当 alpha=10 时，特征变量系数大多在0附近；
当 alpha=1时，岭模型的特征便阿玲系数普遍增大了；
当 alpha=0.1 时，岭回归系数更大了，已经接近线性回归。

而线性回归模型没有经过正则化处理，系数非常大，已经快跑到图表之外了。



(4) 数据集大小对岭回归的影响 - 学习曲线
另一个理解正则化对模型影响的方法，就是固定alpha值，该不安训练集的数据量。

x轴是数据集大小，y轴是学习打分，这样的曲线叫学习曲线。


import numpy as np

from sklearn.model_selection import learning_curve, KFold
# 定义一个绘制学习曲线的函数
def plot_learning_curve(est, X, y):
    # 对数据进行20次拆分用来对模型进行评分
    training_set_size, train_scores, test_scores=learning_curve(
        est, X, y, train_sizes=np.linspace(0.1, 1, 20), cv=KFold(20, shuffle=True,random_state=1))
    
    estimator_name=est.__class__.__name__
    line=plt.plot(training_set_size, train_scores.mean(axis=1), '--', label="training "+estimator_name)
    plt.plot(training_set_size, test_scores.mean(axis=1), '-',
            label="test "+estimator_name, c=line[0].get_color())
    plt.xlabel("Training set size")
    plt.ylabel("Score")
    plt.ylim(0, 1.1)    

plot_learning_curve(Ridge(alpha=1), X, y)
plot_learning_curve(LinearRegression(), X, y)
plt.legend(loc=(0, 1.05), ncol=2, fontsize=11)


- 可见，数据量小的时候，岭回归的训练集和测试集表现差不多，而普通线性回归则差异很大。
- 当数据量很大时，正则化就没那么重要了，两者表现一致。
- 随着数据量的增大，线性回归在训练集上的得分是下降的；说明数据量越大，线性回归越不容易过拟合，或者越难记住已知数据。





(5) 岭迹图 x=alpha, y=coef


# 创建 alpha 集合
alphas = np.logspace(-10,2,100)  # -10 到 2 取100份
# 计算对应的 coef
coefs = []
for alpha in alphas:
    # 获取模型 设置参数
	# 通过修改Ridge(fit_intercept=False)，来让岭回归模型来关闭差值，不让差值调整结果值，这样我们获得的斜率就不是0了。
    rr = Ridge(alpha=alpha, fit_intercept=False)
    rr.fit(X_train, y_train)
    coefs.append(rr.coef_)
# 绘图
plt.plot(alphas,coefs)
# 设置坐标轴 不是以均匀的方式展示 设置x轴线 而是 以10的倍数来显示
plt.xscale('log')
plt.show()





========================================
|-- 使用 L1 正则化的线性模型 - 套索回归 lasso
----------------------------------------


通俗解释
线性回归时使 权重的绝对值相加 让其结果不能大于某个值

一般在特征很多的时候，很多特征对结果的影响几乎为0，我们就可以限制 拉姆达的值，让加和小于某个值，其中权重最小的就会被归零，加和的值还大于拉姆达的话，再让最小的归零，知道小于拉姆达为止。





L1 正则化和L2正则化一样，也会让系数限制在很接近0的范围内。
但是 L1 正则化还会让一部分系数正好等于0，这可以看做是对特征进行自动筛选。
相当于忽略不重要的特征，突出重要的特征。


(1) 套索回归 - 默认参数
# 载入 糖尿病模型
import numpy as np
import matplotlib.pyplot as plt

# 导入数据集拆分工具
from sklearn.model_selection import train_test_split

# 载入数据
from sklearn.datasets import load_diabetes
X,y=load_diabetes().data, load_diabetes().target
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)


# 载入套索回归
from sklearn.linear_model import Lasso
# 使用套索回归拟合
lasso=Lasso().fit(X_train, y_train)

#输出打分
print("training score:", lasso.score(X_train, y_train))
print("testing score:", lasso.score(X_test, y_test))
print("特征数:", np.sum(lasso.coef_ !=0 ))

# 打分只有 0.36， 0.37，只使用了3个特征。
# 训练集结果也很糟糕，说明fasjeng欠拟合。



(2) 套索回归的参数调节

套索回归也有一个正则化参数 alpha，用来控制变量系数被约束到0的强度。
默认是1.

为了降低欠拟合的程度，尝试降低 alpha。
还需要增大 最大迭代次数(max_iter) 的默认设置。



# 使用套索回归拟合
lasso=Lasso(alpha=0.1, max_iter=100000).fit(X_train, y_train)

#输出打分
print("training score:", lasso.score(X_train, y_train))
print("testing score:", lasso.score(X_test, y_test))
print("特征数:", np.sum(lasso.coef_ !=0 ))


输出: 
training score: 0.519480608218357
testing score: 0.47994757514558173
特征数: 7


- 降低alpha值可以拟合出更复杂的模型，从而在训练集和测试集都能获得良好的表现。
- 该结果比 岭回归 稍好，且只用了10个特征中的7个特征。
- 但是，alpha 设置的太低，就去掉了正则化效果，模型就会像线性回归一样，出现过拟合现象。


# 设置 lasso 回归alpha=0.0001
lasso=Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)
#输出打分
print("training score:", lasso.score(X_train, y_train))
print("testing score:", lasso.score(X_test, y_test))
print("特征数:", np.sum(lasso.coef_ !=0 ))

输出
training score: 0.5303811330981303
testing score: 0.4594509683706016
特征数: 10

# alpha太小时，所有变量都用上了。且测试集打分低了10个百分点，说明有过拟合现象。




(3) 套索回归与岭回归的对比

画出不同alpha值的套索回归与岭回归的系数。


# 绘制 alpha=1, 0.1, 0.001 是的模型系数
plt.plot( Lasso(alpha=1).fit(X_train, y_train).coef_, 's', label="Lasso alpha=1")
plt.plot( Lasso(alpha=0.1).fit(X_train, y_train).coef_, '^', label="Lasso alpha=0.1")
plt.plot( Lasso(alpha=0.001).fit(X_train, y_train).coef_, 'v', label="Lasso alpha=0.001")

# 绘制 alpha=0.1 时的岭回归模型
from sklearn.linear_model import Ridge
plt.plot(Ridge(alpha=0.1).fit(X_train, y_train).coef_, "o", label="Ridge alpha=0.1")

plt.legend(ncol=2, loc=(0, 1.05))
#plt.ylim(-25, 25)
plt.xlabel("Coefficient index")
plt.ylabel("Coefficient magnitude")
plt.show()



alpha=1 时，不仅大部分系数为0，且不为0的几个点绝对值也很小。
当alpha=0.1时，大部分系数也为0，但是等于0的个数比alpha=1时少了很多。
而alpha=0.001时，整个模型几乎没有被正则化，大部分系数是非零的，且绝对值较大。

alpha=0.1 的岭回归和 alpha=0.1 的套索回归，预测能力类似，但是岭回归几乎所有的系数都非零。

同等条件选择岭回归。
但是如果特征过多，选择套索回归，因为变量少了更容易解释和理解。




(4) 系数收缩图 x=alpha, y=coef
# 创建 alpha 集合
alphas = np.logspace(-3, 2, 100)  # -3 到 2 取100份
# 计算对应的 coef
coefs = []
for alpha in alphas:
    # 获取模型 设置参数
	# 通过修改Ridge(fit_intercept=False)，来让岭回归模型来关闭差值，不让差值调整结果值，这样我们获得的斜率就不是0了。
    rr = Ridge(alpha=alpha, fit_intercept=False)
    rr.fit(X_train, y_train)
    coefs.append(rr.coef_)
# 绘图
plt.plot(alphas,coefs)
# 设置坐标轴 不是以均匀的方式展示 设置x轴线 而是 以10的倍数来显示
plt.xscale('log')
plt.xlabel("Alpha")
plt.ylabel("Coef")
plt.show()








========================================
|-- 弹性网模型 Elastic net: 套索回归 + 岭回归 // todo
----------------------------------------
通过设置系数，达到在一个模型中组合使用套索回归和岭回归的目的。

用户需要调整2个参数，一个是L1正则化参数，另一个是L2正则化参数。


正则化项L1和L2的直观理解
https://blog.csdn.net/jinping_shi/article/details/52433975







========================================
|-- 其他线性模型
----------------------------------------

logistic regression
线性支持向量机 (Linear SVM)


对于线性模型来说，最主要的桉树就是 正则化参数(Regularization Parameter).
- 在线性回归、岭回归、套索回归中使用alpha参数调节；
- 在logistic regression和 Linear SVM 中通过C参数调节。
- 如果特征太多，使用L1正则化，如lasso回归。
- 如果特征不多，而且每一个都很重要，则使用L2正则化建模，如岭回归。








========================================
|-- 模拟对权重的预测能力：普通线性回归、岭回归与lasso回归比较
----------------------------------------

import numpy as np
import matplotlib.pyplot as plt

# (1)创建数据
np.random.seed(10)  # 随机数种子
samples = 50  # 有几个样本就有几行
features = 100  # 有几个特征就有几列
X = np.random.randn(samples,features)  # 以0为中心，标准差为1的数 参数为形状
# X 作为特征值

# 随机生成权重
w = 10*np.random.randn(features)  # 有几个特征就有几个权重的值  给每个权重扩大10倍

# 随机将一些权重归零
index = np.random.permutation(features)  # 打乱的 各个权重的索引
index[:90]  # 找出前九十个索引
w[index[:90]] = 0  # 把前九十个打乱顺序的所对应的权重值 归零

# 根据现有的特征值与权重值求目标值
y = np.dot(X,w)


# (2) 比较各回归方式 预测权重的效果
from sklearn.linear_model import LinearRegression,Ridge,Lasso
lr = LinearRegression()
rr = Ridge(alpha=1, fit_intercept=False)  #这里主要研究 w 的值，所以为了不受影响，不使用偏差值
lasso = Lasso(alpha=0.8)  #alpha 用来设置权重的上限，不过alpha 的值为0-1的小数 用来表示有用的特征的比例

# 注意：Lasso中的alpha 表示有用特征的比例
# 例如 共有 5 个特征， 有用的 只有一个，那么 alpha = 0.2

# 训练数据
lr.fit(X,y)
rr.fit(X,y)
lasso.fit(X,y)


# 查看各个模型对coef的预测是否正确
# plt.figure(figsize=(12,8))  #设置画布大小
axes1 = plt.subplot(2,2,1)  # 先绘制真实的权重
axes1.plot(w)
axes1.set_title('real')

# 普通线性回归
axes2 = plt.subplot(2,2,2)
axes2.plot(lr.coef_)
axes2.set_title('lr')

# 岭回归
axes3 = plt.subplot(2,2,3)
axes3.plot(rr.coef_)
axes3.set_title('rr')

# 拉索回归
axes4 = plt.subplot(2,2,4)
axes4.plot(lasso.coef_)
axes4.set_title('lasso')

plt.show()








========================================
chapter 5: 朴素贝叶斯 Naive Bayes: 基于先验知识进行分类
----------------------------------------

朴素贝叶斯算法是一种基于贝叶斯理论的有监督学习算法。
朴素，是因为设个算法假设样本特征之间是相互独立的，这个“朴素”假设。
因为该假设，导致NB模型很高效。


概要:
贝叶斯定理简介
朴素贝叶斯的简单应用
伯努利朴素贝叶斯、高斯朴素贝叶斯和多项式朴素贝叶斯
朴素贝叶斯实例 - 判断肿瘤是良性还是恶性





========================================
|-- NB 基本概念: 贝叶斯定理 和 朴素贝叶斯分类器
----------------------------------------

1. 条件概率的定义

(1) A和B同时发生的概率，
	就是A发生的情况下，B发生的概率；
	或者
	就是B发生的情况下，A发生的概率；

写成公式，就是 
P(AB)=P(A)*P(B|A) = P(B)*P(A|B)
右边的等式做一下变换，就是贝叶斯公式 P(A|B)=P(A)*P(B|A) /P(B)


2. 简单应用 
用 0 代表没有下雨，而1代表下雨。

import numpy as np

# 过去7天是否下雨可以用数组表示 
y=np.array( [0,1,1,0,1,0,0] )

# 其他气象信息: 北风、闷热、多云、天气预报是否下雨
X=np.array([
	[0, 1, 0, 1],
	[1, 1, 1, 0],
	[0, 1, 1, 0],
	[0, 0, 0, 1],
	[0, 1, 1, 0],
	[0, 1, 0, 1],
	[1, 0, 0, 1],
])

# 分析下雨、不下雨时每个气象条件的频数
counts={}
for label in np.unique(y):
	counts[label]=X[ y==label ].sum(axis=0)

print("Feature counts:\n{}".format(counts))

# 没下雨时(y=0), 4天天气预报都说下雨，1天北风，2天闷热，0天多云。
# 下雨时(y=1)，天气预报都说没有下雨，1天北风，3天闷热，3天多云。

Feature counts:
{0: array([1, 2, 0, 4]), 1: array([1, 3, 3, 0])}




(2) 使用 伯努利贝叶斯分类器
from sklearn.naive_bayes import BernoulliNB
# 拟合数据
clf=BernoulliNB()
clf.fit(X,y)

# 预测一下训练集
print( clf.predict(X) )
# 打分
print( clf.score(X, y) )

输出：
[0 1 1 0 1 0 0]
1.0


还可以使用 clf.predict_proba() 给出下雨/不下雨的概率。


2) 如果天气预报说没有下雨，且出现多云，倾向于归类到“下雨”

clf.predict( [[0,0,1,0]] ) # array([1])
clf.predict_proba( [[0,0,1,0]] ) #array([[0.13848881, 0.86151119]]) #下雨的概率更大


3) 如果天气预报说下雨，且北风，闷热，无云，倾向于归类到“不下雨”
clf.predict( [[1,1,0,1]] ) #array([0])
clf.predict_proba( [[1,1,0,1]] ) #array([[0.92340878, 0.07659122]]) #不下雨的概率更大


警告：scikit-learn 官网文档给出一段很搞笑的描述：
https://scikit-learn.org/stable/modules/naive_bayes.html
On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.

	虽然朴素贝叶斯是相当好的分类器，但是对于预测具体数值并不擅长，
	因此 predict_proba() 给出的预测概率，不要太当真。




========================================
|-- 朴素贝叶斯的不同方法
----------------------------------------

sklearn 中的朴素贝叶斯有3种方法，分别是：
伯努利朴素贝叶斯 Bernoulli Naive Bayes;
高斯朴素贝叶斯 Gaussian Naive Bayes;
多项式贝叶斯 Multinomial Naive Bayes;


1. 伯努利分布也被称为 “二项分布” 或者 “0-1分布”。
比如我们进行抛硬币的游戏，结果就是正面、反面，我们称抛硬币的结果是符合伯努利分布的。

(1) 
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split

# 生成样本，数量500，分类数5
X,y=make_blobs(n_samples=500, centers=5, random_state=8)

# 拆分数据
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)

# 使用伯努利贝叶斯拟合数据
nb=BernoulliNB()
nb.fit(X_train, y_train)

# 打分
print("trainning score: {:0.3f}".format( nb.score(X_train, y_train) ) )
print("testing score: {:0.3f}".format( nb.score(X_test, y_test) ) )

# 只有一半分类是正确的，很糟糕！
输出 
trainning score: 0.499
testing score: 0.544


2) # 可视化，为什么这么糟糕
import matplotlib.pyplot as plt
x_min, x_max = X[:,0].min()-0.5, X[:,0].max()+0.5
y_min, y_max = X[:,1].min()-0.5, X[:,1].max()+0.5

# 用不同背景色表示不同分类
xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02),
                  np.arange(y_min, y_max, 0.02))
z=nb.predict(np.c_[(xx.ravel(), yy.ravel())]).reshape(xx.shape)
plt.pcolormesh(xx, yy, z, cmap=plt.cm.Pastel1, shading='auto')

# 画训练集和测试集散点图
plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.cool, edgecolor='k')
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.cool, edgecolors='k', marker='*')
plt.xlim( xx.min(), xx.max())
plt.ylim( yy.min(), yy.max())
plt.title("Classifier: BernoulliNB")
plt.show()


# 这就是简单把2条线，分为4个象限，注意有3个颜色。
# 因为使用了伯努利朴素贝叶斯的默认参数 binarize=0.0，所以模型对于数据的判断是
#  如果特征1大于或等于0，且特征2大于或等于0，归为一类；
#  如果特征1小于0，且特征2小于0，归为一类；
#  其余归为一类。
# 所以分类效果很烂。

# 对于多分类，不能使用伯努利朴素贝叶斯模型了。可以使用高斯朴素贝叶斯模型。








========================================
|-- 高斯朴素贝叶斯 (假设特征符合高斯分布)
----------------------------------------

就是假设样本的特征符合高斯分布/正态分布。

from sklearn.naive_bayes import GaussianNB
gnb=GaussianNB()
gnb.fit(X_train, y_train)

# 打分
print("trainning score: {:0.3f}".format( gnb.score(X_train, y_train) ) )
print("testing score: {:0.3f}".format( gnb.score(X_test, y_test) ) )


输出: 
trainning score: 0.939
testing score: 0.968



# 可视化，为什么分类效果这么好

# 用不同背景色表示不同分类
xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02),
                  np.arange(y_min, y_max, 0.02))
z=gnb.predict(np.c_[(xx.ravel(), yy.ravel())]).reshape(xx.shape)
plt.pcolormesh(xx, yy, z, cmap=plt.cm.Pastel1, shading='auto')

# 画训练集和测试集散点图
plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.cool, edgecolor='k')
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.cool, edgecolors='k', marker='*')
plt.xlim( xx.min(), xx.max())
plt.ylim( yy.min(), yy.max())
plt.title("Classifier: GaussianNB")
plt.show()


# 可见，高斯NB的分类边界比伯努利NB复杂的多，且基本分类正确。
# 最常用，因为自然科学和社会科学，大量现象都符合正态分布。




========================================
|-- 多项式朴素贝叶斯分布
----------------------------------------

二项分布通过抛硬币来理解，
多项式分布可以通过掷骰子来理解。

均匀的6面骰子，每次投掷后朝上的一面是1-6这6个数字。如果投掷n次，则每个面朝上的次数的分布，符合多项式分布。



from sklearn.naive_bayes import MultinomialNB

mnb=MultinomialNB()
#mnb.fit(X_train, y_train) # 报错 ValueError: Negative values in data passed to MultinomialNB (input X)
#mnb.score(X_test, y_test)


# 只能传入非负数
# 导入数据预处理工具 MinMaxScaler，作用是把特征值全部转为0-1之间。
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit(X_train)
X_train_scaled=scaler.transform(X_train)
X_test_scaled=scaler.transform(X_test)

# 使用多项式朴素贝叶斯拟合经过预处理的数据
mnb.fit(X_train_scaled, y_train)
mnb.score(X_test_scaled, y_test) #0.32




# 这个打分很糟糕，比伯努利NB还差。可视化

# 用不同背景色表示不同分类
xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02),
                  np.arange(y_min, y_max, 0.02))
z=mnb.predict(np.c_[(xx.ravel(), yy.ravel())]).reshape(xx.shape)
plt.pcolormesh(xx, yy, z, cmap=plt.cm.Pastel1, shading='auto')

# 画训练集和测试集散点图
plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.cool, edgecolor='k')
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.cool, edgecolors='k', marker='*')
plt.xlim( xx.min(), xx.max())
plt.ylim( yy.min(), yy.max())
plt.title("Classifier: MultinomialNB")
plt.show()

# 大部分数据放到了错误的分类中。

# 多项式NB只适合对非负离散数值特征进行分类。典型例子是转化为向量后的文本数据进行分类。





========================================
|-- 真实数据 - 判断中流是良性还是恶性
----------------------------------------

1. 了解数据
from sklearn.datasets import load_breast_cancer
cancer=load_breast_cancer()

print( cancer.keys() )
# dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])

print(cancer.DESCR)

# 共  569 个病例，每个病人 30 个数值特征，
# 2分类的结果(恶性/良性): 212 - Malignant, 357 - Benign

print( cancer.target_names) #['malignant' 'benign']
print( cancer["feature_names"]) #30个特征的名字



2. 建模 

X, y=cancer.data, cancer.target

# 拆分数据
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=38)
print("train set size:", X_train.shape)
print("test set size:", X_test.shape)

# 建模
from sklearn.naive_bayes import GaussianNB
gnb=GaussianNB()
gnb.fit(X_train, y_train)

# 打分
print("trainning score: {:0.3f}".format( gnb.score(X_train, y_train) ) )
print("testing score: {:0.3f}".format( gnb.score(X_test, y_test) ) )

输出 
train: (426, 30)
test: (143, 30)
trainning score: 0.948
testing score: 0.944


(2) 随便预测一个
print("predict:", gnb.predict( [X[312]] ))
print("real:", y[312])

输出：
predict: [1]
real: 1




3. 学习曲线 x=样本量 y=打分
随着样本量的增加，模型的打分变化情况。

import numpy as np
import matplotlib.pyplot as plt

# 导入学习曲线库
from sklearn.model_selection import learning_curve
# 导入随机拆分工具
from sklearn.model_selection import ShuffleSplit

# 定义一个函数绘制学习曲线
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, 
                       n_jobs=1, train_sizes=np.linspace(0.1, 1.0, 5)):
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    # xlab
    plt.xlabel("Traning examples")
    # ylab
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean=np.mean(train_scores, axis=1)
    test_scores_mean=np.mean(test_scores, axis=1)
    plt.grid()
    
    plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label="Cross-validation score")
    
    plt.legend(loc="lower right")
    return plt

# setting
title="Learning Curves (Naive Bayes)"
cv=ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
estimator=GaussianNB()
plot_learning_curve(estimator, title, X, y, ylim=(0.9, 1.01), cv=cv, n_jobs=4)
plt.show()


# 可见，随着样本量的增大，训练集打分逐渐降低，因为要拟合的信息越来越多。
# 而测试集打分基本不变，说明高斯NB在预测方面，对样本量的要求没那么苛刻。如果样本量少，可以考虑NB建模。










========================================
chapter 6: Tree & RF 决策树 与 随机森林
----------------------------------------

概要 
- 决定册数的与基本原理和构造
- 决策树的优势与不足
- 随机森林的基本原理和构造
- 随机森林的优势和不足
- 实战







========================================
|-- 决策树
----------------------------------------

1. 决策树的构建（最大决策深度=1）

import numpy as np
# 画图工具
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# 导入 tree 模型和数据集加载工具
from sklearn import tree, datasets
# 导入拆分工具
from sklearn.model_selection import train_test_split

wine=datasets.load_wine()
# 只选取数据集的前2个特征,为了图形方便展示
X=wine.data[:, :2]
y=wine.target
# 将数据集拆分
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)

# 设定决策树分类器的最大深度为1
clf=tree.DecisionTreeClassifier(max_depth=1)
# 拟合
clf.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(clf.score(X_train, y_train)) )
print("testing score: {:.3f}".format(clf.score(X_test, y_test)) )

# 最关键的参数就是 max_depth，就是问问题的数量，只能回答yes / no.
# 问的问题越多，表示决策树的深度越深。

输出
training score: 0.692
testing score: 0.622


# 可视化

# 定义图像中分区的颜色和散点的颜色
cmap_light=ListedColormap(["#FFAAAA", "#AAFFAA", "#AAAAFF"])
cmap_bold=ListedColormap(["#FF0000", "#00FF00", "#0000FF"])

# 分别用样本的2个特征创建图形和x/y轴
x_min, x_max= X_train[:, 0].min()-1,  X_train[:, 0].max()+1
y_min, y_max= X_train[:, 1].min()-1,  X_train[:, 1].max()+1

xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02), 
                  np.arange(y_min, y_max, 0.02))
Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])

# 给每个分类中的样本分配不同的颜色
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 样本散点图
plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold, edgecolor="k", s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: tree( max_depth = 1)")
plt.show()


只分2类，分类效果不好，不到 70%。





2. max_depth=3

# 尝试加大深度 3
clf2=tree.DecisionTreeClassifier(max_depth=3)
# 拟合
clf2.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(clf2.score(X_train, y_train)) )
print("testing score: {:.3f}".format(clf2.score(X_test, y_test)) )

输出：
training score: 0.887
testing score: 0.822


# 可视化
Z=clf2.predict(np.c_[xx.ravel(), yy.ravel()])

# 给每个分类中的样本分配不同的颜色
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 样本散点图
plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold, edgecolor="k", s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: tree( max_depth = 3)")
plt.show()





3. max_depth=5

# 尝试加大深度 5
clf3=tree.DecisionTreeClassifier(max_depth=5)
# 拟合
clf3.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(clf3.score(X_train, y_train)) )
print("testing score: {:.3f}".format(clf3.score(X_test, y_test)) )

# 出现过拟合倾向了，就是训练集效果远好于测试集。
输出 
training score: 0.925
testing score: 0.778




# 可视化
Z=clf3.predict(np.c_[xx.ravel(), yy.ravel()])

# 给每个分类中的样本分配不同的颜色
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 样本散点图
plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold, edgecolor="k", s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: tree( max_depth = 5)")
plt.show()





4. 决策树的可视化演示
$ pip3 install graphviz -i https://pypi.douban.com/simple/


import graphviz
from sklearn.tree import export_graphviz

# 选择 max_depth=3 的决策树进行可视化
# 输出到文件
export_graphviz(clf2, out_file="wine.dot", class_names=wine.target_names,
               feature_names=wine.feature_names[:2], impurity=False, filled=True)

# 读文件
with open("wine.dot") as f:
    dot_graph=f.read()
# 可视化
graphviz.Source(dot_graph)

# 这种层级关系非常方便向非专业人士解释算法是如何工作的。





5. max_depth与打分曲线

def getScore(depth):
    # 尝试加大深度 5
    clf=tree.DecisionTreeClassifier(max_depth=depth)
    # 拟合
    clf.fit(X_train, y_train)

    # 输出打分
    return [clf.score(X_train, y_train), clf.score(X_test, y_test)]

scores=[]
for i in range(1, 10):
    scores.append( getScore(i) )

scores=np.array(scores)
scores


# 画图
plt.plot(scores[:,0], label="trainning score")
plt.plot(scores[:,1], label="testing score")
plt.xlabel("Max_depth")
plt.ylabel("Score")
plt.legend()
plt.show()

# 可见max_demth超过2就过拟合了。



决策树的优点是：直观，方便解释；不需要对数据预处理；
缺点是：即使使用 max_depth 或者 max_leaf_nodes 等参数对决策树进行预剪枝处理，还时不可避免会过拟合，导致模型的泛化能力大打折扣。





========================================
|-- 随机森林Random Forests (集成学习方法：避免过拟合问题)
----------------------------------------

随机森林也称为 随机决策森林，是一种集合学习方法，既可用于分类，也可用于回归。

集合算法，包括 随机森林(Random Forests)和梯度上升决策树(Gradient Boosted Decision Tree, GBDT)。

随机森林是把不同的几棵决策树打包到一起，每棵树的参数都不相同，然后我们取每棵树预测结果的平均值。
这样既可以保留决策树们的工作成效，又可以降低过拟合的风险。
可以用数学公式推导。略。

1. 继续使用 wine 数据集 

# 导入随机森林分类器
from sklearn.ensemble import RandomForestClassifier
# 导入拆分工具
from sklearn.model_selection import train_test_split

from sklearn import tree, datasets
wine=datasets.load_wine()
# 只选取数据集的前2个特征,为了图形方便展示
X=wine.data[:, :2]
y=wine.target
# 将数据集拆分
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)

# 设定随机森林有6棵树
forest=RandomForestClassifier(n_estimators=6, random_state=3)
# 拟合
forest.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(forest.score(X_train, y_train)) )
print("testing score: {:.3f}".format(forest.score(X_test, y_test)) )

# 测试集的结果比训练集明显差，已经过拟合了。

输出

training score: 0.977
testing score: 0.778


# help(RandomForestClassifier) 
# bootstrap=True 是一个重要的参数，也是默认值。
#   每棵树都是随机的样本，而每棵树也会选择不同的特征，保证每棵树都是不同的。

# max_feature 也是一个重要的参数，默认是 auto=sqrt(特征数量)。太少则每棵树差异太大，太大则每棵树基本都一样。
# n_estimators 是决策树的数量。这些树的概率投票决定着随机森林的输出。




(2). 可视化
## 可视化随机森林
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# 定义图像中分区的颜色和散点的颜色
cmap_light=ListedColormap(["#FFAAAA", "#AAFFAA", "#AAAAFF"])
cmap_bold=ListedColormap(["#FF0000", "#00FF00", "#0000FF"])

# 分别用样本的2个特征创建图形和x/y轴
x_min, x_max= X_train[:, 0].min()-1,  X_train[:, 0].max()+1
y_min, y_max= X_train[:, 1].min()-1,  X_train[:, 1].max()+1

xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02), 
                  np.arange(y_min, y_max, 0.02))
Z=forest.predict(np.c_[xx.ravel(), yy.ravel()])

# 给每个分类中的样本分配不同的颜色
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 样本散点图
plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold, edgecolor="k", s=20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Classifier: RandomForestClassifier")
plt.show()

# 结果更细腻了。
# 可以调节 n_estimator 参数和 random_state 参数，看分类器的表现怎么变化。





2. n_estimator与打分曲线

def getScore(n_est):
    forest=RandomForestClassifier(n_estimators=n_est, random_state=3)
    # 拟合
    forest.fit(X_train, y_train)
    # 输出打分
    return [forest.score(X_train, y_train), forest.score(X_test, y_test)]

scores=[]
for i in range(1, 30):
    scores.append( getScore(i) )

scores=np.array(scores)

# 画图
plt.plot(scores[:,0], label="trainning score")
plt.plot(scores[:,1], label="testing score")
plt.xlabel("n_estimator")
plt.ylabel("Score")
plt.legend()
plt.show()

# n_estimator=17 就是打分极限了





3. 随机森林的优缺点

优点
	应用广泛；
	不需要用户在意参数的调节；
	不需要对数据预处理；
	支持并行处理，就是把 n_jobs 参数设置的<=CPU 核心数；如果设置为-1，则使用全部CPU核心。

缺点：
	向非专业人士展示不方便，优先使用决策树来展示
	对于超高维数据、稀疏数据集等捉襟见肘。这种情况下，线性模型要比随机森林的表现更好一些。
	消耗内存，速度也比线性模型慢。





========================================
|-- 决策树实战 - 预测收入
----------------------------------------

1. 数据集 
简介
https://archive.ics.uci.edu/ml/datasets/Adult

下载 csv文件 
https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data
https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names

(1) 下载数据
import os
os.getcwd()


import pandas as pd
data=pd.read_csv("data/adult.data", header=None, index_col=False,
                names=["age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation", 
                       "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "income"])
# 为了方便展示，选取部分列
data_lite=data[ ["age", "workclass","education","sex", "hours-per-week", "occupation", "income"] ]

print(data.shape)
print(data_lite.shape)

data_lite.head()

输出 
(32561, 15)
(32561, 7)



(2) 数据预处理: 分类变量 to 哑变量

import numpy as np
data_lite.loc[:,"workclass"].unique()
# ？ 表示缺失值

#array([' State-gov', ' Self-emp-not-inc', ' Private', ' Federal-gov',
#       ' Local-gov', ' ?', ' Self-emp-inc', ' Without-pay',
#       ' Never-worked'], dtype=object)



# 使用 get_dummies 处理数据，把分类变量变为 0/1 数值型的。
data_dummies=pd.get_dummies(data_lite)

print("样本原始特征:\n", list(data_lite.columns), "\n" )
print("虚拟变量特征:\n", list(data_dummies.columns) )
print( data_dummies.shape)

data_dummies.head()
#可见原来的7列已经扩展成 46 列了。
# 输出略。




## 把数据分配给X和y
features=data_dummies.loc[:, 'age':'occupation_ Transport-moving']
X=features.values
# 将收入大于50k作为预测目标
y=data_dummies["income_ >50K"].values

# 维度
print(X.shape, y.shape)


(3) 建模
# 导入拆分工具
from sklearn.model_selection import train_test_split
# 将数据集拆分
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

############
from sklearn import tree
# 设定决策树分类器的最大深度为5
clf=tree.DecisionTreeClassifier(max_depth=5)
# 拟合
clf.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(clf.score(X_train, y_train)) )
print("testing score: {:.3f}".format(clf.score(X_test, y_test)) )
print()


############
# 导入随机森林分类器
from sklearn.ensemble import RandomForestClassifier
# 设定随机森林有6棵树
rfc=RandomForestClassifier(n_estimators=7, random_state=3)
# 拟合
rfc.fit(X_train, y_train)

# 输出打分
print("training score: {:.3f}".format(rfc.score(X_train, y_train)) )
print("testing score: {:.3f}".format(rfc.score(X_test, y_test)) )

输出：
training score: 0.803
testing score: 0.796

training score: 0.928
testing score: 0.784

决策树打分0.80，随机森林打分 0.78. 凑合能用吧。做出一个预测，有 80% 的正确率。





========================================
chapter 7: SVM 支持向量机: 专治线性不可分
----------------------------------------

概要
- SVM 的基本原理和构造
- SVM 的核函数 (kernel trick)
	- 多项式内核 Polynomial kernel: 把原始特征进行乘方映射到高维空间;
	- 径向基内核 Radial basic function kernel, RBF /高斯内核 Gaussian kernel
- SVM 的参数调节
- 实例 - 波士顿房价回归分析



边界位置的向量，对确定边界有决定作用，这些变量称为支持向量(Suport vectors)




========================================
|-- 最大边界间隔超平面 Maximum Margin Separating Hyperplane
----------------------------------------
1. 该平面和所有支持向量的距离都是最大的。
import numpy as np
import matplotlib.pyplot as plt

# 造数据，50个点，2类
from sklearn.datasets import make_blobs
X,y= make_blobs(n_samples=50, centers=2, random_state=6)

# 导入支持向量机
from sklearn import svm
clf=svm.SVC(kernel="linear", C=1000)
clf.fit(X, y)
print("score:", clf.score(X, y))

# 可视化
plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)
# 建立图像坐标
ax=plt.gca()
xlim=ax.get_xlim()
ylim=ax.get_ylim()

# 生成2个等差数列
xx=np.linspace(xlim[0], xlim[1], 30)
yy=np.linspace(ylim[0], ylim[1], 30)
YY, XX=np.meshgrid(yy, xx)
xy=np.vstack([XX.ravel(), YY.ravel()]).T #ravel() 拉直，vstack 按列叠放，成为2行，再转置成2列。
Z=clf.decision_function(xy).reshape(XX.shape)

# 把分类的决定边界画出来
ax.contour(XX, YY, Z, colors="k", levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])

ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:,1], s=100, linewidth=2, facecolors="none") #没有颜色，相当于没画。

plt.show()


打分是 
score: 1.0




2.内核换成 RBF内核

clf=svm.SVC(kernel="rbf", C=1000)
clf.fit(X, y)
print("score:", clf.score(X, y))


# 画数据点
plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)
# 建立图像坐标
ax=plt.gca()
xlim=ax.get_xlim()
ylim=ax.get_ylim()

# 生成2个等差数列
xx=np.linspace(xlim[0], xlim[1], 30)
yy=np.linspace(ylim[0], ylim[1], 30)
YY, XX=np.meshgrid(yy, xx)
xy=np.vstack([XX.ravel(), YY.ravel()]).T #ravel() 拉直，vstack 按列叠放，成为2行，再转置成2列。
Z=clf.decision_function(xy).reshape(XX.shape)

# 把分类的决定边界画出来
ax.contour(XX, YY, Z, colors="k", levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])

ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, linewidth=1, facecolors="none") #没有颜色，相当于没画。
plt.title("SVM: kernel='rbf'")
plt.show()

# 分类器是一条曲线，2个支持向量边界也是曲线。
# 计算距离的公式变了，Krbf(x1, x2)=exp( gamma||x1-x2||^2 )
# 其中||x1-x2||代表2点之间的欧几里得距离，gamma是控制RBF内核宽度的参数。




========================================
|-- SVM 核函数与参数选择，及SVM的优缺点
----------------------------------------

1. 不同内核的SVM算法的不同表现

import numpy as np
import matplotlib.pyplot as plt

# 导入支持向量机
from sklearn import svm

# 定义一个画图函数
def make_meshgrid(x, y, h=0.02):
    x_min, x_max = x.min()-1, x.max()+1
    y_min, y_max = y.min()-1, y.max()+1
    xx, yy=np.meshgrid(np.arange(x_min, x_max, h),
                      np.arange(y_min, y_max, h))
    return xx, yy

# 定义一个绘制等高线的函数
def plot_contours(ax, clf, xx, yy, **params):
    Z=clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z=Z.reshape(xx.shape)
    out=ax.contourf(xx, yy, Z, **params)
    return out

# 导入 wine 数据集
from sklearn.datasets import load_wine
wine=load_wine()
# 只选取数据集的前2个特征,为了图形方便展示
X=wine.data[:, :2]
y=wine.target

# 使用 SVM 模型进行拟合
C=1.0 #设定正则化参数
models=[svm.SVC(kernel='linear', C=C),
        svm.LinearSVC(C=C, max_iter=8000), # 这里报warn , 加 max_iter=8000 (默认 1000，7k依旧报错)
        # ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
        svm.SVC(kernel='rbf', gamma=0.7, C=C),
        svm.SVC(kernel='poly', degree=4, C=C)]
[ clf.fit(X, y) for clf in models ]
# 输出打分
scores= [ clf.score(X, y) for clf in models ]
print(scores)

# 设定图的标题
titles=("SVC with linear kernel",
       "LinearSVC (linear kernel)",
       "SVC with RBF kernel(g=0.7)",
       "SVC with polynomial kernel(d=4)")
# 设定子图的个数和排列方式
fig, sub = plt.subplots(2, 2)
plt.subplots_adjust(wspace=0.4, hspace=0.4)
# 使用前面定义的函数进行画图
X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)

for clf, title, ax in zip(models, titles, sub.flatten()):
    plot_contours(ax, clf, xx, yy, cmap=plt.cm.plasma, alpha=0.8)
    ax.scatter(X0, X1, c=y, cmap=plt.cm.plasma, s=20, edgecolors='k')
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xlabel("Feature 0")
    ax.set_ylabel("Feature 1")
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(title)
plt.show()

输出打分 [0.7808988764044944, 0.7640449438202247, 0.8370786516853933, 0.8202247191011236]

结论：
- 线性内核的 SVC 与 LinearSVC 的结果非常近似，但仍有一点点差异。
	* 因为  linearSVC 对L2范数进行最小化，而线性内核的SVC是对L1范数进行最小化。
- 线性内核边界都是线/超平面，而RBF内核的SVC和polynomial内核的SVC分类器边界是非线性的，更加弹性。
	* polynomial 内核的SVC中，起决定作用的是乘方参数degree(对原始数据集的特征进行乘3次方操作)和正则参数C。
	* 而在 RBF 内核的SVC中，器具定作用的是正则化参数C和参数gamma。





2. SVM RBF内核的 gamma 参数
探讨 gamma 值对RBF内核的SVC分类器的影响。

C=1.0
models=[svm.SVC(kernel="rbf", gamma=0.1, C=C),
       svm.SVC(kernel="rbf", gamma=1, C=C),
       svm.SVC(kernel="rbf", gamma=10, C=C),]
[clf.fit(X, y) for clf in models]

# 输出打分
scores= [ clf.score(X, y) for clf in models ]
print(scores)

# 设定标题
titles=("gamma=0.1", "gamma=1", "gamma=10")
# 设置子图个数和排列
fig, sub=plt.subplots(1, 3, figsize=(10, 3))
X0, X1=X[:,0], X[:,1]
xx, yy = make_meshgrid(X0, X1)

# 画图
for clf, title, ax in zip(models, titles, sub.flatten()):
    plot_contours(ax, clf, xx, yy, cmap=plt.cm.plasma, alpha=0.8)
    ax.scatter(X0, X1, c=y, cmap=plt.cm.plasma, s=20, edgecolors='k')
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xlabel("Feature 0")
    ax.set_ylabel("Feature 1")
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(title)
plt.show()

输出打分 [0.8314606741573034, 0.8426966292134831, 0.8932584269662921]

结论：
- gamma 值从0.1增大到到10，
	* gamma越小，则RBF内核的直径越大，更多的点被模型圈定进决定边界中，边界越圆滑，模型越简单。
	* 随着参数的增大，模型倾向于把每个点都放到相应的决定边界中，模型复杂度提高。
- 所以 gamma 值越小，模型越倾向于欠拟合；而gamma值越大，模型越倾向于过拟合。
- 正则化参数C，
	* 越小，模型就越受限，也就是单个数据点对模型影响越小，模型就简单。
	* 越大，每个数据点对模型影响越大，模型就更复杂。




3. RBF内核的 gamma 与打分曲线

from sklearn.model_selection import train_test_split
# 将数据集拆分
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)
print("data size: ", X_train.shape, X_test.shape)

scores=[]
for gm in np.linspace(0.01, 25, 200):
    clf=svm.SVC(kernel="rbf", gamma=gm, C=1)
    clf.fit(X_train, y_train)
    scores.append( [gm, clf.score(X_train, y_train), clf.score(X_test, y_test)] )
scores=np.array(scores)
# print(scores)

# 画图
plt.plot(scores[:,0], scores[:,1], label="trainning score")
plt.plot(scores[:,0], scores[:,2], label="testing score")
plt.xlabel("gamma")
plt.ylabel("Score")
plt.legend()
plt.show()

# 本数据看，gamma=5就可以了，之后就一直过拟合。



4. RBF内核的 C 与打分曲线

from sklearn.model_selection import train_test_split
# 将数据集拆分
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)
print("data size: ", X_train.shape, X_test.shape)

scores=[]
for c2 in np.linspace(0.01, 1, 100):
    clf=svm.SVC(kernel="rbf", gamma=5, C=c2)
    clf.fit(X_train, y_train)
    scores.append( [c2, clf.score(X_train, y_train), clf.score(X_test, y_test)] )
scores=np.array(scores)
# print(scores)

# 画图
plt.plot(scores[:,0], scores[:,1], label="trainning score")
plt.plot(scores[:,0], scores[:,2], label="testing score")
plt.xlabel("C")
plt.ylabel("Score")
plt.legend()
plt.show()

# 本数据看，C=0.5 就可以了，之后就效果不大了。



5. SVM 算法的优缺点

SVM 是很强大的，能在特征很少的情况下生成非常复杂的决定边界，当然特征很多的时候表现也不错。
- 对于1万行以内的数据，SVM应对高维数据集和低位数据集都还算得心应手。
- 但是如果数据集过于大，比如超过10万，SVM就会非常耗时、耗内存。

SVM 还要一个短板，就是对于数据预处理和参数调节要求非常高。
- 为了避免数据预处理和调参，大家倾向于使用 随机森林算法 或 梯度上升决策树(GBDT)算法。
- 对非专业人士不好解释。

SVM游刃有余的场景：
	数据集中样本特征的测度都比较接近，例如图像识别领域；
	还有样本特征数和样本数比较接近时


重要的参数有3个
- 核函数的选择
- 核函数参数的选择，例如 RBF的gamma值
- 正则化参数C。

RBF内核的 gamma 值用来调节内核宽度的， gamma值和C值一起控制模型的复杂度，越大越复杂，越小越简单。
实际使用时， gamma值和C值往往一起调节，才能达到最好的效果。





========================================
|-- SVM 实例 - 波士顿房价预测
----------------------------------------

1. 导入数据 
from sklearn.datasets import load_boston
boston=load_boston()
X,y=load_boston().data, load_boston().target


print(boston.keys())
# print(boston.DESCR) #506 行，13列，第14列是 MEDV  业主自住房屋价格的中位数，千美元为单位。
print(boston.data.shape)
boston.target[1:5]

输出 
dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])
(506, 13)
未来警告 略;
array([21.6, 34.7, 33.4, 36.2])



# 拆分数据
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8)

print("data size:", X_train.shape, X_test.shape)

输出 data size: (379, 13) (127, 13)




2. 内核选择与数据预处理
# 尝试哪个内核效果更好 Linear ,rbf

# 导入支持向量机回归模型
from sklearn.svm import SVR
# 分别测试2个内核
for kernel in ["linear", "rbf"]:
    svr=SVR(kernel=kernel)
    svr.fit(X_train, y_train)
    print("{} score: {:0.3f} {:0.3f}".format(kernel, svr.score(X_train, y_train), svr.score(X_test, y_test)))

# 结果都不好。 rbf 更差。
# 原因可能是特征之间差异过大，没有预处理
输出 
linear score: 0.709 0.696
rbf score: 0.192 0.222




import matplotlib.pyplot as plt

# 可视化每个特征的数量级
plt.plot(X.min(axis=0), 'v', label="min")
plt.plot(X.max(axis=0), '^', label="max")

# y坐标对数形式
plt.yscale("log") #试试去掉这一行，看原始数值
# 图例
plt.legend(loc="best")

plt.xlabel("features")
plt.ylabel("feature magnitude")

#plt.ylim(1e-3, 1e3)
plt.show()

# 确实，每一列的范围差异很大，第一列 -2 到2，而第4列都在0附近。
# 第二列为什么没最小值




# 特征的标准化

# 导入数据预处理工具
from sklearn.preprocessing import StandardScaler
# 对训练集和测试集进行数据预处理
scaler=StandardScaler()
scaler.fit(X_train)
X_train_scaled=scaler.transform(X_train)
X_test_scaled=scaler.transform(X_test)

# 目测最值都在 [-4, 10] 之间
print( X_train_scaled.min(axis=0) )
print( X_train_scaled.max(axis=0) )

# 可视化最值
# 将预处理后的数据特征最大值和最小值用散点图画出来
plt.plot(X_train_scaled.min(axis=0), 'v', label="train set min")
plt.plot(X_train_scaled.max(axis=0), '^', label="train set max")

plt.plot(X_test_scaled.min(axis=0), 'v', label="test set min")
plt.plot(X_test_scaled.max(axis=0), '^', label="test set max")
plt.yscale("log")

plt.legend(loc="best")

plt.xlabel("scaled features")
plt.ylabel("scaled feature magnitude")

#plt.ylim(-10, 10)
plt.show()




3. 预处理后的数据训练模型
for kernel in ["linear", "rbf"]:
    svr=SVR(kernel=kernel)
    svr.fit(X_train_scaled, y_train)
    print("{} score: {:0.3f} {:0.3f}".format(kernel, svr.score(X_train_scaled, y_train), svr.score(X_test_scaled, y_test)))

# linear 打分基本没变化。rbf打分提升很多。

输出 
linear score: 0.706 0.698
rbf score: 0.665 0.695



4. 调节 rbf 内核的其他参数

svr=SVR(kernel='rbf', gamma=0.1, C=100)
svr.fit(X_train_scaled, y_train)
print("rbf score: {:0.3f} {:0.3f}".format( svr.score(X_train_scaled, y_train), svr.score(X_test_scaled, y_test)))
# 这个打分很高了，算是可以接受

rbf score: 0.966 0.894




========================================
chapter 8: 神经网络 neural networks: 深度学习
----------------------------------------

概要
- 神经网络的前世今生(几个低谷)
- NN的原理和非线性矫正
- NN的模型参数调节
- 使用神经网络训练手写数字识别模型




1. 历史
Minsky 1969 年写书 Perceptron，导致 AI winter;
Hinton 等人提出反向传播算法(Back propagation, BP)，解决了2层神经网络所需要的复杂计算问题;
20世纪90年代 SVM出现，不用调参、效率高，称为主流算法，神经网络再一次冰河期。
Hinton给多层神经网络起了个新的名字 - 深度学习。
后面的事情大家都知道了。



2. 神经网络不是 sklearn 包的强项，只有一点点支持: 
	- 多层神经网络 (mutilayer Perceptron, MLP)
最强大的神经网络的还是要用 tensorflow 和 pytorch。






========================================
|-- 神经网络的原理与使用
----------------------------------------
1. 激活函数 - 非线性矫正

主要是 
relu(rectified linear unit)
tanh(rangenshyperbolicus)


# 可视化曲线
import numpy as np
import matplotlib.pyplot as plt 

# 生成一个等差数列
x=np.linspace(-5, 5, 200)

# 画出非线性矫正图形
plt.plot(x, np.tanh(x), label="tanh")
plt.plot(x, np.maximum(x, 0), label="relu")

plt.legend(loc="best")
#plt.xlabel("x")
#plt.ylabel("Y")
plt.show()






2. 神经网络的参数设置
- 每层(包括隐藏层)的节点数，一般设置10个，多的可以增加到1万个；
- 增加隐藏层的层数；这就是深度学习中的 deep 的由来。
- 指定激活函数 activation 一共有4种，"identity", "logistic", "tanh", "relu"(默认);
	* identity 是不做处理 f(x)=x
	* logistic 的激活函数 f(x)=1/[1+exp(-x)]，类似 tanh，但是值域是0到1之间；
	* tanh 的激活函数 f(x)=   , 值域是 -1 到1 之间；
	* relu 的激活函数 f(x)=max(x, 0)

- alpha 值和线性模型的 alpha 值一样，是一个L2惩罚项，用来控制正则化的程度，默认0.0001
- 隐藏层 hidden_layer_sizes 默认[100]，只有一个隐藏层，该层有100个节点；
	* 如果定义为 [10, 10]，那就意味着模型有2个隐藏层，每层有10个节点。



# 导入多层感知机 MPL 神经网络
from sklearn.neural_network import MLPClassifier
# 导入wine数据集
from sklearn.datasets import load_wine
wine=load_wine()
X=wine.data[:, :2]
y=wine.target

# 拆分数据
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# 拟合
mlp=MLPClassifier(solver="lbfgs", max_iter=300, random_state=7)
mlp.fit(X_train, y_train)

# 打分
print("training set:{:0.3f}".format( mlp.score(X_train, y_train)) )
print("tesing set:{:0.3f}".format( mlp.score(X_test, y_test)) )

输出 
training set:0.827
tesing set:0.822




# 可视化
import matplotlib.pyplot as plt 
from matplotlib.colors import ListedColormap

# 使用不同色块表示不同分类
cmap_light=ListedColormap(["#FFAAAA", "#AAFFAA", "#AAAAFF"])
cmap_bold=ListedColormap(["FF0000", "00FF00", "0000FF"])
x_min, x_max = X_train[:, 0].min()-1, X_train[:, 0].max()+1
y_min, y_max = X_train[:, 1].min()-1, X_train[:, 1].max()+1
xx, yy=np.meshgrid(np.arange(x_min, x_max, 0.02),
                  np.arange(y_min, y_max, 0.02))
Z=mlp.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 散点图
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor="k", s=60)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("MLPClassifier:solver=lbfgs")
plt.show()





(1).  减少隐藏层node数
# 拟合
mlp_20=MLPClassifier(solver="lbfgs", hidden_layer_sizes =[10], max_iter=800, random_state=7)
mlp_20.fit(X_train, y_train)

# 打分
print("training set:{:0.3f}".format( mlp_20.score(X_train, y_train)) )
print("tesing set:{:0.3f}".format( mlp_20.score(X_test, y_test)) )


# 画图
Z=mlp_20.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 散点图
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor="k", s=60)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("MLPClassifier:nodes=10")
plt.show()

输出
training set:0.782
tesing set:0.822

- 决定边界丢失了很多细节。每一个隐藏层中，node数大概决定边界的最大直线数，这个值越大，边界越平滑。
- 还有2个方法可以让边界平滑
    * 增加隐藏层数量
    * 激活函数改为 tanh


(2) 隐藏层增加到2层
# 拟合
mlp_2L=MLPClassifier(solver="lbfgs", hidden_layer_sizes =[10, 10], max_iter=800, random_state=8)
mlp_2L.fit(X_train, y_train)

# 打分
print("training set:{:0.3f}".format( mlp_2L.score(X_train, y_train)) )
print("tesing set:{:0.3f}".format( mlp_2L.score(X_test, y_test)) )


# 画图
Z=mlp_2L.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 散点图
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor="k", s=60)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("MLPClassifier:nodes=[10, 10]")
plt.show()

# 边界有更多细节了




(3) 激活函数 activation='tanh'

# 拟合
mlp_2L=MLPClassifier(solver="lbfgs", hidden_layer_sizes =[10, 10], activation='tanh',
                     max_iter=200, random_state=7)
mlp_2L.fit(X_train, y_train)

# 打分
print("training set:{:0.3f}".format( mlp_2L.score(X_train, y_train)) )
print("tesing set:{:0.3f}".format( mlp_2L.score(X_test, y_test)) )


# 画图
Z=mlp_2L.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 散点图
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor="k", s=60)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("MLPClassifier:nodes=[10, 10] activation='tanh'")
plt.show()

# 边界有更多细节了




(4) 提高 alpha 值(默认 0.0001)
# 拟合
mlp_alpha1=MLPClassifier(solver="lbfgs", hidden_layer_sizes =[10, 10], activation='tanh', alpha=1,
                     max_iter=200, random_state=7)
mlp_alpha1.fit(X_train, y_train)

# 打分
print("training set:{:0.3f}".format( mlp_alpha1.score(X_train, y_train)) )
print("tesing set:{:0.3f}".format( mlp_alpha1.score(X_test, y_test)) )


# 画图
Z=mlp_alpha1.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')

# 散点图
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor="k", s=60)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("MLPClassifier:nodes=[10, 10], activation='tanh',alpha=1")
plt.show()

# 边界有更多细节了




小结: 有四种提高神经网络复杂度的方法
	增加每层节点数
	增加隐藏层层数
	调整激活函数
	调整alpha值改变模型的正则化程度









========================================
|-- 实例 - 手写数字识别
----------------------------------------
MNIST 数据集手写体数字识别，就像入门程序猿写 Hello world 一样，是非常基础的必修课。

1. 下载数据

(1) 内置的是 8*8 ，测试识别我的手写体时，效果很不好。也即是泛化很差。
# https://h1ros.github.io/posts/loading-scikit-learns-mnist-dataset/
from sklearn.datasets import load_digits
mnist = load_digits()

print(mnist.keys())
print(mnist.data.shape, mnist.target.shape)
# data是1797行，每行64像素(8*8) 是一个图形每个像素的黑白值。
# target 是0-9的整型数字。


import pandas as pd
print(pd.DataFrame(mnist.data).head())
pd.DataFrame(mnist.target).head()

# 可视化图形
import matplotlib.pyplot as plt
plt.imshow(mnist.images[0]);

# 批量 可视化图形
fig, axes = plt.subplots(2, 10, figsize=(10, 2.8))
for i in range(20):
    axes[i//10, i %10].imshow(mnist.images[i], cmap='gray');
    axes[i//10, i %10].axis('off')
    axes[i//10, i %10].set_title(f"target: {mnist.target[i]}")
    
plt.tight_layout()



(2) 使用外置原始的 28*28 手写体数据
# https://scikit-learn.org/stable/modules/preprocessing.html

# 下载数据
from sklearn.datasets import fetch_openml
X, y = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)
print(X.shape, y.shape) #(70000, 784) (70000,)


# 可视化
import numpy as np
import matplotlib.pyplot as plt
plt.imshow(X[0].reshape([28, 28]));


# 批量 可视化图形
fig, axes = plt.subplots(2, 10, figsize=(10, 2.8))
for i in range(20):
    axes[i//10, i %10].imshow(X[i].reshape([28, 28]), cmap='gray');
    axes[i//10, i %10].axis('off')
    axes[i//10, i %10].set_title(f"target: {y[i]}")
    
plt.tight_layout()




2. 预处理 
# 归一化到 0-1
X=X/255

# 拆分数据
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, test_size=1000, random_state=62)
print(X_train.shape, X_test.shape)


3. 训练 
from sklearn.neural_network import MLPClassifier
mlp_hw=MLPClassifier(solver="lbfgs", hidden_layer_sizes=[100, 100], max_iter=1000,
                    activation='relu', alpha=1e-5, random_state=62)

mlp_hw.fit(X_train, y_train)

# 打分
print("training set:{:0.3f}".format( mlp_hw.score(X_train, y_train)) )
print("tesing set:{:0.3f}".format( mlp_hw.score(X_test, y_test)) )


4. 测试

import numpy as np

# 新建一个 28*28 画布，黑色字体，手写一个数字，保存。
from PIL import Image

def getNum(path="data/28_4.png"):
    fig=plt.figure(figsize=(2, 1))
    image=Image.open(path).convert("F")
    # 调整图像大小
    image=image.resize( (28,28) )
    arr=[]
    for i in range(28):
        for j in range(28):
            pixel=1.0 - float(image.getpixel((j,i)))/255
            arr.append(pixel)
    arr1=np.array(arr).reshape(1, -1)
    plt.imshow( np.array(arr).reshape(28,28), cmap='gray' );
    return mlp_hw.predict(arr1)[0]

getNum() #正确

getNum("data/28_6.png") #错

结论: 还很很傻，人眼能是别的，该网络不一定能识别



推荐使用 keras + TensorFlow | theano。因为他们支持 GPU 加速。
训练神经网络的建议:
	隐藏层的节点数约等于训练集的特征数，但是一般不超过500.
	开始时，可以让模型尽可能复杂，然后再对正则化参数 alpha 调节来提高模型的表现。






========================================
chapter 9: 数据预处理、降维、特征提取及聚类
----------------------------------------

概要:
- 几种常见的数据与处理工具
- PCA 主成分分析用于数据降维
- PCA 主成分分析和NMF非负矩阵分解用于特征提取
- 几种常见的聚类算法



========================================
|-- 数据预处理
----------------------------------------

# 生成数据
import numpy as np
import matplotlib.pyplot as plt

# 生成数据
from sklearn.datasets import make_blobs
# 40个点，2个中心，标准差2
X, y=make_blobs(n_samples=40, centers=2, random_state=50, cluster_std=2)
print(X.shape) #数据是40行，2个特征(x和y)

# 散点图
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.cool)
plt.show()

# 2个特征的范围
print("x: [{:0.2f}, {:0.2f}]".format( X[:,0].min(), X[:,0].max()) )
print("y: [{:0.2f}, {:0.2f}]".format( X[:,1].min(), X[:,1].max()) )




1. StandardScaler: Z 标准化

help(StandardScaler)
- Standardize features by removing the mean and scaling to unit variance.
- The standard score of a sample `x` is calculated as:    z = (x - u) / s
    * where `u` is the mean of the training samples or zero if `with_mean=False`,
    * and `s` is the standard deviation of the training samples or one if `with_std=False`.



from sklearn.preprocessing import StandardScaler
# 预处理
X_1=StandardScaler().fit_transform(X)

# 散点图
plt.scatter(X_1[:,0], X_1[:,1], c=y, cmap=plt.cm.cool)
plt.title("StandardScaler")
plt.show()

# 2个特征的范围
print("x: [{:0.2f}, {:0.2f}]".format( X_1[:,0].min(), X_1[:,0].max()) )
print("y: [{:0.2f}, {:0.2f}]".format( X_1[:,1].min(), X_1[:,1].max()) )

# 可见点之间的相对位置不变，但是x和y的极值都缩小到0附近了
# Z标注化后的数据符合 标准正态分布 N(0,1)



2. MinMaxScaler: 极值标准化[0, 1]
from sklearn.preprocessing import MinMaxScaler
# 预处理
X_2=MinMaxScaler().fit_transform(X)

# 散点图
plt.scatter(X_2[:,0], X_2[:,1], c=y, cmap=plt.cm.cool)
plt.title("MinMaxScaler")
plt.show()

# 2个特征的范围
print("x: [{:0.2f}, {:0.2f}]".format( X_2[:,0].min(), X_2[:,0].max()) )
print("y: [{:0.2f}, {:0.2f}]".format( X_2[:,1].min(), X_2[:,1].max()) )

# x = (x - min)/(max - min) 都在[0, 1]之间



3. RobustScaler: (x-median)/IQR
help(RobustScaler)
- Scale features using statistics that are robust to outliers.
- This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range).
    * The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).


from sklearn.preprocessing import RobustScaler
# 预处理
X_3=RobustScaler().fit_transform(X)

# 散点图
plt.scatter(X_3[:,0], X_3[:,1], c=y, cmap=plt.cm.cool)
plt.title("RobustScaler")
plt.show()

# 2个特征的范围
print("x: [{:0.2f}, {:0.2f}]".format( X_3[:,0].min(), X_3[:,0].max()) )
print("y: [{:0.2f}, {:0.2f}]".format( X_3[:,1].min(), X_3[:,1].max()) )

# 使用中位数和四分位数进行转换，而不是使用均值和方差。
# 能去掉异常值 outlier。



4. Normalizer: 面目全非的转换
将所有样本的特征向量转化为欧几里得距离为1.
- 也就是说数据的分布变为一个直径为1的圆，或者球。
- Normalizer 通常在我们只想保留数据特征向量的方向，忽略其数值的时候使用。


from sklearn.preprocessing import Normalizer
# 预处理
X_4=Normalizer().fit_transform(X)

# 散点图
plt.scatter(X_4[:,0], X_4[:,1], c=y, cmap=plt.cm.cool)
plt.title("Normalizer")
plt.show()

# 2个特征的范围
print("x: [{:0.2f}, {:0.2f}]".format( X_4[:,0].min(), X_4[:,0].max()) )
print("y: [{:0.2f}, {:0.2f}]".format( X_4[:,1].min(), X_4[:,1].max()) )





5. sklearn 中其他的预处理方法
用到了再查文档。

import sklearn.preprocessing
dir(sklearn.preprocessing)


输出: 
['Binarizer', 			*
 'FunctionTransformer',
 'KBinsDiscretizer',
 'KernelCenterer',
 'LabelBinarizer',
 'LabelEncoder',
 'MaxAbsScaler',  		*
 'MinMaxScaler',
 'MultiLabelBinarizer',
 'Normalizer',
 'OneHotEncoder',
 'OrdinalEncoder',
 'PolynomialFeatures',
 'PowerTransformer',
 'QuantileTransformer',		*
 'RobustScaler',
 'SplineTransformer',
 'StandardScaler',
...
'binarize',
 'label_binarize',
 'maxabs_scale',
 'minmax_scale',
 'normalize',
 'power_transform',
 'quantile_transform',
 'robust_scale',
 'scale']



6. 通过预处理提高模型的准确率
from sklearn.datasets import load_wine
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split

wine=load_wine()
X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, random_state=62)

print(X_train.shape, X_test.shape) #(133, 13) (45, 13)


# 训练神经网络
mlp=MLPClassifier(hidden_layer_sizes=[100,100], max_iter=700, random_state=62)
mlp.fit(X_train, y_train)

# 打分
print("training set:{:0.3f}".format( mlp.score(X_train, y_train)) )
print("tesing set:{:0.3f}".format( mlp.score(X_test, y_test)) )

输出: 
training set:0.992
tesing set:0.933




# 数据预处理
scaler=MinMaxScaler()
scaler.fit(X_train)
X_train_scaled=scaler.transform(X_train)
X_test_scaled=scaler.transform(X_test)

# 训练
mlp.fit(X_train_scaled, y_train)

# 打分
print("training set:{:0.3f}".format( mlp.score(X_train_scaled, y_train)) )
print("tesing set:{:0.3f}".format( mlp.score(X_test_scaled, y_test)) )
# 惊呆了！ 完全正确。

输出 
training set:1.000
tesing set:1.000


警告：
# 记住数据预处理的顺序：先拟合原始train数据，再转换原始的train和test数据。
# 而不能 拟合原始test数据，再去转换test数据，这样就失去了数据转换的意义了。




========================================
|-- PCA 降维
----------------------------------------

1. 降维后可视化
# 载入数据
from sklearn.datasets import load_wine
wine=load_wine()

# 载入预处理
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X=wine.data
y=wine.target
X_scaled = scaler.fit_transform(X)
print(X_scaled.shape) # 13个特征


# 为了可视化最显著的特征方向，我们需要做PCA
from sklearn.decomposition import PCA
# 设置主成分数量为2
pca=PCA(n_components=2)
pca.fit(X_scaled) #拟合
X_pca=pca.transform(X_scaled) #转换
print(X_pca.shape) # 2个主成分(特征组合)

# 可视化
import numpy as np
import matplotlib.pyplot as plt

# 将三个分类中的主成分提取出来
X0=X_pca[wine.target==0]
X1=X_pca[wine.target==1]
X2=X_pca[wine.target==2]

# 绘制散点图
plt.scatter(X0[:,0], X0[:,1], c='b', s=60, edgecolor='k')
plt.scatter(X1[:,0], X1[:,1], c='g', s=60, edgecolor='k')
plt.scatter(X2[:,0], X2[:,1], c='r', s=60, edgecolor='k')

plt.legend(wine.target_names, loc="best")
plt.xlabel("PC_1")
plt.ylabel("PC_2")
plt.show()

# 将数据降到2维，从图片中差不多能看到3类的分界线。




2. 原始特征与PCA主成分之间的关系
从数学上讲，需要先理解内积和投影。本文略。

本文只直观绘图。

print(pca.components_.shape)

pca.components_
# 一个主成分，就等于原始特征前面乘以这个系数。
# 系数使正，就是原始特征和PC正相关；为负，就是和PC负相关。

输出: 
(2, 13)
array([[ 0.1443294 , -0.24518758, -0.00205106, -0.23932041,  0.14199204,
         0.39466085,  0.4229343 , -0.2985331 ,  0.31342949, -0.0886167 ,
         0.29671456,  0.37616741,  0.28675223],
       [-0.48365155, -0.22493093, -0.31606881,  0.0105905 , -0.299634  ,
        -0.06503951,  0.00335981, -0.02877949, -0.03930172, -0.52999567,
         0.27923515,  0.16449619, -0.36490283]])



# 使用主成分绘制热图
# plt.figure(figsize=(2,1))
plt.matshow(pca.components_, cmap="plasma")

# y轴作为主成分数
plt.yticks([0,1], ["PC_1", "PC_2"])
plt.colorbar()
# x轴为原始特征数量
plt.xticks( range(len(wine.feature_names)), wine.feature_names, rotation=60, ha="left" )
plt.show()




# 每个主成分解释的变异性百分比
pca.explained_variance_ratio_ # array([0.36198848, 0.1920749 ])



注: help(PCA)
PCA分析中，除了设置主成分个数 n_components 外，还可以设置 n_components=0.9 为要保留的原始特征的90%。





========================================
|-- 特征提取
----------------------------------------
1. 如果特征太多，可以对原始特征进行转换，生成新的“特征”或者说成分，会比直接使用原始特征效果更好。

数据表达“data representation”。

在图像处理领域，如果每个像素点都是为一个特征，则特征成千上万，这时需要“特征提取”(feature extraction)。



2. 使用 LFW(Labeled Faces in the Wild) 人脸识别数据集。
http://vis-www.cs.umass.edu/lfw/

(1) 载入 LFW 人脸识别数据集
# 导入数据集获取工具
from sklearn.datasets import fetch_lfw_people
# 载入人脸数据集
faces=fetch_lfw_people(min_faces_per_person=20, resize=0.8)
image_shape = faces.images[0].shape
image_shape #(100, 75)


# 显示照片
import numpy as np
import matplotlib.pyplot as plt

fig, axes = plt.subplots(3, 4, figsize=(12,9), 
                         subplot_kw={"xticks":(), "yticks":()})
for target, image, ax in zip(faces.target, faces.images, axes.ravel()):
    ax.imshow(image, cmap=plt.cm.gray)
    ax.set_title(faces.target_names[target])
plt.show()



print(faces.target.min(), faces.target.max() ) #共62个人


(2) 预测人脸
# 对数据拆分
import time
start=time.time()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(faces.data/255, faces.target, random_state=0)

# 导入神经网络
from sklearn.neural_network import MLPClassifier
mlp=MLPClassifier(hidden_layer_sizes=[100,100], random_state=0, max_iter=400)
mlp.fit(X_train, y_train)

print("time:{:0.2f}".format(time.time()-start, "s") )

# 打分
print("training set:{:0.3f}".format( mlp.score(X_train, y_train)) )
print("tesing set:{:0.3f}".format( mlp.score(X_test, y_test)) )

# 耗时: 60s
# 测试集记住了一半多一点。能在一分钟内记住 36/62 个人脸，也算很厉害了。
输出 
time:60.08
training set:0.986
tesing set:0.583




(3)白化 data whiten
数据白化能提升模型正确率。
所谓白化，就是降低冗余性，消除相邻像素的相关性，且使所有特征具有相同的方差。

from sklearn.decomposition import PCA

# 使用 PCA 的白化功能处理人脸
pca=PCA(whiten=True, n_components=0.9, random_state=0)
pca.fit(X_train)
X_train_whiten=pca.transform(X_train)
X_test_whiten = pca.transform(X_test)
# (2267, 105) (756, 105)

# 维度
print( X_train.shape, X_test.shape ) #7500 列
print( X_train_whiten.shape, X_test_whiten.shape ) #105 列
输出
(2267, 7500) (756, 7500)
(2267, 105) (756, 105)


# 导入神经网络
from sklearn.neural_network import MLPClassifier
mlp=MLPClassifier(hidden_layer_sizes=[100,100], random_state=0, max_iter=400)
mlp.fit(X_train_whiten, y_train)

# 打分
print("training set:{:0.3f}".format( mlp.score(X_train_whiten, y_train)) )
print("tesing set:{:0.3f}".format( mlp.score(X_test_whiten, y_test)) )
# 训练集满分，测试集打分比白化前又降了。
# 真是玄学。
输出 
training set:1.000
tesing set:0.571



(4) NMF 非负矩阵分解用于特征提取
矩阵分解，就是把一个矩阵写成n个矩阵的连乘。
- 非负矩阵分解，就是原始矩阵中所有的数值必须大于或等于0，当然分解后的矩阵中的数据也是大于或等于0的。
- 直观解释：一堆特征值混乱无序的堆放在空间中，NMF可以看成是从原点(0,0)引出一个(或几个)vector，
	* 用这些vector尽可能的把原始特征值的信息表达出来。

与PCA的区别：
- 如果我们降低NMF的成分数量，它会重新生成新的成分，而新的成分与原来的成分是完全不同的。
- NMF中的成分是没有顺序的，这点和PCA也有所不同。


# 使用 NMF 对LFW人脸数据集进行特征提取，再重新训练神经网络，看看模型的正确率是否变化
import time
start=time.time()

# 导入 NMF
from sklearn.decomposition import NMF
# 使用NMF处理数据: 很慢 15:42->15:
nmf = NMF(n_components=105, max_iter=400, random_state=62).fit(X_train) #默认 max_iter=200
X_train_nmf = nmf.transform(X_train)
X_test_nmf=nmf.transform(X_test)
print("time:{:0.2f}".format(time.time()-start, "s") )

# 打印NMF处理后的数据形状
print("shpe before NMF: ", X_train.shape)
print("shpe after NMF: ", X_train_nmf.shape)

输出 
time:80.86
shpe before NMF:  (2267, 7500)
shpe after NMF:  (2267, 105)



# 使用 NMF 分解后的矩阵 训练神经网络
mlp.fit(X_train_nmf, y_train)

# 打分
print("training set:{:0.3f}".format( mlp.score(X_train_nmf, y_train)) )
print("tesing set:{:0.3f}".format( mlp.score(X_test_nmf, y_test)) )
# 打分更低了！时间比PCA长，效果比PCA差。

输出： 
training set:0.991
tesing set:0.558








========================================
|-- 聚类: k-means, 凝聚聚类, DBSCAN 
----------------------------------------

1. k-means 聚类

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
blobs=make_blobs(random_state=1, centers=1)
X_blobs=blobs[0]

# 可视化
plt.scatter(X_blobs[:,0], X_blobs[:,1], c='r', edgecolor='k')
plt.show()
# 目测确实无差别



# 导入 KMeans 工具
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, random_state=7)
kmeans.fit(X_blobs)

# 可视化
x_min, x_max=X_blobs[:,0].min()-0.5, X_blobs[:,0].max()+0.5
y_min, y_max=X_blobs[:,1].min()-0.5, X_blobs[:,1].max()+0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
Z=kmeans.predict(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)

plt.figure(1)
plt.clf()
plt.imshow(Z, interpolation="nearest",
          extent=(xx.min(), xx.max(), yy.min(), yy.max()),
          cmap=plt.cm.summer,
          aspect='auto', origin='lower')
plt.plot(X_blobs[:,0], X_blobs[:,1], 'r.', markersize=5)

# 用蓝色叉号代表聚类中心
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:,0], centroids[:,1], 
           marker='x', s=150, linewidths=3, color='b', zorder=10)
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks()
plt.yticks()
plt.show()



# 打印 KMeans 进行聚类的标签
print("K均值的聚类标签:\n{}".format(kmeans.labels_))

# k-means 的优点是简单；
# 缺点是它认为每个点到聚类中心的方向都是同等重要的，对于“形状”复杂的数据集，k均值就表现很差。





2. 凝聚聚类算法
简单说，就是最近的聚类，然后剩下的最近的再聚类，一直到最后成为一类。

from scipy.cluster.hierarchy import dendrogram, ward

# 使用连线的方式进行可视化
linkage=ward(X_blobs)
dendrogram(linkage)
ax=plt.gca() #这个有啥用？

plt.xlabel("Sample")
plt.ylabel("Cluster distance")
plt.show()

# 感觉就是R中的层次聚类。
# 也不能处理“形状”复杂的数据集。




3. DBSCAN 算法
全称是“基于密度的有噪声应用空间聚类”(Density-based spatial clustering of applications with noise)

通过对特征空间内的密度进行检测，密度大的地方认为是一类，密度小的地方认为是一个分界线。
所以一开始不需要指定 聚类数量 n_clusters.

实际使用时，先使用 MinMaxScaler 或 StandardScaler 进行预处理，DBSCAN算法的表现会更好。



from sklearn.cluster import DBSCAN
db=DBSCAN()

# 拟合
clusters=db.fit_predict(X_blobs)

#可视化
plt.scatter(X_blobs[:,0], X_blobs[:,1], c=clusters, cmap=plt.cm.cool, s=60, edgecolor='k')

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.show()




# 打印聚类个数
print("聚类标签:\n{}".format(clusters))

# 为什么会有-1？原来 DBSCAN 认为-1是噪声。
# 中间一团密度比较大，归为一类；周围浅色的不属于任何一类，放入噪声类。


(1) DBSCAN 重要的参数
DBSCAN 重要的参数:
- eps: 指定划入同一类的样本距离又多远，eps越大，聚类覆盖的数据点越多；默认0.5；
- min_samples: 某个数据点周围，被看成是聚类核心点的个数；越大则核心数据点越少，噪声越多；越小噪声越小；默认2.


help(DBSCAN)
eps : float, default=0.5
The maximum distance between two samples for one to be considered as in the neighborhood of the other.
This is not a maximum bound on the distances of points within a cluster.
This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.


# 调整 eps 参数 0.5->2
from sklearn.cluster import DBSCAN
db_1=DBSCAN(eps=2)
# 拟合
clusters_1=db_1.fit_predict(X_blobs)

#可视化
plt.scatter(X_blobs[:,0], X_blobs[:,1], c=clusters_1, cmap=plt.cm.cool, s=60, edgecolor='k')

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.title("DBSCAN(eps=2)")
plt.show()
# 只有一类，没有噪音了



# 调整 min_samples 参数 2->20
from sklearn.cluster import DBSCAN
db_2=DBSCAN(min_samples=20)
# 拟合
clusters_2=db_2.fit_predict(X_blobs)

#可视化
plt.scatter(X_blobs[:,0], X_blobs[:,1], c=clusters_2, cmap=plt.cm.cool, s=60, edgecolor='k')

plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.title("DBSCAN(min_samples=20)")
plt.show()
# min_samples 调大，噪音点变多。








========================================
chapter 10: 数据表达与特征工程: OneHotEncoder
----------------------------------------

概要：
- 使用哑变量对分类特征进行转化
- 对数据进行分箱操作
- 几种常用的数据“升维”方法
- 常用的自动特征选择方法





========================================
|-- 数据表达
----------------------------------------
1. 使用哑变量转化类型特征
# 导入 pandas
import pandas as pd
fruits = pd.DataFrame({"value":[5,6,7,8,9],
                      "type":["waterMelon", "banana", "orange", "apple", "grape"]})
display(fruits)


# 转化数据表中的字符串为数值
fruits_dum=pd.get_dummies(fruits)
fruits_dum
# 数值列并没有变化，分类列变成了0-1矩阵


# 令程序将数值也看做字符串
fruits["value"] = fruits["value"].astype(str) #先将数值转为字符串，这一句可选，但是推荐加上。
# 用 get_dummies 转化为字符串
pd.get_dummies(fruits, columns=["value"])





2.对数据进行装箱处理

# 产生数据
import numpy as np
import matplotlib.pyplot as plt
rnd=np.random.RandomState(38)
x=rnd.uniform(-5, 5, size=50)

# 向数据集添加噪音
y_no_noise = np.cos(6*x) +x
X = x.reshape(-1, 1)
y = (y_no_noise + rnd.normal(size=len(x)))/2

plt.plot(X, y, 'o', c='r')
plt.show()




# 分别使用 MLP 算法和 KNN 算法 对这个数据集进行回归分析

from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor

# 生成一个等差数列
line=np.linspace(-5, 5, 1000, endpoint=False).reshape(-1, 1)
mlpr=MLPRegressor().fit(X, y)
knr=KNeighborsRegressor().fit(X, y)

plt.plot(line, mlpr.predict(line), label="MLP")
plt.plot(line, knr.predict(line), label="KNN")
plt.plot(X, y, 'o', c='r')
plt.legend(loc="best")
plt.show()

# 肉眼可见，MLP更平滑，而KNN覆盖更多的点。
# 该采用哪一个呢？





# 对数据进行一下“装箱处理”(binning)，也称为“离散化处理”(discretization)

# 设置箱子个数11
bins=np.linspace(-5, 5, 11)
#将数据进行装箱处理
target_bin=np.digitize(X, bins=bins)
print("装箱范围:\n",bins)

print("前10个数据点的特征值:\n", X[:10])
print("前10个数据点的箱子:\n", target_bin[:10])





(2) OneHotEncoder
# sklearn 的 OneHotEncoder 和 pandas 的 get_dummies 功能基本一致，但是 OneHotEncoder 目前只能用于整型数值的类型变量。

from sklearn.preprocessing import OneHotEncoder
onehot = OneHotEncoder(sparse=False)
onehot.fit(target_bin)
X_in_bin=onehot.transform(target_bin)

print("装箱前的数据形态:\n", target_bin.shape ) #50行1列
print("装箱后的数据形态:\n", X_in_bin.shape ) #50行10列
print("装箱后的前10个数据点:\n", X_in_bin[:10])




# 分别使用 MLP 算法和 KNN 算法 对这个新数据集进行回归分析

# 使用独热编码进行数据表达
line2=onehot.transform(np.digitize(line, bins=bins))

mlpr2=MLPRegressor(max_iter=500).fit(X_in_bin, y)
knr2=KNeighborsRegressor().fit(X_in_bin, y)

plt.plot(line, mlpr2.predict(line2), label="MLP")
plt.plot(line, knr2.predict(line2), label="KNN")
plt.plot(X, y, 'o', c='r')
plt.legend(loc="best")
plt.title("OneHotEncode")
plt.show()

# 在x>0部分，2个拟合几乎完全重合。
# 对比 OneHotEncode 编码前，MLP回归模型变得更复杂；而KNN模型变得更简单。



装箱的好处：可以纠正模型过拟合或者欠拟合的问题。
- 尤其是对大规模高纬度的数据集使用线性模型的时候，装箱处理可以大幅提高线性模型的预测准确度。
- 装箱对于决策树算法没有太多作用，因为这类算法本身就是不停地拆分样本的特征数据，所以不需要再使用装箱操作。 








========================================
|-- 数据“升维”
----------------------------------------
交叉式特征（Interaction Features）和多项式特征(Polynomial Features)

(1) 交叉式特征（Interaction Features）
# 交叉式特征（Interaction Features） 就是在原始特征中添加交互项。

# 测试 np.hstack() 函数
import numpy as np
arr1=[1,2,3,4]
arr2=[10,30,40,50]
arr3=np.hstack( [arr1, arr2] )
arr3 #array([ 1,  2,  3,  4, 10, 30, 40, 50])



# 把原始数据和装箱后的数据堆叠
# 产生数据
import numpy as np
import matplotlib.pyplot as plt
rnd=np.random.RandomState(38)
x=rnd.uniform(-5, 5, size=50)

# 向数据集添加噪音
y_no_noise = np.cos(6*x) +x
X = x.reshape(-1, 1)
y = (y_no_noise + rnd.normal(size=len(x)))/2
print("X\n",X[:3])

# 对数据进行一下“装箱处理”(binning)，也称为“离散化处理”(discretization)
# 设置箱子个数11
bins=np.linspace(-5, 5, 11)
# 将数据进行装箱处理
target_bin = np.digitize(X, bins=bins)
print("\ntarget_bin\n",target_bin[:3])

# OneHotEncoder 只能输入整型
from sklearn.preprocessing import OneHotEncoder
onehot = OneHotEncoder(sparse=False)
onehot.fit(target_bin)
X_in_bin=onehot.transform(target_bin)
print("\nX_in_bin\n", X_in_bin[:3])

# 按列拼接
X_stack = np.hstack([X, X_in_bin])
print(X.shape, X_stack.shape) #(50, 1) (50, 11) 由1列，变为11列。后10列是编码后的。

输出:
X
 [[-1.1522688 ]
 [ 3.59707847]
 [ 4.44199636]]

target_bin
 [[ 4]
 [ 9]
 [10]]

X_in_bin
 [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
(50, 1) (50, 11)




# 分别使用 MLP 算法和 KNN 算法 对这个数据集进行回归分析
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor

# 生成一个等差数列
line=np.linspace(-5, 5, 1000, endpoint=False).reshape(-1, 1)
# 使用独热编码进行数据表达
line2=onehot.transform( np.digitize(line, bins=bins) )

#将数据进行堆叠
line_stack = np.hstack( [line, line2] )

# 训练数据
mlpr3=MLPRegressor().fit(X_stack, y)
knr3=KNeighborsRegressor().fit(X_stack, y)

# 绘图
plt.plot(line, mlpr3.predict(line_stack), linewidth=3, label="MLP")
#plt.plot(line, knr3.predict(line_stack), label="KNN")
plt.ylim(-4, 4)
for vline in bins:
    plt.plot([vline, vline], [-5, 5], ":", c='k')

plt.legend(loc="best")
plt.plot(X, y, 'o', c='r')

plt.title("After adding interaction")
plt.show()

# 每个数据所在的箱体中，MLP增加了斜率。
# 模型复杂度提高了。




(1.2) 交叉相
# 每个箱体中的斜率基本一致了，但是这不是我们需要的。我们希望每个箱体都有各自的截距和斜率。
X_multi = np.hstack([X_in_bin, X*X_in_bin])
print(X.shape, X_in_bin.shape) #(50, 1) (50, 10)

print(X_multi.shape) #(50, 20)
print(X_multi[0])
输出
(50, 1) (50, 10)
(50, 20)
[ 0.         0.         0.         1.         0.         0.
  0.         0.         0.         0.        -0.        -0.
 -0.        -1.1522688 -0.        -0.        -0.        -0.
 -0.        -0.       ]




# 每个箱子内都有自己的斜率
# 训练数据
mlpr4=MLPRegressor().fit(X_multi, y)
knr4=KNeighborsRegressor().fit(X_multi, y)

#将数据进行堆叠
line_multi = np.hstack( [line2, line * line2] )

# 绘图
plt.plot(line, mlpr4.predict(line_multi), linewidth=3, label="MLP")
#plt.plot(line, knr3.predict(line_stack), label="KNN")
plt.ylim(-4, 4)
for vline in bins:
    plt.plot([vline, vline], [-5, 5], ":", c='grey')

plt.plot(X, y, 'o', c='r')

plt.legend(loc="lower right")
plt.title("Adding multiply")
plt.show()

# 线性模型在低维度表现不好，在高纬度表现良好。所以可以升维后使用glm模型。




(2) 多项式特征(Polynomial Features)
多项式: y = ax^4 + bx^3 + cx^2 + dx + e

# 导入多项式特征工具
from sklearn.preprocessing import PolynomialFeatures

# 向数据集添加多项式特征
poly=PolynomialFeatures(degree=20, include_bias=False)
X_poly = poly.fit_transform(X)

print(X.shape, X_poly.shape) #degree=20 就是变成20列
输出: (50, 1) (50, 20)




# 检验一下各项怎么来的, 第一个是1次方，第二个是2次方，类推。
print(X[0], np.power(X[0],2), np.power(X[0],[3, 4,5,6] ) )
print(X_poly[0])

print("\nfeature_names:\n", poly.get_feature_names_out())

输出: 
[-1.1522688] [1.3277234] [-1.52989425  1.76284942 -2.0312764   2.34057643]
[ -1.1522688    1.3277234   -1.52989425   1.76284942  -2.0312764
   2.34057643  -2.6969732    3.10763809  -3.58083443   4.1260838
  -4.75435765   5.47829801  -6.3124719    7.27366446  -8.38121665
   9.65741449 -11.12793745  12.82237519 -14.77482293  17.02456756]

feature_names:
 ['x0' 'x0^2' 'x0^3' 'x0^4' 'x0^5' 'x0^6' 'x0^7' 'x0^8' 'x0^9' 'x0^10'
 'x0^11' 'x0^12' 'x0^13' 'x0^14' 'x0^15' 'x0^16' 'x0^17' 'x0^18' 'x0^19'
 'x0^20']





# 尝试线性回归
from sklearn.linear_model import LinearRegression
lr_poly = LinearRegression().fit(X_poly, y)

line_poly = poly.transform(line)

# 绘制图形
plt.plot(line, lr_poly.predict(line_poly), label="Linear Regressor")
plt.xlim(np.min(X)-0.5, np.max(X)+0.5)
plt.ylim(np.min(y)-0.5, np.max(y)+0.5)
plt.plot(X, y, 'o', c='r')
plt.legend(loc="lower right")
plt.show()

# 这个线性拟合可不是直线！低维数据集，线性拟合通常欠拟合，但是进行多项式扩展后，可以一定程度解决欠拟合问题。

# 除了将特征转为多项式的方法之外，我们还可以用类似正弦函数 sin(), 对数函数 log(), 或指数函数 exp() 等来进行相似的操作。




========================================
|-- 自动特征选择
----------------------------------------
1  单一变量法进行特征选择
很多场景只关心最重要的指标，忽略其他不那么重要的指标：
- 玩具厂，根据年龄划分产品
- 金融公司，根据收入流水确定目标人群的偿债能力

sklearn 中的特征选择方法：
- SelectPercentile: 按照百分比选择特征
- SelectKBest: 选择最重要的K个指标




# 导入某天中国A股全部股票交易信息 
# 上交所: http://www.sse.com.cn/market/price/report/
# 深交所: http://www.szse.cn/market/trend/index.html


# (1) 获取原始json数据
import time
timestamp=str(round(time.time()*1000))
begin=str(0)
end=str(2060) #1515

# http://www.sse.com.cn/market/price/report/
url="http://yunhq.sse.com.cn:32041/v1/sh1/list/exchange/equity?callback=jQuery111208015895779126387_1560941576071&select=date%2Ccode%2Cname%2Copen%2Chigh%2Clow%2Clast%2Cprev_close%2Cchg_rate%2Cvolume%2Camount%2Ctradephase%2Cchange%2Camp_rate%2Ccpxxsubtype%2Ccpxxprodusta&order=&begin="+begin+"&end="+end+"&_="+timestamp;
print("url=", url)

headers = {
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36',
  'Referer': 'http://www.sse.com.cn/market/price/report/'
}

import requests
r = requests.get(url, headers=headers) #, auth=('user', 'pass')
#rs1=r.status_code #200
#rs2=r.headers['content-type'] #'text/html; charset=utf-8'
#rs3=r.encoding #'utf-8' 编码，修改编码
rs4=r.text
#r.json() #只有r.headers['content-type']为json时才能用。否则报错。
#print(rs1,rs2,rs3,"\n")
#rs4 #现在是字符串格式

# (2) 解析json格式为python数组
import json,re
#json
rs=re.sub(r"jQuery111208015895779126387_1560941576071\(", "", rs4)
rs=re.sub(r"\)$", "", rs)
rs=eval(rs)
print( len(rs['list']) )  #25

# (3) 注释每一列的字段名
# select: code,name,open,high,low,last,prev_close,chg_rate,volume,amount,tradephase,change,amp_rate
print('date code,name,open,high,low,last,prev_close,chg_rate,volume,amount,tradephase,change,amp_rate cpxxsubtype cpxxprodusta')
titles='date,code,name,open,high,low,last,prev_close,chg_rate,volume,amount,tradephase,change,amp_rate,cpxxsubtype,cpxxprodusta'.split(",")

#data
alists=rs['list']
#len(alist) #1515
print(alists[0])

alist=alists[0]
for i in range(len(titles)):
    print(i, titles[i], "=", alist[i])

import numpy as np
alists2=np.array(alists)
print(alists2.shape) #(2050, 13)

import pandas as pd
stock=pd.DataFrame(alists2, columns=titles)
stock.head()

# 涨跌幅 chg_rate(%)
# 振幅 amp_rate


print(stock["cpxxsubtype"].unique())
print(stock["cpxxprodusta"].unique())

tmp=stock["chg_rate"].astype(float)
print(tmp.min(), "\t", tmp.max())

tmp=stock["amp_rate"].astype(float)
print(tmp.min(), "\t", tmp.max())



获取数据: 特征和目标

# target 涨幅列
y=stock['chg_rate'].astype(float)
print(y.shape) #2050,)
y[0]

# 特征
features = stock.loc[:, 'open,high,low,last,prev_close,volume,amount,change,amp_rate'.split(",")].astype(float)
X=features.values

print(X.shape) #(2050, 9)
X[:1]




# 由于各个列差异较大，需要标准化

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=62)

# 数据预处理
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled=scaler.transform(X_train)
X_test_scaled=scaler.transform(X_test)

# 设置神经网络层数和alpha
from sklearn.neural_network import MLPRegressor
mlpr=MLPRegressor(random_state=62, hidden_layer_sizes=[100,100], alpha=0.001, max_iter=500)
mlpr.fit(X_train_scaled, y_train)

# 打分
print("training set:{:0.3f}".format( mlpr.score(X_train_scaled, y_train)) )
print("tesing set:{:0.3f}".format( mlpr.score(X_test_scaled, y_test)) )
# 打分挺高了
输出 
training set:0.992
tesing set:0.982



# 列举涨跌幅>=10%
wanted=stock.loc[:, ("date","code","name", "last", "prev_close", "chg_rate")]
print("涨停榜\n", wanted[y>=9].sort_values(by=['chg_rate', "last"], ascending=[False,True] ))

print("\n跌停榜\n", wanted[y<= -9].sort_values(by=['chg_rate', "last"], ascending=[False,True] ))




(1.3) 使用 SelectPercentile 进行特征选择
from sklearn.feature_selection import SelectPercentile
#设置特征选择参数
select = SelectPercentile(percentile=50)
select.fit(X_train_scaled, y_train)
X_train_selected = select.transform(X_train_scaled)

# 输出维度
print(X_train_scaled.shape, X_train_selected.shape) #9列变为4列

# 查看哪些被保留了
mask=select.get_support()
print(mask)
输出
(1537, 9) (1537, 4)
[False False False False False  True  True  True  True]


# 使用图像表示特征选择的结果
import matplotlib.pyplot as plt
plt.matshow(mask.reshape(1,-1), cmap=plt.cm.cool)
plt.xlabel("Featuers Selected")
plt.show() # 红色是保留的。




# 再次使用选择的特征，训练神经网络
X_test_selected=select.transform(X_test_scaled)

# 设置神经网络层数和alpha
from sklearn.neural_network import MLPRegressor
mlpr_sp=MLPRegressor(random_state=62, hidden_layer_sizes=[100,100], alpha=0.001, max_iter=500)
mlpr_sp.fit(X_train_selected, y_train)

# 打分
print("training set:{:0.3f}".format( mlpr_sp.score(X_train_selected, y_train)) )
print("tesing set:{:0.3f}".format( mlpr_sp.score(X_test_selected, y_test)) )
输出 
training set:0.974
tesing set:0.943


# 打分降低了。
# 说明我们的数据不包括噪音，去掉的都是有用的信息。

# 单一变量法进行特征筛选，不依赖于具体建模的算法。






2. 基于模型的特征选择
步骤：先使用一个监督学习模型，判断数据特征的重要性，保留最重要的特征。
	- 这一步使用的模型和最终使用的不一定一样。



# 导入基于模型选择特征的工具
from sklearn.feature_selection import SelectFromModel

# 导入随机森林模型
from sklearn.ensemble import RandomForestRegressor

# 设置模型 n_estimators 参数
sfm=SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=38), threshold="median")

# 使用模型拟合数据
sfm.fit(X_train_scaled, y_train)
X_train_sfm =sfm.transform(X_train_scaled)

# 打印形状
print(X_train_scaled.shape, X_train_sfm.shape) #9列变5列

# 查看哪些被保留了
mask_sfm=sfm.get_support()
print(mask_sfm)
输出
(1537, 9) (1537, 5)
[ True False False False  True  True False  True  True]




# 使用图像表示特征选择的结果
import matplotlib.pyplot as plt
plt.matshow(mask_sfm.reshape(1,-1), cmap=plt.cm.cool)
plt.xlabel("Featuers Selected")
plt.show()
# 红色是保留的。随机森林法 和 单一变量法，选择的特征不同。




# 再次使用选择的特征，训练神经网络
X_test_sfm=sfm.transform(X_test_scaled)

# 设置神经网络层数和alpha
from sklearn.neural_network import MLPRegressor
mlpr_sfm=MLPRegressor(random_state=62, hidden_layer_sizes=[100,100], alpha=0.001, max_iter=500)
mlpr_sfm.fit(X_train_sfm, y_train)

# 打分
print("training set:{:0.3f}".format( mlpr_sfm.score(X_train_sfm, y_train)) )
print("tesing set:{:0.3f}".format( mlpr_sfm.score(X_test_sfm, y_test)) )
输出
training set:0.989
tesing set:0.976

# 打分比原始的降低了。但是比单一变量法略高。
# 说明我们的数据不包括噪音，去掉的都是有用的信息。 






3. 迭代式特征选择
递归特征剔除法(Recursive Feature Elimination, RFE): https://machinelearningmastery.com/rfe-feature-selection-in-python/
- 使用某个模型对特征进行筛选，之后建立2个模型，去掉某个特征前后的模型，


from sklearn.feature_selection import RFE
rfe=RFE(RandomForestRegressor(n_estimators=100, random_state=38), n_features_to_select=5)

rfe.fit(X_train_scaled, y_train)
mask=rfe.get_support()
mask #array([ True, False,  True, False,  True, False, False,  True,  True])




# 使用图像表示特征选择的结果
import matplotlib.pyplot as plt
plt.matshow(mask.reshape(1,-1), cmap=plt.cm.cool)
plt.xlabel("Featuers Selected")
plt.show()





# 再次使用选择的特征，训练神经网络
X_train_rfe=rfe.transform(X_train_scaled)
X_test_rfe=rfe.transform(X_test_scaled)

# 设置神经网络层数和alpha
from sklearn.neural_network import MLPRegressor
mlpr_rfe=MLPRegressor(random_state=62, hidden_layer_sizes=[100,100], alpha=0.001, max_iter=500)
mlpr_rfe.fit(X_train_rfe, y_train)

# 打分
print("training set:{:0.3f}".format( mlpr_rfe.score(X_train_rfe, y_train)) )
print("tesing set:{:0.3f}".format( mlpr_rfe.score(X_test_rfe, y_test)) )
输出
training set:0.992
tesing set:0.986

# 打分比原始的略高，最好的结果了。








========================================
chapter 11: 模型评估与优化
----------------------------------------

不同的数据集，不同的模型表现如何？
- 使用交叉验证对模型进行评估
- 使用网络搜索寻找模型的最优参数
- 对分类模型的可信度进行评估





========================================
|-- 使用交叉验证进行模型评估
----------------------------------------
统计学中，交叉验证用于评估模型的泛化能力。

除了之前的 train_test_split 函数外，还有更自动化的拆分测试工具。就是交叉验证法(Cross Validation)。

K折叠交叉验证法(k-fold cross validation): 
- 把数据随机拆分成k份，对k-1份训练，使用1份测试。
- 循环k次，则每份都做过测试集。
- 对外输出5次测试集的打分平均数。

1. 随机拆分交叉验证法 shuffle-split cross validation
# 导入红酒数据集
from sklearn.datasets import load_wine
# 导入交叉验证工具
from sklearn.model_selection import cross_val_score
# 导入用于分类的支持向量机模型
from sklearn.svm import SVC

# 载入数据
wine = load_wine()
# 设置 SVC 的核函数为 linear
svc = SVC(kernel='linear')
# 使用交叉验证法对SVC进行评分
scores=cross_val_score(svc, wine.data, wine.target, cv=5) #默认分成5份

print(scores, "\nMean score:{:0.3f}".format(scores.mean()) )
输出
[0.88888889 0.94444444 0.97222222 1.         1.        ] 
Mean score:0.961


# sklearn对分类模型，默认使用的是 分层k交叉验证法。
print(wine.target) #可见酒分了3类。
# 所谓的分层，就是如果目标是80%男，20%女，则拆分数据后每一份中男女也是这个比例。

输出 
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]








2.随机拆分法(shuffle-split corss-validation)¶

# 原理：先从数据中随机抽取一部分作为训练集，再从其余部分随机抽取一部分作为测试集，进行评分后再迭代，
#    直到达到我们希望的迭代次数。

from sklearn.model_selection import ShuffleSplit
# 设置拆分为10份
shuffle_split = ShuffleSplit(train_size=0.7, test_size=0.2, n_splits=10)
# 交叉验证
scores=cross_val_score(svc, wine.data, wine.target, cv=shuffle_split)
print(scores, "\nMean score:{:0.3f}".format(scores.mean()) )
输出 

[0.91666667 0.97222222 0.94444444 0.97222222 0.97222222 0.97222222
 0.91666667 0.91666667 0.97222222 1.        ] 
Mean score:0.956





3. 挨个儿试试法( leave-one-out )
# 原理：每次取一个作为测试集，其他作为训练集。缺点是耗时，特别是对大数据集。

from sklearn.model_selection import LeaveOneOut
loo = LeaveOneOut()
# 交叉验证
scores=cross_val_score(svc, wine.data, wine.target, cv=loo)
print(scores, "\nMean score:{:0.3f}".format(scores.mean()) )
输出 
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.
 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] 
Mean score:0.955







========================================
|-- 使用网络搜索优化模型参数
----------------------------------------
除了逐个尝试看打分外，还可以使用网络搜索法一次性找到更优的参数设置。



1. 简单网络搜索
# 目的: 尝试2个参数的组合 max_iter=100,1000,5000,10000, alpha=10,1,0.1,0.01;

from sklearn.datasets import load_wine
wine = load_wine()

# 导入套索回归模型
from sklearn.linear_model import Lasso
# 导入数据集拆分工具
from sklearn.model_selection import train_test_split
# 将数据集拆分为训练集与测试集
X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, random_state=38)

# 设置初试分数0
best_score=0
# 设置 alpha 参数4个遍历
for alpha in [0.01, 0.1, 1, 10]:
    #设置最大迭代次数, 4个遍历
    for max_iter in [100, 1000, 5000, 10000]:
        lasso=Lasso(alpha=alpha, max_iter=max_iter)
        lasso.fit(X_train, y_train)
        score=lasso.score(X_test, y_test)
        # print(alpha, max_iter, score)
        # 最高分数
        if score > best_score:
            best_score=score
            best_parameters={"alpha":alpha, "max_iter":max_iter}
# 输出
print("best_score:{:0.3f}".format(best_score))
print("best_parameters:{}".format(best_parameters))

输出
best_score:0.889
best_parameters:{'alpha': 0.01, 'max_iter': 100}




# 上述做法是有缺陷的，因为16次评分使用的是同一个训练集和测试集。不能反映出新数据及的情况。

# 比如换一个拆分方法，打分也会变 random_state=0
X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, random_state=0)

best_score=0
for alpha in [0.01, 0.1, 1, 10]:
    for max_iter in [100, 1000, 5000, 10000]:
        lasso=Lasso(alpha=alpha, max_iter=max_iter)
        lasso.fit(X_train, y_train)
        score=lasso.score(X_test, y_test)
        if score > best_score:
            best_score=score
            best_parameters={"alpha":alpha, "max_iter":max_iter}
# 输出
print("best_score:{:0.3f}".format(best_score))
print("best_parameters:{}".format(best_parameters))

# 改变拆分方法后，最佳参数变为 alpha=0.1, 最佳打分也下降了
输出
best_score:0.830
best_parameters:{'alpha': 0.1, 'max_iter': 100}







2. 与交叉验证结合的网络搜索
import numpy as np
from sklearn.model_selection import cross_val_score

best_score=0
for alpha in [0.01, 0.1, 1, 10]:
    for max_iter in [100, 1000, 5000, 10000]:
        lasso=Lasso(alpha=alpha, max_iter=max_iter)
        scores = cross_val_score(lasso, X_train, y_train, cv=6)
        score=np.mean(scores)
        if score > best_score:
            best_score=score
            best_parameters={"alpha":alpha, "max_iter":max_iter}
# 输出
print("best_score:{:0.3f}".format(best_score))
print("best_parameters:{}".format(best_parameters))
输出 
best_score:0.865
best_parameters:{'alpha': 0.01, 'max_iter': 100}




# 再试试验证集上，使用最优参数
lasso=Lasso(alpha=0.01, max_iter=100).fit(X_train, y_train)
print("test score:{:0.3f}".format(lasso.score(X_test, y_test))) 
# test score:0.819
# 这个打分有点低。因为lasso会舍弃特征，而我们的特征本来就不多，说明舍弃的列不那么冗余。





# 使用内置的 GridSearchCV 简化上述循环

#导入网络搜索工具
from sklearn.model_selection import GridSearchCV
params = {'alpha':[0.01, 0.1, 1, 10], 'max_iter':[100,1000,5000,10000]}
# 定义模型和参数
grid_search = GridSearchCV(lasso, params, cv=6)
grid_search.fit(X_train, y_train)

# 打分
print("test score:{:0.3f}".format(grid_search.score(X_test, y_test)))
print("best params:", grid_search.best_params_)

# 最佳条件和上面的for循环一样。
# 注意: grid_search.best_score_ 中存储的是交叉验证的最高分，而不是在测试集上的得分。
print("\nbest_score_:{:0.3f}".format(grid_search.best_score_))

输出 
test score:0.819
best params: {'alpha': 0.01, 'max_iter': 100}

best_score_:0.865






========================================
|-- 分类模型的可信度评估
----------------------------------------
1. 分类模型中的预测准确率
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

X, y= make_blobs(n_samples=200, random_state=1, centers=2, cluster_std=5)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.cool, edgecolor='k')
plt.show()
# 假设深红色 1，浅色0



# 导入高斯贝叶斯模型
from sklearn.naive_bayes import GaussianNB
# 数据集拆分
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=68)
# 训练高斯贝叶斯模型
gnb=GaussianNB()
gnb.fit(X_train, y_train)

predict_prob = gnb.predict_proba(X_test)
print(predict_prob.shape) #(50, 2)
predict_prob[:3]
# 第一行是归为2个分类的概率 98.8% vs 1.2%
输出 
(50, 2)
array([[0.98849996, 0.01150004],
       [0.0495985 , 0.9504015 ],
       [0.01648034, 0.98351966]])






# 可视化分类过程中的表现

x_min, x_max= X[:,0].min()-0.5, X[:,0].max()+0.5
y_min, y_max= X[:,1].min()-0.5, X[:,1].max()+0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                    np.arange(y_min, y_max, 0.02))
Z=gnb.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]
Z=Z.reshape(xx.shape)

#绘制等高线
plt.contourf(xx, yy, Z, cmap=plt.cm.summer, alpha=0.8)
plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.cool, edgecolor='k')
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.cool, edgecolor='k', alpha=0.6)
# 设置坐标轴范围 
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
# 设置横纵轴的单位
plt.xticks(())
plt.yticks(())

plt.title("GaussianNB predict_proba")
plt.show()
# 背景颜色代表概率值。半透明的点是测试集。

# 并不是每个分类算法都有 predict_proba 属性。
#  不过我们还可以使用另一种方法检查分类的可信度，就是决定系数 decision_function.







2. 分类模型中的决定系数
决定系数只返回一个值，正数代表属于分类1，负数代表属于分类2.
由于高斯朴素贝叶斯没有 decision_function，我们换成 支持向量机SVC 算法来进行建模。


from sklearn.svm import SVC
svc=SVC().fit(X_train, y_train)
dec_func= svc.decision_function(X_test)
# 打印决定系数的前5个
print(dec_func[:5])
# 负数一类，正数一类。 [-1.36071347  1.53694862  1.78825594 -0.96133081  1.81826853]





# 可视化工作原理
Z=svc.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z=Z.reshape(xx.shape)
# 绘制等高线
plt.contourf(xx, yy, Z, cmap=plt.cm.summer, alpha=0.8)

# 绘制散点图
plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap=plt.cm.cool, edgecolor='k')
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=plt.cm.cool, edgecolor='k', alpha=0.6)

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())

plt.title("SVC decision_function")
# 设置横纵轴的单位
plt.xticks(())
plt.yticks(())
plt.show()

# 和上图类似，有一个斜对角线分界线。
#  但是深背景色的核心位置不同。处于区域中心的是高度可信的，而边缘区域和交界区域的，则是“模棱两可”区域。


本例汇总使用的是二元分类任务，但是 predict_proba 和 decision_function 同样适用于多元分类任务。
- 感兴趣的读者可以调整 make_blobs 的 centers 参数进行试验。





3. 其它模型评价指标
之前一直使用的 .score(),
- 对于分类模型，给出的是模型分类的准确率(accuracy)；
- 对于回归模型，给出的是回归分析中的R^2分数，翻译为 可决系数，或 拟合优度。（决定系数）
    * R^2 的计算方法是用 “回归平方和”(explain sum of squares, ESS) 除以“总变差”(total sum of squares, TSS)

R^2 = 1 - 求和(y-y_hat)^2/求和(y-y_bar)^2



其它指标
- 精度 Precision
- 召回率 Recall
- f1分数 f1-score
- ROC(Receiver Operating Characteristic Curve)
- AUC(Area Under Curve)

这些指标经常和 网格搜索算法 配合使用，选择合适的模型和参数。
- 如果使用 GridSearchCV 类，需要改变评分方法，只需要修改 scoring 参数即可。
- 随机森林，修改评分为 roc_auc:
    * grid=GridSearchCV(RandomForestClassifier(), param_grid=param_grid, scoring='roc_auc')






========================================
chapter 12: 建立算法的管道模型
----------------------------------------



概要
- 管道模型的基本概念和使用
- 使用管道模型进行模型选择
- 使用管道模型进行参数调优
- 实例：使用管道模型对股票涨跌进行回归分析


========================================
|-- 管道模型的概念与用法
----------------------------------------
1. 基本概念

# 常规步骤：数据预处理，交叉验证模型评估模型，使用网格搜索找到最优参数。

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier

# 生成数据，200个样本，分类2， 标准差为5
X,y = make_blobs(n_samples=200, centers=2, cluster_std=5)
# 拆分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=38)
# 预处理
scaler = StandardScaler().fit(X_train)
X_train_scaled=scaler.transform(X_train)
X_test_scaled=scaler.transform(X_test)

print("shape of the datasets: ", X_train_scaled.shape, X_test_scaled.shape) #(150, 2) (50, 2)
# shape of the datasets:  (150, 2) (50, 2)




# 神经网络是典型的需要数据预处理的算法模型。

# 原始的训练集
plt.scatter(X_train[:,0], X_train[:,1])
# 经过预处理的数据集
plt.scatter(X_train_scaled[:,0], X_train_scaled[:,1], marker='^', edgecolors='k')

plt.title("training set & scaled training set")
plt.show()
# 处理后的数据更加“聚拢”。





# 使用神经网络进行拟合，使用网格搜索确定最优参数
from sklearn.model_selection import GridSearchCV
# 设定参数组合 5*5=25个组合
params={"hidden_layer_sizes":[(50,), (100,), (100,100), (50,100), (100,50)],
       "alpha":[0.0001, 0.001,0.01, 0.1, 1]}
grid=GridSearchCV(MLPClassifier(max_iter=3000, random_state=38),
                 param_grid=params, cv=3)
# 拟合
grid.fit(X_train_scaled, y_train) #耗时 60s

# 打分
print("best score:{:0.3f}".format(grid.best_score_))
print("best params_:{}".format(grid.best_params_))
print("\ntest score:{}".format(grid.score(X_test_scaled, y_test)) )

# 这个过程其实是错误的。
#   因为我们对 X_train做的标准化fit，
#   而GridSearchCV时传入的是X_train_scaled，对该数据有切分为 train 和 validation 2部分，内部按照 validation 最高分输出的参数组合。
#   而外部，我们使用该参数组合 MLP fit 是相对于X_train_scaled，对 X_test_scaled 做 prediction 打分。
#   内部 scaler fit 应该对 内部的train，而不能是对数据总体。

输出:
best score:0.747
best params_:{'alpha': 1, 'hidden_layer_sizes': (100,)}

test score:0.76






2. 管道模型
# 每次划分都要手动预处理，需要做参数组合数次，太麻烦。

# Pipeline 能起到形式简化的作用
from sklearn.pipeline import Pipeline

# 在流水线上安装2个设备，一个数据预处理的 StandardScaler， 一个最大迭代次数1600的MLP多层感知神经网络。
pipeline = Pipeline([('scaler', StandardScaler()), 
                     ('mlp', MLPClassifier(max_iter=1600, random_state=38))])
# 使用管道模型对训练集进行拟合
pipeline.fit(X_train, y_train)
# 对测试集打分
print("test score:{:0.3f}".format(pipeline.score(X_test, y_test))) #0.880
# test score:0.760





3. 使用管道模型进行网格搜索
# 注意：参数加上管道中工具的前缀，中间使用双下划线__连接。
params={"mlp__hidden_layer_sizes":[(50,), (100,), (100,100), (50,100), (100,50)], #
       "mlp__alpha":[0.0001, 0.001,0.01, 0.1]}
grid=GridSearchCV(pipeline, param_grid=params, cv=3)
# 拟合
grid.fit(X_train, y_train) #耗时 60s

# 打分
print("best score:{:0.3f}".format(grid.best_score_))
print("best params_:{}".format(grid.best_params_))
print("\ntest score:{}".format(grid.score(X_test, y_test)) )
输出:
best score:0.733
best params_:{'mlp__alpha': 0.0001, 'mlp__hidden_layer_sizes': (100,)}

test score:0.76




# 检查步骤
pipeline.steps
输出: 
[('scaler', StandardScaler()),
 ('mlp', MLPClassifier(max_iter=1600, random_state=38))]







========================================
|-- 使用管道模型对股票涨幅进行回归分析
----------------------------------------
1. 下载数据
# (1) 获取原始json数据
import time
timestamp=str(round(time.time()*1000))
begin=str(0)
end=str(2060) #1515

# http://www.sse.com.cn/market/price/report/
url="http://yunhq.sse.com.cn:32041/v1/sh1/list/exchange/equity?callback=jQuery111208015895779126387_1560941576071&select=date%2Ccode%2Cname%2Copen%2Chigh%2Clow%2Clast%2Cprev_close%2Cchg_rate%2Cvolume%2Camount%2Ctradephase%2Cchange%2Camp_rate%2Ccpxxsubtype%2Ccpxxprodusta&order=&begin="+begin+"&end="+end+"&_="+timestamp;
print("url=", url)

headers = {
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36',
  'Referer': 'http://www.sse.com.cn/market/price/report/'
}

import requests
r = requests.get(url, headers=headers) #, auth=('user', 'pass')
#rs1=r.status_code #200
#rs2=r.headers['content-type'] #'text/html; charset=utf-8'
#rs3=r.encoding #'utf-8' 编码，修改编码
rs4=r.text
#r.json() #只有r.headers['content-type']为json时才能用。否则报错。
#print(rs1,rs2,rs3,"\n")
#rs4 #现在是字符串格式

# (2) 解析json格式为python数组
import json,re
#json
rs=re.sub(r"jQuery111208015895779126387_1560941576071\(", "", rs4)
rs=re.sub(r"\)$", "", rs)
rs=eval(rs)
print( len(rs['list']) )  #25

# (3) 注释每一列的字段名
# select: code,name,open,high,low,last,prev_close,chg_rate,volume,amount,tradephase,change,amp_rate
print('date code,name,open,high,low,last,prev_close,chg_rate,volume,amount,tradephase,change,amp_rate cpxxsubtype cpxxprodusta')
titles='date,code,name,open,high,low,last,prev_close,chg_rate,volume,amount,tradephase,change,amp_rate,cpxxsubtype,cpxxprodusta'.split(",")

#data
alists=rs['list']
#len(alist) #1515
print(alists[0])

alist=alists[0]
for i in range(len(titles)):
    print(i, titles[i], "=", alist[i])

import numpy as np
alists2=np.array(alists)
print(alists2.shape) #(2052, 16)

import pandas as pd
stock=pd.DataFrame(alists2, columns=titles)

stock['amount']=stock['amount'].astype(np.float64)/10000 #单位 万元
stock['volume']=stock['volume'].astype(np.float64)/100 # 单位 手

stock['chg_rate']=stock['chg_rate'].astype(np.number)
stock['last']=stock['last'].astype(np.number)

stock['amp_rate']=stock['amp_rate'].astype(np.number)

stock['open']=stock['open'].astype(np.number)
stock['high']=stock['high'].astype(np.number)
stock['low']=stock['low'].astype(np.number)

stock['prev_close']=stock['prev_close'].astype(np.number)
stock['high']=stock['high'].astype(np.number)

stock['change']=stock['change'].astype(np.number)

stock.head()
# 涨跌幅 chg_rate(%)
# 振幅 amp_rate






import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

X=stock.loc[:, "open":"amp_rate"]
X=X.drop(["chg_rate", "tradephase"], axis=1)
y=stock["chg_rate"]

print("size:", X.shape, y.shape) #size: (2053, 9) (2053,)




# 从9列特征中预测涨幅, 使用MLP多层感知神经网络

# 导入交叉验证
from sklearn.model_selection import cross_val_score
# 导入MLP神经网络回归
from sklearn.neural_network import MLPRegressor
scores=cross_val_score(MLPRegressor(random_state=38, max_iter=800), X, y, cv=3)
print("mean score:{:0.3f}".format( scores.mean()) ) 
# 这个打分怎么小于0呢？而且小这么多!?
# 因为没有预处理，各个特征的极值差异过大。
# mean score:-430.876




2.  预处理和MLP模型的管道
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import StandardScaler

# 对比两种方法的语法
pipeline=Pipeline([ ("scaler",StandardScaler()),
                  ("mlp", MLPRegressor(random_state=38, max_iter=800) )])
pipe = make_pipeline(StandardScaler(), MLPRegressor(random_state=38, max_iter=800))
# make_pipeline 看着更简洁。
print(pipeline.steps)
print(pipe.steps)
输出:
[('scaler', StandardScaler()), ('mlp', MLPRegressor(max_iter=800, random_state=38))]
[('standardscaler', StandardScaler()), ('mlpregressor', MLPRegressor(max_iter=800, random_state=38))]




# 进行交叉验证

# 这次评分是建立在管道模型pipe上，也就是数偶在交叉验证中，每次都会对数据集进行StandardScaler预处理，再拟合MLP回归模型。
scores = cross_val_score(pipe, X, y, cv=3)
print( "mean score:{:0.3f}".format(scores.mean()) )
# 这个打分也不算多好，但至少正常点了。
# mean score:0.720






3. 添加特征选择步骤

# 尝试使用随机森林模型，对数据集进行特征筛选。

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestRegressor
pipe = make_pipeline(StandardScaler(),
                    SelectFromModel(RandomForestRegressor(random_state=38)),
                    MLPRegressor(random_state=38, max_iter=800))
pipe.steps
输出:
[('standardscaler', StandardScaler()),
 ('selectfrommodel',
  SelectFromModel(estimator=RandomForestRegressor(random_state=38))),
 ('mlpregressor', MLPRegressor(max_iter=800, random_state=38))]





# 进行交叉验证
scores = cross_val_score(pipe, X, y, cv=3)
print( "mean score:{:0.3f}".format(scores.mean()) )
# 打分略有变化
# mean score:0.725





# 查看每一步的属性，比如第二步选了哪些特征？
pipe.fit(X, y)
mask = pipe.named_steps["selectfrommodel"].get_support()
mask #可见，只有最后2个特征被用于模型。
# array([False, False, False, False, False, False, False,  True,  True])


# true relation
print( X["change"] / X["prev_close"]*100 - stock['chg_rate'] )



========================================
|-- 管道进行模型选择和参数调优
----------------------------------------
要复用上述数据，所以接着运行。

1. 模型选择
# 目的: 看看 随机森林好， 还是 MLP 多层感知神经网络好。而MLP需要数据预处理。

from sklearn.model_selection import GridSearchCV

# 定义参数
params=[
    {"reg":[MLPRegressor(random_state=38, max_iter=1000)],
        "scaler":[StandardScaler(), None]},
    
    {"reg":[RandomForestRegressor(random_state=38)],
       "scaler":[None]}
]
# 实例化
pipe = Pipeline([("scaler", StandardScaler()), ("reg", MLPRegressor())])
grid=GridSearchCV(pipe, params, cv=3)

# 拟合数据
grid.fit(X, y)

# 打分
print( "best model:{}".format( grid.best_params_) )
print( "best score:{:0.2f}".format(grid.best_score_) )

输出:
best model:{'reg': RandomForestRegressor(random_state=38), 'scaler': None}
best score:0.88





2. 参数调优
# 在参数字典中增加 MLP 隐藏层 和随机森林中 estimator 数量的选项
params= [
    {"reg":[MLPRegressor(random_state=38, max_iter=1000)], 
    "scaler":[StandardScaler(), None],
    "reg__hidden_layer_sizes":[(50,), (100,), (100,100)]},
    
    {"reg": [RandomForestRegressor(random_state=38)],
    "scaler":[None],
    "reg__n_estimators":[10, 50, 100]}
]

# 建立管道模型
pipe = Pipeline([("scaler", StandardScaler()),
                ("reg", MLPRegressor())])
# 建立网格搜索
grid = GridSearchCV(pipe, params, cv=3)

# 拟合网格
grid.fit(X, y)

# 打分
print( "best model:{}".format( grid.best_params_) )
print( "best score:{:0.2f}".format(grid.best_score_) )

#出现反转，最好的模型又变成了 多层神经网络。
输出:
best model:{'reg': MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=1000, random_state=38), 'reg__hidden_layer_sizes': (100, 100), 'scaler': StandardScaler()}
best score:0.90



grid.score(X, y) #训练集上的打分
# 0.98843





========================================
chapter 13 文本数据处理: 自然语言处理(Natural Language Processing, NLP)
----------------------------------------

概要 
- 文本数据的特征提取
- 中文文本的分词方法
- 用 n-Gram 模型优化文本数据
- 是引用 tf-idf 模型改善特征提取
- 删除停用词 (Stopwords)



========================================
|-- 词袋模型
----------------------------------------
1. 英语句子分词
# 导入向量化工具
from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()
# 拟合文本数据
en=["The quick brown fox jumps over a lazy dog"]
vect.fit(en)

print("words:{}".format( len(vect.vocabulary_) ))
print("words:{}".format(vect.vocabulary_))
输出:
words:8
words:{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumps': 3, 'over': 5, 'lazy': 4, 'dog': 1}



2. 中文句子分词

# 而中文不能自动分词
cn=["那只敏捷的棕色狐狸跳过了一只懒惰的狗"]
vect.fit(cn)
vect.vocabulary_
# {'那只敏捷的棕色狐狸跳过了一只懒惰的狗': 0}



# 中文使用 结巴分词 pip3 install jieba -i https://pypi.douban.com/simple/
# jieba-0.42.1
import jieba
cn1=jieba.lcut(cn[0])
print("cn1:", cn1)
# cn1: ['那', '只', '敏捷', '的', '棕色', '狐狸', '跳过', '了', '一只', '懒惰', '的', '狗']



# 使用空格连起来
cn2= [' '.join(cn1)]
print("cn2:", cn2)

vect.fit(cn2)
vect.vocabulary_
# cn2: ['那 只 敏捷 的 棕色 狐狸 跳过 了 一只 懒惰 的 狗']
# {'敏捷': 2, '棕色': 3, '狐狸': 4, '跳过': 5, '一只': 0, '懒惰': 1}





3. 句子向量化，词袋模型 bag of words
# 定义词袋模型
print(cn2)
bag_of_words = vect.transform( cn2 )
print("2->", bag_of_words)
print("3->", repr(bag_of_words) )

# 打印词袋模型的密度表达
print( bag_of_words.toarray() )
输出:
['那 只 敏捷 的 棕色 狐狸 跳过 了 一只 懒惰 的 狗']
2->   (0, 0)	1
  (0, 1)	1
  (0, 2)	1
  (0, 3)	1
  (0, 4)	1
  (0, 5)	1
3-> <1x6 sparse matrix of type '<class 'numpy.int64'>'
	with 6 stored elements in Compressed Sparse Row format>
[[1 1 1 1 1 1]]






## 话一句复杂的句子
cn_1=jieba.lcut("懒惰的狐狸不如敏捷的狐狸敏捷，敏捷的狐狸不如懒惰的狐狸懒惰")
cn_2=[" ".join(cn_1)]
print(cn_2)

# 建立新的词袋模型
new_bag = vect.transform(cn_2)
print("词袋特征:", repr(new_bag) )
print("词袋密度:", new_bag.toarray())
# 0位 一只 出现0次；1位 懒惰 出现3次；... 4位 狐狸 出现4次；...

# 词袋模型，仅仅是用数组表示每个单词出现的频率，并不表示每个单词的位置。
输出 
['懒惰 的 狐狸 不如 敏捷 的 狐狸 敏捷 ， 敏捷 的 狐狸 不如 懒惰 的 狐狸 懒惰']
词袋特征: <1x6 sparse matrix of type '<class 'numpy.int64'>'
	with 3 stored elements in Compressed Sparse Row format>
词袋密度: [[0 3 3 0 4 0]]





========================================
|-- 对文本数据的优化处理
----------------------------------------

1. 使用 n-Gram 改善词袋模型
除了词语的频率，词语的顺序也很重要。

# 随便写一句话
import jieba
joke = jieba.lcut("道士看到和尚亲吻了尼姑的嘴唇")
# 插入空格
joke = [' '.join(joke)]

# 转为向量
# 导入向量化工具
from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()
vect.fit(joke)
joke_feature = vect.transform(joke)
# 打印
print("单词表:", vect.vocabulary_)
print("特征表达:", joke_feature.toarray())
输出:
单词表: {'道士': 5, '看到': 4, '和尚': 1, '亲吻': 0, '尼姑': 3, '嘴唇': 2}
特征表达: [[1 1 1 1 1 1]]




# 调整顺序
joke2 = jieba.lcut("尼姑看到道士亲吻了和尚的嘴唇")
# 插入空格
joke2=[' '.join(joke2)]
# 特征提取
joke_feature2 = vect.transform(joke2)

# 打印
print("单词表:", vect.vocabulary_)
print("特征表达:", joke_feature2.toarray())
# 这2句对于人类是明显不同的，但对于这个机器模型是一模一样的。这肯定有问题。
输出:
单词表: {'道士': 5, '看到': 4, '和尚': 1, '亲吻': 0, '尼姑': 3, '嘴唇': 2}
特征表达: [[1 1 1 1 1 1]]




# 可以使用 CountVectorizer 的 ngram_range 参数进行参数调节。
# n 是整型，
#  n=2的模型叫做 bi-Gram，表示 n-Gram 会对相邻的2个单词进行配对；
#  n=3的模型叫做 tri-Gram，表示 n-Gram 会对相邻的3个单词进行配对。

vect=CountVectorizer(ngram_range=(2,2))
# 重新进行文本数据的特征提取
cv=vect.fit(joke)
joke_feature=cv.transform(joke)

print("单词表:", cv.vocabulary_)
print("特征表达:", joke_feature.toarray())
输出 
单词表: {'道士 看到': 4, '看到 和尚': 3, '和尚 亲吻': 1, '亲吻 尼姑': 0, '尼姑 嘴唇': 2}
特征表达: [[1 1 1 1 1]]



# 调整顺序后
# 特征提取
joke_feature2 = cv.transform(joke2)

# 打印
print("单词表:", cv.vocabulary_)
print("特征表达:", joke_feature2.toarray())
# 机器已经不认为这是同一句话了。
输出
单词表: {'道士 看到': 4, '看到 和尚': 3, '和尚 亲吻': 1, '亲吻 尼姑': 0, '尼姑 嘴唇': 2}
特征表达: [[0 0 0 0 0]]





2. 使用 tf-idf 算法处理文本数据
tf-idf: term frequency-inverse document frequency,翻译 “词频-逆向文件频率”。
- 用来评估某个词对于一个语料库中某一份文件的重要程度。
- 如果一个词在A文件中频率很高，在其他文件中频率很低，那么 tf-idf 就认为这个词能区分文件，很重要。反之则认为该单词重要程度较低。

公式：tf-idf 的计算公式有很多实现，这是其中一种。
- 单词频率 tf= Nij / 求和(k, Nkj); 其中Nij表示某个词在语料库中某文件中出现的次数；分母表示该文件中所有单词出现的次数之和。
- idf 的计算公式 idf = log( (N+1)/(Nw+1) ) + 1; 其中 N表示语料库中文件的总数；Nw表示语料库中包含上述单词的文件数量。
- 最终计算 tf-idf 值的公式: tf-idf = tf * idf

(1) 常规 SVM 分类评价文本
# 这里使用一套英文 评价数据，一个是文本评论，一个是情感分类 positive or negative
# http://ai.stanford.edu/~amaas/data/sentiment/
def readFiles2Array(filename):
    arr=[];
    fr=open(filename)
    for lineR in fr.readlines():
        line=lineR.strip()
        arr.append(line)
    fr.close()
    return arr;

reviews=readFiles2Array("data/reviews.txt")
labels=readFiles2Array("data/labels.txt")

# 查看一条评论
print(reviews[0], "\n", labels[0])



# 拆分数据
import numpy as np
from sklearn.model_selection import train_test_split

# 为了快速，只取前1000条
total_size=2000
X_train, X_test, y_train, y_test = train_test_split( np.array(reviews)[:total_size], np.array(labels)[:total_size] )
print(X_train.shape, X_test.shape) #(18750,) (6250,)
# (1500,) (500,)



# 拟合
vect = CountVectorizer().fit(X_train)
# 文本转为向量
X_train_vect = vect.transform(X_train)
# 
print("训练集样本特征数量:{}".format(len(vect.get_feature_names_out()) ) )
print("最后10个训练集样本特征:{}".format( vect.get_feature_names_out()[-10:] ))

X_train_vect.shape #(750, 14330) 有1.4万特征
输出
训练集样本特征数量:19861
最后10个训练集样本特征:['zorro' 'zsigmond' 'zu' 'zubeidaa' 'zucco' 'zuckerman' 'zuzz' 'zwick'
 'zz' 'zzzz']
(1500, 19861)






# 有监督的学习算法，进行交叉验证评分

# 导入线性SVC分类模型
from sklearn.svm import LinearSVC
# 导入交叉验证工具
from sklearn.model_selection import cross_val_score
# 使用交叉验证对模型进行打分
scores = cross_val_score(LinearSVC(max_iter=3000), X_train_vect, y_train)
print("mean score:{:0.3f}".format(scores.mean()))
# 1k data: 0.836
# 2k data: 0.858



# 泛化到测试集的表现
X_test_vect = vect.transform(X_test)
# 使用 SVC 拟合训练数据集
clf=LinearSVC(max_iter=3000).fit(X_train_vect, y_train)
print("train score:{:0.3f}".format(clf.score(X_train_vect, y_train)))
print("test score:{:0.3f}".format(clf.score(X_test_vect, y_test)))
# test score:
# 1k data: 0.844
# 2k data: 0.860




(2) tf-idf 分类
from sklearn.feature_extraction.text import TfidfTransformer
# 使用tf-idf工具转化训练集和测试集
tfidf = TfidfTransformer(smooth_idf=False)
tfidf.fit(X_train_vect)
X_train_tfidf = tfidf.transform(X_train_vect)
X_test_tfidf = tfidf.transform(X_test_vect)
# 将处理前后的特征打印进行比较
print("tf-idf前的特征:\n", X_train_vect[:6, 20:30].toarray())

print("tf-idf处理后的特征:")
import pandas as pd
display( pd.DataFrame(X_train_tfidf[:6, 20:30].toarray() ) )

# 可见处理前是单词频数，处理后是词频乘以逆向文档频率，是一个浮点数。



# 使用 tf-idf 后的新数据

# 使用交叉验证对模型进行打分
scores = cross_val_score(LinearSVC(max_iter=3000), X_train_tfidf, y_train)
print("mean score of CV:{:0.3f}".format(scores.mean()))

# 训练线性 SVC 模型
clf=LinearSVC(max_iter=3000).fit(X_train_tfidf, y_train)
print("train score:{:0.3f}".format(clf.score(X_train_tfidf, y_train)))
print("test score:{:0.3f}".format(clf.score(X_test_tfidf, y_test)))
# 测试集有所提升






3. 删除文本数据中的停用词 stopwords
停用词 stopwords，指的是出现频率很高，但没有实际意义的单词，通常包括各种语气词、连词、介词等。
- 筛选策略，就是去掉频率过高的词；或者使查表过滤。sklearn 也内置了一个list。
- 中文常见的表有: 哈工大停用词表,百度停用词表


# 导入停用词表
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
print("sklearn内置停用词个数:", len(ENGLISH_STOP_WORDS))
print("前20个和后20个:\n",list(ENGLISH_STOP_WORDS)[:20], "\n", list(ENGLISH_STOP_WORDS)[-20:])
输出 
sklearn内置停用词个数: 318
前20个和后20个:
['until', 'into', 'whereby', 'anywhere', 'some', 'around', 'us', 'five', 'these', 'ten', 'anyhow', 'interest', 'by', 'couldnt', 'throughout', 'nothing', 'put', 'hundred', 'thence', 'per'] 
['had', 'may', 'somewhere', 'co', 'therefore', 'with', 'were', 'him', 'many', 'fifteen', 'own', 'me', 'mine', 'being', 'between', 'wherever', 'three', 'done', 'move', 'fifty']



# 删除停用词后，看模型打分是否有提升

from sklearn.feature_extraction.text import TfidfVectorizer
# 激活停用词参数
tfidf = TfidfVectorizer(smooth_idf=False, stop_words="english")
# 拟合
tfidf.fit(X_train)
# 将训练集转为向量
X_train_tfidf= tfidf.transform(X_train)


# 使用交叉验证对模型进行打分
scores3 = cross_val_score(LinearSVC(max_iter=3000), X_train_tfidf, y_train)
print("mean score of CV:{:0.3f}".format(scores3.mean()))

# 训练线性 SVC 模型
X_test_tfidf= tfidf.transform(X_test)
clf=LinearSVC(max_iter=3000).fit(X_train_tfidf, y_train)
print("train score:{:0.3f}".format(clf.score(X_train_tfidf, y_train)))
print("test score:{:0.3f}".format(clf.score(X_test_tfidf, y_test))) #略有提升
输出
mean score of CV:0.902
train score:1.000
test score:0.918





进一步学习

自然语言处理
- 推荐宝 NLTK， pip install nltk; 功能包括 分词，为文本加标注，词干提取(stemming)及词干还原(Lemmatization)
- 话题建模(topic modeling) 和 文档聚类(document clustering)，差不多是文本数据的降维。话题指的是ML后，将相似的文本进行聚类的结果。
    * 和PCA或NMF算法不同，而是使用了 “潜狄利克雷分布”(Latent Dirichlet Alocation, LDA)
- 自然语言处理的工具 word2vec 库。
- 使用 TensorFlow 建立 循环神经网络(RNN) 也实现了重大突破。



========================================
chapter 14 数据获取到话题提取: 爬虫(Requests/ bs4/ RegExp)
----------------------------------------
1. 简单页面的爬取
! pip list | grep -i request
# Rrequests 库是基于 urllib的，但是更好用。
# requests                          2.25.1

(1) 查询User agent
# 1) 从网页复制
# https://www.ip138.com/useragent/
# Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.164 Safari/537.36

# 2) 从网页F12 复制
# 打开百度，F12键-network，再F5刷新，随便选择一个加载内容，点header - Request，看其中内的 User-Agent:
UserAgent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.164 Safari/537.36"


(2) 熟悉网站结构
# 政策-最新： http://www.gov.cn/zhengce/zuixin.htm
url= "http://www.gov.cn/zhengce/content/2021-11/03/content_5648645.htm"



(3) 爬取并保存到本地
import requests
headers={
    "User-Agent": UserAgent
}

# 发起请求
r=requests.get(url, headers=headers)
#print(r.text)# 主要内容乱码

#打印编码方式
print(r.encoding) #ISO-8859-1
# 而原网页正文写的是 charset="utf-8" 

# 重新设置编码方式
r.encoding="urf-8"
print(r.encoding) #urf-8
#print(r.text) #正常打印
# 但是包含html标记，不需要它们。


# 要么保存为 html
with open("dustbin/test.html", 'w', encoding='utf8') as f:
    f.write(r.text)
# 要么解析其中的文本，保存为csv文件，见下文。



========================================
|-- 稍微复杂的爬虫
----------------------------------------
上面的例子其实降低了我们的效率，单个网址我们直接用浏览器看反而更高效。
我们希望看到一个列表(部门，标题，链接)，大致判断感兴趣的内容，再点开看细节。怎么获得这样的列表呢？

目标 最新政策 list 的爬取 url="http://www.gov.cn/zhengce/zuixin.htm"
- 分析细节页面，打开该页面，F12查看，要闻列表

1. 正则表达式简介
略，看 basic 专题。


2. 使用 BeautiSoup 进行 html 解析
python的两个解析html的库: lxml 和 BeautifulSoup。

# $ pip3 install beautifulsoup4 -i https://pypi.douban.com/simple/
# $ pip3 list | grep -i soup
#beautifulsoup4                    4.10.0
# $ pip3 list | grep -i xml
#defusedxml                        0.7.1
#lxml                              4.6.3


# 下载html信息
url= "http://www.gov.cn/zhengce/content/2021-11/03/content_5648645.htm"

import requests
headers={
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.164 Safari/537.36"
}
r=requests.get(url, headers=headers)
# 重新设置编码方式
r.encoding="urf-8"


# 解析html信息
from bs4 import BeautifulSoup
soup = BeautifulSoup(r.text, 'lxml', from_encoding='utf8') #指定使用 lxml 库作为解析库，比自带的标准库快
#print(soup)

# 获取标题
print(soup.title) #带着 html 标签
print(soup.title.string) #去标签
print(soup.title.get_text()) #或者
输出:
<title>国务院关于2020年度国家科学技术奖励的决定（国发〔2021〕22号）_政府信息公开专栏</title>
国务院关于2020年度国家科学技术奖励的决定（国发〔2021〕22号）_政府信息公开专栏
国务院关于2020年度国家科学技术奖励的决定（国发〔2021〕22号）_政府信息公开专栏






# 获取段落
print(soup.p.string)

# 怎么获取全部的段落？
texts=soup.find_all('p')
for text in texts:
    if text.string == None: #过滤空白段
        continue
    print(text.string)
# 文本略。


# 获取链接
link=soup.find_all('a')[-1] #最后的一个链接
link.get('href')






3. 爬取新闻目录页面并保存为csv
import requests, csv, re
from bs4 import BeautifulSoup

#下载
url="http://www.gov.cn/zhengce/zuixin.htm"
user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.164 Safari/537.36"

policies=requests.get(url, headers={"User-Agent": user_agent})
policies.encoding='utf-8'

#解析
p=BeautifulSoup(policies.text, 'lxml')

# 用正则表达式匹配所有包含 content 的单词的a标签
contents =p.find_all("a", href=re.compile('content'))

# 定义一个空列表
rows=[]

#设计一个for循环，提取每个链接汇总的标题
for content in contents:
    href=content.get('href')
    row=(content.string[0:4], content.string, href)
    rows.append(row)

# 定义表头
header=["发文部门", "标题", "链接"]
# 保存文件；原文 encoding='gb18030'
with open("dustbin/policies.csv", 'w', encoding='utf8') as f:
    writer=csv.writer(f)
    writer.writerow(header)
    writer.writerows( rows)
print("==done==")


# check
$ head -n 3 dustbin/policies.csv
发文部门,标题,链接
中共中央,中共中央关于党的百年奋斗重大成就和历史经验的决议,/zhengce/2021-11/16/content_5651269.htm
国务院办,国务院办公厅关于对国务院第八次大督查发现的典型经验做法给予表扬的通报,http://www.gov.cn/zhengce/content/2021-11/15/content_5650982.htm








========================================
|-- 对文本数据进行话题提取
----------------------------------------
当爬取的内容很多时，看着很花时间。能否快速了解几万字的核心内容呢？
- 可以使用 “潜在地理克雷分布”(Latent Dirichlet Allocation)对文本做话题提取。

1. 下载大量内容
# 百度搜 段子，找到一个全是文本段子的网站。

import requests, csv, re
from bs4 import BeautifulSoup

user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.164 Safari/537.36"

# 定义一个空列表
rows=[]
urls=[]

for i in range(1, 10):
    url="http://www.duanziku.com/gaoxiaoduanzi/gxdz{}.html".format(i)
    webpage=requests.get(url, headers={"User-Agent": user_agent})
    webpage.encoding='utf-8'

    #解析
    p=BeautifulSoup(webpage.text, 'lxml')

    # 用正则表达式匹配所有包含 gaoxiaoduanzi 的单词的a标签
    contents =p.find_all("a", href=re.compile('gaoxiaoduanzi.*\d+\.html$'))

    #设计一个for循环，提取标题和链接
    for content in contents:
        href=content.get('href')
        if(content.string==None):
            continue
        row=(content.string, href)
        if href in urls:
            continue;
        else:
            urls.append(href)
            if len(urls)<5:
                print(row)
            rows.append(row)
len(rows)



def getJoke(url):
    webpage=requests.get(url, headers={"User-Agent": user_agent})
    webpage.encoding='utf-8'
    p=BeautifulSoup(webpage.text, 'lxml')
    contents =p.find("span", style="font-size:16px;")
    if contents==None:
        return;
    contents=contents.text
    
    # 去掉换行
    w=re.sub("\r\n", "", contents)

    # 切分文本
    contents2 = re.split("\n+", w)
    # 保存
    db=[]
    for joke in contents2:
        if joke=="":
            continue
        joke = re.sub("\d{1,2}[\.\、]", "", joke) #去掉首数字
        db.append(joke)
    return db
# test
getJoke( rows[0][1])[0:3]
# 内容略


def getJoke2(url):
    webpage=requests.get(url, headers={"User-Agent": user_agent})
    webpage.encoding='utf-8'
    p=BeautifulSoup(webpage.text, 'lxml')

    p=p.find("table")
    if p==None:
        return;
    contents =p.find_all("p")
    
    if contents==None:
        print("broken page:", url)
        return;
    
    db=[]
    for text in contents:
        text2=re.sub("\s+","", text.text)
        if len(text2)<10 or re.match("[0-9]",text2)==None:
            #pass
            continue
        joke =text2
        joke = re.sub("\d{1,2}[\.\、]", "", joke) #去掉首数字
        db.append(joke)
    return db
getJoke2( rows[6][1])[0:3]
# 内容略




import random
import time

jokes=[]
for i in range(0, len(rows)):
    #print(i, len(jokes))
    url=rows[i][1]
    # 获取页面并解析出文本
    time.sleep(0.5 + 2.5*random.random())
    db=getJoke( url )
    if db==None:
        db=getJoke2( url )
    if db==None or len(db)==0:
        print("broken:", url )
        continue
    for joke in db:
        if joke not in jokes:
            jokes.append(joke)
# 总数
print(len(jokes)) #411




# 保存到文件
fw=open("dustbin/jokes.txt", 'w')
for joke in jokes:
    fw.write(joke + "\n")
fw.close()





3.使用 潜在狄利克雷分布 进行话题提取
潜在狄利克雷分布(Latent Dirichlet Allocation, LDA)是基于不同的词语共同出现的频率来进行分组的模型。
- 比如，某个文档中，“妹子”“吃货”这2个词经常同时出现，LDA模型就会将2个词归入同一个话题(Topic)。


# 载入文件
fr=open("dustbin/jokes.txt", 'r')
lines=fr.readlines()
line=str(lines) #把数组变成一个字符串
fr.close()

print( len(lines) ) #411
print(len(line)) #23419

# 分词
import jieba
line2=jieba.cut(line)
x=" ".join(line2)
# 保存
with open("dustbin/cutjokes.txt", 'w') as f:
    f.write(x);




from sklearn.feature_extraction.text import TfidfVectorizer
# 导入 LDA 模型
from sklearn.decomposition import LatentDirichletAllocation

# 打印提取后的高频词
def print_topics(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        message = "topic #%d:" % topic_idx
        message += " ".join([ feature_names[i]
                            for i in topic.argsort()[:-n_top_words -1:-1]])
        print(message)
    print()


#定义每个话题提取20个高频词
n_top_words=20
# tfidf 最大特征数 1000
tf=TfidfVectorizer(max_features=1000)

# 载入分词处理过的文本文件
fr=open("dustbin/cutjokes.txt")
#转化训练数据
X_train = tf.fit_transform(fr)
fr.close()

# 指定 LDA 模型提取10个话题
lda=LatentDirichletAllocation(n_components=6)
lda.fit(X_train)

print_topics(lda, tf.get_feature_names_out(), n_top_words )




# check 
for topic_idx, topic in enumerate(lda.components_):
    print(topic_idx, ">>>", topic[0:6])

tmp=tf.get_feature_names_out()
tmp

for i in topic.argsort()[:-n_top_words -1:-1]:
    print(i, tmp[i])





3. 进一步学习
爬虫：推荐 Scrapy，是目前最常用的用于开发爬虫的工具之一。

自然语言处理：循环神经网络 RNN。





========================================
chapter 15:需求现状及未来学习方向
----------------------------------------

1. 人工智能的细分领域

1: 算法、机器学习等
2: GPU、智能芯片等
3: 机器人
4: 图像识别/计算机视觉
5: 自然语言处理 
6: 智能/精准营销
7: 语音识别
8: 推荐系统
9: 搜索引擎
10: 智能交通/自动驾驶




2. 未来学习方向

(1) 用于大数据分析的计算引擎

核外学习 out-of-core-learning, 外存学习;
	数据是通过外部硬盘甚至网络提供的
集群式并行计算;
	Spark 计算引擎: 主流分布式计算平台, MapReduce 算法，支持Python, scala, java, R 等.

R 语言。


(2) 深度学习开源框架 
TensorFlow，Caffe 和 Keras。

Keras 上手快。


(3) 使用概率模型进行推理
PyMC: 是一个实现贝叶斯统计模型和马尔科夫链蒙特卡洛采样工具拟合算法的库。
Stan: 是一个非常尖端的用于统计建模和高效统计计算的平台。

2个库都要求用户对概率统计有一定的了解。





3. 技能的磨练

(1) Kaggle 算法大赛平台，OpenML平台
可以看到更多数据和任务。

(2) 工业级场景的应用
sklearn 适合快速原型。
而最求效率的，还可以使用 Java, C++, Scalar, Go 等重写。

(3) 对算法的 A/B 测试





========================================
----------------------------------------







========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------







========================================
----------------------------------------



========================================
----------------------------------------


========================================
----------------------------------------



