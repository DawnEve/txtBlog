mysql高级话题：(争取好用,高效,稳定,可靠)
分区、表约束、
触发器、游标、存储过程、访问控制、事务、
视图、

索引优化
分库分表 sharding,


========================================
深入理解mysql 索引特性(面试高频,屡试不爽的mysql索引总结)
----------------------------------------
https://juejin.im/post/5d4e7f496fb9a06afe128655





========================================
key和index
----------------------------------------
1 索引分类
主键索引（必须指定为“PRIMARY KEY”，没有PRIMARY Index）、
唯一索引（unique index，一般写成unique key）、
普通索引(index，只有这一种才是纯粹的index)等，也是基于是不是把index看作了key。
           比如 create table t(id int, unique indexinx_tx_id (id));--index当作了key使用

(2) 最重要的也就是，不管如何描述，需要理解index是纯粹的index（普通的key，或者普通索引index），还是被当作key（如：unique index、unique key和primary key），若当作key时则会有两种意义或起两种作用。




2.MySQL Key值（PRI, UNI, MUL）的含义：
PRI主键约束；
UNI唯一约束；
MUL可以重复。

注：若是普通的key或者普通的index（实际上，普通的key与普通的index同义）。

(2)desc tableName;
我们看到Key那一栏，可能会有4种值，即'啥也没有','PRI','UNI','MUL'
1). 如果Key是空的, 那么该列值的可以重复，表示该列没有索引, 或者是一个非唯一的复合索引的非前导列
2). 如果Key是PRI,  那么该列是主键的组成部分
3). 如果Key是UNI,  那么该列是一个唯一值索引的第一列(前导列)，且不能含有空值(NULL)
4). 如果Key是MUL,  那么该列的值可以重复, 该列是一个非唯一索引的前导列(第一列)或者是一个唯一性索引的组成部分但是可以含有空值NULL

注：
1)、如果对于一个列的定义，同时满足上述4种情况的多种，比如一个列既是PRI，又是UNI（如果是PRI，则一定是UNI）
那么"desc 表名"; 的时候，显示的Key值按照优先级来显示 PRI->UNI->MUL
那么此时，显示PRI。

2)、如果某列不能含有空值，同时该表没有主键，则一个唯一性索引列可以显示为PRI，

3)、如果多列构成了一个唯一性复合索引，那么一个唯一性索引列可以显示为MUL。（因为虽然索引的多列组合是唯一的，比如ID+NAME是唯一的，但是每一个单独的列依然可以有重复的值，因为只要ID+NAME是唯一的即可）



https://www.cnblogs.com/jpfss/p/8308656.html







========================================
分库分表 sharding 
----------------------------------------
https://juejin.im/post/5d4b6dc1f265da03c1288332




========================================
加索引，能提高update语句的效率，至少一个数量级
----------------------------------------

直接创建索引
CREATE INDEX index_name ON table(column(length))

修改表结构的方式添加索引
ALTER TABLE table_name ADD INDEX index_name ON (column(length))



实例：
> CREATE INDEX index_pasID ON feature_apa(pasID(200)) #实测，确实把update语句的执行时间从 33s/100条 降低到 0.5s/100条。
> desc feature_apa;
+---------------+--------------+------+-----+---------+----------------+
| Field         | Type         | Null | Key | Default | Extra          |
+---------------+--------------+------+-----+---------+----------------+
| id            | int(4)       | NO   | PRI | NULL    | auto_increment |
| pasID         | varchar(200) | NO   | MUL | NULL    |                |
| chr           | varchar(100) | YES  |     | NULL    |                |
| pos           | bigint(4)    | YES  |     | NULL    |                |
| score         | int(10)      | YES  |     | NULL    |                |
| strand        | char(1)      | YES  |     | NULL    |                |
| baseD4        | char(4)      | YES  |     | NULL    |                |
| existIn2Cells | int(2)       | YES  |     | 0       |                |
| PY_gene       | varchar(50)  | YES  |     |         |                |
+---------------+--------------+------+-----+---------+----------------+
9 rows in set (0.00 sec)


# update2() end; i=150626; w=110897; 耗时914.14s; 20191008-110218 #之前，执行到中间加的索引，总时间应该比这个长。
# update2() end; i=150626; w=110897; 耗时88.38s; 20191008-110649 #添加索引之后，有重复一次
# update2() end; i=150626; w=110897; 耗时187.44s; 20191008-111911 #开启事务之后更慢了（耗时加倍），看来为保证都执行就要牺牲性能。



这时给title字段添加一个BTREE索引：
mysql> ALTER TABLE article ADD INDEX index_article_title ON title(200);



========================================
mysql拷贝表/复制表的几种方法：定期备份或破坏性操作前备份
----------------------------------------
1. 拷贝表结构到新表中。 （不会拷贝表中的数据）
CREATE TABLE 新表 LIKE 旧表;<br>或<br>CREATE TABLE 新表 SELECT * FROM 旧表 WHERE 1=2;
　　

2. 拷贝数据到新表中。（新表不会有主键，索引）
CREATE TABLE 新表 AS  
(  
    SELECT *  
    FROM 旧表
);
　　

3. 真正的复制一个表。可以用下面的语句。
CREATE TABLE 新表 LIKE 旧表;  
INSERT INTO 新表 SELECT * FROM 旧表;  <br>或<br>CREATE TABLE 新表 SELECT * FROM 旧表;
　　

4. 操作不同的数据库。
CREATE TABLE 新表 LIKE 库1.旧表;  
CREATE TABLE 库2.新表 LIKE 库1.旧表; 


　　
5. 拷贝一个表中其中的一些字段。
CREATE TABLE 新表 AS  
(  
    SELECT 字段1, 字段2,... FROM 旧表
);

我们也可以拷贝一部分数据
CREATE TABLE 新表 AS
(
	SELECT * FROM 旧表 WHERE LEFT(username,1) = 's'
);



复制旧表的数据到新表
(假设两个表结构一样) 
INSERT INTO 新表 SELECT * FROM 旧表;

(假设两个表结构不一样)
INSERT INTO 新表
(
　　字段1,字段2,.......
) SELECT 
　　字段1,字段2,...... 
FROM 旧表;


6. 将新建的表的字段改名。
CREATE TABLE 新表 AS  
(  
    SELECT id, username AS uname, password AS pass FROM 旧表
);


7.将表1结构复制到表2
SELECT * INTO 表2 FROM 表1 WHERE 1=2;

将表1内容全部复制到表2 
SELECT * INTO 表2 FROM 表1;


8. 创建表的同时定义表中的字段信息。
CREATE TABLE 新表
(
    id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY
)
AS
(
    SELECT * FROM 旧表  
);


9. 显示旧表的创建命令
show create table 旧表;





拷贝表：
https://www.cnblogs.com/lyjing/p/8483742.html






========================================
为一个有主键的表添加一个递增列 (拷贝表/复制表)
----------------------------------------
1.尝试1：失败

(1)# 复制表的推荐方案：表结构、键、数据完全一样
create table feature_gene2 like `feature_gene`; #复制表结构
Insert into feature_gene2 select * from feature_gene; #复制表内容

##CREATE TABLE feature_gene2 AS SELECT * FROM feature_gene; #没有主键

## drop table feature_gene2;

(2)直接插入递增列报错
mysql> alter table feature_gene2
    -> add id int auto_increment first;
ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key

(3)再试，没报错，但是总数不对
mysql> alter table feature_gene2
   add id int unique key auto_increment first;
#Query OK, 0 rows affected (3.08 sec)
Records: 0  Duplicates: 0  Warnings: 0

查看，已经新增了一列
mysql> select * from feature_gene2 limit 5;
+----+------------ ---------+-------+--------+-----------+-----------+--------+------- --------+-------------+-----------+-------+------+
| id | gene_id              | chr   | source | start     | end       | strand | gene_type      | gene_status | gene_name | level | tag  |
+----+----------- ----------+-------+--------+-----------+-----------+--------+-------- -------+-------------+-----------+-------+------+
|  1 | ENSG00000000003.14_2 | chrX  | HAVANA |  99882106 |  99894988 | -      | protein_coding |             | TSPAN6    |     2 |      |
|  2 | ENSG00000000005.6_3  | chrX  | HAVANA |  99839933 |  99854882 | +      | protein_coding |             | TNMD      |     2 |      |


但是总共
mysql> select count(*) from feature_gene2;
+----------+
| count(*) |
+----------+
|    60627 |
+----------+

但是最后一行行号却很大：
mysql> select id,gene_type,gene_name from feature_gene2 limit 60625, 20;
+-------+-----------+------------+
| id    | gene_type | gene_name  |
+-------+-----------+------------+
| 60631 | antisense | AC008945.2 |
| 60632 | lincRNA   | AC093843.2 |
+-------+-----------+------------+

经核实，有几个行号被跳过了，原因不明。反复试都这样。
[1] 11257
[1] 22650
[1] 33285
[1] 44131
[1] 54953




2. 方法2: 改进版，成功实现
create table feature_gene3 like `feature_gene`; #创建表结构，保留字段、主键
ALTER TABLE `feature_gene3` DROP PRIMARY KEY; #去掉新表主键
alter table feature_gene3 add id int PRIMARY KEY auto_increment first; #为新表添加递增列

## 为新表添加数据
insert into feature_gene3(gene_id,chr,source,start,end,strand,gene_type,gene_status,gene_name,level,tag) select * from feature_gene;



## 删除旧表
## drop table feature_gene
rename table feature_gene to feature_gene_h1; #备份历史1，没有id列
## 重命名新表
rename table feature_gene3 to feature_gene;




3.这样也可行：创建新表，同时添加新列，插入旧表数据
CREATE TABLE feature_gene2
(
    id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY
)
AS
(
    SELECT * FROM feature_gene  
);






## 参考百度上两个回答：
1.
alter table feature_gene2
add id int primary key auto_increment first;
新建的字段，肯定是没有设置过主键的。
add语句就可以解决。既然是id，所以建议放在第一列，因此最后可加first。

已经有主键：
alter table feature_gene2
add id int auto_increment first;


2.
id不是主键的情况下，
alter table temp 
change id id int primary key auto_increment; 

如果id已经是主键，
alter table temp 
change id id int auto_increment;





========================================
优化Python对MySQL批量插入的效率
----------------------------------------
用Python的MySQLdb模块可以实现，支持存储过程。

开始测试的时候用单条insert语句循环n次，最后commit，结果慢的要死，插一万条用了两分钟，十万条我去吃了个饭回来还在插。十万条用存储过程插单库也用了50多秒。


一些SQL优化的知识。
主要有三条：

1、insert的时候尽量多条一起插，不要单条插。这样可以减少日志量，降低日志刷盘数据量和频率，效率提高很多。

2、在事务中进行插入。也就是每次部分commit，否则每条insert commit 创建事务的消耗也是不小的。

3、 数据插入的时候保持有序。比如Innodb用的是B+树索引，对B+树的插入如果是在索引中间就会需要树节点分裂合并，这也会有一定的消耗。


可以看到1000万以下数据的优化效果是明显的，但是单合并数据+事务的方式在1000万以上性能会有急剧下降，因为此时已经达到了innodb_buffer的上限，随机插入每次需要大量的磁盘操作，性能下降明显。而有序插入在1000万以上时也表现稳定，因为索引定位方便，不需要频繁对磁盘读写，维持较高性能。


import MySQLdb
db=MySQLdb.connect(host='127.0.0.1', user='root', passwd='123456', port=7070)
cur=db.cursor()
cur.execute('use dbtest')
cur.execute('truncate table tb5')
for t in range(0,100):
	sql = 'insert into tb5 (id, val) values '
	for i in range(1,100000):
		sql += ' ('+`t*100000+i`+', "tb5EXTRA"),'
	sql += ' ('+`t`+'00000, "tb5EXTRA")'
	cur.execute(sql)
	db.commit()
cur.close()
db.close()

共插入了1000万条数据，sublime显示用了333.3s，比之前插入10万条都减少了很多，测试插10万条只需要3s。



ref:
https://www.cnblogs.com/hyace/p/4173831.html



========================================
mysql 内置函数
----------------------------------------

1.连接字符串 concat():
select concat('|',word,"|"), tag_ox from word_ms limit 10;
+---------- ------------+--------+
| concat('|',word,"|") | tag_ox |
+-------- --------------+--------+
| |silver|             | A2     |
| |yesterday|          | A1     |



2.CONCAT_WS() 代表 CONCAT With Separator ，是CONCAT()的特殊形式。   第一个参数是其它参数的分隔符。分隔符的位置放在要连接的两个字符串之间。分隔符可以是一个字符串，也可以是其它参数。如果分隔符为 NULL，则结果为 NULL。函数会忽略任何分隔符参数后的 NULL 值。

mysql> SELECT CONCAT_WS(',','First name','Second name','Last Name');
-> 'First name,Second name,Last Name'

mysql> SELECT CONCAT_WS(',','First name',NULL,'Last Name');
-> 'First name,Last Name'

mysql CONCAT_WS()不会忽略任何空字符串。 (然而会忽略所有的 NULL）。





========================================
|-- 时间戳 转为 时间日期字符串: from_unixtime(int, '%Y%m%d')
----------------------------------------
1. 数据库中 updatedAt 保存时间戳：距离unix起点的秒数
找到今天修改的值:

mysql> SELECT p_id, symbol, updatedAt FROM paper_tbl WHERE from_unixtime(updatedAt, '%Y%m%d') = '20230918';
+------+--------+------------+
| p_id | symbol | updatedAt  |
+------+--------+------------+
|  651 | DNMT3A | 1695004993 |
|  652 | KMT2D  | 1695006040 |
|  653 | SP140  | 1695026590 |
+------+--------+------------+
3 rows in set (0.00 sec)











========================================
如何查询出自增列的断号（被删掉的）
----------------------------------------
select (id-1) as id_jump from word_ms a where not exists (select 1 from word_ms where id+1=a.id) order by id_jump;


ref:
https://bbs.csdn.net/topics/360242343



========================================
事务 及 事务的ACID特性
----------------------------------------
1.什么是事务？
所谓事务是用户定义的一个数据库序列，这些操作要么全做，要么全不做，是一个不可分割的工作单位。

在SQL中，定义事务的语句一般有三条：
    BEGIN TRANSACTION;
    COMMIT;
    ROLLBACK;

事务通常是以BEGIN TRANSACTION开始，以COMMIT或ROLLBACK结束。
    COMMIT：表示提交，即提交事务的所有操作
    ROLLBACK：表示回滚



2. 事务的ACID特性
【重点】事务具有4个特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持续性（Durability）。这4个特性简称为ACID特性

（1）原子性
事务是一个原子性质的操作单元，事务里面的对数据库的操作要么都执行，要么都不执行。

（2）一致性
在事务开始之前和完成之后，数据都必须保持一致状态，必须保证数据库的完整性。也就是说，数据必须符合数据库的规则。

（3）隔离性
一个事务的执行不能被其他事务干扰。即一个事务的内部操作及使用的数据对其他并发事务是隔离的，并发执行的各个事务之间不能互相干扰。

事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。


（4）持久性
持久性也成为永久性，指一个事务一旦提交，它对数据库中数据的改变就应该是永久的。


ref:
https://blog.csdn.net/weixin_51201930/article/details/123572289


========================================
|-- 数据库的脏读、不可重复读、幻读以及四种隔离级别（并发事务）
----------------------------------------
https://blog.csdn.net/weixin_51201930/article/details/123572289
1. 定义
当多个事务并发处理同一条数据时，如果事务隔离性不合理，就会产生我们今天要介绍的内容，具体的说就是：脏读、不可重复读和幻读！

在事务的四个特性里面，其中隔离性总共分为四种级别：由低到高依次为 Read uncommitted 、Read committed 、Repeatable read 、Serializable ，这四个级别可以逐个解决脏读 、不可重复读 、幻读等这几类问题。


Read uncommitted：俗称读未提交，指的是一个事务还没提交时，它做的变更就能被别的事务看到。

Read committed：俗称读提交，指的是一个事务提交之后，它做的变更才会被其他事务看到。

Repeatable read：俗称可重复读，指的是一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的，同时当其他事务在未提交时，变更是不可见的。

Serializable：俗称串行化，顾名思义就是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。


(1)脏读: 
读也俗称“读未提交”，顾名思义，就是某一事务A读取到了事务B未提交的数据。而B又回滚了。

脏读最大的问题就是可能会读到不存在的数据。
当数据库的事务隔离级别为读未提交，就会发生脏读现象！


(2)不可重复读
不可重复读，有时候也会说成“读已提交”。

就是在一个事务内，多次读取同一个数据，却返回了不同的结果。实际上，这是因为在该事务间隔读取数据的期间，有其他事务对这段数据进行了修改，并且已经提交，就会发生不可重复读事故。

当数据库的事务隔离级别为读未提交、读提交时，就会发生不可重复读现象！

不可重复读和脏读的区别：前者是“读已提交”，后者是“读未提交”


(3)幻读

出现幻读和不可重复读的原因很像，都是在多次操作数据的时候发现结果和原来的不一样了，出现了其他事务干扰的现象。但是，幻读的偏重点是添加和删除数据，多次操作数据得到的记录数不一样；不可重复读的偏重点是修改数据，多次读取数据发现数据的值不一样了。

A事务的修改涉及到表中的全部数据行。此时，突然事务 B 插入了一条数据并提交了，当事务 A 提交了修改数据操作之后，再次读取全部数据，结果发现还有一条数据未更新，给人感觉好像产生了幻觉一样。这就是幻读！

当有别的事务，在插入或者删除同一条数据的时候，就容易产生幻读的现象！
当数据库的事务隔离级别为读未提交、读提交、可重复读时，就会发生幻读现象！





2. 如何解决
为了解决上述问题，数据库通过锁机制来解决并发访问的问题。

以 Mysql 为例，根据锁定对象不同，分为：行级锁和表级锁；根据并发事务锁定的关系上看，分为：共享锁定和独占锁定。

共享锁定会防止独占锁定，但允许其他的共享锁定；而独占锁定既防止共享锁定也能防止其他独占锁定；为了更改数据，数据库在进行更改的行上施加了行级独占锁定，insert、update、delete和selsct for update语句都会隐式采用必要的行锁定，当冲突加剧，会上升到表级锁定，此时会影响到其他表的访问操作。

直接使用锁机制管理是很复杂的，基于锁机制，数据库给用户提供了不同的事务隔离级别，只要设置了事务隔离级别，数据库就会分析事务中的 sql 语句然后自动选择合适的锁，可以依次有效的解决脏读、不可重复读和幻读问题！


四种隔离级别
(1) 读未提交（Read Uncommitted）：在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）


(2) 读已提交（Read Committed）：这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别也支持所谓的不可重复读（NonrepeatableRead），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果


(3) 可重复读（Repeatable Read）:这是MySQL的默认事务隔离级别，同一事务的多个实例在并发读取数据时，会看到同样的数据。不过理论上，这会导致另一个棘手的问题：幻读（Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。

可重复读并不是加行锁，只是通过mvcc保存了数据快照，其他事务仍然可以进行更改操作，但是在同一个事务里因为数据快照，所以查到的数据相同


(4) 可串行化（Serializable）：这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。


整体的来说，事务的隔离级别和数据库并发性是成反比的，隔离级别越高，并发性越低。
四种隔离级别越往后越影响性能，如何选取根据业务需求而定。
以下是四种隔离级别中对脏读、不可重复读、幻读的影响情况。
                          脏读   不可重复读  幻读
Read Uncommitted 读未提交  Yes    Yes        Yes
Read Committed 读已提交    no     Yes        Yes
Repeatable Read 可重复读   no     no         Yes
Serializable 串行化        no     no         no


读未提交（Read Uncommitted）依旧存在脏读、不可重复读和幻读；
读已提交（Read Committed）解决了脏读问题，因为发生脏读的条件就是读未提交的数据；
可重复读（Repeatable Read）进一步解决了不可重复读的问题，从隔离名称就可以看出，但还存在幻读问题；
可串行化（Serializable）解决了脏读、幻读、不可重复读问题，但是设立这种隔离级别会大大消耗性能。

mysql数据库默认开启重复读隔离级别。
大多数数据库默认开启读提交隔离级别：Oracle






ref:
https://blog.csdn.net/weixin_51201930/article/details/123572289
https://zhuanlan.zhihu.com/p/514835631


========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------



========================================
----------------------------------------

