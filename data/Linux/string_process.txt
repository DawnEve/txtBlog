linux文本处理

linux简介6|命令行文本处理工具 


awk和sed简明教程  http://agetouch.blog.163.com/blog/static/228535090201732824532207/
教材： O'Reilly 出版的 sed & awk, 2nd Edition http://shop.oreilly.com/product/9781565922259.do
awk https://www.cnblogs.com/jiqianqian/p/7944013.html


linux shell文本处理三大利器：
grep 查找
sed 行编辑器
awk 列文本处理工具
一行指令，轻松搞定。


IBM sed实例(sed 轻量级流编辑器)
https://www.ibm.com/developerworks/cn/linux/shell/sed/sed-1/  还有2和3.

IBM awk实例(awk 适合于文本处理和报表生成。一旦学会就会成为您战略代码库的重要部分的语言)
https://www.ibm.com/developerworks/cn/linux/shell/awk/awk-1/ 还有2和3.
https://www.ibm.com/developerworks/cn/education/aix/au-gawk/index.html








========================================
目录、文件基本命令
----------------------------------------
获取帮助
Linux帮助信息十分丰富，能解决大多数使用上的问题，剩余情况可以通过搜索引擎，或者求助来解决
$ man <command>, 如: 
 man ls 
 man mkdir


常用目录操作命令：
显示当前所在目录的绝对路径	pwd
切换当前目录	cd
创建目录	mkdir
删除目录	rmdir
列出目录下文件清单	ls



Linux中分为绝对路径和相对路径
	绝对路径：从根目录起始的路径，如/usr/bin
	相对路径：从当前路径起始的路径，如../test
	两个特殊的路径：’.’和 ‘..’分别表示当前路径和上层 路径



cat命令：1. 显示文件；2. 创建文件；3. 合并文件
用法	意义
$cat filename	#查看文件filename的内容
$cat > filename	#创建文件filename
$cat >>filename	#追加文件filename
$cat file1 file2 > file3	#上下合并文件file1、file2为文件file3

$paste name passwd >one #左右合并文件成一个文件，逐行合并。
$paste -d"#" name passwd  #用#作分隔符。
注：-d指定分隔符，不加此参数默认为制表符。



常用文件操作命令：
命令名称		用途
cat	查看、创建、合并文件
more	以翻页形式查看文件内容（只能向下翻页）
less	以翻页形式查看文件内容（可上下翻页）
head或者tail	查看一个文件的前n行或者后n行，默认是10行



wc	统计文件的行数、字符数、字节数
cp	复制文件或者目录
mv	移动文件文件夹、或者文件重命名
rm	删除文件或者目录
ln	链接文件(软链接或硬链接)


链接 ln
用法: ln [options] [source] [destination] 
链接分为软连接(-s)和硬链接
	-s	建立软链接
	它的功能是为某一个文件在另外一个位置建立一个同步的链接，当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。

$ ln -s /etc/passwd #相当于在当前文件内放一个快捷方式，指向/etc/passwd。
$ ls -lth
total 16K
lrwxrwxrwx 1 wangjl wangjl 11 Sep  1 16:50 passwd -> /etc/passwd




如何由关键词找到某个文件？
(1)先找到确切文件名
/etc$ ls * -l | grep sysi  #查找/etc/目录下包含sysi的文件名
ls: cannot open directory chatscripts: Permission denied
-rw-r--r-- 1 root root 1543 Apr 16  2012 rc-sysinit.conf
(2)定位绝对路径
$ locate rc-sysinit.conf #定位文件绝对路径
/etc/init/rc-sysinit.conf









========================================
命令 shuf 随机挑选文本文件的若干行
----------------------------------------
1. 示例
$ shuf -n5 t.txt > L #则随机在t.txt 中抽取5列 到L中









========================================
命令find详解: 最常见和强大的查找命令
----------------------------------------
命令格式 find <path> < expression > < cmd >
参数介绍
path： 所要搜索的目录及其所有子目录。默认为当前目录。
expression： 所要搜索的文件的特征。
cmd： 对搜索结果进行特定的处理。
如果什么参数也不加，find 默认搜索当前目录及其子目录，并且不过滤任何结果（也就是返回所有文件），将它们全都显示在屏幕上。


大概可以归类为以下几种：
(1).根据文件(名称、类型、深度、大小)或正则表达式进行匹配。
(2).根据日期和时间查找。
(3).根据文件权限(所属组，拥有者)查找。
(4).使用-exec或-ok来执行shell命令。




1.根据文件(名称、类型、深度、大小)或正则表达式进行匹配。
(1)列出当前目录及子目录下所有文件和文件夹  find .
./c2_ROW02_R2_trimmed.fq.gz
./text
./text/aa.txt

(2).在/home下查找以.txt结尾的文件名 find /home -name "*.txt"

(3).在/home下查找以.txt结尾的文件名 但忽略大小写。 find /home -iname "*.txt"
当前目录查找文件名以一个大写字母开头的文件：
$ find . -maxdepth 1 -name "[A-Z]*" -print
./Untitled1.ipynb
./Download


(4) 查找当前目录及子目录下所有以.txt和.pdf结尾的文件
$ find . -name "*.txt" -o -name "*.pdf" #或者
$ find . \( -name "*.txt" -o -name "*.pdf" \)
##./c2_ROW02_R2.fastq_trimming_report.txt
##./text/aa.txt
##./word.pdf

要查找所有隐藏的文件
$ find ~ -name ".*" |head





(5)匹配文件路径或文件 find /usr/ -path "*local"
$ find . -path "*bb*"

(6)基于正则表达式匹配文件路径 
$ find . -regex ".*\.(txt|pdf)$"  #不能匹配
$ find . -regex ".*\.\(txt\|pdf\)$" #需要添加转义符，把()|等特殊字符转义
##./c2_ROW02_R2.fastq_trimming_report.txt
##./text/aa.txt
##./word.pdf


(7)找出/home/wangjl/test 路径下不是以.txt结尾的文件
$ find /home/wangjl/test ! -name "*.txt"
##/home/wangjl/test
##/home/wangjl/test/c2_ROW02_R2_trimmed_fastqc.html
##/home/wangjl/test/c2_ROW02_R2_trimmed_fastqc.zip



(8)根据文件类型查找
$ find . -type f #文件
$ find . -type l #链接
$ find . -type d #文件夹

$ find . -type f -name *.R |head #在当前文件夹内查找后缀是.R的文件
$ find . -type d -name test* #在当前目录查找开头是test的文件夹


(9)最大-最小文件深度
$ find ~ -maxdepth 2 -type f
$ find ~ -mindepth 10 -type f
## /home/wangjl/R/x86_64-redhat-linux-gnu-library/3.5/Rcpp/include/Rcpp/api/meat/module/Module.h




(10)根据文件大小进行匹配
文件大小单元： b 块（512字节）, c 字节, w字（2字节）, k千字节, M兆字节, G吉字节

`b' for 512-byte blocks (this is the default if no suffix is used)
`c' for bytes
`w' for two-byte words
`k' for Kibibytes (KiB, units of 1024 bytes)
`M' for Mebibytes (MiB, units of 1024 * 1024 = 1048576 bytes)
`G' for Gibibytes (GiB, units of 1024 * 1024 * 1024 = 1073741824 bytes)


查找大于10k的文件 $ find . -type f -size +10k
查找小于10K的文件 $ find . -type f -size -10k
找到大于50MB且小于100MB的所有文件 $ find /home/wangjl/data/ref/ -type f -size +50M -size -100M |xargs ls -lth
$ find . -type f -size +40c -size -100c |xargs ls -lth

某文件夹内(不含子文件夹)，小于1k的a开头的文件：
$ find /home/wangjl -maxdepth 1 -type f -name "a*" -size -1000c






2.根据日期和时间查找。
UNIX/Linux文件系统每个文件都有三种时间戳： 
访问时间（-atime/天，-amin/分钟）：用户最近一次访问时间。 
修改时间（-mtime/天，-mmin/分钟）：文件最后一次修改时间。 
变化时间（-ctime/天，-cmin/分钟）：文件数据元（例如权限等）最后一次修改时间。

用减号-来限定更改时间在距今n日以内的文件，而用加号+来限定更改时间在距今n日以前的文件。

(1) 搜索最近2天访问过的文件 $ find . -type f -atime -2
查找超过2天访问的文件(近两天没有访问过) $ find . -type f -atime +2
恰好2天前访问的文件 $ find . -type f -atime 2 

查找最近30分钟修改的当前目录下的.php文件 $ find . -name '*.php' -mmin -30


(2)查找访问时间超过10分钟的文件 $ find . -type f -amin +10
查找最近1小时内更改的所有文件 $ find . -cmin -60 #能查到权限变动等

(3)查找最后2-50天修改的文件 $ find . -type f -mtime +2 -mtime -50 | xargs ls -lth

(4)找出比test.cpp修改时间更长的文件 $ find . -type f -newer test.cpp
否定是!: $ find . ! -newer cc.txt

-newer file1 ! file2 查找更改时间比文件 file1 新但比文件 file2 旧的文件。
$ find . -newer aa.txt \! \( -newer cc.txt \) #结果不清不楚的


ubuntu测试结果符合描述：
$ ls -lth
-rw-rw-r-- 1 wangjl wangjl 334 Nov 15 18:57 unprecedented #新文件
...
-rw-rw-r-- 1 wangjl wangjl 326 Nov 15 17:20 backwards #老文件

# 查找创建日期介于 backwards 之后，unprecedented之前的文件，限定是文件(type)，限定只在当前文件夹内(maxdepth)找：
$ find /home/wangjl -maxdepth 1 -type f -newer backwards ! -newer unprecedented

# 移动这些文件到文件夹 word_ms 中：
$ find /home/wangjl -maxdepth 1 -type f -newer backwards ! -newer unprecedented -exec mv {} word_ms \;








3.根据文件权限(所属组，拥有者)查找。
(1)rwx权限
1)查找文件权限为777的文件 find . -type f -perm 777
2)查找文件权限不是644的文件 $ find . -type f ! -perm  644

3)查找只读文件 $ find / -type f -perm /u=r |head
$ find / -type f -perm /u=r 2>/dev/null/ #为什么过滤语句报错？//todo
4)查找可执行py文件 $ find . -type f -name "*.py" -perm /a=x

5)还有一种表达方法：在八进制数字前面要加一个横杠-，表示都匹配，如-007就相当于777，-006相当于666

# ls -l
-rwxrwxr-x   2 sam   adm        0 10月 31 01:01 http3.conf
-rw-rw-rw-   1 sam   adm    34890 10月 31 00:57 httpd1.conf
-rwxrwxr-x   2 sam   adm        0 10月 31 01:01 httpd.conf
drw-rw-rw-   2 gem   group   4096 10月 26 19:48 sam
-rw-rw-rw-   1 root  root    2792 10月 31 20:19 temp

# find . -perm 006
# find . -perm -006
./sam
./httpd1.conf
./temp

-perm mode:文件许可正好符合mode
-perm +mode:文件许可部分符合mode
-perm -mode: 文件许可完全符合mode



(2)ACL权限
1)查找所有SUID文件 $ find / -perm /u=s 2>/dev/null | head
2)查找权限为644的所有SGID为文件 $ find / -perm 2644
$ find / -perm 2644 2>/dev/null #过滤掉报错行，主要是 Permission denied
3)找到具有551权限的粘滞位文件(Sticky Bit) $ find / -perm 1551 2>/dev/null
4)查找所有SGID文件 $ find / -perm /g=s 2>/dev/null | head

(3)所有者
1)当前文件夹内，所有者为wangjl的文件或目录  $ find . -user wangjl
2)查找属于组root的所有文件 $ find ~ -group root
3)-nogroup 查找无有效所属组的文件，即该文件所属的组在 / etc/groups 中不存在。
$ find / -nogroup 2>/dev/null

4)-nouser 查找无有效属主的文件，即该文件的属主在 / etc/passwd 中不存在。
$ find / -nouser 2>/dev/null


为了查找属主帐户已经被删除的文件，可以使用-nouser选项。这样就能够找到那些属主在/etc/passwd文件中没有有效帐户的文件。在使用-nouser选项时，不必给出用户名； find命令能够为你完成相应的工作。

例如，希望在/home目录下查找所有的这类文件，可以用：
$ find /home -nouser -print








4.使用-exec或-ok执行其他shell命令。
任何形式的命令都可以在-exec选项中使用。
exec选项后面跟随着所要执行的命令或脚本，然后是一对儿{ }，一个空格和一个\，最后是一个分号。为了使用exec选项

(1)查找到拥有者为wangjl 的文件，并将它的拥有者改为hou 
$ find . -type f -user wangjl -exec chown hou {} \; #整个命令前加sudo才能执行

(2)查找所有以.txt文件结尾的文件，并把它写到all.txt文件中去
$ find . -type f -name "*.txt" -exec cat {} \; >all.txt
## cat: ./all.txt: input file is output file

(3)列出所有长度为0 
$ find . -empty
$ find . -type f -empty #查找空文件
$ find . -type d -empty #查找空白文件夹

(4)查找所有777个权限文件，并使用chmod命令将权限设置为644
$ find . -type f -perm 0777 -print -exec chmod 644 {} \;

(5)查找并删除1个或多个文件
查找和删除多个文件，如.mp3或.txt，然后使用。 
$ find -type f -name "*.py" -exec rm -f {} \;

记住：在shell中用任何方式删除文件之前，应当先查看相应的文件，一定要小心！当使用诸如mv或rm命令时，可以使用-exec选项的安全模式。它将在对每个匹配到的文件进行操作之前提示你。
$ find -type f -name "*.py" -ok rm -f {} \;

$ find . -maxdepth 1 -type f -name "*.py" -ok rm -f {} \;
< rm ... ./b.py > ? y
< rm ... ./a.py > ? n

按y键删除文件，按n键不删除，直接回车也不删除。


(6) find找到文件，使用grep搜索文件内容。
find命令首先匹配所有文件名为“ passwd*”的文件，例如passwd、passwd.old、passwd.bak，
然后执行grep命令看看在这些文件中是否存在一个sam用户。

$ find /etc -name "passwd*" -exec grep "sam" {} \; 2>/dev/null
sam:x:1002:1002::/usr/sam:/bin/bash













========================================
|-- find 命令各种疑难杂症
----------------------------------------
1. 删除匹配文件: 在查找命令后加上 -delete
删除以.txt结尾的文件 $ find . -type f -name "*.TXT" -delete


2. find命令-prune用法很严格！
-prune 使用这一选项可以使 find 命令不在当前指定的目录中查找，如果同时使用 - depth 选项，那么 - prune 将被 find 命令忽略。

#在当前目录下查找文件，但不希望在./text目录下查找
希望在/apps目录下查找文件，但不希望在/apps/bin目录下查找: $ find /apps -path "/apps/bin" -prune -o -print

$ find . -path ./text -prune -o -name "*.txt"
严格的用法：find 查找文件的目录 -path 需要排除的目录 -prune -o -name 需要查询的内容
注意事项：
1)-prune 必须和 -path， -o 一起使用
2）-prune -o 的顺序不 能调换
3）-name等必须放在-prune -o后面才能使用

如果不做名字筛选，也可以直接打印text目录外的所有文件： $ find . -path ./text -prune -o -print

-o 是or的意思。这里解释不通，why?//todo



3. 逻辑运算
(1)尝试解释： 避开多个文件夹
比如要在/usr/sam目录下查找不在dir1子目录之内的所有文件
$ find /usr/sam -path "/usr/sam/dir1" -prune -o -print

  find [-path ..] [expression] 在路径列表的后面的是表达式
-path "/usr/sam" -prune -o -print 是 -path "/usr/sam" -a -prune -o -print 的简写。
表达式按顺序求值, -a 和 -o 都是短路求值，与 shell 的 && 和 || 类似如果 -path "/usr/sam" 为真，则求值 -prune , -prune 返回真，与逻辑表达式为真；否则不求值 -prune，与逻辑表达式为假。
如果 -path "/usr/sam" -a -prune 为假，则求值 -print ，-print返回真，或逻辑表达式为真；否则不求值 -print，或逻辑表达式为真。

这个表达式组合特例可以用伪码写为
if -path "/usr/sam" then
    -prune
else
    -print
#


(2) 避开多个文件夹
find /usr/sam \( -path /usr/sam/dir1 -o -path /usr/sam/file1 \) -prune -o -print
圆括号表示表达式的结合。

\ 表示引用，即指示 shell 不对后面的字符作特殊解释，而留给 find 命令去解释其意义。
查找某一确定文件，-name等选项加在-o 之后

#find /usr/sam \(-path /usr/sam/dir1 -o -path /usr/sam/file1 \) -prune -o -name "temp" -print







4. find---xargs以及find--- -exec结合使用
xargs - build and execute command lines from standard input

在使用find命令的-exec选项处理匹配到的文件时， find命令将所有匹配到的文件一起传递给exec执行。但有些系统对能够传递给exec的命令长度有限制，这样在find命令运行几分钟之后，就会出现 溢出错误。错误信息通常是“参数列太长”或“参数列溢出”。这就是xargs命令的用处所在，特别是与find命令一起使用。

find命令把匹配到的文件传递给xargs命令，而xargs命令每次只获取一部分文件而不是全部，不像-exec选项那样。这样它可以先处理最先获取的一部分文件，然后是下一批，并如此继续下去。

在有些系统中，使用-exec选项会为处理每一个匹配到的文件而发起一个相应的进程，并非将匹配到的文件全部作为参数一次执行；这样在有些情况下就会出现进程过多，系统性能下降的问题，因而效率不高；

而使用xargs命令则只有一个进程。另外，在使用xargs命令时，究竟是一次获取所有的参数，还是分批取得参数，以及每一次获取参数的数目都会根据该命令的选项及系统内核中相应的可调参数来确定。

来看看xargs命令是如何同find命令一起使用的，并给出一些例子。


(1)下面的例子查找系统中的每一个普通文件，然后使用xargs命令来测试它们分别属于哪类文件

#find . -type f -print | xargs file
./num.txt:                                                 ASCII text
./.kde/Autostart/Autorun.desktop: UTF-8 Unicode English text
./.kde/Autostart/.directory:      ISO-8859 text\
......


(2)在整个系统中查找内存信息转储文件(core dump) ，然后把结果保存到/tmp/core.log 文件中：
$ find / -name "core" -print 2>/dev/null | xargs echo "" >/tmp/core.log

上面这个执行太慢，我改成在当前目录下查找
#find . -name "file*" -print 2>/dev/null | xargs echo "" > /temp/core.log
# cat /temp/core.log
./file6


(3)在当前目录下查找所有用户具有读、写和执行权限的文件，并收回相应的写权限：
# find . -perm -7 -print | xargs chmod o-w

(4)用grep命令在所有的普通文件中搜索hostname这个词：
# find . -type f -print 2>/dev/null | xargs grep "hostname" --color=auto

# 在当前目录下的所有普通文件中搜索hostnames：
# find . -name \* -type f -print 2>/dev/null | xargs grep "hostname" --color=auto


注意，在上面的例子中， \用来取消find命令中的*在shell中的特殊含义。

find命令配合使用exec和xargs可以使用户对所匹配到的文件执行几乎所有的命令。





refer:
https://blog.csdn.net/z_xiao_xue/article/details/53925680
https://www.cnblogs.com/Ido-911/p/9638612.html

linux下find---xargs以及find--- -exec结合使用: https://www.cnblogs.com/whiteprism/p/6599542.html




========================================
字符处理命令概述及资源
----------------------------------------
gerp 查找, awk 根据内容分析并处理, sed 编辑.
(1)grep, egrep, fgrep, rgrep - print lines matching a pattern 打印匹配模式的行。
(2)awk(关键字:分析&处理) 一行一行的分析处理。
mawk - pattern scanning and text processing language模式扫描与文本处理语言。
(3)sed(关键字: 编辑) 以行为单位的文本编辑工具 
sed - stream editor for filtering and transforming text 用于过滤和转换文本的流编辑器。特色是该编辑器能用于pipeline中。

awk和sed简明教程：
	http://agetouch.blog.163.com/blog/static/228535090201732824532207/

AWK 简明教程: http://coolshell.cn/articles/9070.html
sed 简明教程: http://coolshell.cn/articles/9104.html
三十分钟学会AWK: https://segmentfault.com/a/1190000007338373



[论坛]awk,sed,grep,cut等文本操作命令: http://bbs.chinaunix.net/forum-24-1.html


How to Use AWK
http://sparky.rice.edu/~hartigan/awk.html
http://sparky.rice.edu/~hartigan/sed.html


========================================
命令cut基于列处理文件
----------------------------------------

使用,	-	+等分割的数据文件的处理，按列显示。
cut -d: -f1 /ect/passwd
	-d	指定分割字符（默认是tab）
	-f	指定输出的列号
	-c	基于字符进行分割
		cut -c2-6 /etc/passwd


实例1:
$ grep wangjl /etc/passwd 
wangjl:x:1001:1002::/home/wangjl:/bin/bash


$ grep wangjl /etc/passwd | cut -d: -f3 #输出wangjl用户名对应的uid。
1001




实例: 获取长度为6-8的密码
$ date | md5sum | cut -c 1-7
008eb43

$ echo "who are you" | md5sum | cut -c 4-12
6d9239413






========================================
命令wc（word count）：统计并输出文件的字节数、字数、行数
----------------------------------------
$ wc cat.txt
  7  23 102 cat.txt
行数、单词数、字节数、文件名
	-l  统计行数
	-w  统计单词数
	-c  统计字节数
	-m 统计字符数

$ wc -l /etc/passwd #统计当前系统的用户数量
48 /etc/passwd


========================================
命令sort：用于对文本内容进行排序
----------------------------------------
1. 常用功能

$ cat out2.txt | sort -r #对指定文本文件的行，按照首字母的倒叙输出
$ sort out2.txt #或者直接使用sort，和上文输出结果相同。

	-r 进行倒序排序
	-n 基于数字进行排序，默认是基于ascii码排序
	-f 忽略大小写
	-u 删除重复行，该参数可替代 uniq 命令。
	-t c	使用c作为分隔符分割为列进行排序
	-k x 当进行基于指定字符分割为列的排序时，指定基于哪个列排序


$ sort /etc/passwd -t : -k 1 -r | head -n 5 #使用:分割，并使用第一列排序，倒叙。只显示前5行。
zhouxm:x:1006:1007:,,,:/home/zhouxm:/bin/bash
zhangd:x:1007:1008:,,,:/home/zhangd:/bin/bash
www-data:x:33:33:www-data:/var/www:/bin/sh
wuyh:x:1009:1010:,,,:/home/wuyh:/bin/bash
wuhj:x:1004:1005::/home/wuhj:/bin/bash













2. 更精细的控制

sort命令很强大、控制很细致，要点就是k参数的设置：
KEYDEF is F[.C][OPTS][,F[.C][OPTS]] for start and stop position, where F is a
field number and C a character position in the field; both are origin 1, and
the stop position defaults to the line's end. 

$ cat facebook.txt #定义一个测试文本
baidu 100 5000
google 110 5000
sohu 100 4500
guge 50 3000

$ sort -k 1.2r facebook.txt #按照第一个单词1的第二个字母2，r倒序排序
guge 50 3000
google 110 5000
sohu 100 4500
baidu 100 5000

$ sort -k 2 facebook.txt #按照第二列排序，默认是ascii顺序。注意50在最后。
sohu 100 4500
baidu 100 5000
google 110 5000
guge 50 3000

$ sort -k 2n facebook.txt #按照第二列排序，n指定为按照数字排序。
guge 50 3000
baidu 100 5000
sohu 100 4500
google 110 5000



#按照公司名字第二个字母正序，按照第二列倒序
$ sort -k 1.2 -k 2nr facebook.txt
baidu 100 5000
sohu 100 4500
google 110 5000
guge 50 3000
然而，google行并没有拍到sohu行前面，why？
因为默认是到结尾的，所以需要指定结尾，否则就变成了从第二个字母开始到最后的字符进行排序，当然oh在oo前面了。第一个参数就已经毫无争议的排好序了。
$ sort -k 1.2,1.2 -k 2nr facebook.txt
baidu 100 5000
google 110 5000
sohu 100 4500
guge 50 3000

这样仅仅指定第一列第二个字符排序，则google和sohu行谁在前还是有争议的。
$ sort -k 1.2,1.2 -k 2n facebook.txt #如果-k 2n不加倒序，则sohu行的100<110，跑到前面了。
baidu 100 5000
sohu 100 4500
google 110 5000
guge 50 3000

当然更精确的描述应该是：
$ sort -k 1.2,1.2 -k 2,2nr facebook.txt
baidu 100 5000
google 110 5000
sohu 100 4500
guge 50 3000


按照第二列去重
$ sort -k 2,2 -u facebook.txt 
baidu 100 5000
google 110 5000
guge 50 3000



(2) 按照第一列用户名排序，每个用户名只保留一次
注意： 
$ w | sed '1,2d' | sort -k 1 -u
caow     pts/16   10.21.100.150    11:02    4:16m  0.13s  0.13s -bash
chengww  pts/6    10.20.11.152     15:16    2:23   0.11s  0.11s -bash
hou      pts/10   10.20.89.188     Thu20   18:02m 17.84s 11.95s /usr/lib64/R/bin/exec/R
huangcy  pts/21   10.20.89.188     Tue10   17:40m  1.63s  1.63s -bash
huangcy  pts/22   10.20.89.188     Tue10    1:19m  0.18s  0.18s -bash
huangcy  pts/23   10.20.89.188     Tue10    3days  2:04   2:04  /data/jinwf/huangcy/software/Anaconda3/bin/python /data/jinwf/huangcy/software/Anaconda3/bin/jupyter-notebook
huangcy  pts/24   10.20.89.188     Tue10    2days  0.16s  0.16s -bash
...
# 这个命令做不到按第一列去重的，因为它指定了-k的start，没指定end，导致从第一列到最后一列都判断到，认为没有重复。


# 正确的做法是 -k 1,1 同时设置start和end的列编号。
$ w | sed '1,2d' | sort -k 1,1 -u
caow     pts/16   10.21.100.150    11:02    4:14m  0.13s  0.13s -bash
chengww  pts/6    10.20.11.152     15:16   21.00s  0.11s  0.11s -bash
hou      pts/10   10.20.89.188     Thu20   18:00m 17.83s 11.94s /usr/lib64/R/bin/exec/R
huangcy  pts/21   10.20.89.188     Tue10   17:38m  1.63s  1.63s -bash
niesy    pts/1    10.20.89.255     Thu09    0.00s  2.94s  2.94s -bash
wangjl   pts/37   10.21.9.239      Thu15    0.00s  3.80s  0.03s w
wangw    pts/12   10.20.1.45       Tue09   23:07m 19:27  19:26  top -u wangw
wangxf   pts/0    10.20.5.167      09:06    6:07m  0.10s  0.00s tmux a -t 0
yangwl   pts/9    10.20.4.17       10:30    4:29m  0.22s  0.02s less barcodes.tsv
zhouwg   pts/18   10.20.63.188     14:55   21:29   0.08s  0.08s -bash









更多讨论见 生物慕课 公众号 2018.4.20: sort文本排序命令。


========================================
命令uniq: 用于删除相邻的重复行(去掉重复项，输出重复项)
----------------------------------------
新建文本test2.txt:
$ cat >test2.txt
good
bad
good
better

$ sort test2.txt -u #删除重复行
bad
better
good

$ cat test2.txt | uniq #删除相邻的重复行，不相邻删不掉
good
bad
good
better

$ cat test2.txt |sort|uniq #删除相邻的重复行
bad
better
good


如果想输出重复项，使用以下命令：
$ cat test2.txt | sort | uniq -d
good

想统计重复项的重复次数-c
$ cat test2.txt | sort | uniq -dc
      2 good


统计每个字符出现的次数
$ cat test2.txt | sort | uniq -c
      1 bad
      1 better
      2 good


========================================
linux文件里的内容按列对齐整齐输出: column -t
----------------------------------------
column -t 

1. 基础案例: 默认是使用空格作为分隔符

只用head，则各个列没对齐
$ head macs2_result/total_peaks.narrowPeak
chr1    9939    10249   total_peak_1    215     .       15.5844 30.4403 21.5529 155
chr1    180654  180854  total_peak_2    61      .       7.5     11.9286 6.10164 111
chr1    3477235 3477437 total_peak_3    101     .       9.83607 16.6594 10.1581 100
chr1    53348062        53348262        total_peak_4    74      .       8.33333 13.6284 7.46158 103
chr1    196980518       196980718       total_peak_5    56      .       7.01754 11.4905 5.69445 99


对齐列
$ head macs2_result/total_peaks.narrowPeak | column -t 
chr1   9939       10249      total_peak_1   215  .  15.5844  30.4403  21.5529  155
chr1   180654     180854     total_peak_2   61   .  7.5      11.9286  6.10164  111
chr1   3477235    3477437    total_peak_3   101  .  9.83607  16.6594  10.1581  100
chr1   53348062   53348262   total_peak_4   74   .  8.33333  13.6284  7.46158  103
chr1   196980518  196980718  total_peak_5   56   .  7.01754  11.4905  5.69445  99





2. 高级案例： 使用 tab 作为分隔符呢？
https://unix.stackexchange.com/questions/57222/how-can-i-use-column-to-delimit-on-tabs-and-not-spaces

原始表格，各种错位。
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation 
Intergenic      chr1    1       10873   +       N       1900000000
promoter-TSS (NR_046018)        chr1    10874   11974   +       P       1
non-coding (NR_046018, exon 1 of 3)     chr1    11975   12227   +       pseudo  165001

尝试 column -t -s '\t' 竟然是使用 字母t 作为分隔符。。。
要怎么打一个真正的tab分隔符呢？

(1) column -ts $'\t' 
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation | column -t -s $'\t'
Intergenic                           chr1  1      10873  +  N       1900000000
promoter-TSS (NR_046018)             chr1  10874  11974  +  P       1
non-coding (NR_046018, exon 1 of 3)  chr1  11975  12227  +  pseudo  165001


(2) 或者 column -ts "$(printf '\t')"
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation | column -ts "$(printf '\t')"
输出整齐同上。

(3) $(printf '\011') can be used, as 011 (octal representation of decimal 9) is the ANSI code for a horizontal tab character:
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation | column -t -s "$(printf '\011')"
输出整齐同上。


(4) 迂回策略，先tr替换为没出现过的单字符，再在 column 中-s 指定分隔符
$ head /home/wangjl/soft/homer/data/genomes/hg38/hg38.basic.annotation | tr '\t' '|' | column -t -s '|'
输出整齐同上。





========================================
命令diff：用于两个文本的比较
----------------------------------------
$ diff a.txt a-new.txt
	-i 忽略大小写
	-b 忽略空格数量的改变
	-u 统一显示比较信息（一般用以生成patch文件）
		diff -u a.txt a-new.txt > final.patch

$ diff out2.txt out2-new.txt 
16a17  #append新增一行
> www
21d21 #delete删除一行
< tmp
24c24	#change修改一行。
< vmlinuz	#前一个文件
---
> vmlinux	#后一个文件


增加参数-u之后：
$ diff -u out2.txt out2-new.txt > final2.patch

$ cat final2.patch 
--- out2.txt	2016-06-10 19:46:54.647591447 -0700
+++ out2-new.txt	2016-06-10 23:13:34.459199288 -0700
@@ -14,11 +14,11 @@
 proc
 root
 run
+www
 sbin
 selinux
 srv
 sys
-tmp
 usr
 var
-vmlinuz
+vmlinux

用于制作补丁。



========================================
命令aspell用以显示检查英文拼写
----------------------------------------
$ aspell list < out3.txt #列出错误信息
cannnot
nowhera
$ aspell check out3.txt #交互式改单词




========================================
命令tr (translate)：处理文本内容（删除、替换），重定向接收文件
----------------------------------------
$ cat out3.txt
ls: cannot accesses nowhere: No such file or directory


删除关键字
	$ tr -d 'TMD' < aa.txt
转换大小写
	$ tr 'a-z' 'A-Z' < aa.txt

$ tr -d 'No' <out3.txt
ls: cannt accesses nwhere:  such file r directry

$ cat out3.txt | tr 'a-z' 'A-Z' > out33.txt 
$ cat out33.txt
$ rm out33.txt 
或者
$ tr 'a-z' 'A-Z' <out3.txt
或者
$  tr 'a-z' 'A-Z' <out3.txt  > out33.txt 
$ cat out33.txt 
LS: CANNOT ACCESSES NOWHERE: NO SUCH FILE OR DIRECTORY

$ cat out3.txt
ls: cannot accesses nowhere: No such file or directory

并没有影响原文！如果想改原文，怎么办？重定向，但是不能重定向到原文件！








========================================
按列合并2个文件: paste /join 
----------------------------------------

1. 简单合并 paste file1 file2 ...
例子:
$ cat 1.txt
1 a good
2 d good
3 c bad

$ cat 2.txt 
a 100
b 200
c 300

$ paste 1.txt 2.txt
1 a good        a 100
2 d good        b 200
3 c bad c 300

默认的分隔符是制表符，也可以用-d指定分隔符：
$ paste -d '-' 1.txt 2.txt
1 a good-a 100
2 d good-b 200
3 c bad-c 300





2. join 按照某一列合并 (要先按照该列sort)
$ sort -k 2 1.txt >1_.txt
$  cat 1_.txt 
1 a good
3 c bad
2 d good


按照第一个文件的第2列，和第二个文件的第1列，合并。
该共有列输出为第一列，只输出2个文件中能匹配上的行。
$ join -1 2 1_.txt -2 1 2.txt
a 1 good 100
c 3 bad 300



















========================================
grep 查找与正则表达式
----------------------------------------
正则规则：http://blog.csdn.net/newthinker_wei/article/details/8219293

常用快查：grep命令 - 基于关键字搜索文本
$ grep 'wangjl' /etc/passwd #查找/etc/passwd文件中包含'wangjl'关键字的行
$ find / -user wangjl 2>/dev/null | grep Video	#查找根目录下用户名user为wangjl，且包含Video关键字的文件名。
$ find / -user wangjl 2>&1 | grep Video #或者把标准错误合并到标准输出，然后管道到grep去搜索。

	-i 在搜索的时候忽略大小写
	-n 显示结果所在行数
	-v 输出不带关键字的行，相当于取反操作
	-Ax 在输出的时候包含结果所在行之后的指定行数
	-Bx 在输出的时候包含结果所在行之前的指定行数

$ find / -user wangjl 2>&1 | grep Video -i #忽略大小写
$ grep wang /etc/passwd #在文件/etc/passwd中输出匹配关键字wang的行
$ grep wang /etc/passwd -n #显示行号



可以在grep中使用perl语法，加-P参数。grep -E它是相同的。egrep不会起作用（这将是贪婪匹配的）。
* (0 or more) greedy matching
+ (1 or more) greedy matching
*? (0 or more) non-greedy matching
+? (1 or more) non-greedy matching

for VIM:
* (0 or more) greedy matching
\+ (1 or more) greedy matching
\{-} (0 or more) non-greedy matching
\{-n,} (at least n) non-greedy matching



1.grep(关键字: 截取) 文本搜集工具, 结合正则表达式非常强大。
一般格式： grep 正则 fileName
比如： $ grep 'wangjl' /etc/passwd #查找并返回passwd文件中包含wangjl的行。

主要参数 []
	-c : 只输出匹配的行
	-I : 不区分大小写
	-h : 查询多文件时不显示文件名
	-l : 查询多文件时, 只输出包含匹配字符的文件名
	-n : 显示匹配的行号及行
	-v : 显示不包含匹配文本的所有行
	
基本工作方式: grep 要匹配的内容 文件名
例如:
grep 'test' d* 显示所有以d开头的文件中包含test的行
grep 'test' aa bb cc 显示在 aa bb cc 文件中包含test的行
grep '[a-z]\{5}\' aa 显示所有包含字符串至少有5个连续小写字母的串


2.正则表达式分类：
1)基本的正则表达式（Basic Regular Expression 又叫 Basic RegEx  简称 BREs）
2)扩展的正则表达式 -E（Extended Regular Expression 又叫 Extended RegEx 简称 EREs）
3)Perl 的正则表达式 -P（Perl Regular Expression 又叫 Perl RegEx 简称 PREs）
 说明：只有掌握了正则表达式，才能全面地掌握 Linux 下的常用文本工具（例如：grep、egrep、GUN sed、 Awk 等） 的用法
注意：linux的正则支持有三种，其中perl兼容的是功能最强大的。
默认的是基本的正则，不支持数字元字符，但是加上-P指定Perl兼容的正则，就支持数字元字符了。

$ grep -P '\d' /etc/passwd #任何带数字的行
		\d: 任何数字 [0-9] grep默认不支持
		\D: 任何非数字[^0-9] grep默认不支持



3.正则表达式的数量词
正则表达式可能被以下重复修饰符中的一个修饰：
    ?      {0,1}The preceding item is optional and matched at most once.
    *      {0,}The preceding item will be matched zero or more times.
    +      {1,}The preceding item will be matched one or more times.
    {n}    The preceding item is matched exactly n times.
    {n,}   The preceding item is matched n or more times.
    {,m}   The  preceding  item  is matched at most m times.  This is a GNU extension.
    {n,m}  The preceding item is matched at least n  times,  but  not  more than m times.
比如：$ grep 'ro*t' /etc/passwd   # {0,}表示0个或多个o。


$ cd /etc
$ grep 'ro+t' passwd   #啥也没有
$ grep -P 'ro+t' passwd #使用Perl兼容的可以-P
root:x:0:0:root:/root:/bin/bash

$ grep 'ro\+t' passwd #或者使用转义字符\
root:x:0:0:root:/root:/bin/bash


4.正则的分组()
使用小括号()，默认需要加转义字符\(text\)
如果加入-P参数，则可以直接输入(text)

$ grep -P '(34)+' passwd #带有1个及以上34的行



5.范围
[]表示范围，比如
	[0-9]表示任意一位数字，等价于\d
	[a-z]任意一个小写字母，
	[A-Z]任意大写字母，
	[a-zA-Z]任意字母
[]内的^表示否定。
$ grep -P "[0-9]{4,6}" passwd # 含4到6个数字的行
$ grep -P "[^0-9]" passwd #匹配含有非数字的行
$ grep -P "[^0-9a-zA-Z]" passwd #匹配含有非数字、非大小写字母的行





6.位置界定^$
 ^首位
 $结尾
$ grep '^c.*e$' passwd #找c开头，e结尾的行




7.任意字符串 .* # .在[]外表示任意字符，*表示前面字符任意多个，加起来.*就是任意字符串了。
$ grep -P '^r.*' passwd #r开头，后面跟着任意字符
$ grep -P 'm.*c' passwd #m后面有c的行，默认贪婪匹配，匹配尽量后的c

如果想使用非贪婪匹配，找到最近的c就停止匹配，可以首尾加上分隔符\b 
$ grep -P '\bm.*c\b' passwd





8.贪婪匹配
更好的非贪婪匹配是量词后加上?
当"?"字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，“o+?”将匹配单个“o”，而“o?”将匹配所有“o”。

注：非贪婪只对量词有效果，如 * + {1，9} 等。
默认是贪婪模式；在量词后面直接加上一个问号？就是非贪婪模式。

$ grep -P 'm.*?c' passwd #从每行的m开始，找到第一个c停止
$ echo 12,23,24|grep -P '([0-9]\d{1,})' 
$ echo 12,23,24|grep -P '([0-9]\d{1,}?)' 



9.利用grep打印匹配的上下几行
https://www.cnblogs.com/wangkongming/p/3684905.html

如果在只是想匹配模式的上下几行，grep可以实现。
$ grep -5 'parttern' inputfile   //打印匹配行的前后5行
$ grep -C 5 'parttern' inputfile //打印匹配行的前后5行

$ grep -A 5 'parttern' inputfile //打印匹配行的后5行
$ grep -B 5 'parttern' inputfile //打印匹配行的前5行




10.设置centOS下的grep高亮显示匹配项（ubuntu默认已经高亮）
方法1：设置别名
编辑 vim ~/.bashrc
添加如下一行内容：
alias grep='grep --color=auto'
source ~/.bashrc    //使配置生效；

方法2：
https://www.linuxidc.com/Linux/2014-09/106871.htm


11. 逻辑运算
$ grep pattern1 filename | grep pattern2  #"与"操作（其实就是多次筛选）。显示既匹配 pattern1又匹配 pattern2 的行。

$ grep -E 'k1|k2' filename #或 （如果没有空格，则可以不加引号）
$ grep 'pattern1\|pattern2' filename

$ grep -v "k1" filename #非, 不满足当前条件的所有内容行.


// 找出文件（filename）中包含123或者包含abc的行
grep -E '123|abc'filename
egrep '123|abc' filename   // 用egrep同样可以实现. egrep相当于grep -E。
awk '/123|abc/' filename  // awk 的实现方式




100.其他细节
\s	匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]。
\S	匹配任何非空白字符。等价于[^ \f\n\r\t\v]。










========================================
awk(关键字:分析&处理) 一行一行的分析处理
----------------------------------------
gawk是awk的增强版，如果有些教程中的awk命令不能运行，请考虑安装gawk。
$ sudo apt install gawk



最好的学习资料非《The Awk Programming Language》不可，这本书是Awk 的创始人，也《The C Programming Language》的作者，老头子文笔非常之好，非常简单的语言，把问题解释的非常清楚，要让我选诺贝尔文学奖获得者，我一定会第一给他投票，老头子实在是太NB了，他的其他书《The Practice of Programming》和《Unix 编程环境》也很不错，强烈推荐。除此之外，还要想看更多Awk例子的，可以看看《Effective Awk Programming》， 千万别看什么《Sed && Awk》，全是废话，垃圾书。
或者web的 https://www.theunixschool.com/p/awk-sed.html




1.定义与作用
mawk - pattern scanning and text processing language模式扫描与文本处理语言。
An AWK program is a sequence of pattern {action} pairs and user function definitions.

典型用途：使用AWK可以做很多任务，下面是其中一些
	文本处理
	输出格式化的文本报表
	执行算数运算
	执行字符串操作等等

(2)Awk 程序的结构：awk 'BEGIN{ print "start" } pattern { commands } END{ print "end" }  file

Awk程序由3部分组成：BEGIN语句块，END语句块，和能够使用模式（正则表达式/关系表达式）匹配的通用语句，这三部分中的任何一部分都可以省略。



2.工作流

BEGIN语句块在程序最开始执行，通常完成一些初始化的工作，END语句块在程序的最后执行，通常在最后格式化输出结果。
模式部分的工作原理如下：首先获取一行，检查该行是否与提供的样式匹配，如果匹配就执行与该样式对应的{}中的语句。



要成为AWK编程专家，你需要先知道它的内部实现机制，AWK遵循了非常简单的工作流 - 读取，执行和重复，下图描述了AWK的工作流。
(1)BEGIN block: BEGIN {awk-commands} #可选
(2)Read a line from input stream
(3)Execute AWK commands on a line: /pattern/ {awk-commands}
(4)Repeat if it is not End of file,GOTO(2)
(5)END block: END {awk-commands} #可选

awk的处理流程是:
1) 读第一行, 将第一行资料填入变量 $0, $1... 等变量中
2) 依据条件限制, 执行动作
3) 接下来执行下一行









3.一般格式：$ awk 'pattern1{动作1}pattern2{动作2}' filename
	awk 也可以读取来自前一个指令的 standard input。
	相对于sed常常用于一整行处理, awk则比较倾向于一行当中分成数个"字段"(区域)来处理, 默认的分隔符是空格键或tab键

示例文件marks.txt
$ cat marks.txt
1)  Amit    Physics  80
2)  Rahul   Maths    90
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89


示例文件netstat.txt。
$ netstat -na |head -n 16 > netstat.txt
$ ls #用vim删掉第一行
netstat.txt
$ cat netstat.txt
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp        0     36 172.16.112.86:22        172.16.113.174:55033    ESTABLISHED
tcp        0      0 172.16.112.86:22        172.16.113.174:56000    ESTABLISHED
tcp6       0      0 :::22                   :::*                    LISTEN
tcp6       0      0 :::3306                 :::*                    LISTEN
udp        0      0 127.0.1.1:53            0.0.0.0:*
udp        0      0 0.0.0.0:68              0.0.0.0:*
udp        0      0 0.0.0.0:631             0.0.0.0:*
udp        0      0 0.0.0.0:55418           0.0.0.0:*
udp        0      0 0.0.0.0:5353            0.0.0.0:*
udp6       0      0 :::58832                :::*
udp6       0      0 :::5353                 :::*
raw6       0      0 :::58                   :::*                    7



(1)可以用 $ awk '{print}' marks.txt 来代替cat命令，查看文件内容。

awk可以接受标准输入的数据，例如:$ last -n 5 | awk '{print $1 "\t" $3}'  #输入最后5次登陆，只显示用户名和ip。
这里大括号内$1"\t"$3 之间不加空格也可以, 不过最好还是加上个空格, 另外注意"\t"是有双引号的, 因为本身这些内容都在单引号内
$0 代表整行 $1代表第一个区域, 依此类推

#打印第3和4列，用制表符隔开:
$ awk '{print $3 "\t" $4}' marks.txt
Physics 80
Maths   90
Biology 87
English 85
History 89

所以, AWK一次处理是一行, 而一次中处理的最小单位是一个区域。
比如: 一行中带有a的，打印整行。
$ awk '/a/{print $0}' marks.txt 可以简化为
$ awk '/a/{print}' marks.txt 可以进一步简化为
$ awk '/a/' marks.txt 没有制定操作，就是默认打印匹配到的整行。
2)  Rahul   Maths    90
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89



(2)逻辑判断 > < >= <= == !== , 赋值直接使用=

另外还有3个变量, 
	NF: 当前行的第几个字段(number of fields in the current record). 每个字段保存在$1, $2, ..., $NF中.  The built-in variable NF is set to the number of fields.
	NR 目前处理到第几行(current record number in the total input stream).
	FS 目前的分隔符(input record separator, initially = "\n").

	
例1 $ cat /etc/passwd | awk '{FS=":"} $3<10 {print $1 "\t" $3}'
首先定义分隔符为:, 
然后判断, 注意看, 判断没有写在{}中, 
然后执行动作, 
FS=":"这是一个动作, 赋值动作, 不是一个判断, 所以写在{}中

$  awk  'BEGIN{FS=":"} {print $1,$3,$6}' /etc/passwd
也等价于：（-F的意思就是指定分隔符）
$ awk  -F: '{print $1,$3,$6}' /etc/passwd

注：如果你要指定多个分隔符，你可以这样来：
awk -F '[;:]'




例2 $ awk '/linux/ {print NR}' out.txt #将带有linux的行的行号打印出来, 注意//之间可以使用正则表达式
$ cat out.txt
this is a linux system.
line2
Ubuntu is a release of linux.
$ awk '/linux/ {print NR}' out.txt
1
3


例3：&&并的使用。显示第3列是0且第6列是LISTEN的行。
$ awk '$3==0 && $6=="LISTEN"' netstat.txt
tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
tcp6       0      0 :::3306                 :::*                    LISTEN

如果需要保留表头，可以引入内建变量NR：
$ awk '$3==0 && $6=="LISTEN" || NR==1' netstat.txt
Active Internet connections (servers and established)
tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
tcp6       0      0 :::3306                 :::*                    LISTEN




(3) BEGIN END , 给程序员一个初始化和收尾的工作

BEGIN之后列出的操作在{}内将在awk开始扫描输入之前执行, 而END{}内的操作, 将在扫描完输入文件后执行.

例1：通过BEGIN给文本marks.txt添加表头
$ awk 'BEGIN{printf "Sr No\tName\tSub\tMarks\n"}{print}' marks.txt
Sr No   Name    Sub     Marks
1)  Amit    Physics  80
2)  Rahul   Maths    90
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89

例2： prints all lines that start with an AWK identifier.打印开头符合identifier的行。
$ head /etc/passwd | awk 'BEGIN { identifier = "ro*t" } $0 ~ "^" identifier' /etc/passwd 
（r开头，后面若干个0，接着是t的）


-v 变量赋值选项：该选项将一个值赋予一个变量，它会在程序开始之前进行赋值。
例3：$ awk -v name=Jimmy 'BEGIN{printf "Name = %s\n", name}'
Name=Jimmy




(4)条件、循环控制语句，及从文件输入pattern。

awk {}内, 可以使用 if else ,for(i=0;i<10;i++), i=1 while(i<NF)
可见, awk的很多用法都等同于C语言, 比如"\t" 分隔符, print的格式, if, while, for 等等。


## 注意：每个句子结束要使用分号
awk '{if ($1==1) print "A"; else if ($1==2) print "B"; else print "C"}'

例：提取RNA的末端坐标，+链则需要3端，-链需要5端
chr10	101688	101706	E00300:157:HWCKKCCXX:6:2220:10094:68096	255	-
chr10	112858	112912	E00300:165:H3CMMALXX:8:2219:25997:49689	255	+
chr10	118384	118494	E00300:165:H3CMMALXX:5:2105:10987:37471	255	+

$ head 225.bed |awk '{if($6=="+") print $1":"$3":"$6; else print $1":"$2":"$6}'
chr10:101688:-
chr10:112912:+
chr10:118494:+





例：计数文件中独特单词的个数。count the number of unique "real words".

1)把模式写入文件$ cat cmd.awk
    BEGIN { FS = "[^A-Za-z]+" }
    { for(i = 1 ; i <= NF ; i++)  word[$i] = "" }
    END { delete word[""]
          for ( i in word )  cnt++
          print cnt
    }

$ cat out.txt
this is a linux system.
line2
Ubuntu is a release of linux.

2)对文件out.txt使用以上规则计数
$ awk -f cmd.awk out.txt #-f表示从文件中读取pattern。
9


(5)运算符

算术运算符：+-*/%
# awk 'BEGIN {a=50;b=20;print "(a+b)=",(a+b)}'
(a+b)= 70
# awk 'BEGIN {a=50;b=20;print "(a%b)=",(a%b)}'
(a%b)= 10 #取余数

自增自减与C语言一致。
# awk 'BEGIN { a = 10; b = a--; printf "a = %d, b = %d\n", a, b }'
# 猜一下a和b分别是多少？

赋值操作符有+=, -=, *=, /=, %=,乘方符^=,**=(这个不支持)
# awk 'BEGIN{a=2;a^=10;printf "a=%d\n",a}'
a=1024

关系操作符>,<,>=,<=,==,!=
# awk 'BEGIN{a=10; b=10; if(a==b)print"a==b"}'
a==b



逻辑操作符 或(||),且(&&)非,(!)
打印第2到4行
# awk '{if (NR >= 2 && NR <= 4) printf "line%d:%s\n", NR,$0}' marks.txt
line2:2)  Rahul   Maths    90
line3:3)  Shyam   Biology  87
line4:4)  Kedar   English  85

三元操作符
# awk 'BEGIN{a=10;b=2;max=(a>b)?a:b;print "max=",max;}'
max= 10

字符串连接，就是用空格连接字符串变量。
# awk 'BEGIN{a="Hello,"; b="world";c=a b;print c;}'
Hello,world

数组成员操作符
# awk 'BEGIN{arr[0]=10;arr[1]=11;arr[2]=12; for(i in arr) printf "arr[%d]=%d\n",i,arr[i]}'
arr[0]=10
arr[1]=11
arr[2]=12







4.awk正则表达式
正则表达式操作符使用 ~ 和 !~ 分别代表匹配和不匹配。
也可以按字段匹配。~ 表示模式开始。/ /中是模式，表达式前后添加反斜线，与js类似吧
# tail -n 40 /var/log/nginx/access.log | awk '$0 ~ /ip\[127\.0\.0\.1\]/'
# tail -n 40 /var/log/auth.log | awk '/wangjl/'

awk可以像grep一样的去匹配每一行，就像这样：
例1：awk '/LISTEN/' netstat.txt
tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
tcp6       0      0 :::3306                 :::*                    LISTEN



例2： 第六个字段符合正则的行
$ awk '$6 ~ /ESTA/ || NR==1 {print NR,$4,$5,$6}' OFS=";" netstat.txt
1;Local;Address;Foreign
4;172.16.112.86:22;172.16.113.174:55033;ESTABLISHED
5;172.16.112.86:22;172.16.113.174:56000;ESTABLISHED

例3： LISTEN 状态的行
$ netstat -an| awk '$6 ~ /LISTEN/ ||NR==2 {print NR,$4,$5,$6}' OFS="\t"
2       Local   Address Foreign
3       127.0.1.1:53    0.0.0.0:*       LISTEN
4       0.0.0.0:22      0.0.0.0:*       LISTEN
7       :::22   :::*    LISTEN
8       :::3306 :::*    LISTEN

例4：可以使用 “/LISTEN|ESTABLISHED/” 来匹配 LISTEN 或者 ESTABLISHED :
$ netstat -an| awk '$6 ~ /LISTEN|ESTABLISHED/ ||NR==2 {print NR,$4,$5,$6}' OFS="\t"

例5：取反
p$ awk '$6 !~ /LISTEN|ESTABLISHED/ ||NR==1 {print NR,$4,$5,$6}' OFS="\t" netstat.txt
1       Local   Address Foreign
8       127.0.1.1:53    0.0.0.0:*

或者$ awk '!/LISTEN/' netstat.txt








5.内置变量与自定义变量
(1)内置变量参考[$ man awk 的 7. Builtin-variables]

内置变量Builtin-variables
The following variables are built-in and initialized before program execution.程序执行前初始化
	ARGC      number of command line arguments.
	ARGV      array of command line arguments, 0..ARGC-1.
	CONVFMT   format for internal conversion of numbers to string, initially = "%.6g".
	ENVIRON   array  indexed  by  environment  variables.  An environment string, var=value is stored as
			ENVIRON[var] = value.
	FILENAME  当前文件名 name of the current input file.
	FNR       文件内自己的行号 current record number in FILENAME.
	FS        字段分隔符 splits records into fields as a regular expression. 域分割符，相当于sort 命令和cut 命令中的－d选项
	NF        当前行字段数 number of fields in the current record.
	NR        当前记录数 current record number in the total input stream. 在执行过程中，相当于当前行号
	OFMT      format for printing numbers; initially = "%.6g".
	OFS       显示时字段分割符 inserted between fields on output, initially = " ". 输出时，域分割符，默认是”\t”
	ORS       terminates each record on output, initially = "\n".
	RLENGTH   length set by the last call to the built-in function, match().
	RS        记录分隔符，默认是换行符 input record separator, initially = "\n".
	RSTART    index set by the last call to match().
	SUBSEP    used to build multiple array subscripts, initially = "\034".


例1: 内置变量 FILENAME 表示当前文件名：
$ awk 'END {print FILENAME}' marks.txt
marks.txt

例2: OFS指定显示分隔符
$ awk  -F: '{print $1,$3,$6}' OFS=";" /etc/passwd | head
root;0;/root
daemon;1;/usr/sbin
bin;2;/bin
sys;3;/dev
sync;4;/bin



(2)
自定义变量统计含a的行数：
$ awk '/a/{++cnt} END {print "Count=", cnt}' marks.txt
Count= 4

使用语句打印出来哪些行含有r：
$ awk '/r/{++cnt; print} END {print "Count=", cnt}' marks.txt
4)  Kedar   English  85
5)  Hari    History  89
Count= 2








6.内置函数、自定义函数与系统命令
AWK提供了很多方便的内建函数供编程人员使用。最好先知道大概，使用的时候再查手册。
https://www.gnu.org/software/gawk/manual/gawk.html#Built_002din
内置函数： http://www.cnblogs.com/chengmo/archive/2010/10/08/1845913.html

数学函数
	atan2(y, x)
	cos(expr)
	exp(expr)
	int(expr)
	log(expr)
	rand
	sin(expr)
	sqrt(expr)
	srand([expr])

字符串函数
	asort(arr [, d [, how] ])
	asorti(arr [, d [, how] ])
	gsub(regex, sub, string)
	index(str, sub)
	length(str)
	match(str, regex)
	split(str, arr, regex)
	sprintf(format, expr-list)
	strtonum(str)
	sub(regex, sub, string)
	substr(str, start, l)
	tolower(str)
	toupper(str)

时间函数
	systime
	mktime(datespec)
	strftime([format [, timestamp[, utc-flag]]])

字节操作函数
	and
	compl
	lshift
	rshift
	or
	xor

其它
	close(expr) 关闭管道文件


例1: 使用command | getline var可以实现将命令的输出写入到变量var。
$ awk 'BEGIN {
     "date" | getline current_time
     close("date")
     print "Report printed on " current_time
}'

只能读取一行，其中command是linux命令。
$ awk 'BEGIN{"ls"|getline txts; close("ls"); print txts}'
Desktop


函数是程序基本的组成部分，AWK允许我们自己创建自定义的函数。一个大型的程序可以被划分为多个函数，每个函数之间可以独立的开发和测试，提供可重用的代码。

下面是用户自定义函数的基本语法
function function_name(argument1, argument2, ...) { 
   function body
}
return 用于用户自定义函数的返回值。


例2:自定义加法
首先，创建一个functions.awk文件，包含下面的awk命令
$ cat awk_2.cmd
function addition(num1, num2) {
   result = num1 + num2
   return result
}
BEGIN {
   res = addition(10, 20)
   print "10 + 20 = " res
}

$ awk -f awk_2.cmd
10 + 20 = 30


例3:执行系统命令
system函数用于执行操作系统命令并且返回命令的退出码到awk。
END {
     system("date | mail -s 'awk run done' root")
}

$ awk 'BEGIN{system("date")}'
2017年 08月 12日 星期六 06:37:36 CST
$ awk 'BEGIN{system("date | tr [A-Z] [a-z]")}'
2017年 08月 12日 星期六 06:37:50 cst











7.文件操作：拆分文件、重定向与管道

awk拆分文件很简单，使用重定向就好了。
重定向操作符跟在print和printf函数的后面，与shell中的用法基本一致。
	print DATA > output-file #覆盖式写入
	print DATA >> output-file #追加到文件结尾

例如，下面两条命令输出是一致的
$ echo "Hello, World !!!" > /tmp/message.txt
$ awk 'BEGIN { print "Hello, World !!!" > "/tmp/message.txt" }'

例1：按第6例分隔文件，相当的简单（其中的NR!=1表示不处理表头,$6!=""表示如果第六行是空则忽略掉）。
$ awk 'NR!=1 && $6!="" {print>$6}' netstat.txt
$ ls
7  ESTABLISHED  LISTEN  marks.txt  netstat.txt


例2：也可以把指定的列输出到文件：
$ ls
marks.txt  netstat.txt
$ awk 'NR!=1 && $6!="" {print $4,$5 > $6}' netstat.txt
$ ls
7  ESTABLISHED  LISTEN  marks.txt  netstat.txt
$ cat LISTEN
127.0.1.1:53 0.0.0.0:*
0.0.0.0:22 0.0.0.0:*
:::22 :::*
:::3306 :::*



例3:再复杂一点：（注意其中的if-else-if语句，可见awk其实是个脚本解释器）
$ ls
marks.txt  netstat.txt
$ awk 'NR!=1{
	if($6 ~ /TIME|ESTABLISHED/) print > "1.txt";
	else if($6 ~ /LISTEN/) print > "2.txt";
	else print > "3.txt" 
}' netstat.txt
$ ls
1.txt  2.txt  3.txt  marks.txt  netstat.txt
$ cat 3.txt
udp        0      0 127.0.1.1:53            0.0.0.0:*
udp        0      0 0.0.0.0:68              0.0.0.0:*
udp        0      0 0.0.0.0:631             0.0.0.0:*
udp        0      0 0.0.0.0:55418           0.0.0.0:*
udp        0      0 0.0.0.0:5353            0.0.0.0:*
udp6       0      0 :::58832                :::*
udp6       0      0 :::5353                 :::*
raw6       0      0 :::58                   :::*   



例4:管道
除了将输出重定向到文件之外，我们还可以将输出重定向到其它程序，与shell中一样，我们可以使用管道操作符|。
r$ echo "hello"|tr [a-z] [A-Z]
HELLO
$ awk 'BEGIN { print "hello, world !!!" | "tr [a-z] [A-Z]" }'
HELLO, WORLD !!!


例5:双向连接
AWK中可以使用|&进行双向连接，那么什么是双向连接呢？一种常见的场景是我们发送数据到另一个程序处理，然后读取处理结果，这种场景下就需要打开一个到另外一个进程的双向管道了。第二个进程会与gawk程序并行执行，这里称其为 协作进程。与单向连接使用|操作符不同的是，双向连接使用|&操作符。

do {
    print data |& "subprogram"
    "subprogram" |& getline results
} while (data left to process)
close("subprogram")

第一次I/O操作使用了|&操作符，gawk会创建一个到运行其它程序的子进程的双向管道，print的输出被写入到了subprogram的标准输入，而这个subprogram的标准输出在gawk中使用getline函数进行读取。





8.统计
例1：下面的命令计算所有的C文件，CPP文件和H文件的文件大小总和。
$ ls -l  *.cpp *.c *.h | awk '{sum+=$5} END {print sum}'

# ls -l *.zip
-rw-rw-r-- 1 wangjl wangjl 25072402 8月   4 10:49 mapdata_2.2-6.zip
-rw-rw-r-- 1 wangjl wangjl  3631621 8月   4 10:37 maps_3.2.0.zip
-rw-rw-r-- 1 wangjl wangjl  1823908 8月   4 08:10 maptools_0.9-2.zip
-rw-rw-r-- 1 wangjl wangjl  1219777 8月   4 10:47 plyr_1.8.4.zip
-rw-rw-r-- 1 wangjl wangjl  1538955 8月   4 08:09 sp_1.2-5.zip
# ls -l *.zip | awk '{sum+=$5} END{print sum}'
33286663

对test文件某列求和：-F，用，号分隔，求第3行的和
$ awk -F,  '{sum += $3};END {print sum}' test



例2：（利用数组）统计每种状态（第六列）的数量

# awk 'BEGIN{print "State,count"}NR!=1 {a[$6]++} END{for(i in a) print i "," a[i]}' netstat.txt
State,count
,7
LISTEN,4
ESTABLISHED,2
7,1



例3: （利用关联数组）统计每个用户(第一列)的进程的占了多少内存（注：sum的RSS那一列）
# ps aux | head
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.3 185340  3444 ?        Ss   7月31   0:04 /sbin/init splash
root         2  0.0  0.0      0     0 ?        S    7月31   0:00 [kthreadd]
root         4  0.0  0.0      0     0 ?        S<   7月31   0:00 [kworker/0:0H]
root         6  0.0  0.0      0     0 ?        S    7月31   0:19 [ksoftirqd/0]
root         7  0.0  0.0      0     0 ?        S    7月31   1:06 [rcu_sched]
root         8  0.0  0.0      0     0 ?        S    7月31   0:00 [rcu_bh]
root         9  0.0  0.0      0     0 ?        S    7月31   0:00 [migration/0]
root        10  0.0  0.0      0     0 ?        S<   7月31   0:00 [lru-add-drain]
root        11  0.0  0.0      0     0 ?        S    7月31   0:04 [watchdog/0]

# ps aux | awk 'NR!=1 {a[$1]+=$6;} END{ for(i in a) print i "," a[i] "KB"}'
vboxadd,177912KB
syslog,644KB
colord,528KB
nobody,816KB
avahi,2324KB
wangjl,259812KB
whoopsie,3292KB
rtkit,292KB
root,115712KB
message+,3088KB

按照使用数量倒序排序
$ ps aux | awk 'NR!=1 {a[$1]+=$6;} END{ for(i in a) print i " " a[i] " KB"}' | sort -k2,2nr
rqfu 95183176 KB
xfwang 6944028 KB
hou 1385552 KB
wangjl 260916 KB
root 105544 KB
rstudio+ 28628 KB
yao 14248 KB
pengfei 13064 KB
postfix 4552 KB
polkitd 3536 KB
dbus 1672 KB
yllin 1280 KB




例4:综合统计
利用GEGIN,END语句求学生成绩表的行列小计。

$ cat score.txt
Marry   2143 78 84 77
Jack    2321 66 78 45
Tom     2122 48 77 71
Mike    2537 87 97 95
Bob     2415 40 57 62


$ cat cal.awk
#!/bin/awk -f
#运行前
BEGIN {
    math = 0
    english = 0
    computer = 0
 
    printf "NAME    NO.   MATH  ENGLISH  COMPUTER   TOTAL\n"
    printf "----------- ---------------- ----------- -------\n"
}
#运行中
{
    math+=$3
    english+=$4
    computer+=$5
    printf "%-6s %-6s %4d %8d %8d %8d\n", $1, $2, $3,$4,$5, $3+$4+$5
}
#运行后
END {
    printf "-------- -------------- ------------- ----------\n"
    printf "  TOTAL:%10d %8d %8d \n", math, english, computer
    printf "AVERAGE:%10.2f %8.2f %8.2f\n", math/NR, english/NR, computer/NR
}


我们来看一下执行结果：（也可以这样运行 ./cal.awk score.txt）
$ awk -f cal.awk score.txt #-f表示从文件读取awk命令
NAME    NO.   MATH  ENGLISH  COMPUTER   TOTAL
-------- ----------- ------------ --------------
Marry  2143     78       84       77      239
Jack   2321     66       78       45      189
Tom    2122     48       77       71      196
Mike   2537     87       97       95      279
Bob    2415     40       57       62      159
------- -------------- ----------- -------------
  TOTAL:       319      393      350
AVERAGE:     63.80    78.60    70.00





9.环境变量
即然说到了脚本，我们来看看怎么和环境变量交互：
（使用-v参数和ENVIRON，使用ENVIRON的环境变量需要export）
# x=5
# y=10
# export y
# echo $x $y

# cat score.txt
Marry   2143 78 84 77
Jack    2321 66 78 45
Tom     2122 48 77 71
Mike    2537 87 97 95
Bob     2415 40 57 62

# awk -v val=$x '{print $1,$2,$3,$4+val, $5+ENVIRON["y"]}' OFS="\t" score.txt
Marry   2143    78      89      87
Jack    2321    66      83      55
Tom     2122    48      82      81
Mike    2537    87      102     105
Bob     2415    40      62      72





10.杂项
# 从file文件中找出长度大于80的行
$ awk 'length>80' file

#按连接数查看客户端IP
netstat -ntu | awk '{print $5}' | cut -d: -f1 | sort | uniq -c | sort -nr

#打印99乘法表
seq 9 | sed 'H;g' | awk -v RS='' '{for(i=1;i<=NF;i++)printf("%dx%d=%d%s", i, NR, i*NR, i==NR?"\n":"\t")}'




11.经典案例

(1)awk选取该文件表头为name的那一列数据
$ head gwas.bed
chrom	chromStart	chromEnd	name
chr1	161475749	161475750	rs10494360
chr1	194329560	194329561	rs10921544
chr1	9408958	9408959	rs11121380
chr1	117280695	117280696	rs112300936
chr1	44558671	44558672	rs114940806
chr1	55505646	55505647	rs11591147
chr1	117751364	117751365	rs12046117
chr1	162449408	162449409	rs12047961

$ awk 'NR==1{for(i=1;i<=NF;i++)a[$i]=i;next}{print $a["name"]}' gwas.bed 
rs10494360
rs10921544
rs11121380
rs112300936
rs114940806
rs11591147
rs12046117
rs12047961

分析: 
一共两句，第一句是处理第一行，第二句是处理之后的所有行。
重点是a这个数组，第一句定义 a["chrom"]=1, a["chromStart"]=2, a["chromEnd"]=3, a["name"]=4.
第二句引用a["name"]=4，所以print $a["name"]就是print $4



结束语： awk 是相当复杂的工具, 真正使用时, 再细看man awk吧. 
内建变量，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Built_002din-Variables
流控方面，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Statements
内建函数，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Built_002din
正则表达式，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Regexp

https://segmentfault.com/a/1190000007338373
http://www.gnu.org/software/gawk/manual/gawk.html
https://www.gnu.org/software/gawk/manual/gawk.html





========================================
|-- awk 速查表
----------------------------------------
https://www.cnblogs.com/xudong-bupt/p/3721210.html

awk是行处理器: 相比较屏幕处理的优点，在处理庞大文件时不会出现内存溢出或是处理缓慢的问题，通常用来格式化文本信息
awk处理过程: 依次对每一行进行处理，然后输出
awk命令形式: awk [-F|-f|-v] 'BEGIN{} //{command1; command2} END{}' file
[-F|-f|-v]   大参数，-F指定分隔符，-f调用脚本，-v定义变量 var=value
''          引用代码块
BEGIN   初始化代码块，在对每一行进行处理之前，初始化代码，主要是引用全局变量，设置FS分隔符
//           匹配代码块，可以是字符串或正则表达式
{}           命令代码块，包含一条或多条命令
；          多条命令使用分号分隔
END      结尾代码块，在对每一行进行处理之后再执行的代码块，主要是进行最终计算或输出结尾摘要信息



## 特殊要点:
$0           表示整个当前行
$1           每行第一个字段
NF          字段数量变量
NR          每行的记录号，多文件记录递增
FNR        与NR类似，不过多文件记录不递增，每个文件都从1开始
\t            制表符
\n           换行符
FS          BEGIN时定义分隔符
RS       输入的记录分隔符， 默认为换行符(即文本是按一行一行输入)
~            匹配，与==相比不是精确比较
!~           不匹配，不精确比较
==         等于，必须全部相等，精确比较
!=           不等于，精确比较
&&　     逻辑与
||             逻辑或
+            匹配时表示1个或1个以上
/[0-9][0-9]+/   两个或两个以上数字
/[0-9][0-9]*/    一个或一个以上数字
FILENAME 文件名
OFS      输出字段分隔符， 默认也是空格，可以改为制表符等
ORS        输出的记录分隔符，默认为换行符,即处理结果也是一行一行输出到屏幕
-F'[:#/]'   定义三个分隔符


## print & $0
print 是awk打印指定内容的主要命令
awk '{print}'  /etc/passwd   ==   awk '{print $0}'  /etc/passwd  
awk '{print " "}' /etc/passwd                                           //不输出passwd的内容，而是输出相同个数的空行，进一步解释了awk是一行一行处理文本
awk '{print "a"}'   /etc/passwd                                        //输出相同个数的a行，一行只有一个a字母
awk -F":" '{print $1}'  /etc/passwd 
awk -F: '{print $1; print $2}'   /etc/passwd                   //将每一行的前二个字段，分行输出，进一步理解一行一行处理文本
awk  -F: '{print $1,$3,$6}' OFS="\t" /etc/passwd        //输出字段1,3,6，以制表符作为分隔符



## -f指定脚本文件
awk -f script.awk  file
BEGIN{
FS=":"
}
{print $1} //效果与awk -F":" '{print $1}'相同,只是分隔符使用FS在代码自身中指定

$ awk 'BEGIN{X=0} /^$/{ X+=1 } END{print "I find",X,"blank lines."}' test 
I find 4 blank lines.

$ ls -l|awk 'BEGIN{sum=0} !/^d/{sum+=$5} END{print "total size is",sum}'    //计算文件大小
total size is 17487


## -F指定分隔符
$1 指指定分隔符后，第一个字段，$3第三个字段， \t是制表符
一个或多个连续的空格或制表符看做一个定界符，即多个空格看做一个空格
awk -F":" '{print $1}'  /etc/passwd
awk -F":" '{print $1 $3}'  /etc/passwd                       //$1与$3相连输出，不分隔
awk -F":" '{print $1,$3}'  /etc/passwd                       //多了一个逗号，$1与$3使用空格分隔
awk -F":" '{print $1 " " $3}'  /etc/passwd                  //$1与$3之间手动添加空格分隔
awk -F":" '{print "Username:" $1 "\t\t Uid:" $3 }' /etc/passwd       //自定义输出  
awk -F: '{print NF}' /etc/passwd                                //显示每行有多少字段
awk -F: '{print $NF}' /etc/passwd                              //将每行第NF个字段的值打印出来
 awk -F: 'NF==4 {print }' /etc/passwd                       //显示只有4个字段的行
awk -F: 'NF>2{print $0}' /etc/passwd                       //显示每行字段数量大于2的行
awk '{print NR,$0}' /etc/passwd                                 //输出每行的行号
awk -F: '{print NR,NF,$NF,"\t",$0}' /etc/passwd      //依次打印行号，字段数，最后字段值，制表符，每行内容
awk -F: 'NR==5{print}'  /etc/passwd                         //显示第5行
awk -F: 'NR==5 || NR==6{print}'  /etc/passwd       //显示第5行和第6行
route -n|awk 'NR!=1{print}'                                       //不显示第一行


## 匹配代码块
//纯字符匹配   !//纯字符不匹配   ~//字段值匹配    !~//字段值不匹配   ~/a1|a2/字段值匹配a1或a2   
awk '/mysql/' /etc/passwd
awk '/mysql/{print }' /etc/passwd
awk '/mysql/{print $0}' /etc/passwd                   //三条指令结果一样
awk '!/mysql/{print $0}' /etc/passwd                  //输出不匹配mysql的行
awk '/mysql|mail/{print}' /etc/passwd
awk '!/mysql|mail/{print}' /etc/passwd
awk -F: '/mail/,/mysql/{print}' /etc/passwd         //区间匹配
awk '/[2][7][7]*/{print $0}' /etc/passwd               //匹配包含27为数字开头的行，如27，277，2777...
awk -F: '$1~/mail/{print $1}' /etc/passwd           //$1匹配指定内容才显示
awk -F: '{if($1~/mail/) print $1}' /etc/passwd     //与上面相同
awk -F: '$1!~/mail/{print $1}' /etc/passwd          //不匹配
awk -F: '$1!~/mail|mysql/{print $1}' /etc/passwd        


## IF语句
必须用在{}中，且比较内容用()扩起来
awk -F: '{if($1~/mail/) print $1}' /etc/passwd                                       //简写
awk -F: '{if($1~/mail/) {print $1}}'  /etc/passwd                                   //全写
awk -F: '{if($1~/mail/) {print $1} else {print $2}}' /etc/passwd            //if...else...


## 条件表达式
==   !=   >   >=  
awk -F":" '$1=="mysql"{print $3}' /etc/passwd  
awk -F":" '{if($1=="mysql") print $3}' /etc/passwd          //与上面相同 
awk -F":" '$1!="mysql"{print $3}' /etc/passwd                 //不等于
awk -F":" '$3>1000{print $3}' /etc/passwd                      //大于
awk -F":" '$3>=100{print $3}' /etc/passwd                     //大于等于
awk -F":" '$3<1{print $3}' /etc/passwd                            //小于
awk -F":" '$3<=1{print $3}' /etc/passwd                         //小于等于


## 逻辑运算符
&&　|| 
awk -F: '$1~/mail/ && $3>8 {print }' /etc/passwd         //逻辑与，$1匹配mail，并且$3>8
awk -F: '{if($1~/mail/ && $3>8) print }' /etc/passwd
awk -F: '$1~/mail/ || $3>1000 {print }' /etc/passwd       //逻辑或
awk -F: '{if($1~/mail/ || $3>1000) print }' /etc/passwd 


## 数值运算
awk -F: '$3 > 100' /etc/passwd    
awk -F: '$3 > 100 || $3 < 5' /etc/passwd  
awk -F: '$3+$4 > 200' /etc/passwd
awk -F: '/mysql|mail/{print $3+10}' /etc/passwd                    //第三个字段加10打印 
awk -F: '/mysql/{print $3-$4}' /etc/passwd                             //减法
awk -F: '/mysql/{print $3*$4}' /etc/passwd                             //求乘积
awk '/MemFree/{print $2/1024}' /proc/meminfo                  //除法
awk '/MemFree/{print int($2/1024)}' /proc/meminfo           //取整


## 输出分隔符OFS
awk '$6 ~ /FIN/ || NR==1 {print NR,$4,$5,$6}' OFS="\t" netstat.txt
awk '$6 ~ /WAIT/ || NR==1 {print NR,$4,$5,$6}' OFS="\t" netstat.txt        
//输出字段6匹配WAIT的行，其中输出每行行号，字段4，5,6，并使用制表符分割字段


## 输出处理结果到文件
①在命令代码块中直接输出    route -n|awk 'NR!=1{print > "./fs"}'   
②使用重定向进行输出           route -n|awk 'NR!=1{print}'  > ./fs


## 格式化输出
netstat -anp|awk '{printf "%-8s %-8s %-10s\n",$1,$2,$3}' 
printf表示格式输出
%格式化输出分隔符
-8长度为8个字符
s表示字符串类型
打印每行前三个字段，指定第一个字段输出字符串类型(长度为8)，第二个字段输出字符串类型(长度为8),
第三个字段输出字符串类型(长度为10)
netstat -anp|awk '$6=="LISTEN" || NR==1 {printf "%-10s %-10s %-10s \n",$1,$2,$3}'
netstat -anp|awk '$6=="LISTEN" || NR==1 {printf "%-3s %-10s %-10s %-10s \n",NR,$1,$2,$3}'


## IF语句
awk -F: '{if($3>100) print "large"; else print "small"}' /etc/passwd
small
small
small
large
small
small
awk -F: 'BEGIN{A=0;B=0} {if($3>100) {A++; print "large"} else {B++; print "small"}} END{print A,"\t",B}' /etc/passwd 
                                                                                                                  //ID大于100,A加1，否则B加1
awk -F: '{if($3<100) next; else print}' /etc/passwd                         //小于100跳过，否则显示
awk -F: 'BEGIN{i=1} {if(i<NF) print NR,NF,i++ }' /etc/passwd   
awk -F: 'BEGIN{i=1} {if(i<NF) {print NR,NF} i++ }' /etc/passwd
另一种形式
awk -F: '{print ($3>100 ? "yes":"no")}'  /etc/passwd 
awk -F: '{print ($3>100 ? $3":\tyes":$3":\tno")}'  /etc/passwd


## while语句
awk -F: 'BEGIN{i=1} {while(i<NF) print NF,$i,i++}' /etc/passwd 
7 root 1
7 x 2
7 0 3
7 0 4
7 root 5
7 /root 6


## 数组
netstat -anp|awk 'NR!=1{a[$6]++} END{for (i in a) print i,"\t",a[i]}'
netstat -anp|awk 'NR!=1{a[$6]++} END{for (i in a) printf "%-20s %-10s %-5s \n", i,"\t",a[i]}'
9523                               1     
9929                               1     
LISTEN                            6     
7903                               1     
3038/cupsd                   1     
7913                               1     
10837                             1     
9833                               1     




## 应用1
awk -F: '{print NF}' helloworld.sh                            //输出文件每行有多少字段
awk -F: '{print $1,$2,$3,$4,$5}' helloworld.sh                //输出前5个字段
awk -F: '{print $1,$2,$3,$4,$5}' OFS='\t' helloworld.sh       //输出前5个字段并使用制表符分隔输出
awk -F: '{print NR,$1,$2,$3,$4,$5}' OFS='\t' helloworld.sh    //制表符分隔输出前5个字段，并打印行号

## 应用2
awk -F'[:#]' '{print NF}'  helloworld.sh                         //指定多个分隔符: #，输出每行多少字段
awk -F'[:#]' '{print $1,$2,$3,$4,$5,$6,$7}' OFS='\t' helloworld.sh   //制表符分隔输出多字段

## 应用3
awk -F'[:#/]' '{print NF}' helloworld.sh         //指定三个分隔符，并输出每行字段数
awk -F'[:#/]' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12}' helloworld.sh     //制表符分隔输出多字段

## 应用4
计算/home目录下，普通文件的大小，使用KB作为单位
ls -l|awk 'BEGIN{sum=0} !/^d/{sum+=$5} END{print "total size is:",sum/1024,"KB"}'
ls -l|awk 'BEGIN{sum=0} !/^d/{sum+=$5} END{print "total size is:",int(sum/1024),"KB"}'         //int是取整的意思

## 应用5
统计netstat -anp 状态为LISTEN和CONNECT的连接数量分别是多少
netstat -anp|awk '$6~/LISTEN|CONNECTED/{sum[$6]++} END{for (i in sum) printf "%-10s %-6s %-3s \n", i," ",sum[i]}'

## 应用6
统计/home目录下不同用户的普通文件的总数是多少？
ls -l|awk 'NR!=1 && !/^d/{sum[$3]++} END{for (i in sum) printf "%-6s %-5s %-3s \n",i," ",sum[i]}'   
mysql        199 
root           374 
统计/home目录下不同用户的普通文件的大小总size是多少？
ls -l|awk 'NR!=1 && !/^d/{sum[$3]+=$5} END{for (i in sum) printf "%-6s %-5s %-3s %-2s \n",i," ",sum[i]/1024/1024,"MB"}'

## 应用7
输出成绩表
awk 'BEGIN{math=0;eng=0;com=0;printf "Lineno.   Name    No.    Math   English   Computer    Total\n";printf "------------ ----------------- -------------------------------\n"}{math+=$3; eng+=$4; com+=$5;printf "%-8s %-7s %-7s %-7s %-9s %-10s %-7s \n",NR,$1,$2,$3,$4,$5,$3+$4+$5} END{printf "-------------- ----------------------- -----------------------\n";printf "%-24s %-7s %-9s %-20s \n","Total:",math,eng,com;printf "%-24s %-7s %-9s %-20s \n","Avg:",math/NR,eng/NR,com/NR}' test0

[root@localhost home]# cat test0 
Marry   2143 78 84 77
Jack    2321 66 78 45
Tom     2122 48 77 71
Mike    2537 87 97 95
Bob     2415 40 57 62







========================================
|-- awk求两列的差集: NR==FNR; 数组; next命令;
----------------------------------------

1. Awk: check if value is not in array
#https://stackoverflow.com/questions/17994025/awk-check-if-value-is-not-in-array

awk 'NR==FNR{a[$0];next}NF<3||!($22 in a)' file1 file2 

(1)目的：
1).打印file2前3行header； 
2).如果file2 的22列有值没有在array中(file1的整列)，则打印file2的哪一行


(2)解析:
1)NR,表示awk开始执行程序后所读取的数据行数.
FNR,与NR功用类似,不同的是awk每打开一个新文件,FNR便从0重新累计.
所以，由NR==FNR为真时,判断当前读入的是第一个文件file1；

由NR==FNR为假时,判断当前读入了第二个文件,然后跳过{a[$0];next},

2) next 命令，它告诉 awk 跳过你所提供的所有剩下的模式和表达式，直接处理下一个输入行。
next 命令帮助你阻止运行命令执行过程中多余的步骤。

next命令在编写高效的命令脚本时候是非常重要的，它可以提高脚本速度。

ref: https://www.cnblogs.com/linuxprobe/p/5745381.html

3)NF  字段数量变量
NF<3 一行字段少于3，或者22列不在第一个数组中的，默认操作是打印整行。





2.引申：求两列的差集，这两列位于不同的文件中

求hsa00001.keg文件带C开头的列的第二列，和kegg2gene.txt的第一列的差集。前者是大集合。


$ grep '^C' hsa00001.keg | grep hsa >hsa00001.keg.C
$ awk 'NR==FNR{a[$1];next} !($2 in a)' kegg2gene.txt  hsa00001.keg.C

描述：把第一个文件读入数组，









========================================
|-- awk 对某一列拆分后输出：由gtf文件拆分出 基因范围bed文件
----------------------------------------

1.awk 内置函数 例子
$ awk 'BEGIN{info="this is a test2010test!";gsub(/[0-9]+/,"!",info);print info}'   
this is a test!test!




2. 实例 
最后一列怎么拆分呢？使用awk内置函数。

$ awk -F "\t" '{print $1"\t"$4"\t"$5"\t"$7"\t"$9}' gencode.v38.annotation.GENE.gtf | head
chr1    11869   14409   +       gene_id "ENSG00000223972.5"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"; level 2; hgnc_id "HGNC:37102"; havana_gene "OTTHUMG00000000961.2";
chr1    14404   29570   -       gene_id "ENSG00000227232.5"; gene_type "unprocessed_pseudogene"; gene_name "WASH7P"; level 2; hgnc_id "HGNC:38034"; havana_gene "OTTHUMG00000000958.1";


$ awk -F "\t" '{
split($9, arr, "; ");
gsub( "gene_id ", "", arr[1] );
gsub( "gene_type ", "", arr[2] );
gsub( "gene_name ", "", arr[3] );

gsub("\"", "", arr[1]);
gsub("\"", "", arr[2]);
gsub("\"", "", arr[3]);

print $1"\t"$4"\t"$5"\t"$7"\t"arr[3]"\t"arr[2]"\t"arr[1]}' gencode.v38.annotation.GENE.gtf | head | column -t

输出：
chr1  11869  14409   +  DDX11L1       transcribed_unprocessed_pseudogene  ENSG00000223972.5
chr1  14404  29570   -  WASH7P        unprocessed_pseudogene              ENSG00000227232.5
chr1  17369  17436   -  MIR6859-1     miRNA                               ENSG00000278267.1
chr1  29554  31109   +  MIR1302-2HG   lncRNA                              ENSG00000243485.5
chr1  30366  30503   +  MIR1302-2     miRNA                               ENSG00000284332.1
chr1  34554  36081   -  FAM138A       lncRNA                              ENSG00000237613.2
chr1  52473  53312   +  OR4G4P        unprocessed_pseudogene              ENSG00000268020.3
chr1  57598  64116   +  OR4G11P       transcribed_unprocessed_pseudogene  ENSG00000240361.2
chr1  65419  71585   +  OR4F5         protein_coding                      ENSG00000186092.7
chr1  89295  133723  -  RP11-34P13.7  lncRNA                              ENSG00000238009.6




更多函数参考: http://blog.sina.com.cn/s/blog_5357c0af0101mito.html





========================================
|-- awk 小技巧: 去掉某一列; 
----------------------------------------
1.awk去掉指定列并打印其余列技巧
$ head odometry_loc.txt.00000 | awk '{print $1=null,$0}'



========================================
sed(关键字: 编辑) 以行为单位的文本编辑工具 
----------------------------------------

命令sed: 用正则表达式搜索并替换文本
$ cat out.txt
this is a linux system.
line2
Ubuntu is a release of linux.


$ sed 's/linux/unix/g' out.txt  #正则替换，/g是全局替换
this is a unix system.
line2
Ubuntu is a release of unix.


$ sed '1,2s/linux/unix/g' out.txt  #只替换从m到n行
this is a unix system.
line2
Ubuntu is a release of linux.


$ sed -e 's/linux/unix/g' -e 's/line/row/g' out.txt #多个替换正则表达式
this is a unix system.
row2
Ubuntu is a release of unix.

$ cat >regFile #把sed的规则保存到文件regFile中
1,2s/linux/unix/g
^C
$ cat regFile
1,2s/linux/unix/g
$ sed -f regFile out.txt #-f参数调用之前保存到文件中的规则，进行替换。
this is a unix system.
line2
Ubuntu is a release of linux.


删除文件最后一行
$ sed -i '$d' fileName






####################
1.定义与作用
sed - stream editor for filtering and transforming text 
用于过滤和转换文本的流编辑器。特色是该编辑器能用于pipeline中。

sed全名叫stream editor，流编辑器，用程序的方式来编辑文本（非交互式的），相当的hacker啊。
sed基本上就是玩正则模式匹配，所以，玩sed的人，正则表达式一般都比较强。

sed可以直接修改档案, 不过一般不推荐这么做, 可以分析 standard input

文档： 
	man: http://www.gnu.org/software/sed/manual/sed.html
	https://www.gnu.org/software/sed/
	list: http://sed.sourceforge.net/
	detail: http://www.grymoire.com/Unix/Sed.html




2.基本工作方式: sed [-nef] '[动作]' [输入文本]
	-n : 安静模式, 一般sed用法中, 来自stdin的数据一般会被列出到屏幕上, 如果使用-n参数后, 只有经过sed处理的那一行被列出来.
	-e : 多重编辑, 比如你同时又想删除某行, 又想改变其他行, 那么可以用 sed -e '1,5d' -e 's/abc/xxx/g' filename
	-f : 首先将 sed的动作写在一个档案内, 然后通过 sed -f scriptfile 就可以直接执行 scriptfile 内的sed动作。
	-i : 直接编辑, 这回就是真的改变文件中的内容了, 别的都只是改变显示. (不推荐使用)
	
动作:
	a 新增, a 后面可以接字符串, 而这个字符串会在新的一行出现. (下一行)
	c 取代, c 后面的字符串, 这些字符串可以取代 n1,n2之间的行
	d 删除, 后面不接任何东西
	i 插入, 后面的字符串, 会在上一行出现
	p 打印, 将选择的资料列出, 通常和 sed -n 一起运作 sed -n '3p' 只打印第3行
	s 取代, 类似vi中的取代, 1,20s/old/new/g

例： $ echo "this is a book" | sed 's/is/was/g'
thwas was a book

	[line-address]q 退出, 匹配到某行退出, 提高效率
	[line-address]r 匹配到的行读取某文件(注意是文件名) 例如: sed '1r qqq.txt' abc , 注意, 写入的文本是写在了第1行的后边, 也就是第2行
	[line-address]w file, 匹配到的行写入某文件  例如: sed -n '/m/w qqq' abc , 从abc中读取带m的行写到qqq文件中, 注意, 这个写入带有覆盖性.


常用例子:
sed '1d' abc 删除 abc 档案里的第一行, 注意, 这时会显示除了第一行之外的所有行, 因为第一行已经被删除了(实际文件并没有被删除,而只是显示的时候被删除了)

sed -n '1d' abc 什么内容也不显示, 因为经过sed处理的行, 是个删除操作, 所以不显示.

sed '2,$d' abc 删除abc中从第二行到最后一行所有的内容, 注意, $符号正则表达式中表示行末尾, 但是这里并没有说那行末尾, 就会指最后一行末尾, ^开头, 如果没有指定哪行开头, 那么就是第一行开头

sed '$d' abc 只删除了最后一行, 因为并没有指定是那行末尾, 就认为是最后一行末尾

sed '/test/d' abc 删除文件中所有带 test 的行

sed '/test/a RRRRRRR' abc 将 RRRRRRR 追加到所有的带 test 行的下一行 

sed '1,5c RRRRRRR' abc 从1到5行被 RRRRRRR 替换

sed '/test/c RRRRRRR' abc 将 RRRRRRR 替换所有带 test 的行, 当然, 这里也可以是通过行来进行替换, 比如 sed '1,5c RRRRRRR' abc





3.用s命令替换
(1)一般替换
$ cat out.txt #原文
this is a linux system.
line2
Ubuntu is a release of linux.

$ sed 's/is/are/' out.txt #把is用are替换
thare is a linux system.
line2
Ubuntu are a release of linux.

$ sed 's/is/are/g' out.txt #/g 表示一行上的替换所有的匹配
thare are a linux system.
line2
Ubuntu are a release of linux.

$ sed '1s/linux/Unix/g' out.txt #1s表示只替换第一行。
this is a Unix system.
line2
Ubuntu is a release of linux.


上面的sed并没有对文件的内容改变，只是把处理过后的内容输出，如果你要写回文件，你可以使用重定向。
$ sed '1s/linux/Unix/g' out.txt >out2.txt
或者使用-i参数（不推荐）直接修改原文件
$ sed -i 's/linux/Windows/g' out2.txt
wangjl@ubt16:~/str$ cat out2.txt
this is a Unix system.
line2
Ubuntu is a release of Windows.


在每一行最前面加注释符#：
$ sed 's/^/#/g' out.txt
#this is a linux system.
#line2
#Ubuntu is a release of linux.

在每一行最后面加上英文分号;
$ sed 's/$/;/g' out.txt
this is a linux system.;
line2;
Ubuntu is a release of linux.;

(2)每一行多个替换(-e参数)
同时在行首加上#，行尾加上;
$ sed -e 's/^/#/g' -e 's/$/;/g' out.txt
#this is a linux system.;
#line2;
#Ubuntu is a release of linux.;

(3)正则表达式的一些最基本的东西：
	^ 表示一行的开头。如：/^#/ 以#开头的匹配。
	$ 表示一行的结尾。如：/}$/ 以}结尾的匹配。
	\< 表示词首。 如：\<abc 表示以 abc 为首的詞。
	\> 表示词尾。 如：abc\> 表示以 abc 結尾的詞。
	. 表示任何单个字符。
	* 表示某个字符出现了0次或多次。
	[ ] 字符集合。 如：[abc] 表示匹配a或b或c，还有 [a-zA-Z] 表示匹配所有的26个字符。如果其中有^表示反，如 [^a] 表示非a的字符
	
注意： sed的正则用的是BREs/EREs，不支持非贪婪模式。
https://segmentfault.com/q/1010000002416121

例:去掉某html中的tags：
index.html
<b>This</b> is what <span style="text-decoration: underline;">the Boss</span> meant. Understand?

# 如果你这样搞的话，就会有问题
$ sed 's/<.*>//g' index.html
 meant. Understand?

# 要解决上面的那个问题，就得像下面这样。
# 其中的'[^>]' 指定了非>的字符重复0次或多次。
$ sed 's/<[^>]*>//g' index.html
This is what the Boss meant. Understand?


(4)替换每一行的第2个匹配项
$ sed 's/i/I/2' out.txt
this Is a linux system.
line2
Ubuntu is a release of lInux.

$ sed 's/i/I/1' out.txt
thIs is a linux system.
lIne2
Ubuntu Is a release of linux.

$ sed 's/i/I/' out.txt #默认就是只替换每行的第一个
thIs is a linux system.
lIne2
Ubuntu Is a release of linux. 

$ sed 's/i/I/2g' out.txt #替换每行第三个及以后的
this Is a lInux system.
line2
Ubuntu is a release of lInux.


(5)引用匹配项
我们可以使用&来当做被匹配的变量，然后可以在其左右加点东西。
$ sed 's/i/[&]/g' out.txt
th[i]s [i]s a l[i]nux system.
l[i]ne2
Ubuntu [i]s a release of l[i]nux.

或者使用圆括号匹配，用\1,\2等表示对匹配的引用。
$ sed 's/\(\w*\) is a \([^.]*\)/\1-\2/g' out.txt
this-linux system.
line2
Ubuntu-release of linux.
为什么[^.]没起到过滤.号的作用？
注意： 因为sed只替换了匹配的，没有匹配的保持不变。

比如 echo 'aaaaaaactttt'|sed 's/\(.*\)a/\1b/'
输出为 aaaaaabctttt

想达到去除.号的目的，需要把.移到正则()外面：
$ sed 's/\(\w*\) is a \(.*\)\./\1-\2/g' out.txt
this-linux system
line2
Ubuntu-release of linux

正则为：(\w*) is a (). 
匹配为：(this) is a (linux system).
然后：\1就是this，\2就是linux system







4.更多命令

(1)N命令: 把下一行的内容纳入当成缓冲区做匹配。
n N    Read/append the next line of input into the pattern space.

$ sed 's/i/I/' out2.txt 
thIs is a linux system.
lIne2
Ubuntu Is a release of linux.
It Is a wonderfull release.

加了N命令后，偶数行合并到奇数行结尾，原文大概成了这样
this is a linux system.\nline2
Ubuntu is a release of linux.\nIt is a wonderfull release.

$ sed 'N;s/i/I/' out2.txt #偶数行没做替换
thIs is a linux system.
line2
Ubuntu Is a release of linux.
It is a wonderfull release.

$ sed 'N;s/i/I/' out.txt #最后一个没有偶数行的也没有处理(?bug)
thIs is a linux system.
line2
Ubuntu is a release of linux.

我们甚至可以替换掉这个换行符/n 
$ sed 'N;s/\n//' out2.txt
this is a linux system.line2
Ubuntu is a release of linux.It is a wonderfull release.


$ sed -e 'N;s/i/I/' -e 'N;s/\n//' out2.txt #又出现bug第二个没有替换
thIs is a linux system.line2
Ubuntu is a release of linux.
It is a wonderfull release.


(2)a命令和i命令
a命令就是append(在行后一行添加)， i命令就是insert(在行前一行添加)，它们是用来添加行的。

1)
# 其中的1i表明，其要在第1行前插入一行（insert）
$ sed '1i desc\n------------' out.txt
desc
------------
this is a linux system.
line2
Ubuntu is a release of linux.

# 其中的1a表明，其要在第一行后追加一行（append）
$ sed '1a ------------' out.txt
this is a linux system.
------------
line2
Ubuntu is a release of linux.


# 其中的$a表明，其要在最后一行后追加一行（append）
$ sed '$a ------------' out.txt
this is a linux system.
line2
Ubuntu is a release of linux.
------------

2)我们可以运用匹配来添加文本：
注意其中的/is/a，这意思是匹配到/is/后就追加一行
$ sed '/is/a ---' out.txt
this is a linux system.
---
line2
Ubuntu is a release of linux.
---


(3)c命令-替换匹配行
$ sed '2c this is another line' out.txt
this is a linux system.
this is another line
Ubuntu is a release of linux.

$ sed '/linux/c replace the line with linux' out.txt
replace the line with linux
line2
replace the line with linux



(4)d命令-删除匹配行
$ sed '2d' out.txt
this is a linux system.
Ubuntu is a release of linux.

$ sed '2,$d' out.txt
this is a linux system.

$ sed '/is/d' out.txt
line2



(5)p命令-打印命令
你可以把这个命令当成grep式的命令

$ sed '/is/p' out.txt
this is a linux system.
this is a linux system.
line2
Ubuntu is a release of linux.
Ubuntu is a release of linux.

发现有些输出两遍，这是因为sed处理时会把处理的信息输出。
添加-n参数后就可以了。
$ sed -n '/is/p' out.txt
this is a linux system.
Ubuntu is a release of linux.


#从一个模式到另一个模式（跨行了）
$ sed -n '/ine/,/nux/p' out.txt
line2
Ubuntu is a release of linux.


#从第1行到匹配到的行
$ sed -n '1,/line/p' out.txt
this is a linux system.
line2


#从第二行打印，每隔4行打印一行（用途：显示fastq文件的序列行信息）
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '{n;p;n;n}'
或者
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '2~4p'
# first~step: Match every step'th line starting with line first.  





5.四个sed的基本知识点

(0)Pattern Space
我们来看一下sed处理文本的伪代码，并了解一下Pattern Space的概念：
foreach line in file {
    //把行放入Pattern_Space
    Pattern_Space <= line;
 
    // 对每个pattern space执行sed命令
    Pattern_Space <= EXEC(sed_cmd, Pattern_Space);
 
    // 如果没有指定 -n 则输出处理后的Pattern_Space
    if (sed option hasn't "-n")  {
       print Pattern_Space
    }
}

(1)Address
第一个是关于address，几乎上述所有的命令都是这样的（注：其中的!表示匹配成功后是否执行命令）
[address[,address]][!]{cmd}

address可以是一个数字，也可以是一个模式，
你可以通过逗号要分隔两个address 表示两个address的区间，参执行命令cmd，伪代码如下：
bool bexec = false
foreach line in file {
    if ( match(address1) ){
        bexec = true;
    }
 
    if ( bexec == true) {
        EXEC(sed_cmd);
    }
 
    if ( match (address2) ) {
        bexec = false;
    }
}

关于address可以使用相对位置，如：
r$ sed '/sys/,+1s/^/#/g' out.txt
#this is a linux system.
#line2
Ubuntu is a release of linux.

$ sed '/sys/,+2s/^/#/g' out.txt
#this is a linux system.
#line2
#Ubuntu is a release of linux.


(2)命令打包
第二个是cmd可以是多个，它们可以用分号分开，可以用大括号括起来作为嵌套命令。下面是几个例子：

对1行到第2行，执行命令/ine/d
$ sed '1,2 {/ine/d}' out.txt
this is a linux system.
Ubuntu is a release of linux.

# 对1行到第3行，匹配/is/成功后，再匹配/linux/，成功后执行d命令
$ sed '1,3{/is/{/linux/d}}' out2.txt
line2
It is a wonderfull release.

# 从第一行到最后一行，如果匹配到is，则删除之；如果有line，则替换为anotherLine
$ sed '1,${/is/d;s/line/anotherLine/g}' out2.txt
anotherLine2



(3)Hold Space[比较难，慢慢看，多看几遍]
第三个我们再来看一下 Hold Space

接下来，我们需要了解一下Hold Space的概念，我们先来看四个命令：
	g： 将hold space中的内容拷贝到pattern space中，原来pattern space里的内容清除
	G： 将hold space中的内容append到pattern space\n后
	h： 将pattern space中的内容拷贝到hold space中，原来的hold space里的内容被清除
	H： 将pattern space中的内容append到hold space\n后
	x： 交换pattern space和hold space的内容

这些命令有什么用？我们来看两个示例吧，用到的示例文件是：
$ cat >t.txt
one
two
three


第一个示例：
$ sed 'H;g' t.txt


第二个示例，反序了一个文件的行：
$ sed '1!G;h;$!d' t.txt

其中的 '1!G;h;$!d' 可拆解为三个命令
	1!G —— 只有第一行不执行G命令，将hold space中的内容append回到pattern space
	h —— 第一行都执行h命令，将pattern space中的内容拷贝到hold space中
	$!d —— 除了最后一行不执行d命令，其它行都执行d命令，删除当前行










6.杂七杂八
(1)sed '/^$/d' my.txt 为什么起到过滤空行的效果呢？
sed是按行进行处理的。
^$ 是正则表达式，其中^表示以什么开头，$表示以什么结尾，两个连载一起就是空行的意思，
d指令是sed里面的删除，
整个语句的意思就是从my.txt文件中删除空行空行

(2)# 在每一行后面增加一空行 
$ sed G my.txt

(3)可以修改定界符，用#代替/
$ a='are'
$ echo $a
are

$ sed 's/is/$a/g' out.txt #单引号内就是字符替换
th$a $a a linux system.
line2
Ubuntu $a a release of linux.

$ sed "s#is#$a#g" out.txt #双引号内才可以变量替换
thare are a linux system.
line2
Ubuntu are a release of linux.

$ sed "s/is/$a/g" out.txt
thare are a linux system.
line2
Ubuntu are a release of linux.






========================================
|-- sed 把一个文件插入到另一个文件头部：e命令
----------------------------------------

1. 实例: 为一个很大的 sam文件添加头信息，不想拷贝一份原文件了
$ cat head.txt 
@a1
@a2

$ cat num.txt 
1
2
3
4


$ sed '1 e cat head.txt' num.txt
@a1
@a2
1
2
3
4

实质性更改 num.txt 
$ sed -i '1 e cat head.txt' num.txt

$ head num.txt 
@a1
@a2
1
2
3
4




2. 解释
https://unix.stackexchange.com/questions/337435/sed-insert-file-at-top-of-another

GNU sed provides a e command that executes the command in parameter. The result is output is immediately.

(2) From the manual:
e [COMMAND]
This command allows one to pipe input from a shell command into pattern space. Without parameters, the `e' command executes the command that is found in pattern space and replaces the pattern space with the output; a trailing newline is suppressed.

If a parameter is specified, instead, the `e' command interprets it as a command and sends its output to the output stream.

Note that, unlike the 'r' command, the output of the command will be printed immediately; the 'r' command instead delays the output to the end of the current cycle.










========================================
分割文件命令: split及其与解压命令的组合
----------------------------------------
1. 语法
(1) split 指令将大文件分割成较小的文件，在默认情况下将按照每1000行切割成一个小文件。
只能输入文本文件，不能gz压缩。如果是gz压缩过的，建议配合gunzip -c同时使用 
$ gunzip -c xx.gz | split -l 800 -d prefix_

(2) split [--help][--version][-<行数>][-b <字节>][-C <字节>][-l <行数>][要切割的文件][输出文件名]
参数说明：
	-l, --lines=NUMBER     指定每份行数 put NUMBER lines per output file
	-d, --numeric-suffixes[=FROM]  子文件用数字当后缀 use numeric suffixes instead of alphabetic;                          
									FROM changes the start value (default 0)
		## 不加-d，则默认子文件用字母组合做后缀: xaa, xab, xac etc...

	-<行数> : 指定每多少行切成一个小文件
	-b<字节> : 指定每多少字节切成一个小文件

	--help : 在线帮助
	--version : 显示版本信息
	-C<字节> : 与参数"-b"相似，但是在切 割时将尽量维持每行的完整性
	[输出文件名] : 设置切割后文件的前置文件名(prefix前缀)， split会自动在前置文件名后再加上编号

# 如果是测序文件.fq，则需要保持每4行的完整性，所以-b和-C参数都不建议使用。


(3) split提供两种方式对文件进行切割：
	根据行数切割，通过-l参数指定需要切割的行数
	根据大小切割，通过-b参数指定需要切割的大小
		$ split -b 500M -d --verbose 2020011702-www.happylauliu.cn.gz split-size



2. 示例

(1) 按行分割
$ split -6 README       #将README文件每六行分割成一个文件 
$ split -l 4000000 myFastq.fq 


(2) 按文件大小分割
$ split -b 8G large.tgz large.tgz.part-  #分成8G大小的文件


(3) 分割gz压缩文件
zless超级慢，建议用gunzip -c 代替。
$ zless recal.fastq.gz | split -l 4000000 - prefix

推荐 
$ gunzip -c xx.fastq.gz | split -l 100000000 -d - FqChunk_

(4) 如果想分割后再压缩呢？使用filter参数: split --bytes=1024M --filter='gzip > $FILE.gz' /path/to/input /path/to/output
$ split -l 100 -d --filter='gzip > $FILE.gz' ../ncbi_error_report.txt small_ #博主测试通过

## ref https://serverfault.com/questions/598647/how-to-split-a-file-and-compress-directly
$ zcat ERR3152365.fastq.gz | split -a 3 -d -l 1200000 --numeric-suffixes --filter='pigz -p 8 > $FILE.fq.gz' - splitout/part_'







3. 分割gz压缩文件的性能测试
有一个115G的gz文本文件，每1e8行分割到一个文件，需要大概50多个子文件。
-rw-r--r--. 1 wangjl user 115G Dec 29 21:41 /data4/public_data/10x_colon_RNA/CRCSZ04_mergeAllData/mergedRawData2/CRCSZ05N_S1_L001_R1_001.fastq.gz

(1) 手动版 耗时 几个小时
$ zcat /data4/public_data/10x_colon_RNA/CRCSZ04_mergeAllData/mergedRawData2/CRCSZ05N_S1_L001_R1_001.fastq.gz | head -n 100000000 | tail -n 100000000 >temp2/origin/chunk_1.fq &
$ zcat /data4/public_data/10x_colon_RNA/CRCSZ04_mergeAllData/mergedRawData2/CRCSZ05N_S1_L001_R1_001.fastq.gz | head -n 200000000 | tail -n 100000000 >temp2/origin/chunk_1.fq &
...
$ ls -lth 
-rw-r--r--. 1 wangjl user 8.4G Jan 18 21:56 chunk_3.fq       
-rw-r--r--. 1 wangjl user 8.4G Jan 18 21:54 chunk_2.fq       
-rw-r--r--. 1 wangjl user 8.4G Jan 18 21:53 chunk_1.fq
## 缺点：手动执行，一次几条，全部执行250G的内存都扛不住。
## 越往后越慢，貌似head 是全部读进内存，然后再取出来的，越往后无用功越多。


(2) 特慢 耗时一晚上没完成
$ zless /data4/public_data/10x_colon_RNA/CRCSZ04_mergeAllData/mergedRawData2/CRCSZ05N_S1_L001_R1_001.fastq.gz | split -l 100000000 -d - FqChunk_
$ ls -lth
-rw-r--r--. 1 wangjl user 8.4G Jan 19 01:40 FqChunk_02
-rw-r--r--. 1 wangjl user 8.4G Jan 18 23:55 FqChunk_01
-rw-r--r--. 1 wangjl user 8.4G Jan 18 22:50 FqChunk_00
## zless 很慢，差不多60分钟产出一个文件。

(3) 最优解 耗时1个小时多
$ gunzip -c /data4/public_data/10x_colon_RNA/CRCSZ04_mergeAllData/mergedRawData2/CRCSZ05N_S1_L001_R1_001.fastq.gz | split -l 100000000 -d - FqChunk_
$ ls -lth
-rw-r--r--. 1 wangjl user 8.4G Jan 19 09:47 FqChunk_02
-rw-r--r--. 1 wangjl user 8.4G Jan 19 09:46 FqChunk_01
-rw-r--r--. 1 wangjl user 8.4G Jan 19 09:44 FqChunk_00
## 【极力推荐】 gunzip -c 差不多 1.5min产出一个文件。

测试: gunzip -c 和 zcat是不是一样呢？
$ zcat /data4/public_data/10x_colon_RNA/CRCSZ04_mergeAllData/mergedRawData2/CRCSZ05N_S1_L001_R1_001.fastq.gz | split -l 100000000 -d - FqChunkTest_

-rw-r--r--. 1 wangjl user 8.4G Jan 19 14:21 FqChunkTest_02
-rw-r--r--. 1 wangjl user 8.4G Jan 19 14:20 FqChunkTest_01
-rw-r--r--. 1 wangjl user 8.4G Jan 19 14:19 FqChunkTest_00
## 差不多也是1.5min一个。更简练。


(4) 再压缩小文件。需要20小时。
$ zcat /data4/public_data/10x_colon_RNA/CRCSZ04_mergeAllData/mergedRawData2/CRCSZ05N_S1_L001_R1_001.fastq.gz | split -l 100000000 -d --filter='gzip > $FILE.gz' - FqChunkTest2_

-rw-r--r--. 1 wangjl user 2.1G Jan 19 15:32 FqChunkTest2_02.gz
-rw-r--r--. 1 wangjl user 2.0G Jan 19 15:10 FqChunkTest2_01.gz
-rw-r--r--. 1 wangjl user 2.0G Jan 19 14:48 FqChunkTest2_00.gz
## 压缩率4倍。但是慢了20倍，大概23min一个。





4. 合并文件 
$ cat split-size01 split-size02 >two-file-merge

https://www.shayanderson.com/linux/split-tar-and-gzip-file-into-smaller-multiple-files.htm
(1) TAR and GZIP the large file:
$ tar -czvf newlarge.tgz largefile.dat

(2)use the split command to split the files into multiple smaller files:
$ split -b 300m "newlarge.tgz" "newlarge.tgz.part-"

newlarge.tgz.part-aa
newlarge.tgz.part-ab
newlarge.tgz.part-ac

(3) join the split files and extract the TAR and GZIP large file
$ cat newlarge* > newlarge.tgz

(4) 解压缩
$ tar -xvf newlarge.tgz



========================================
字符串处理综合案例
----------------------------------------

主要是基因组信息处理实例。


========================================
|-- 提取gene_id列和gene_name列
----------------------------------------
要求：从ID文件中提取gene_id列和gene_name列，要求保持gene_id列的唯一性，及其与gene_name列的对应。

$ head -n 15 ID
gene_id "ENSG00000223972.5_2"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"; level 2
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000456328.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000456328.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000456328.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000456328.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000227232.5_2"; gene_type "unprocessed_pseudogene"; gene_name "WASH7P"; level 2
gene_id "ENSG00000227232.5_2"; transcript_id "ENST00000488147.1_1"; gene_type "unprocessed_pseudogene"; gene_name "WASH7P"
gene_id "ENSG00000227232.5_2"; transcript_id "ENST00000488147.1_1"; gene_type "unprocessed_pseudogene"; gene_name "WASH7P"


$ wc ID
  2629275  21034200 302140702 ID


第一步：用sort按照第二列排序，并去重：
$ cat ID | sort -k 2,2 -u > ID.one
$ wc ID.one
  60461  483688 5452889 ID.one


第二步：用cut切出来第2和第6列，用sed替换其中的"和;符号。
$ cat ID.one | cut -d " " -f 2,6 |sed -e "s/\"//g" -e "s/;//g" >ID.end


查看结果文件：
$ head ID.end
ENSG00000000003.14_2 TSPAN6
ENSG00000000005.5_2 TNMD
ENSG00000000419.12_2 DPM1

$ wc ID.end
  60461  120922 1702210 ID.end



或者其他切割顺序：
$ head ID.one|cut -d ";" -f 1,3
gene_id "ENSG00000000003.14_2"; gene_name "TSPAN6"
gene_id "ENSG00000000005.5_2"; gene_name "TNMD"
gene_id "ENSG00000000419.12_2"; gene_name "DPM1"
gene_id "ENSG00000000457.13_2"; gene_name "SCYL3"

$ head ID.one|cut -d ";" -f 1,3|sed -e "s/;//g" -e "s/gene_id //g" -e "s/gene_name //g" -e "s/\"//g"
ENSG00000000003.14_2 TSPAN6
ENSG00000000005.5_2 TNMD
ENSG00000000419.12_2 DPM1
ENSG00000000457.13_2 SCYL3
ENSG00000000460.16_4 C1orf112



========================================
|-- 多个连续换行只保留一个换行？//todo
----------------------------------------
实例
$ cat input
1
2

3


4




5

希望的结果：多个换行的，只保留一个换行，紧邻的行还是紧邻。
1
2

3

4

5


1.接近的实现
$ sed '/^$/d' input | awk '{print $0"\n"}' >output
缺点是实现了目标，但是以前连续的行(上例中的1和2行)之间也加了空行。


2.怎么只保留多个连续空行的一个，且不增加其他新空行






========================================
|-- 2行合并为一行
----------------------------------------
把fa文件两行合并为一行
$ sed -n '{N;s/\n/\t/p}' all.4.fasta >all.4.fasta.oneLine
## >chr10:157702-157706(-)	aaaa




========================================
超大文件的打开与预览 (上G的文件 vim 会非常慢)
----------------------------------------
Linux下打开超大文件方法
在Linux下用VIM打开大小几个G、甚至几十个G的文件时，是非常慢的。

这时，我们可以利用下面的方法分割文件，然后再打开。

1 查看文件的前多少行
head -10000 /var/lib/mysql/slowquery.log > temp.log
上面命令的意思是：把slowquery.log文件前10000行的数据写入到temp.log文件中。


2 查看文件的后多少行
tail -10000 /var/lib/mysql/slowquery.log > temp.log
上面命令的意思是：把slowquery.log文件后10000行的数据写入到temp.log文件中。


3 查看文件的几行到几行
sed -n '10,10000p' /var/lib/mysql/slowquery.log > temp.log
上面命令的意思是：把slowquery.log文件第10到10000行的数据写入到temp.log文件中。


4 根据查询条件导出
cat catalina.log | grep   '2017-09-06 15:15:42' > test.log


5 实时监控文件输出
tail -f catalina.out





========================================
----------------------------------------



========================================
----------------------------------------






========================================
----------------------------------------



========================================
----------------------------------------


