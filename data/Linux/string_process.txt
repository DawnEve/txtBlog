linux文本处理

linux简介6|命令行文本处理工具 


awk和sed简明教程  http://agetouch.blog.163.com/blog/static/228535090201732824532207/
教材： O'Reilly 出版的 sed & awk, 2nd Edition http://shop.oreilly.com/product/9781565922259.do
awk https://www.cnblogs.com/jiqianqian/p/7944013.html


linux shell文本处理三大利器：
grep 查找
sed 行编辑器
awk 列文本处理工具
一行指令，轻松搞定。


IBM sed实例(sed 轻量级流编辑器)
https://www.ibm.com/developerworks/cn/linux/shell/sed/sed-1/  还有2和3.

IBM awk实例(awk 适合于文本处理和报表生成。一旦学会就会成为您战略代码库的重要部分的语言)
https://www.ibm.com/developerworks/cn/linux/shell/awk/awk-1/ 还有2和3.
https://www.ibm.com/developerworks/cn/education/aix/au-gawk/index.html








========================================
目录、文件基本命令
----------------------------------------
获取帮助
Linux帮助信息十分丰富，能解决大多数使用上的问题，剩余情况可以通过搜索引擎，或者求助来解决
$ man <command>, 如: 
 man ls 
 man mkdir


常用目录操作命令：
显示当前所在目录的绝对路径	pwd
切换当前目录	cd
创建目录	mkdir
删除目录	rmdir
列出目录下文件清单	ls



Linux中分为绝对路径和相对路径
	绝对路径：从根目录起始的路径，如/usr/bin
	相对路径：从当前路径起始的路径，如../test
	两个特殊的路径：’.’和 ‘..’分别表示当前路径和上层 路径



cat命令：1. 显示文件；2. 创建文件；3. 合并文件
用法	意义
$cat filename	#查看文件filename的内容
$cat > filename	#创建文件filename
$cat >>filename	#追加文件filename
$cat file1 file2 > file3	#上下合并文件file1、file2为文件file3

$paste name passwd >one #左右合并文件成一个文件，逐行合并。
$paste -d"#" name passwd  #用#作分隔符。
注：-d指定分隔符，不加此参数默认为制表符。



常用文件操作命令：
命令名称		用途
cat	查看、创建、合并文件
more	以翻页形式查看文件内容（只能向下翻页）
less	以翻页形式查看文件内容（可上下翻页）
head或者tail	查看一个文件的前n行或者后n行，默认是10行



wc	统计文件的行数、字符数、字节数
cp	复制文件或者目录
mv	移动文件文件夹、或者文件重命名
rm	删除文件或者目录
ln	链接文件(软链接或硬链接)


链接 ln
用法: ln [options] [source] [destination] 
链接分为软连接(-s)和硬链接
	-s	建立软链接
	它的功能是为某一个文件在另外一个位置建立一个同步的链接，当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。

$ ln -s /etc/passwd #相当于在当前文件内放一个快捷方式，指向/etc/passwd。
$ ls -lth
total 16K
lrwxrwxrwx 1 wangjl wangjl 11 Sep  1 16:50 passwd -> /etc/passwd




如何由关键词找到某个文件？
(1)先找到确切文件名
/etc$ ls * -l | grep sysi  #查找/etc/目录下包含sysi的文件名
ls: cannot open directory chatscripts: Permission denied
-rw-r--r-- 1 root root 1543 Apr 16  2012 rc-sysinit.conf
(2)定位绝对路径
$ locate rc-sysinit.conf #定位文件绝对路径
/etc/init/rc-sysinit.conf





========================================
命令find详解: 最常见和强大的查找命令
----------------------------------------
命令格式 find <path> < expression > < cmd >
参数介绍
path： 所要搜索的目录及其所有子目录。默认为当前目录。
expression： 所要搜索的文件的特征。
cmd： 对搜索结果进行特定的处理。
如果什么参数也不加，find 默认搜索当前目录及其子目录，并且不过滤任何结果（也就是返回所有文件），将它们全都显示在屏幕上。


大概可以归类为以下几种：
(1).根据文件(名称、类型、深度、大小)或正则表达式进行匹配。
(2).根据日期和时间查找。
(3).根据文件权限(所属组，拥有者)查找。
(4).借助-exec与其他命令结合使用。




1.根据文件(名称、类型、深度、大小)或正则表达式进行匹配。
(1)列出当前目录及子目录下所有文件和文件夹  find .
./c2_ROW02_R2_trimmed.fq.gz
./text
./text/aa.txt

(2).在/home下查找以.txt结尾的文件名 find /home -name "*.txt"
(3).在/home下查找以.txt结尾的文件名 但忽略大小写。 find /home -iname "*.txt"
(4) 查找当前目录及子目录下所有以.txt和.pdf结尾的文件
$ find . -name "*.txt" -o -name "*.pdf" #或者
$ find . \( -name "*.txt" -o -name "*.pdf" \)
##./c2_ROW02_R2.fastq_trimming_report.txt
##./text/aa.txt
##./word.pdf

要查找所有隐藏的文件
$ find ~ -name ".*" |head


(5)匹配文件路径或文件 find /usr/ -path "*local"
$ find . -path "*bb*"

(6)基于正则表达式匹配文件路径 
$ find . -regex ".*\.(txt|pdf)$"  #不能匹配
$ find . -regex ".*\.\(txt\|pdf\)$" #需要添加转义符，把()|等特殊字符转义
##./c2_ROW02_R2.fastq_trimming_report.txt
##./text/aa.txt
##./word.pdf


(7)找出/home/wangjl/test 路径下不是以.txt结尾的文件
$ find /home/wangjl/test ! -name "*.txt"
##/home/wangjl/test
##/home/wangjl/test/c2_ROW02_R2_trimmed_fastqc.html
##/home/wangjl/test/c2_ROW02_R2_trimmed_fastqc.zip

(8)根据文件类型查找
$ find . -type f #文件
$ find . -type l #链接
$ find . -type d #文件夹

$ find . -type f -name *.R |head #在当前文件夹内查找后缀是.R的文件
$ find . -type d -name test* #在当前目录查找开头是test的文件夹


(9)最大-最小文件深度
$ find ~ -maxdepth 2 -type f
$ find ~ -mindepth 10 -type f
## /home/wangjl/R/x86_64-redhat-linux-gnu-library/3.5/Rcpp/include/Rcpp/api/meat/module/Module.h




(10)根据文件大小进行匹配
文件大小单元： b 块（512字节）, c 字节, w字（2字节）, k千字节, M兆字节, G吉字节

`b' for 512-byte blocks (this is the default if no suffix is used)
`c' for bytes
`w' for two-byte words
`k' for Kibibytes (KiB, units of 1024 bytes)
`M' for Mebibytes (MiB, units of 1024 * 1024 = 1048576 bytes)
`G' for Gibibytes (GiB, units of 1024 * 1024 * 1024 = 1073741824 bytes)


查找大于10k的文件 $ find . -type f -size +10k
查找小于10K的文件 $ find . -type f -size -10k
找到大于50MB且小于100MB的所有文件 $ find /home/wangjl/data/ref/ -type f -size +50M -size -100M |xargs ls -lth
$ find . -type f -size +40c -size -100c |xargs ls -lth








2.根据日期和时间查找。
UNIX/Linux文件系统每个文件都有三种时间戳： 
访问时间（-atime/天，-amin/分钟）：用户最近一次访问时间。 
修改时间（-mtime/天，-mmin/分钟）：文件最后一次修改时间。 
变化时间（-ctime/天，-cmin/分钟）：文件数据元（例如权限等）最后一次修改时间。

(1) 搜索最近2天访问过的文件 $ find . -type f -atime -2
查找超过2天访问的文件(近两天没有访问过) $ find . -type f -atime +2
恰好2天前访问的文件 $ find . -type f -atime 2 

(2)查找访问时间超过10分钟的文件 $ find . -type f -amin +10
查找最近1小时内更改的所有文件 $ find . -cmin -60 #能查到权限变动等

(3)查找最后2-50天修改的文件 $ find . -type f -mtime +2 -mtime -50 | xargs ls -lth

(4)找出比test.cpp修改时间更长的文件 $ find . -type f -newer test.cpp
否定是!: $ find . ! -newer cc.txt






3.根据文件权限(所属组，拥有者)查找。
(1)rwx权限
1)查找文件权限为777的文件 find . -type f -perm 777
2)查找文件权限不是644的文件 $ find . -type f ! -perm  644

3)查找只读文件 $ find / -type f -perm /u=r |head
$ find / -type f -perm /u=r 2>/dev/null/ #为什么过滤语句报错？//todo
4)查找可执行py文件 $ find . -type f -name "*.py" -perm /a=x


(2)ACL权限
1)查找所有SUID文件 $ find / -perm /u=s 2>/dev/null | head
2)查找权限为644的所有SGID为文件 $ find / -perm 2644
$ find / -perm 2644 2>/dev/null #过滤掉报错行，主要是 Permission denied
3)找到具有551权限的粘滞位文件(Sticky Bit) $ find / -perm 1551 2>/dev/null
4)查找所有SGID文件 $ find / -perm /g=s 2>/dev/null | head

(3)所有者
1)当前文件夹内，所有者为wangjl的文件或目录  $ find . -user wangjl
2)查找属于组root的所有文件 $ find ~ -group root
3)-nogroup 查找无有效所属组的文件，即该文件所属的组在 / etc/groups 中不存在。
$ find / -nogroup 2>/dev/null

4)-nouser 查找无有效属主的文件，即该文件的属主在 / etc/passwd 中不存在。
$ find / -nouser 2>/dev/null



4.借助-exec与其他命令结合使用。
(1)查找到拥有者为wangjl 的文件，并将它的拥有者改为hou 
$ find . -type f -user wangjl -exec chown hou {} \; #整个命令前加sudo才能执行

(2)查找所有以.txt文件结尾的文件，并把它写到all.txt文件中去
$ find . -type f -name "*.txt" -exec cat {} \; >all.txt
## cat: ./all.txt: input file is output file

(3)列出所有长度为0 
$ find . -empty
$ find . -type f -empty #查找空文件
$ find . -type d -empty #查找空白文件夹

(4)查找所有777个权限文件，并使用chmod命令将权限设置为644
$ find . -type f -perm 0777 -print -exec chmod 644 {} \;

(5)查找并删除1个或多个文件
查找和删除多个文件，如.mp3或.txt，然后使用。 
$ find -type f -name "*.py" -exec rm -f {} \;







5.疑难杂症
(1) 删除匹配文件: 在查找命令后加上 -delete
删除以.txt结尾的文件 $ find . -type f -name "*.TXT" -delete


(2)find命令-prune用法很严格！
-prune 使用这一选项可以使 find 命令不在当前指定的目录中查找，如果同时使用 - depth 选项，那么 - prune 将被 find 命令忽略。
#在当前目录下查找文件，但不希望在./text目录下查找
$ find . -path ./text -prune -o -name "*.txt"
严格的用法：find 查找文件的目录 -path 需要排除的目录 -prune -o -name 需要查询的内容
注意事项：
1)-prune 必须和 -path， -o 一起使用
2）-prune -o 的顺序不 能调换
3）-name等必须放在-prune -o后面才能使用

如果不做名字筛选，也可以直接打印text目录外的所有文件： $ find . -path ./text -prune -o -print

-o 是or的意思。这里解释不通，why?//todo


(3)-newer file1 ! file2 查找更改时间比文件 file1 新但比文件 file2 旧的文件。
$ find . -newer aa.txt \! \( -newer cc.txt \) #结果不清不楚的 todo





refer:
https://blog.csdn.net/z_xiao_xue/article/details/53925680
https://www.cnblogs.com/Ido-911/p/9638612.html





========================================
字符处理命令概述及资源
----------------------------------------
gerp 查找, awk 根据内容分析并处理, sed 编辑.
(1)grep, egrep, fgrep, rgrep - print lines matching a pattern 打印匹配模式的行。
(2)awk(关键字:分析&处理) 一行一行的分析处理。
mawk - pattern scanning and text processing language模式扫描与文本处理语言。
(3)sed(关键字: 编辑) 以行为单位的文本编辑工具 
sed - stream editor for filtering and transforming text 用于过滤和转换文本的流编辑器。特色是该编辑器能用于pipeline中。

awk和sed简明教程：
	http://agetouch.blog.163.com/blog/static/228535090201732824532207/

AWK 简明教程: http://coolshell.cn/articles/9070.html
sed 简明教程: http://coolshell.cn/articles/9104.html
三十分钟学会AWK: https://segmentfault.com/a/1190000007338373



[论坛]awk,sed,grep,cut等文本操作命令: http://bbs.chinaunix.net/forum-24-1.html


How to Use AWK
http://sparky.rice.edu/~hartigan/awk.html
http://sparky.rice.edu/~hartigan/sed.html


========================================
命令cut基于列处理文件
----------------------------------------

使用,	-	+等分割的数据文件的处理，按列显示。
cut -d: -f1 /ect/passwd
	-d	指定分割字符（默认是tab）
	-f	指定输出的列号
	-c	基于字符进行分割
		cut -c2-6 /etc/passwd

$ grep wangjl /etc/passwd 
wangjl:x:1001:1002::/home/wangjl:/bin/bash

$ grep wangjl /etc/passwd | cut -d: -f3 #输出wangjl用户名对应的uid。
1001





========================================
命令wc（word count）：统计并输出文件的字节数、字数、行数
----------------------------------------
$ wc cat.txt
  7  23 102 cat.txt
行数、单词数、字节数、文件名
	-l  统计行数
	-w  统计单词数
	-c  统计字节数
	-m 统计字符数

$ wc -l /etc/passwd #统计当前系统的用户数量
48 /etc/passwd


========================================
命令sort：用于对文本内容进行排序
----------------------------------------
$ cat out2.txt | sort -r #对指定文本文件的行，按照首字母的倒叙输出
$ sort out2.txt #或者直接使用sort，和上文输出结果相同。

	-r 进行倒序排序
	-n 基于数字进行排序，默认是基于ascii码排序
	-f 忽略大小写
	-u 删除重复行
	-t c	使用c作为分隔符分割为列进行排序
	-k x 当进行基于指定字符分割为列的排序时，指定基于哪个列排序


$ sort /etc/passwd -t : -k 1 -r | head -n 5 #使用:分割，并使用第一列排序，倒叙。只显示前5行。
zhouxm:x:1006:1007:,,,:/home/zhouxm:/bin/bash
zhangd:x:1007:1008:,,,:/home/zhangd:/bin/bash
www-data:x:33:33:www-data:/var/www:/bin/sh
wuyh:x:1009:1010:,,,:/home/wuyh:/bin/bash
wuhj:x:1004:1005::/home/wuhj:/bin/bash





sort命令很强大、控制很细致，要点就是k参数的设置：
KEYDEF is F[.C][OPTS][,F[.C][OPTS]] for start and stop position, where F is a
field number and C a character position in the field; both are origin 1, and
the stop position defaults to the line's end. 

$ cat facebook.txt #定义一个测试文本
baidu 100 5000
google 110 5000
sohu 100 4500
guge 50 3000

$ sort -k 1.2r facebook.txt #按照第一个单词1的第二个字母2，r倒序排序
guge 50 3000
google 110 5000
sohu 100 4500
baidu 100 5000

$ sort -k 2 facebook.txt #按照第二列排序，默认是ascii顺序。注意50在最后。
sohu 100 4500
baidu 100 5000
google 110 5000
guge 50 3000

$ sort -k 2n facebook.txt #按照第二列排序，n指定为按照数字排序。
guge 50 3000
baidu 100 5000
sohu 100 4500
google 110 5000



#按照公司名字第二个字母正序，按照第二列倒序
$ sort -k 1.2 -k 2nr facebook.txt
baidu 100 5000
sohu 100 4500
google 110 5000
guge 50 3000
然而，google行并没有拍到sohu行前面，why？
因为默认是到结尾的，所以需要指定结尾，否则就变成了从第二个字母开始到最后的字符进行排序，当然oh在oo前面了。第一个参数就已经毫无争议的排好序了。
$ sort -k 1.2,1.2 -k 2nr facebook.txt
baidu 100 5000
google 110 5000
sohu 100 4500
guge 50 3000

这样仅仅指定第一列第二个字符排序，则google和sohu行谁在前还是有争议的。
$ sort -k 1.2,1.2 -k 2n facebook.txt #如果-k 2n不加倒序，则sohu行的100<110，跑到前面了。
baidu 100 5000
sohu 100 4500
google 110 5000
guge 50 3000

当然更精确的描述应该是：
$ sort -k 1.2,1.2 -k 2,2nr facebook.txt
baidu 100 5000
google 110 5000
sohu 100 4500
guge 50 3000


更多讨论见 生物慕课 公众号 2018.4.20: sort文本排序命令。


========================================
命令uniq: 用于删除相邻的重复行(去掉重复项，输出重复项)
----------------------------------------
新建文本test2.txt:
$ cat >test2.txt
good
bad
good
better

$ sort test2.txt -u #删除重复行
bad
better
good

$ cat test2.txt | uniq #删除相邻的重复行，不相邻删不掉
good
bad
good
better

$ cat test2.txt |sort|uniq #删除相邻的重复行
bad
better
good


如果想输出重复项，使用以下命令：
$ cat test2.txt | sort | uniq -d
good

想统计重复项的重复次数-c
$ cat test2.txt | sort | uniq -dc
      2 good


统计每个字符出现的次数
$ cat test2.txt | sort | uniq -c
      1 bad
      1 better
      2 good


========================================
命令diff：用于两个文本的比较
----------------------------------------
$ diff a.txt a-new.txt
	-i 忽略大小写
	-b 忽略空格数量的改变
	-u 统一显示比较信息（一般用以生成patch文件）
		diff -u a.txt a-new.txt > final.patch

$ diff out2.txt out2-new.txt 
16a17  #append新增一行
> www
21d21 #delete删除一行
< tmp
24c24	#change修改一行。
< vmlinuz	#前一个文件
---
> vmlinux	#后一个文件


增加参数-u之后：
$ diff -u out2.txt out2-new.txt > final2.patch

$ cat final2.patch 
--- out2.txt	2016-06-10 19:46:54.647591447 -0700
+++ out2-new.txt	2016-06-10 23:13:34.459199288 -0700
@@ -14,11 +14,11 @@
 proc
 root
 run
+www
 sbin
 selinux
 srv
 sys
-tmp
 usr
 var
-vmlinuz
+vmlinux

用于制作补丁。



========================================
命令aspell用以显示检查英文拼写
----------------------------------------
$ aspell list < out3.txt #列出错误信息
cannnot
nowhera
$ aspell check out3.txt #交互式改单词




========================================
命令tr (translate)：处理文本内容（删除、替换），重定向接收文件
----------------------------------------
$ cat out3.txt
ls: cannot accesses nowhere: No such file or directory


删除关键字
	$ tr -d 'TMD' < aa.txt
转换大小写
	$ tr 'a-z' 'A-Z' < aa.txt

$ tr -d 'No' <out3.txt
ls: cannt accesses nwhere:  such file r directry

$ cat out3.txt | tr 'a-z' 'A-Z' > out33.txt 
$ cat out33.txt
$ rm out33.txt 
或者
$ tr 'a-z' 'A-Z' <out3.txt
或者
$  tr 'a-z' 'A-Z' <out3.txt  > out33.txt 
$ cat out33.txt 
LS: CANNOT ACCESSES NOWHERE: NO SUCH FILE OR DIRECTORY

$ cat out3.txt
ls: cannot accesses nowhere: No such file or directory

并没有影响原文！如果想改原文，怎么办？重定向，但是不能重定向到原文件！








========================================
grep 查找与正则表达式
----------------------------------------
正则规则：http://blog.csdn.net/newthinker_wei/article/details/8219293

常用快查：grep命令 - 基于关键字搜索文本
$ grep 'wangjl' /etc/passwd #查找/etc/passwd文件中包含'wangjl'关键字的行
$ find / -user wangjl 2>/dev/null | grep Video	#查找根目录下用户名user为wangjl，且包含Video关键字的文件名。
$ find / -user wangjl 2>&1 | grep Video #或者把标准错误合并到标准输出，然后管道到grep去搜索。

	-i 在搜索的时候忽略大小写
	-n 显示结果所在行数
	-v 输出不带关键字的行，相当于取反操作
	-Ax 在输出的时候包含结果所在行之后的指定行数
	-Bx 在输出的时候包含结果所在行之前的指定行数

$ find / -user wangjl 2>&1 | grep Video -i #忽略大小写
$ grep wang /etc/passwd #在文件/etc/passwd中输出匹配关键字wang的行
$ grep wang /etc/passwd -n #显示行号



可以在grep中使用perl语法，加-P参数。grep -E它是相同的。egrep不会起作用（这将是贪婪匹配的）。
* (0 or more) greedy matching
+ (1 or more) greedy matching
*? (0 or more) non-greedy matching
+? (1 or more) non-greedy matching

for VIM:
* (0 or more) greedy matching
\+ (1 or more) greedy matching
\{-} (0 or more) non-greedy matching
\{-n,} (at least n) non-greedy matching



1.grep(关键字: 截取) 文本搜集工具, 结合正则表达式非常强大。
一般格式： grep 正则 fileName
比如： $ grep 'wangjl' /etc/passwd #查找并返回passwd文件中包含wangjl的行。

主要参数 []
	-c : 只输出匹配的行
	-I : 不区分大小写
	-h : 查询多文件时不显示文件名
	-l : 查询多文件时, 只输出包含匹配字符的文件名
	-n : 显示匹配的行号及行
	-v : 显示不包含匹配文本的所有行
	
基本工作方式: grep 要匹配的内容 文件名
例如:
grep 'test' d* 显示所有以d开头的文件中包含test的行
grep 'test' aa bb cc 显示在 aa bb cc 文件中包含test的行
grep '[a-z]\{5}\' aa 显示所有包含字符串至少有5个连续小写字母的串


2.正则表达式分类：
1)基本的正则表达式（Basic Regular Expression 又叫 Basic RegEx  简称 BREs）
2)扩展的正则表达式 -E（Extended Regular Expression 又叫 Extended RegEx 简称 EREs）
3)Perl 的正则表达式 -P（Perl Regular Expression 又叫 Perl RegEx 简称 PREs）
 说明：只有掌握了正则表达式，才能全面地掌握 Linux 下的常用文本工具（例如：grep、egrep、GUN sed、 Awk 等） 的用法
注意：linux的正则支持有三种，其中perl兼容的是功能最强大的。
默认的是基本的正则，不支持数字元字符，但是加上-P指定Perl兼容的正则，就支持数字元字符了。

$ grep -P '\d' /etc/passwd #任何带数字的行
		\d: 任何数字 [0-9] grep默认不支持
		\D: 任何非数字[^0-9] grep默认不支持



3.正则表达式的数量词
正则表达式可能被以下重复修饰符中的一个修饰：
    ?      {0,1}The preceding item is optional and matched at most once.
    *      {0,}The preceding item will be matched zero or more times.
    +      {1,}The preceding item will be matched one or more times.
    {n}    The preceding item is matched exactly n times.
    {n,}   The preceding item is matched n or more times.
    {,m}   The  preceding  item  is matched at most m times.  This is a GNU extension.
    {n,m}  The preceding item is matched at least n  times,  but  not  more than m times.
比如：$ grep 'ro*t' /etc/passwd   # {0,}表示0个或多个o。


$ cd /etc
$ grep 'ro+t' passwd   #啥也没有
$ grep -P 'ro+t' passwd #使用Perl兼容的可以-P
root:x:0:0:root:/root:/bin/bash

$ grep 'ro\+t' passwd #或者使用转义字符\
root:x:0:0:root:/root:/bin/bash


4.正则的分组()
使用小括号()，默认需要加转义字符\(text\)
如果加入-P参数，则可以直接输入(text)

$ grep -P '(34)+' passwd #带有1个及以上34的行



5.范围
[]表示范围，比如
	[0-9]表示任意一位数字，等价于\d
	[a-z]任意一个小写字母，
	[A-Z]任意大写字母，
	[a-zA-Z]任意字母
[]内的^表示否定。
$ grep -P "[0-9]{4,6}" passwd # 含4到6个数字的行
$ grep -P "[^0-9]" passwd #匹配含有非数字的行
$ grep -P "[^0-9a-zA-Z]" passwd #匹配含有非数字、非大小写字母的行





6.位置界定^$
 ^首位
 $结尾
$ grep '^c.*e$' passwd #找c开头，e结尾的行




7.任意字符串 .* # .在[]外表示任意字符，*表示前面字符任意多个，加起来.*就是任意字符串了。
$ grep -P '^r.*' passwd #r开头，后面跟着任意字符
$ grep -P 'm.*c' passwd #m后面有c的行，默认贪婪匹配，匹配尽量后的c

如果想使用非贪婪匹配，找到最近的c就停止匹配，可以首尾加上分隔符\b 
$ grep -P '\bm.*c\b' passwd





8.贪婪匹配
更好的非贪婪匹配是量词后加上?
当"?"字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，“o+?”将匹配单个“o”，而“o?”将匹配所有“o”。

注：非贪婪只对量词有效果，如 * + {1，9} 等。
默认是贪婪模式；在量词后面直接加上一个问号？就是非贪婪模式。

$ grep -P 'm.*?c' passwd #从每行的m开始，找到第一个c停止
$ echo 12,23,24|grep -P '([0-9]\d{1,})' 
$ echo 12,23,24|grep -P '([0-9]\d{1,}?)' 



9.利用grep打印匹配的上下几行
https://www.cnblogs.com/wangkongming/p/3684905.html

如果在只是想匹配模式的上下几行，grep可以实现。
$ grep -5 'parttern' inputfile   //打印匹配行的前后5行
$ grep -C 5 'parttern' inputfile //打印匹配行的前后5行

$ grep -A 5 'parttern' inputfile //打印匹配行的后5行
$ grep -B 5 'parttern' inputfile //打印匹配行的前5行




10.设置centOS下的grep高亮显示匹配项（ubuntu默认已经高亮）
方法1：设置别名
编辑 vim ~/.bashrc
添加如下一行内容：
alias grep='grep --color=auto'
source ~/.bashrc    //使配置生效；

方法2：
https://www.linuxidc.com/Linux/2014-09/106871.htm


11. 逻辑运算
$ grep pattern1 filename | grep pattern2  #"与"操作（其实就是多次筛选）。显示既匹配 pattern1又匹配 pattern2 的行。

$ grep -E 'k1|k2' filename #或 （如果没有空格，则可以不加引号）
$ grep 'pattern1\|pattern2' filename

$ grep -v "k1" filename #非, 不满足当前条件的所有内容行.


// 找出文件（filename）中包含123或者包含abc的行
grep -E '123|abc'filename
egrep '123|abc' filename   // 用egrep同样可以实现. egrep相当于grep -E。
awk '/123|abc/' filename  // awk 的实现方式




100.其他细节
\s	匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]。
\S	匹配任何非空白字符。等价于[^ \f\n\r\t\v]。










========================================
awk(关键字:分析&处理) 一行一行的分析处理
----------------------------------------
gawk是awk的增强版，如果有些教程中的awk命令不能运行，请考虑安装gawk。
$ sudo apt install gawk


1.定义与作用
mawk - pattern scanning and text processing language模式扫描与文本处理语言。
An AWK program is a sequence of pattern {action} pairs and user function definitions.

典型用途：使用AWK可以做很多任务，下面是其中一些
	文本处理
	输出格式化的文本报表
	执行算数运算
	执行字符串操作等等







2.工作流
要成为AWK编程专家，你需要先知道它的内部实现机制，AWK遵循了非常简单的工作流 - 读取，执行和重复，下图描述了AWK的工作流。
(1)BEGIN block: BEGIN {awk-commands} #可选
(2)Read a line from input stream
(3)Execute AWK commands on a line: /pattern/ {awk-commands}
(4)Repeat if it is not End of file,GOTO(2)
(5)END block: END {awk-commands} #可选

awk的处理流程是:
1) 读第一行, 将第一行资料填入变量 $0, $1... 等变量中
2) 依据条件限制, 执行动作
3) 接下来执行下一行









3.一般格式：$ awk 'pattern1{动作1}pattern2{动作2}' filename
	awk 也可以读取来自前一个指令的 standard input。
	相对于sed常常用于一整行处理, awk则比较倾向于一行当中分成数个"字段"(区域)来处理, 默认的分隔符是空格键或tab键

示例文件marks.txt
$ cat marks.txt
1)  Amit    Physics  80
2)  Rahul   Maths    90
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89


示例文件netstat.txt。
$ netstat -na |head -n 16 > netstat.txt
$ ls #用vim删掉第一行
netstat.txt
$ cat netstat.txt
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp        0     36 172.16.112.86:22        172.16.113.174:55033    ESTABLISHED
tcp        0      0 172.16.112.86:22        172.16.113.174:56000    ESTABLISHED
tcp6       0      0 :::22                   :::*                    LISTEN
tcp6       0      0 :::3306                 :::*                    LISTEN
udp        0      0 127.0.1.1:53            0.0.0.0:*
udp        0      0 0.0.0.0:68              0.0.0.0:*
udp        0      0 0.0.0.0:631             0.0.0.0:*
udp        0      0 0.0.0.0:55418           0.0.0.0:*
udp        0      0 0.0.0.0:5353            0.0.0.0:*
udp6       0      0 :::58832                :::*
udp6       0      0 :::5353                 :::*
raw6       0      0 :::58                   :::*                    7



(1)可以用 $ awk '{print}' marks.txt 来代替cat命令，查看文件内容。

awk可以接受标准输入的数据，例如:$ last -n 5 | awk '{print $1 "\t" $3}'  #输入最后5次登陆，只显示用户名和ip。
这里大括号内$1"\t"$3 之间不加空格也可以, 不过最好还是加上个空格, 另外注意"\t"是有双引号的, 因为本身这些内容都在单引号内
$0 代表整行 $1代表第一个区域, 依此类推

#打印第3和4列，用制表符隔开:
$ awk '{print $3 "\t" $4}' marks.txt
Physics 80
Maths   90
Biology 87
English 85
History 89

所以, AWK一次处理是一行, 而一次中处理的最小单位是一个区域。

$ awk '/a/{print $0}' marks.txt 可以简化为
$ awk '/a/{print}' marks.txt 可以进一步简化为
$ awk '/a/' marks.txt 没有制定操作，就是默认打印匹配到的整行。
2)  Rahul   Maths    90
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89



(2)逻辑判断 > < >= <= == !== , 赋值直接使用=

另外还有3个变量, 
	NF: 当前行的第几个字段(number of fields in the current record). 每个字段保存在$1, $2, ..., $NF中.  The built-in variable NF is set to the number of fields.
	NR 目前处理到第几行(current record number in the total input stream).
	FS 目前的分隔符(input record separator, initially = "\n").

	
例1 $ cat /etc/passwd | awk '{FS=":"} $3<10 {print $1 "\t" $3}'
首先定义分隔符为:, 
然后判断, 注意看, 判断没有写在{}中, 
然后执行动作, 
FS=":"这是一个动作, 赋值动作, 不是一个判断, 所以写在{}中

$  awk  'BEGIN{FS=":"} {print $1,$3,$6}' /etc/passwd
也等价于：（-F的意思就是指定分隔符）
$ awk  -F: '{print $1,$3,$6}' /etc/passwd

注：如果你要指定多个分隔符，你可以这样来：
awk -F '[;:]'




例2 $ awk '/linux/ {print NR}' out.txt #将带有linux的行的行号打印出来, 注意//之间可以使用正则表达式
$ cat out.txt
this is a linux system.
line2
Ubuntu is a release of linux.
$ awk '/linux/ {print NR}' out.txt
1
3


例3：&&并的使用。显示第3列是0且第6列是LISTEN的行。
$ awk '$3==0 && $6=="LISTEN"' netstat.txt
tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
tcp6       0      0 :::3306                 :::*                    LISTEN

如果需要保留表头，可以引入内建变量NR：
$ awk '$3==0 && $6=="LISTEN" || NR==1' netstat.txt
Active Internet connections (servers and established)
tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
tcp6       0      0 :::3306                 :::*                    LISTEN




(3) BEGIN END , 给程序员一个初始化和收尾的工作

BEGIN之后列出的操作在{}内将在awk开始扫描输入之前执行, 而END{}内的操作, 将在扫描完输入文件后执行.

例1：通过BEGIN给文本marks.txt添加表头
$ awk 'BEGIN{printf "Sr No\tName\tSub\tMarks\n"}{print}' marks.txt
Sr No   Name    Sub     Marks
1)  Amit    Physics  80
2)  Rahul   Maths    90
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89

例2： prints all lines that start with an AWK identifier.打印开头符合identifier的行。
$ head /etc/passwd | awk 'BEGIN { identifier = "ro*t" } $0 ~ "^" identifier' /etc/passwd 
（r开头，后面若干个0，接着是t的）


-v 变量赋值选项：该选项将一个值赋予一个变量，它会在程序开始之前进行赋值。
例3：$ awk -v name=Jimmy 'BEGIN{printf "Name = %s\n", name}'
Name=Jimmy




(4)条件、循环控制语句，及从文件输入pattern。

awk {}内, 可以使用 if else ,for(i=0;i<10;i++), i=1 while(i<NF)
可见, awk的很多用法都等同于C语言, 比如"\t" 分隔符, print的格式, if, while, for 等等。


## 注意：每个句子结束要使用分号
awk '{if ($1==1) print "A"; else if ($1==2) print "B"; else print "C"}'

例：提取RNA的末端坐标，+链则需要3端，-链需要5端
chr10	101688	101706	E00300:157:HWCKKCCXX:6:2220:10094:68096	255	-
chr10	112858	112912	E00300:165:H3CMMALXX:8:2219:25997:49689	255	+
chr10	118384	118494	E00300:165:H3CMMALXX:5:2105:10987:37471	255	+

$ head 225.bed |awk '{if($6=="+") print $1":"$3":"$6; else print $1":"$2":"$6}'
chr10:101688:-
chr10:112912:+
chr10:118494:+





例：计数文件中独特单词的个数。count the number of unique "real words".

1)把模式写入文件$ cat cmd.awk
    BEGIN { FS = "[^A-Za-z]+" }
    { for(i = 1 ; i <= NF ; i++)  word[$i] = "" }
    END { delete word[""]
          for ( i in word )  cnt++
          print cnt
    }

$ cat out.txt
this is a linux system.
line2
Ubuntu is a release of linux.

2)对文件out.txt使用以上规则计数
$ awk -f cmd.awk out.txt #-f表示从文件中读取pattern。
9


(5)运算符

算术运算符：+-*/%
# awk 'BEGIN {a=50;b=20;print "(a+b)=",(a+b)}'
(a+b)= 70
# awk 'BEGIN {a=50;b=20;print "(a%b)=",(a%b)}'
(a%b)= 10 #取余数

自增自减与C语言一致。
# awk 'BEGIN { a = 10; b = a--; printf "a = %d, b = %d\n", a, b }'
# 猜一下a和b分别是多少？

赋值操作符有+=, -=, *=, /=, %=,乘方符^=,**=(这个不支持)
# awk 'BEGIN{a=2;a^=10;printf "a=%d\n",a}'
a=1024

关系操作符>,<,>=,<=,==,!=
# awk 'BEGIN{a=10; b=10; if(a==b)print"a==b"}'
a==b



逻辑操作符 或(||),且(&&)非,(!)
打印第2到4行
# awk '{if (NR >= 2 && NR <= 4) printf "line%d:%s\n", NR,$0}' marks.txt
line2:2)  Rahul   Maths    90
line3:3)  Shyam   Biology  87
line4:4)  Kedar   English  85

三元操作符
# awk 'BEGIN{a=10;b=2;max=(a>b)?a:b;print "max=",max;}'
max= 10

字符串连接，就是用空格连接字符串变量。
# awk 'BEGIN{a="Hello,"; b="world";c=a b;print c;}'
Hello,world

数组成员操作符
# awk 'BEGIN{arr[0]=10;arr[1]=11;arr[2]=12; for(i in arr) printf "arr[%d]=%d\n",i,arr[i]}'
arr[0]=10
arr[1]=11
arr[2]=12







4.awk正则表达式
正则表达式操作符使用 ~ 和 !~ 分别代表匹配和不匹配。
也可以按字段匹配。~ 表示模式开始。/ /中是模式，表达式前后添加反斜线，与js类似吧
# tail -n 40 /var/log/nginx/access.log | awk '$0 ~ /ip\[127\.0\.0\.1\]/'
# tail -n 40 /var/log/auth.log | awk '/wangjl/'

awk可以像grep一样的去匹配每一行，就像这样：
例1：awk '/LISTEN/' netstat.txt
tcp        0      0 127.0.1.1:53            0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN
tcp6       0      0 :::3306                 :::*                    LISTEN



例2： 第六个字段符合正则的行
$ awk '$6 ~ /ESTA/ || NR==1 {print NR,$4,$5,$6}' OFS=";" netstat.txt
1;Local;Address;Foreign
4;172.16.112.86:22;172.16.113.174:55033;ESTABLISHED
5;172.16.112.86:22;172.16.113.174:56000;ESTABLISHED

例3： LISTEN 状态的行
$ netstat -an| awk '$6 ~ /LISTEN/ ||NR==2 {print NR,$4,$5,$6}' OFS="\t"
2       Local   Address Foreign
3       127.0.1.1:53    0.0.0.0:*       LISTEN
4       0.0.0.0:22      0.0.0.0:*       LISTEN
7       :::22   :::*    LISTEN
8       :::3306 :::*    LISTEN

例4：可以使用 “/LISTEN|ESTABLISHED/” 来匹配 LISTEN 或者 ESTABLISHED :
$ netstat -an| awk '$6 ~ /LISTEN|ESTABLISHED/ ||NR==2 {print NR,$4,$5,$6}' OFS="\t"

例5：取反
p$ awk '$6 !~ /LISTEN|ESTABLISHED/ ||NR==1 {print NR,$4,$5,$6}' OFS="\t" netstat.txt
1       Local   Address Foreign
8       127.0.1.1:53    0.0.0.0:*

或者$ awk '!/LISTEN/' netstat.txt








5.内置变量与自定义变量
(1)内置变量参考[$ man awk 的 7. Builtin-variables]

内置变量Builtin-variables
The following variables are built-in and initialized before program execution.程序执行前初始化
	ARGC      number of command line arguments.
	ARGV      array of command line arguments, 0..ARGC-1.
	CONVFMT   format for internal conversion of numbers to string, initially = "%.6g".
	ENVIRON   array  indexed  by  environment  variables.  An environment string, var=value is stored as
			ENVIRON[var] = value.
	FILENAME  当前文件名 name of the current input file.
	FNR       文件内自己的行号 current record number in FILENAME.
	FS        字段分隔符 splits records into fields as a regular expression.
	NF        当前行字段数 number of fields in the current record.
	NR        当前记录数 current record number in the total input stream.
	OFMT      format for printing numbers; initially = "%.6g".
	OFS       显示时字段分割符 inserted between fields on output, initially = " ".
	ORS       terminates each record on output, initially = "\n".
	RLENGTH   length set by the last call to the built-in function, match().
	RS        记录分隔符，默认是换行符 input record separator, initially = "\n".
	RSTART    index set by the last call to match().
	SUBSEP    used to build multiple array subscripts, initially = "\034".


例1: 内置变量 FILENAME 表示当前文件名：
$ awk 'END {print FILENAME}' marks.txt
marks.txt

例2: OFS指定显示分隔符
$ awk  -F: '{print $1,$3,$6}' OFS=";" /etc/passwd | head
root;0;/root
daemon;1;/usr/sbin
bin;2;/bin
sys;3;/dev
sync;4;/bin



(2)
自定义变量统计含a的行数：
$ awk '/a/{++cnt} END {print "Count=", cnt}' marks.txt
Count= 4

使用语句打印出来哪些行含有r：
$ awk '/r/{++cnt; print} END {print "Count=", cnt}' marks.txt
4)  Kedar   English  85
5)  Hari    History  89
Count= 2








6.内置函数、自定义函数与系统命令
AWK提供了很多方便的内建函数供编程人员使用。最好先知道大概，使用的时候再查手册。
https://www.gnu.org/software/gawk/manual/gawk.html#Built_002din
内置函数： http://www.cnblogs.com/chengmo/archive/2010/10/08/1845913.html

数学函数
	atan2(y, x)
	cos(expr)
	exp(expr)
	int(expr)
	log(expr)
	rand
	sin(expr)
	sqrt(expr)
	srand([expr])

字符串函数
	asort(arr [, d [, how] ])
	asorti(arr [, d [, how] ])
	gsub(regex, sub, string)
	index(str, sub)
	length(str)
	match(str, regex)
	split(str, arr, regex)
	sprintf(format, expr-list)
	strtonum(str)
	sub(regex, sub, string)
	substr(str, start, l)
	tolower(str)
	toupper(str)

时间函数
	systime
	mktime(datespec)
	strftime([format [, timestamp[, utc-flag]]])

字节操作函数
	and
	compl
	lshift
	rshift
	or
	xor

其它
	close(expr) 关闭管道文件


例1: 使用command | getline var可以实现将命令的输出写入到变量var。
$ awk 'BEGIN {
     "date" | getline current_time
     close("date")
     print "Report printed on " current_time
}'

只能读取一行，其中command是linux命令。
$ awk 'BEGIN{"ls"|getline txts; close("ls"); print txts}'
Desktop


函数是程序基本的组成部分，AWK允许我们自己创建自定义的函数。一个大型的程序可以被划分为多个函数，每个函数之间可以独立的开发和测试，提供可重用的代码。

下面是用户自定义函数的基本语法
function function_name(argument1, argument2, ...) { 
   function body
}
return 用于用户自定义函数的返回值。


例2:自定义加法
首先，创建一个functions.awk文件，包含下面的awk命令
$ cat awk_2.cmd
function addition(num1, num2) {
   result = num1 + num2
   return result
}
BEGIN {
   res = addition(10, 20)
   print "10 + 20 = " res
}

$ awk -f awk_2.cmd
10 + 20 = 30


例3:执行系统命令
system函数用于执行操作系统命令并且返回命令的退出码到awk。
END {
     system("date | mail -s 'awk run done' root")
}

$ awk 'BEGIN{system("date")}'
2017年 08月 12日 星期六 06:37:36 CST
$ awk 'BEGIN{system("date | tr [A-Z] [a-z]")}'
2017年 08月 12日 星期六 06:37:50 cst







7.文件操作：拆分文件、重定向与管道

awk拆分文件很简单，使用重定向就好了。
重定向操作符跟在print和printf函数的后面，与shell中的用法基本一致。
	print DATA > output-file #覆盖式写入
	print DATA >> output-file #追加到文件结尾

例如，下面两条命令输出是一致的
$ echo "Hello, World !!!" > /tmp/message.txt
$ awk 'BEGIN { print "Hello, World !!!" > "/tmp/message.txt" }'

例1：按第6例分隔文件，相当的简单（其中的NR!=1表示不处理表头,$6!=""表示如果第六行是空则忽略掉）。
$ awk 'NR!=1 && $6!="" {print>$6}' netstat.txt
$ ls
7  ESTABLISHED  LISTEN  marks.txt  netstat.txt


例2：也可以把指定的列输出到文件：
$ ls
marks.txt  netstat.txt
$ awk 'NR!=1 && $6!="" {print $4,$5 > $6}' netstat.txt
$ ls
7  ESTABLISHED  LISTEN  marks.txt  netstat.txt
$ cat LISTEN
127.0.1.1:53 0.0.0.0:*
0.0.0.0:22 0.0.0.0:*
:::22 :::*
:::3306 :::*



例3:再复杂一点：（注意其中的if-else-if语句，可见awk其实是个脚本解释器）
$ ls
marks.txt  netstat.txt
$ awk 'NR!=1{
	if($6 ~ /TIME|ESTABLISHED/) print > "1.txt";
	else if($6 ~ /LISTEN/) print > "2.txt";
	else print > "3.txt" 
}' netstat.txt
$ ls
1.txt  2.txt  3.txt  marks.txt  netstat.txt
$ cat 3.txt
udp        0      0 127.0.1.1:53            0.0.0.0:*
udp        0      0 0.0.0.0:68              0.0.0.0:*
udp        0      0 0.0.0.0:631             0.0.0.0:*
udp        0      0 0.0.0.0:55418           0.0.0.0:*
udp        0      0 0.0.0.0:5353            0.0.0.0:*
udp6       0      0 :::58832                :::*
udp6       0      0 :::5353                 :::*
raw6       0      0 :::58                   :::*   



例4:管道
除了将输出重定向到文件之外，我们还可以将输出重定向到其它程序，与shell中一样，我们可以使用管道操作符|。
r$ echo "hello"|tr [a-z] [A-Z]
HELLO
$ awk 'BEGIN { print "hello, world !!!" | "tr [a-z] [A-Z]" }'
HELLO, WORLD !!!


例5:双向连接
AWK中可以使用|&进行双向连接，那么什么是双向连接呢？一种常见的场景是我们发送数据到另一个程序处理，然后读取处理结果，这种场景下就需要打开一个到另外一个进程的双向管道了。第二个进程会与gawk程序并行执行，这里称其为 协作进程。与单向连接使用|操作符不同的是，双向连接使用|&操作符。

do {
    print data |& "subprogram"
    "subprogram" |& getline results
} while (data left to process)
close("subprogram")

第一次I/O操作使用了|&操作符，gawk会创建一个到运行其它程序的子进程的双向管道，print的输出被写入到了subprogram的标准输入，而这个subprogram的标准输出在gawk中使用getline函数进行读取。





8.统计
例1：下面的命令计算所有的C文件，CPP文件和H文件的文件大小总和。
$ ls -l  *.cpp *.c *.h | awk '{sum+=$5} END {print sum}'

# ls -l *.zip
-rw-rw-r-- 1 wangjl wangjl 25072402 8月   4 10:49 mapdata_2.2-6.zip
-rw-rw-r-- 1 wangjl wangjl  3631621 8月   4 10:37 maps_3.2.0.zip
-rw-rw-r-- 1 wangjl wangjl  1823908 8月   4 08:10 maptools_0.9-2.zip
-rw-rw-r-- 1 wangjl wangjl  1219777 8月   4 10:47 plyr_1.8.4.zip
-rw-rw-r-- 1 wangjl wangjl  1538955 8月   4 08:09 sp_1.2-5.zip
# ls -l *.zip | awk '{sum+=$5} END{print sum}'
33286663

对test文件某列求和：-F，用，号分隔，求第3行的和
$ awk -F,  '{sum += $3};END {print sum}' test



例2：（利用数组）统计每种状态（第六列）的数量

# awk 'BEGIN{print "State,count"}NR!=1 {a[$6]++} END{for(i in a) print i "," a[i]}' netstat.txt
State,count
,7
LISTEN,4
ESTABLISHED,2
7,1



例3: （利用关联数组）统计每个用户(第一列)的进程的占了多少内存（注：sum的RSS那一列）
# ps aux | head
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.3 185340  3444 ?        Ss   7月31   0:04 /sbin/init splash
root         2  0.0  0.0      0     0 ?        S    7月31   0:00 [kthreadd]
root         4  0.0  0.0      0     0 ?        S<   7月31   0:00 [kworker/0:0H]
root         6  0.0  0.0      0     0 ?        S    7月31   0:19 [ksoftirqd/0]
root         7  0.0  0.0      0     0 ?        S    7月31   1:06 [rcu_sched]
root         8  0.0  0.0      0     0 ?        S    7月31   0:00 [rcu_bh]
root         9  0.0  0.0      0     0 ?        S    7月31   0:00 [migration/0]
root        10  0.0  0.0      0     0 ?        S<   7月31   0:00 [lru-add-drain]
root        11  0.0  0.0      0     0 ?        S    7月31   0:04 [watchdog/0]

# ps aux | awk 'NR!=1 {a[$1]+=$6;} END{ for(i in a) print i "," a[i] "KB"}'
vboxadd,177912KB
syslog,644KB
colord,528KB
nobody,816KB
avahi,2324KB
wangjl,259812KB
whoopsie,3292KB
rtkit,292KB
root,115712KB
message+,3088KB

按照使用数量倒序排序
$ ps aux | awk 'NR!=1 {a[$1]+=$6;} END{ for(i in a) print i " " a[i] " KB"}' | sort -k2,2nr
rqfu 95183176 KB
xfwang 6944028 KB
hou 1385552 KB
wangjl 260916 KB
root 105544 KB
rstudio+ 28628 KB
yao 14248 KB
pengfei 13064 KB
postfix 4552 KB
polkitd 3536 KB
dbus 1672 KB
yllin 1280 KB




例4:综合统计
利用GEGIN,END语句求学生成绩表的行列小计。

$ cat score.txt
Marry   2143 78 84 77
Jack    2321 66 78 45
Tom     2122 48 77 71
Mike    2537 87 97 95
Bob     2415 40 57 62


$ cat cal.awk
#!/bin/awk -f
#运行前
BEGIN {
    math = 0
    english = 0
    computer = 0
 
    printf "NAME    NO.   MATH  ENGLISH  COMPUTER   TOTAL\n"
    printf "---------------------------------------------\n"
}
#运行中
{
    math+=$3
    english+=$4
    computer+=$5
    printf "%-6s %-6s %4d %8d %8d %8d\n", $1, $2, $3,$4,$5, $3+$4+$5
}
#运行后
END {
    printf "---------------------------------------------\n"
    printf "  TOTAL:%10d %8d %8d \n", math, english, computer
    printf "AVERAGE:%10.2f %8.2f %8.2f\n", math/NR, english/NR, computer/NR
}


我们来看一下执行结果：（也可以这样运行 ./cal.awk score.txt）
$ awk -f cal.awk score.txt #-f表示从文件读取awk命令
NAME    NO.   MATH  ENGLISH  COMPUTER   TOTAL
---------------------------------------------
Marry  2143     78       84       77      239
Jack   2321     66       78       45      189
Tom    2122     48       77       71      196
Mike   2537     87       97       95      279
Bob    2415     40       57       62      159
---------------------------------------------
  TOTAL:       319      393      350
AVERAGE:     63.80    78.60    70.00





9.环境变量
即然说到了脚本，我们来看看怎么和环境变量交互：
（使用-v参数和ENVIRON，使用ENVIRON的环境变量需要export）
# x=5
# y=10
# export y
# echo $x $y

# cat score.txt
Marry   2143 78 84 77
Jack    2321 66 78 45
Tom     2122 48 77 71
Mike    2537 87 97 95
Bob     2415 40 57 62

# awk -v val=$x '{print $1,$2,$3,$4+val, $5+ENVIRON["y"]}' OFS="\t" score.txt
Marry   2143    78      89      87
Jack    2321    66      83      55
Tom     2122    48      82      81
Mike    2537    87      102     105
Bob     2415    40      62      72





10.杂项
# 从file文件中找出长度大于80的行
$ awk 'length>80' file

#按连接数查看客户端IP
netstat -ntu | awk '{print $5}' | cut -d: -f1 | sort | uniq -c | sort -nr

#打印99乘法表
seq 9 | sed 'H;g' | awk -v RS='' '{for(i=1;i<=NF;i++)printf("%dx%d=%d%s", i, NR, i*NR, i==NR?"\n":"\t")}'




11.经典案例

(1)awk选取该文件表头为name的那一列数据
$ head gwas.bed
chrom	chromStart	chromEnd	name
chr1	161475749	161475750	rs10494360
chr1	194329560	194329561	rs10921544
chr1	9408958	9408959	rs11121380
chr1	117280695	117280696	rs112300936
chr1	44558671	44558672	rs114940806
chr1	55505646	55505647	rs11591147
chr1	117751364	117751365	rs12046117
chr1	162449408	162449409	rs12047961

$ awk 'NR==1{for(i=1;i<=NF;i++)a[$i]=i;next}{print $a["name"]}' gwas.bed 
rs10494360
rs10921544
rs11121380
rs112300936
rs114940806
rs11591147
rs12046117
rs12047961

分析: 
一共两句，第一句是处理第一行，第二句是处理之后的所有行。
重点是a这个数组，第一句定义 a["chrom"]=1, a["chromStart"]=2, a["chromEnd"]=3, a["name"]=4.
第二句引用a["name"]=4，所以print $a["name"]就是print $4



结束语： awk 是相当复杂的工具, 真正使用时, 再细看man awk吧. 
内建变量，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Built_002din-Variables
流控方面，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Statements
内建函数，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Built_002din
正则表达式，参看：http://www.gnu.org/software/gawk/manual/gawk.html#Regexp

https://segmentfault.com/a/1190000007338373
http://www.gnu.org/software/gawk/manual/gawk.html
https://www.gnu.org/software/gawk/manual/gawk.html





========================================
|-- awk 速查表
----------------------------------------
https://www.cnblogs.com/xudong-bupt/p/3721210.html

awk是行处理器: 相比较屏幕处理的优点，在处理庞大文件时不会出现内存溢出或是处理缓慢的问题，通常用来格式化文本信息
awk处理过程: 依次对每一行进行处理，然后输出
awk命令形式: awk [-F|-f|-v] 'BEGIN{} //{command1; command2} END{}' file
[-F|-f|-v]   大参数，-F指定分隔符，-f调用脚本，-v定义变量 var=value
''          引用代码块
BEGIN   初始化代码块，在对每一行进行处理之前，初始化代码，主要是引用全局变量，设置FS分隔符
//           匹配代码块，可以是字符串或正则表达式
{}           命令代码块，包含一条或多条命令
；          多条命令使用分号分隔
END      结尾代码块，在对每一行进行处理之后再执行的代码块，主要是进行最终计算或输出结尾摘要信息



## 特殊要点:
$0           表示整个当前行
$1           每行第一个字段
NF          字段数量变量
NR          每行的记录号，多文件记录递增
FNR        与NR类似，不过多文件记录不递增，每个文件都从1开始
\t            制表符
\n           换行符
FS          BEGIN时定义分隔符
RS       输入的记录分隔符， 默认为换行符(即文本是按一行一行输入)
~            匹配，与==相比不是精确比较
!~           不匹配，不精确比较
==         等于，必须全部相等，精确比较
!=           不等于，精确比较
&&　     逻辑与
||             逻辑或
+            匹配时表示1个或1个以上
/[0-9][0-9]+/   两个或两个以上数字
/[0-9][0-9]*/    一个或一个以上数字
FILENAME 文件名
OFS      输出字段分隔符， 默认也是空格，可以改为制表符等
ORS        输出的记录分隔符，默认为换行符,即处理结果也是一行一行输出到屏幕
-F'[:#/]'   定义三个分隔符


## print & $0
print 是awk打印指定内容的主要命令
awk '{print}'  /etc/passwd   ==   awk '{print $0}'  /etc/passwd  
awk '{print " "}' /etc/passwd                                           //不输出passwd的内容，而是输出相同个数的空行，进一步解释了awk是一行一行处理文本
awk '{print "a"}'   /etc/passwd                                        //输出相同个数的a行，一行只有一个a字母
awk -F":" '{print $1}'  /etc/passwd 
awk -F: '{print $1; print $2}'   /etc/passwd                   //将每一行的前二个字段，分行输出，进一步理解一行一行处理文本
awk  -F: '{print $1,$3,$6}' OFS="\t" /etc/passwd        //输出字段1,3,6，以制表符作为分隔符



## -f指定脚本文件
awk -f script.awk  file
BEGIN{
FS=":"
}
{print $1} //效果与awk -F":" '{print $1}'相同,只是分隔符使用FS在代码自身中指定

$ awk 'BEGIN{X=0} /^$/{ X+=1 } END{print "I find",X,"blank lines."}' test 
I find 4 blank lines.

$ ls -l|awk 'BEGIN{sum=0} !/^d/{sum+=$5} END{print "total size is",sum}'    //计算文件大小
total size is 17487


## -F指定分隔符
$1 指指定分隔符后，第一个字段，$3第三个字段， \t是制表符
一个或多个连续的空格或制表符看做一个定界符，即多个空格看做一个空格
awk -F":" '{print $1}'  /etc/passwd
awk -F":" '{print $1 $3}'  /etc/passwd                       //$1与$3相连输出，不分隔
awk -F":" '{print $1,$3}'  /etc/passwd                       //多了一个逗号，$1与$3使用空格分隔
awk -F":" '{print $1 " " $3}'  /etc/passwd                  //$1与$3之间手动添加空格分隔
awk -F":" '{print "Username:" $1 "\t\t Uid:" $3 }' /etc/passwd       //自定义输出  
awk -F: '{print NF}' /etc/passwd                                //显示每行有多少字段
awk -F: '{print $NF}' /etc/passwd                              //将每行第NF个字段的值打印出来
 awk -F: 'NF==4 {print }' /etc/passwd                       //显示只有4个字段的行
awk -F: 'NF>2{print $0}' /etc/passwd                       //显示每行字段数量大于2的行
awk '{print NR,$0}' /etc/passwd                                 //输出每行的行号
awk -F: '{print NR,NF,$NF,"\t",$0}' /etc/passwd      //依次打印行号，字段数，最后字段值，制表符，每行内容
awk -F: 'NR==5{print}'  /etc/passwd                         //显示第5行
awk -F: 'NR==5 || NR==6{print}'  /etc/passwd       //显示第5行和第6行
route -n|awk 'NR!=1{print}'                                       //不显示第一行


## 匹配代码块
//纯字符匹配   !//纯字符不匹配   ~//字段值匹配    !~//字段值不匹配   ~/a1|a2/字段值匹配a1或a2   
awk '/mysql/' /etc/passwd
awk '/mysql/{print }' /etc/passwd
awk '/mysql/{print $0}' /etc/passwd                   //三条指令结果一样
awk '!/mysql/{print $0}' /etc/passwd                  //输出不匹配mysql的行
awk '/mysql|mail/{print}' /etc/passwd
awk '!/mysql|mail/{print}' /etc/passwd
awk -F: '/mail/,/mysql/{print}' /etc/passwd         //区间匹配
awk '/[2][7][7]*/{print $0}' /etc/passwd               //匹配包含27为数字开头的行，如27，277，2777...
awk -F: '$1~/mail/{print $1}' /etc/passwd           //$1匹配指定内容才显示
awk -F: '{if($1~/mail/) print $1}' /etc/passwd     //与上面相同
awk -F: '$1!~/mail/{print $1}' /etc/passwd          //不匹配
awk -F: '$1!~/mail|mysql/{print $1}' /etc/passwd        


## IF语句
必须用在{}中，且比较内容用()扩起来
awk -F: '{if($1~/mail/) print $1}' /etc/passwd                                       //简写
awk -F: '{if($1~/mail/) {print $1}}'  /etc/passwd                                   //全写
awk -F: '{if($1~/mail/) {print $1} else {print $2}}' /etc/passwd            //if...else...


## 条件表达式
==   !=   >   >=  
awk -F":" '$1=="mysql"{print $3}' /etc/passwd  
awk -F":" '{if($1=="mysql") print $3}' /etc/passwd          //与上面相同 
awk -F":" '$1!="mysql"{print $3}' /etc/passwd                 //不等于
awk -F":" '$3>1000{print $3}' /etc/passwd                      //大于
awk -F":" '$3>=100{print $3}' /etc/passwd                     //大于等于
awk -F":" '$3<1{print $3}' /etc/passwd                            //小于
awk -F":" '$3<=1{print $3}' /etc/passwd                         //小于等于


## 逻辑运算符
&&　|| 
awk -F: '$1~/mail/ && $3>8 {print }' /etc/passwd         //逻辑与，$1匹配mail，并且$3>8
awk -F: '{if($1~/mail/ && $3>8) print }' /etc/passwd
awk -F: '$1~/mail/ || $3>1000 {print }' /etc/passwd       //逻辑或
awk -F: '{if($1~/mail/ || $3>1000) print }' /etc/passwd 


## 数值运算
awk -F: '$3 > 100' /etc/passwd    
awk -F: '$3 > 100 || $3 < 5' /etc/passwd  
awk -F: '$3+$4 > 200' /etc/passwd
awk -F: '/mysql|mail/{print $3+10}' /etc/passwd                    //第三个字段加10打印 
awk -F: '/mysql/{print $3-$4}' /etc/passwd                             //减法
awk -F: '/mysql/{print $3*$4}' /etc/passwd                             //求乘积
awk '/MemFree/{print $2/1024}' /proc/meminfo                  //除法
awk '/MemFree/{print int($2/1024)}' /proc/meminfo           //取整


## 输出分隔符OFS
awk '$6 ~ /FIN/ || NR==1 {print NR,$4,$5,$6}' OFS="\t" netstat.txt
awk '$6 ~ /WAIT/ || NR==1 {print NR,$4,$5,$6}' OFS="\t" netstat.txt        
//输出字段6匹配WAIT的行，其中输出每行行号，字段4，5,6，并使用制表符分割字段


## 输出处理结果到文件
①在命令代码块中直接输出    route -n|awk 'NR!=1{print > "./fs"}'   
②使用重定向进行输出           route -n|awk 'NR!=1{print}'  > ./fs


## 格式化输出
netstat -anp|awk '{printf "%-8s %-8s %-10s\n",$1,$2,$3}' 
printf表示格式输出
%格式化输出分隔符
-8长度为8个字符
s表示字符串类型
打印每行前三个字段，指定第一个字段输出字符串类型(长度为8)，第二个字段输出字符串类型(长度为8),
第三个字段输出字符串类型(长度为10)
netstat -anp|awk '$6=="LISTEN" || NR==1 {printf "%-10s %-10s %-10s \n",$1,$2,$3}'
netstat -anp|awk '$6=="LISTEN" || NR==1 {printf "%-3s %-10s %-10s %-10s \n",NR,$1,$2,$3}'


## IF语句
awk -F: '{if($3>100) print "large"; else print "small"}' /etc/passwd
small
small
small
large
small
small
awk -F: 'BEGIN{A=0;B=0} {if($3>100) {A++; print "large"} else {B++; print "small"}} END{print A,"\t",B}' /etc/passwd 
                                                                                                                  //ID大于100,A加1，否则B加1
awk -F: '{if($3<100) next; else print}' /etc/passwd                         //小于100跳过，否则显示
awk -F: 'BEGIN{i=1} {if(i<NF) print NR,NF,i++ }' /etc/passwd   
awk -F: 'BEGIN{i=1} {if(i<NF) {print NR,NF} i++ }' /etc/passwd
另一种形式
awk -F: '{print ($3>100 ? "yes":"no")}'  /etc/passwd 
awk -F: '{print ($3>100 ? $3":\tyes":$3":\tno")}'  /etc/passwd


## while语句
awk -F: 'BEGIN{i=1} {while(i<NF) print NF,$i,i++}' /etc/passwd 
7 root 1
7 x 2
7 0 3
7 0 4
7 root 5
7 /root 6


## 数组
netstat -anp|awk 'NR!=1{a[$6]++} END{for (i in a) print i,"\t",a[i]}'
netstat -anp|awk 'NR!=1{a[$6]++} END{for (i in a) printf "%-20s %-10s %-5s \n", i,"\t",a[i]}'
9523                               1     
9929                               1     
LISTEN                            6     
7903                               1     
3038/cupsd                   1     
7913                               1     
10837                             1     
9833                               1     




## 应用1
awk -F: '{print NF}' helloworld.sh                                                       //输出文件每行有多少字段
awk -F: '{print $1,$2,$3,$4,$5}' helloworld.sh                                 //输出前5个字段
awk -F: '{print $1,$2,$3,$4,$5}' OFS='\t' helloworld.sh                 //输出前5个字段并使用制表符分隔输出
awk -F: '{print NR,$1,$2,$3,$4,$5}' OFS='\t' helloworld.sh           //制表符分隔输出前5个字段，并打印行号

## 应用2
awk -F'[:#]' '{print NF}'  helloworld.sh                                                  //指定多个分隔符: #，输出每行多少字段
awk -F'[:#]' '{print $1,$2,$3,$4,$5,$6,$7}' OFS='\t' helloworld.sh   //制表符分隔输出多字段

## 应用3
awk -F'[:#/]' '{print NF}' helloworld.sh                                               //指定三个分隔符，并输出每行字段数
awk -F'[:#/]' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12}' helloworld.sh     //制表符分隔输出多字段

## 应用4
计算/home目录下，普通文件的大小，使用KB作为单位
ls -l|awk 'BEGIN{sum=0} !/^d/{sum+=$5} END{print "total size is:",sum/1024,"KB"}'
ls -l|awk 'BEGIN{sum=0} !/^d/{sum+=$5} END{print "total size is:",int(sum/1024),"KB"}'         //int是取整的意思

## 应用5
统计netstat -anp 状态为LISTEN和CONNECT的连接数量分别是多少
netstat -anp|awk '$6~/LISTEN|CONNECTED/{sum[$6]++} END{for (i in sum) printf "%-10s %-6s %-3s \n", i," ",sum[i]}'

## 应用6
统计/home目录下不同用户的普通文件的总数是多少？
ls -l|awk 'NR!=1 && !/^d/{sum[$3]++} END{for (i in sum) printf "%-6s %-5s %-3s \n",i," ",sum[i]}'   
mysql        199 
root           374 
统计/home目录下不同用户的普通文件的大小总size是多少？
ls -l|awk 'NR!=1 && !/^d/{sum[$3]+=$5} END{for (i in sum) printf "%-6s %-5s %-3s %-2s \n",i," ",sum[i]/1024/1024,"MB"}'

## 应用7
输出成绩表
awk 'BEGIN{math=0;eng=0;com=0;printf "Lineno.   Name    No.    Math   English   Computer    Total\n";printf "------------------------------------------------------------\n"}{math+=$3; eng+=$4; com+=$5;printf "%-8s %-7s %-7s %-7s %-9s %-10s %-7s \n",NR,$1,$2,$3,$4,$5,$3+$4+$5} END{printf "------------------------------------------------------------\n";printf "%-24s %-7s %-9s %-20s \n","Total:",math,eng,com;printf "%-24s %-7s %-9s %-20s \n","Avg:",math/NR,eng/NR,com/NR}' test0

[root@localhost home]# cat test0 
Marry   2143 78 84 77
Jack    2321 66 78 45
Tom     2122 48 77 71
Mike    2537 87 97 95
Bob     2415 40 57 62







========================================
sed(关键字: 编辑) 以行为单位的文本编辑工具 
----------------------------------------

命令sed: 用正则表达式搜索并替换文本
$ cat out.txt
this is a linux system.
line2
Ubuntu is a release of linux.


$ sed 's/linux/unix/g' out.txt  #正则替换，/g是全局替换
this is a unix system.
line2
Ubuntu is a release of unix.


$ sed '1,2s/linux/unix/g' out.txt  #只替换从m到n行
this is a unix system.
line2
Ubuntu is a release of linux.


$ sed -e 's/linux/unix/g' -e 's/line/row/g' out.txt #多个替换正则表达式
this is a unix system.
row2
Ubuntu is a release of unix.

$ cat >regFile #把sed的规则保存到文件regFile中
1,2s/linux/unix/g
^C
$ cat regFile
1,2s/linux/unix/g
$ sed -f regFile out.txt #-f参数调用之前保存到文件中的规则，进行替换。
this is a unix system.
line2
Ubuntu is a release of linux.


删除文件最后一行
$ sed -i '$d' fileName






####################
1.定义与作用
sed - stream editor for filtering and transforming text 
用于过滤和转换文本的流编辑器。特色是该编辑器能用于pipeline中。

sed全名叫stream editor，流编辑器，用程序的方式来编辑文本（非交互式的），相当的hacker啊。
sed基本上就是玩正则模式匹配，所以，玩sed的人，正则表达式一般都比较强。

sed可以直接修改档案, 不过一般不推荐这么做, 可以分析 standard input

文档： 
	man: http://www.gnu.org/software/sed/manual/sed.html
	https://www.gnu.org/software/sed/
	list: http://sed.sourceforge.net/
	detail: http://www.grymoire.com/Unix/Sed.html




2.基本工作方式: sed [-nef] '[动作]' [输入文本]
	-n : 安静模式, 一般sed用法中, 来自stdin的数据一般会被列出到屏幕上, 如果使用-n参数后, 只有经过sed处理的那一行被列出来.
	-e : 多重编辑, 比如你同时又想删除某行, 又想改变其他行, 那么可以用 sed -e '1,5d' -e 's/abc/xxx/g' filename
	-f : 首先将 sed的动作写在一个档案内, 然后通过 sed -f scriptfile 就可以直接执行 scriptfile 内的sed动作。
	-i : 直接编辑, 这回就是真的改变文件中的内容了, 别的都只是改变显示. (不推荐使用)
	
动作:
	a 新增, a 后面可以接字符串, 而这个字符串会在新的一行出现. (下一行)
	c 取代, c 后面的字符串, 这些字符串可以取代 n1,n2之间的行
	d 删除, 后面不接任何东西
	i 插入, 后面的字符串, 会在上一行出现
	p 打印, 将选择的资料列出, 通常和 sed -n 一起运作 sed -n '3p' 只打印第3行
	s 取代, 类似vi中的取代, 1,20s/old/new/g

例： $ echo "this is a book" | sed 's/is/was/g'
thwas was a book

	[line-address]q 退出, 匹配到某行退出, 提高效率
	[line-address]r 匹配到的行读取某文件(注意是文件名) 例如: sed '1r qqq.txt' abc , 注意, 写入的文本是写在了第1行的后边, 也就是第2行
	[line-address]w file, 匹配到的行写入某文件  例如: sed -n '/m/w qqq' abc , 从abc中读取带m的行写到qqq文件中, 注意, 这个写入带有覆盖性.


常用例子:
sed '1d' abc 删除 abc 档案里的第一行, 注意, 这时会显示除了第一行之外的所有行, 因为第一行已经被删除了(实际文件并没有被删除,而只是显示的时候被删除了)

sed -n '1d' abc 什么内容也不显示, 因为经过sed处理的行, 是个删除操作, 所以不显示.

sed '2,$d' abc 删除abc中从第二行到最后一行所有的内容, 注意, $符号正则表达式中表示行末尾, 但是这里并没有说那行末尾, 就会指最后一行末尾, ^开头, 如果没有指定哪行开头, 那么就是第一行开头

sed '$d' abc 只删除了最后一行, 因为并没有指定是那行末尾, 就认为是最后一行末尾

sed '/test/d' abc 删除文件中所有带 test 的行

sed '/test/a RRRRRRR' abc 将 RRRRRRR 追加到所有的带 test 行的下一行 

sed '1,5c RRRRRRR' abc 从1到5行被 RRRRRRR 替换

sed '/test/c RRRRRRR' abc 将 RRRRRRR 替换所有带 test 的行, 当然, 这里也可以是通过行来进行替换, 比如 sed '1,5c RRRRRRR' abc





3.用s命令替换
(1)一般替换
$ cat out.txt #原文
this is a linux system.
line2
Ubuntu is a release of linux.

$ sed 's/is/are/' out.txt #把is用are替换
thare is a linux system.
line2
Ubuntu are a release of linux.

$ sed 's/is/are/g' out.txt #/g 表示一行上的替换所有的匹配
thare are a linux system.
line2
Ubuntu are a release of linux.

$ sed '1s/linux/Unix/g' out.txt #1s表示只替换第一行。
this is a Unix system.
line2
Ubuntu is a release of linux.


上面的sed并没有对文件的内容改变，只是把处理过后的内容输出，如果你要写回文件，你可以使用重定向。
$ sed '1s/linux/Unix/g' out.txt >out2.txt
或者使用-i参数（不推荐）直接修改原文件
$ sed -i 's/linux/Windows/g' out2.txt
wangjl@ubt16:~/str$ cat out2.txt
this is a Unix system.
line2
Ubuntu is a release of Windows.


在每一行最前面加注释符#：
$ sed 's/^/#/g' out.txt
#this is a linux system.
#line2
#Ubuntu is a release of linux.

在每一行最后面加上英文分号;
$ sed 's/$/;/g' out.txt
this is a linux system.;
line2;
Ubuntu is a release of linux.;

(2)每一行多个替换(-e参数)
同时在行首加上#，行尾加上;
$ sed -e 's/^/#/g' -e 's/$/;/g' out.txt
#this is a linux system.;
#line2;
#Ubuntu is a release of linux.;

(3)正则表达式的一些最基本的东西：
	^ 表示一行的开头。如：/^#/ 以#开头的匹配。
	$ 表示一行的结尾。如：/}$/ 以}结尾的匹配。
	\< 表示词首。 如：\<abc 表示以 abc 为首的詞。
	\> 表示词尾。 如：abc\> 表示以 abc 結尾的詞。
	. 表示任何单个字符。
	* 表示某个字符出现了0次或多次。
	[ ] 字符集合。 如：[abc] 表示匹配a或b或c，还有 [a-zA-Z] 表示匹配所有的26个字符。如果其中有^表示反，如 [^a] 表示非a的字符
	
注意： sed的正则用的是BREs/EREs，不支持非贪婪模式。
https://segmentfault.com/q/1010000002416121

例:去掉某html中的tags：
index.html
<b>This</b> is what <span style="text-decoration: underline;">the Boss</span> meant. Understand?

# 如果你这样搞的话，就会有问题
$ sed 's/<.*>//g' index.html
 meant. Understand?

# 要解决上面的那个问题，就得像下面这样。
# 其中的'[^>]' 指定了非>的字符重复0次或多次。
$ sed 's/<[^>]*>//g' index.html
This is what the Boss meant. Understand?


(4)替换每一行的第2个匹配项
$ sed 's/i/I/2' out.txt
this Is a linux system.
line2
Ubuntu is a release of lInux.

$ sed 's/i/I/1' out.txt
thIs is a linux system.
lIne2
Ubuntu Is a release of linux.

$ sed 's/i/I/' out.txt #默认就是只替换每行的第一个
thIs is a linux system.
lIne2
Ubuntu Is a release of linux. 

$ sed 's/i/I/2g' out.txt #替换每行第三个及以后的
this Is a lInux system.
line2
Ubuntu is a release of lInux.


(5)引用匹配项
我们可以使用&来当做被匹配的变量，然后可以在其左右加点东西。
$ sed 's/i/[&]/g' out.txt
th[i]s [i]s a l[i]nux system.
l[i]ne2
Ubuntu [i]s a release of l[i]nux.

或者使用圆括号匹配，用\1,\2等表示对匹配的引用。
$ sed 's/\(\w*\) is a \([^.]*\)/\1-\2/g' out.txt
this-linux system.
line2
Ubuntu-release of linux.
为什么[^.]没起到过滤.号的作用？
注意： 因为sed只替换了匹配的，没有匹配的保持不变。

比如 echo 'aaaaaaactttt'|sed 's/\(.*\)a/\1b/'
输出为 aaaaaabctttt

想达到去除.号的目的，需要把.移到正则()外面：
$ sed 's/\(\w*\) is a \(.*\)\./\1-\2/g' out.txt
this-linux system
line2
Ubuntu-release of linux

正则为：(\w*) is a (). 
匹配为：(this) is a (linux system).
然后：\1就是this，\2就是linux system







4.更多命令

(1)N命令: 把下一行的内容纳入当成缓冲区做匹配。
n N    Read/append the next line of input into the pattern space.

$ sed 's/i/I/' out2.txt 
thIs is a linux system.
lIne2
Ubuntu Is a release of linux.
It Is a wonderfull release.

加了N命令后，偶数行合并到奇数行结尾，原文大概成了这样
this is a linux system.\nline2
Ubuntu is a release of linux.\nIt is a wonderfull release.

$ sed 'N;s/i/I/' out2.txt #偶数行没做替换
thIs is a linux system.
line2
Ubuntu Is a release of linux.
It is a wonderfull release.

$ sed 'N;s/i/I/' out.txt #最后一个没有偶数行的也没有处理(?bug)
thIs is a linux system.
line2
Ubuntu is a release of linux.

我们甚至可以替换掉这个换行符/n 
$ sed 'N;s/\n//' out2.txt
this is a linux system.line2
Ubuntu is a release of linux.It is a wonderfull release.


$ sed -e 'N;s/i/I/' -e 'N;s/\n//' out2.txt #又出现bug第二个没有替换
thIs is a linux system.line2
Ubuntu is a release of linux.
It is a wonderfull release.


(2)a命令和i命令
a命令就是append(在行后一行添加)， i命令就是insert(在行前一行添加)，它们是用来添加行的。

1)
# 其中的1i表明，其要在第1行前插入一行（insert）
$ sed '1i desc\n------------' out.txt
desc
------------
this is a linux system.
line2
Ubuntu is a release of linux.

# 其中的1a表明，其要在第一行后追加一行（append）
$ sed '1a ------------' out.txt
this is a linux system.
------------
line2
Ubuntu is a release of linux.


# 其中的$a表明，其要在最后一行后追加一行（append）
$ sed '$a ------------' out.txt
this is a linux system.
line2
Ubuntu is a release of linux.
------------

2)我们可以运用匹配来添加文本：
注意其中的/is/a，这意思是匹配到/fish/后就追加一行
$ sed '/is/a ---' out.txt
this is a linux system.
---
line2
Ubuntu is a release of linux.
---


(3)c命令-替换匹配行
$ sed '2c this is another line' out.txt
this is a linux system.
this is another line
Ubuntu is a release of linux.

$ sed '/linux/c replace the line with linux' out.txt
replace the line with linux
line2
replace the line with linux



(4)d命令-删除匹配行
$ sed '2d' out.txt
this is a linux system.
Ubuntu is a release of linux.

$ sed '2,$d' out.txt
this is a linux system.

$ sed '/is/d' out.txt
line2



(5)p命令-打印命令
你可以把这个命令当成grep式的命令

$ sed '/is/p' out.txt
this is a linux system.
this is a linux system.
line2
Ubuntu is a release of linux.
Ubuntu is a release of linux.

发现有些输出两遍，这是因为sed处理时会把处理的信息输出。
添加-n参数后就可以了。
$ sed -n '/is/p' out.txt
this is a linux system.
Ubuntu is a release of linux.


#从一个模式到另一个模式（跨行了）
$ sed -n '/ine/,/nux/p' out.txt
line2
Ubuntu is a release of linux.


#从第1行到匹配到的行
$ sed -n '1,/line/p' out.txt
this is a linux system.
line2


#从第二行打印，每隔4行打印一行（用途：显示fastq文件的序列行信息）
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '{n;p;n;n}'
或者
$ zcat c2_R1.fastq.gz |head -n 8|sed -n '2~4p'
# first~step: Match every step'th line starting with line first.  





5.四个sed的基本知识点

(0)Pattern Space
我们来看一下sed处理文本的伪代码，并了解一下Pattern Space的概念：
foreach line in file {
    //把行放入Pattern_Space
    Pattern_Space <= line;
 
    // 对每个pattern space执行sed命令
    Pattern_Space <= EXEC(sed_cmd, Pattern_Space);
 
    // 如果没有指定 -n 则输出处理后的Pattern_Space
    if (sed option hasn't "-n")  {
       print Pattern_Space
    }
}

(1)Address
第一个是关于address，几乎上述所有的命令都是这样的（注：其中的!表示匹配成功后是否执行命令）
[address[,address]][!]{cmd}

address可以是一个数字，也可以是一个模式，
你可以通过逗号要分隔两个address 表示两个address的区间，参执行命令cmd，伪代码如下：
bool bexec = false
foreach line in file {
    if ( match(address1) ){
        bexec = true;
    }
 
    if ( bexec == true) {
        EXEC(sed_cmd);
    }
 
    if ( match (address2) ) {
        bexec = false;
    }
}

关于address可以使用相对位置，如：
r$ sed '/sys/,+1s/^/#/g' out.txt
#this is a linux system.
#line2
Ubuntu is a release of linux.

$ sed '/sys/,+2s/^/#/g' out.txt
#this is a linux system.
#line2
#Ubuntu is a release of linux.


(2)命令打包
第二个是cmd可以是多个，它们可以用分号分开，可以用大括号括起来作为嵌套命令。下面是几个例子：

对1行到第2行，执行命令/ine/d
$ sed '1,2 {/ine/d}' out.txt
this is a linux system.
Ubuntu is a release of linux.

# 对1行到第3行，匹配/is/成功后，再匹配/linux/，成功后执行d命令
$ sed '1,3{/is/{/linux/d}}' out2.txt
line2
It is a wonderfull release.

# 从第一行到最后一行，如果匹配到is，则删除之；如果有line，则替换为anotherLine
$ sed '1,${/is/d;s/line/anotherLine/g}' out2.txt
anotherLine2



(3)Hold Space[比较难，慢慢看，多看几遍]
第三个我们再来看一下 Hold Space

接下来，我们需要了解一下Hold Space的概念，我们先来看四个命令：
	g： 将hold space中的内容拷贝到pattern space中，原来pattern space里的内容清除
	G： 将hold space中的内容append到pattern space\n后
	h： 将pattern space中的内容拷贝到hold space中，原来的hold space里的内容被清除
	H： 将pattern space中的内容append到hold space\n后
	x： 交换pattern space和hold space的内容

这些命令有什么用？我们来看两个示例吧，用到的示例文件是：
$ cat >t.txt
one
two
three


第一个示例：
$ sed 'H;g' t.txt


第二个示例，反序了一个文件的行：
$ sed '1!G;h;$!d' t.txt

其中的 '1!G;h;$!d' 可拆解为三个命令
	1!G —— 只有第一行不执行G命令，将hold space中的内容append回到pattern space
	h —— 第一行都执行h命令，将pattern space中的内容拷贝到hold space中
	$!d —— 除了最后一行不执行d命令，其它行都执行d命令，删除当前行










6.杂七杂八
(1)sed '/^$/d' my.txt 为什么起到过滤空行的效果呢？
sed是按行进行处理的。
^$ 是正则表达式，其中^表示以什么开头，$表示以什么结尾，两个连载一起就是空行的意思，
d指令是sed里面的删除，
整个语句的意思就是从my.txt文件中删除空行空行

(2)# 在每一行后面增加一空行 
$ sed G my.txt

(3)可以修改定界符，用#代替/
$ a='are'
$ echo $a
are

$ sed 's/is/$a/g' out.txt #单引号内就是字符替换
th$a $a a linux system.
line2
Ubuntu $a a release of linux.

$ sed "s#is#$a#g" out.txt #双引号内才可以变量替换
thare are a linux system.
line2
Ubuntu are a release of linux.

$ sed "s/is/$a/g" out.txt
thare are a linux system.
line2
Ubuntu are a release of linux.






========================================
字符串处理综合案例
----------------------------------------

主要是基因组信息处理实例。


========================================
|-- 提取gene_id列和gene_name列
----------------------------------------
要求：从ID文件中提取gene_id列和gene_name列，要求保持gene_id列的唯一性，及其与gene_name列的对应。

$ head -n 15 ID
gene_id "ENSG00000223972.5_2"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"; level 2
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000456328.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000456328.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000456328.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000456328.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000223972.5_2"; transcript_id "ENST00000450305.2_1"; gene_type "transcribed_unprocessed_pseudogene"; gene_name "DDX11L1"
gene_id "ENSG00000227232.5_2"; gene_type "unprocessed_pseudogene"; gene_name "WASH7P"; level 2
gene_id "ENSG00000227232.5_2"; transcript_id "ENST00000488147.1_1"; gene_type "unprocessed_pseudogene"; gene_name "WASH7P"
gene_id "ENSG00000227232.5_2"; transcript_id "ENST00000488147.1_1"; gene_type "unprocessed_pseudogene"; gene_name "WASH7P"


$ wc ID
  2629275  21034200 302140702 ID


第一步：用sort按照第二列排序，并去重：
$ cat ID | sort -k 2,2 -u > ID.one
$ wc ID.one
  60461  483688 5452889 ID.one


第二步：用cut切出来第2和第6列，用sed替换其中的"和;符号。
$ cat ID.one | cut -d " " -f 2,6 |sed -e "s/\"//g" -e "s/;//g" >ID.end


查看结果文件：
$ head ID.end
ENSG00000000003.14_2 TSPAN6
ENSG00000000005.5_2 TNMD
ENSG00000000419.12_2 DPM1

$ wc ID.end
  60461  120922 1702210 ID.end



或者其他切割顺序：
$ head ID.one|cut -d ";" -f 1,3
gene_id "ENSG00000000003.14_2"; gene_name "TSPAN6"
gene_id "ENSG00000000005.5_2"; gene_name "TNMD"
gene_id "ENSG00000000419.12_2"; gene_name "DPM1"
gene_id "ENSG00000000457.13_2"; gene_name "SCYL3"

$ head ID.one|cut -d ";" -f 1,3|sed -e "s/;//g" -e "s/gene_id //g" -e "s/gene_name //g" -e "s/\"//g"
ENSG00000000003.14_2 TSPAN6
ENSG00000000005.5_2 TNMD
ENSG00000000419.12_2 DPM1
ENSG00000000457.13_2 SCYL3
ENSG00000000460.16_4 C1orf112



========================================
|-- 多个连续换行只保留一个换行？//todo
----------------------------------------
实例
$ cat input
1
2

3


4




5

希望的结果：多个换行的，只保留一个换行，紧邻的行还是紧邻。
1
2

3

4

5


1.接近的实现
$ sed '/^$/d' input | awk '{print $0"\n"}' >output
缺点是实现了目标，但是以前连续的行(上例中的1和2行)之间也加了空行。


2.怎么只保留多个连续空行的一个，且不增加其他新空行






========================================
|-- 2行合并为一行
----------------------------------------
把fa文件两行合并为一行
$ sed -n '{N;s/\n/\t/p}' all.4.fasta >all.4.fasta.oneLine
## >chr10:157702-157706(-)	aaaa




========================================
----------------------------------------






========================================
----------------------------------------



========================================
----------------------------------------






========================================
----------------------------------------



========================================
----------------------------------------


